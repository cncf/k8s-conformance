I1230 03:20:40.801787      25 e2e.go:116] Starting e2e run "d6d95677-1855-4e28-8bac-3e4b37116b4e" on Ginkgo node 1
Dec 30 03:20:40.814: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1672370440 - will randomize all specs

Will run 362 of 7067 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:76
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:76
{"msg":"Test Suite starting","completed":0,"skipped":0,"failed":0}
Dec 30 03:20:40.928: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
Dec 30 03:20:40.929: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
E1230 03:20:40.930398      25 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
E1230 03:20:40.930398      25 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
Dec 30 03:20:40.952: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Dec 30 03:20:41.000: INFO: 31 / 31 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Dec 30 03:20:41.000: INFO: expected 5 pod replicas in namespace 'kube-system', 5 are Running and Ready.
Dec 30 03:20:41.000: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Dec 30 03:20:41.008: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Dec 30 03:20:41.008: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Dec 30 03:20:41.008: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'nodelocaldns' (0 seconds elapsed)
Dec 30 03:20:41.009: INFO: e2e test version: v1.25.3
Dec 30 03:20:41.010: INFO: kube-apiserver version: v1.25.3
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:76
Dec 30 03:20:41.010: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
Dec 30 03:20:41.015: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.087 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:76

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:76
    Dec 30 03:20:40.928: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    Dec 30 03:20:40.929: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    E1230 03:20:40.930398      25 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
    Dec 30 03:20:40.952: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    Dec 30 03:20:41.000: INFO: 31 / 31 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    Dec 30 03:20:41.000: INFO: expected 5 pod replicas in namespace 'kube-system', 5 are Running and Ready.
    Dec 30 03:20:41.000: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Dec 30 03:20:41.008: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
    Dec 30 03:20:41.008: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
    Dec 30 03:20:41.008: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'nodelocaldns' (0 seconds elapsed)
    Dec 30 03:20:41.009: INFO: e2e test version: v1.25.3
    Dec 30 03:20:41.010: INFO: kube-apiserver version: v1.25.3
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:76
    Dec 30 03:20:41.010: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    Dec 30 03:20:41.015: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:20:41.05
Dec 30 03:20:41.050: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename conformance-tests 12/30/22 03:20:41.053
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:20:41.067
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:20:41.07
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 12/30/22 03:20:41.072
Dec 30 03:20:41.073: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:187
Dec 30 03:20:41.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "conformance-tests-4876" for this suite. 12/30/22 03:20:41.085
{"msg":"PASSED [sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]","completed":1,"skipped":50,"failed":0}
------------------------------
• [0.042 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:20:41.05
    Dec 30 03:20:41.050: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename conformance-tests 12/30/22 03:20:41.053
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:20:41.067
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:20:41.07
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 12/30/22 03:20:41.072
    Dec 30 03:20:41.073: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/framework.go:187
    Dec 30 03:20:41.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "conformance-tests-4876" for this suite. 12/30/22 03:20:41.085
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:20:41.092
Dec 30 03:20:41.092: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
E1230 03:20:41.092621      25 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
E1230 03:20:41.092621      25 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
STEP: Building a namespace api object, basename services 12/30/22 03:20:41.093
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:20:41.106
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:20:41.109
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791
STEP: creating service endpoint-test2 in namespace services-2952 12/30/22 03:20:41.112
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2952 to expose endpoints map[] 12/30/22 03:20:41.122
Dec 30 03:20:41.125: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Dec 30 03:20:42.136: INFO: successfully validated that service endpoint-test2 in namespace services-2952 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-2952 12/30/22 03:20:42.136
Dec 30 03:20:42.146: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-2952" to be "running and ready"
Dec 30 03:20:42.150: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.143332ms
Dec 30 03:20:42.150: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Dec 30 03:20:44.155: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009376659s
Dec 30 03:20:44.156: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Dec 30 03:20:46.155: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.009070175s
Dec 30 03:20:46.155: INFO: The phase of Pod pod1 is Running (Ready = true)
Dec 30 03:20:46.155: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2952 to expose endpoints map[pod1:[80]] 12/30/22 03:20:46.158
Dec 30 03:20:46.171: INFO: successfully validated that service endpoint-test2 in namespace services-2952 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 12/30/22 03:20:46.171
Dec 30 03:20:46.171: INFO: Creating new exec pod
Dec 30 03:20:46.177: INFO: Waiting up to 5m0s for pod "execpodttcbt" in namespace "services-2952" to be "running"
Dec 30 03:20:46.181: INFO: Pod "execpodttcbt": Phase="Pending", Reason="", readiness=false. Elapsed: 4.051737ms
Dec 30 03:20:48.187: INFO: Pod "execpodttcbt": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009748203s
Dec 30 03:20:50.187: INFO: Pod "execpodttcbt": Phase="Running", Reason="", readiness=true. Elapsed: 4.010174858s
Dec 30 03:20:50.187: INFO: Pod "execpodttcbt" satisfied condition "running"
Dec 30 03:20:51.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-2952 exec execpodttcbt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Dec 30 03:20:51.442: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Dec 30 03:20:51.442: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 30 03:20:51.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-2952 exec execpodttcbt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.50.81 80'
Dec 30 03:20:51.640: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.50.81 80\nConnection to 10.233.50.81 80 port [tcp/http] succeeded!\n"
Dec 30 03:20:51.640: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Creating pod pod2 in namespace services-2952 12/30/22 03:20:51.64
Dec 30 03:20:51.648: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-2952" to be "running and ready"
Dec 30 03:20:51.652: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.980458ms
Dec 30 03:20:51.652: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Dec 30 03:20:53.657: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008909406s
Dec 30 03:20:53.657: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Dec 30 03:20:55.658: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.010214037s
Dec 30 03:20:55.658: INFO: The phase of Pod pod2 is Running (Ready = true)
Dec 30 03:20:55.658: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2952 to expose endpoints map[pod1:[80] pod2:[80]] 12/30/22 03:20:55.662
Dec 30 03:20:55.683: INFO: successfully validated that service endpoint-test2 in namespace services-2952 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 12/30/22 03:20:55.683
Dec 30 03:20:56.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-2952 exec execpodttcbt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Dec 30 03:20:56.891: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Dec 30 03:20:56.891: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 30 03:20:56.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-2952 exec execpodttcbt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.50.81 80'
Dec 30 03:20:57.102: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.50.81 80\nConnection to 10.233.50.81 80 port [tcp/http] succeeded!\n"
Dec 30 03:20:57.102: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-2952 12/30/22 03:20:57.102
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2952 to expose endpoints map[pod2:[80]] 12/30/22 03:20:57.118
Dec 30 03:20:57.131: INFO: successfully validated that service endpoint-test2 in namespace services-2952 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 12/30/22 03:20:57.131
Dec 30 03:20:58.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-2952 exec execpodttcbt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Dec 30 03:20:58.366: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Dec 30 03:20:58.366: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 30 03:20:58.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-2952 exec execpodttcbt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.50.81 80'
Dec 30 03:20:58.565: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.50.81 80\nConnection to 10.233.50.81 80 port [tcp/http] succeeded!\n"
Dec 30 03:20:58.565: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod2 in namespace services-2952 12/30/22 03:20:58.565
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2952 to expose endpoints map[] 12/30/22 03:20:58.58
Dec 30 03:20:58.589: INFO: successfully validated that service endpoint-test2 in namespace services-2952 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Dec 30 03:20:58.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2952" for this suite. 12/30/22 03:20:58.609
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","completed":2,"skipped":53,"failed":0}
------------------------------
• [SLOW TEST] [17.523 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:20:41.092
    Dec 30 03:20:41.092: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    E1230 03:20:41.092621      25 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
    STEP: Building a namespace api object, basename services 12/30/22 03:20:41.093
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:20:41.106
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:20:41.109
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:791
    STEP: creating service endpoint-test2 in namespace services-2952 12/30/22 03:20:41.112
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2952 to expose endpoints map[] 12/30/22 03:20:41.122
    Dec 30 03:20:41.125: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
    Dec 30 03:20:42.136: INFO: successfully validated that service endpoint-test2 in namespace services-2952 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-2952 12/30/22 03:20:42.136
    Dec 30 03:20:42.146: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-2952" to be "running and ready"
    Dec 30 03:20:42.150: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.143332ms
    Dec 30 03:20:42.150: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Dec 30 03:20:44.155: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009376659s
    Dec 30 03:20:44.156: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Dec 30 03:20:46.155: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.009070175s
    Dec 30 03:20:46.155: INFO: The phase of Pod pod1 is Running (Ready = true)
    Dec 30 03:20:46.155: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2952 to expose endpoints map[pod1:[80]] 12/30/22 03:20:46.158
    Dec 30 03:20:46.171: INFO: successfully validated that service endpoint-test2 in namespace services-2952 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 12/30/22 03:20:46.171
    Dec 30 03:20:46.171: INFO: Creating new exec pod
    Dec 30 03:20:46.177: INFO: Waiting up to 5m0s for pod "execpodttcbt" in namespace "services-2952" to be "running"
    Dec 30 03:20:46.181: INFO: Pod "execpodttcbt": Phase="Pending", Reason="", readiness=false. Elapsed: 4.051737ms
    Dec 30 03:20:48.187: INFO: Pod "execpodttcbt": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009748203s
    Dec 30 03:20:50.187: INFO: Pod "execpodttcbt": Phase="Running", Reason="", readiness=true. Elapsed: 4.010174858s
    Dec 30 03:20:50.187: INFO: Pod "execpodttcbt" satisfied condition "running"
    Dec 30 03:20:51.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-2952 exec execpodttcbt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Dec 30 03:20:51.442: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Dec 30 03:20:51.442: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Dec 30 03:20:51.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-2952 exec execpodttcbt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.50.81 80'
    Dec 30 03:20:51.640: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.50.81 80\nConnection to 10.233.50.81 80 port [tcp/http] succeeded!\n"
    Dec 30 03:20:51.640: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Creating pod pod2 in namespace services-2952 12/30/22 03:20:51.64
    Dec 30 03:20:51.648: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-2952" to be "running and ready"
    Dec 30 03:20:51.652: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.980458ms
    Dec 30 03:20:51.652: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Dec 30 03:20:53.657: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008909406s
    Dec 30 03:20:53.657: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Dec 30 03:20:55.658: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.010214037s
    Dec 30 03:20:55.658: INFO: The phase of Pod pod2 is Running (Ready = true)
    Dec 30 03:20:55.658: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2952 to expose endpoints map[pod1:[80] pod2:[80]] 12/30/22 03:20:55.662
    Dec 30 03:20:55.683: INFO: successfully validated that service endpoint-test2 in namespace services-2952 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 12/30/22 03:20:55.683
    Dec 30 03:20:56.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-2952 exec execpodttcbt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Dec 30 03:20:56.891: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Dec 30 03:20:56.891: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Dec 30 03:20:56.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-2952 exec execpodttcbt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.50.81 80'
    Dec 30 03:20:57.102: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.50.81 80\nConnection to 10.233.50.81 80 port [tcp/http] succeeded!\n"
    Dec 30 03:20:57.102: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod1 in namespace services-2952 12/30/22 03:20:57.102
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2952 to expose endpoints map[pod2:[80]] 12/30/22 03:20:57.118
    Dec 30 03:20:57.131: INFO: successfully validated that service endpoint-test2 in namespace services-2952 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 12/30/22 03:20:57.131
    Dec 30 03:20:58.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-2952 exec execpodttcbt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Dec 30 03:20:58.366: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Dec 30 03:20:58.366: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Dec 30 03:20:58.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-2952 exec execpodttcbt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.50.81 80'
    Dec 30 03:20:58.565: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.50.81 80\nConnection to 10.233.50.81 80 port [tcp/http] succeeded!\n"
    Dec 30 03:20:58.565: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod2 in namespace services-2952 12/30/22 03:20:58.565
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2952 to expose endpoints map[] 12/30/22 03:20:58.58
    Dec 30 03:20:58.589: INFO: successfully validated that service endpoint-test2 in namespace services-2952 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Dec 30 03:20:58.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-2952" for this suite. 12/30/22 03:20:58.609
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:337
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:20:58.615
Dec 30 03:20:58.615: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename kubectl 12/30/22 03:20:58.616
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:20:58.63
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:20:58.632
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:324
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:337
STEP: creating a replication controller 12/30/22 03:20:58.635
Dec 30 03:20:58.636: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-5845 create -f -'
Dec 30 03:20:59.880: INFO: stderr: ""
Dec 30 03:20:59.880: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 12/30/22 03:20:59.88
Dec 30 03:20:59.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-5845 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec 30 03:21:00.028: INFO: stderr: ""
Dec 30 03:21:00.028: INFO: stdout: "update-demo-nautilus-gcckn "
STEP: Replicas for name=update-demo: expected=2 actual=1 12/30/22 03:21:00.028
Dec 30 03:21:05.033: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-5845 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec 30 03:21:05.139: INFO: stderr: ""
Dec 30 03:21:05.140: INFO: stdout: "update-demo-nautilus-22nb9 update-demo-nautilus-gcckn "
Dec 30 03:21:05.140: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-5845 get pods update-demo-nautilus-22nb9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 30 03:21:05.237: INFO: stderr: ""
Dec 30 03:21:05.237: INFO: stdout: ""
Dec 30 03:21:05.237: INFO: update-demo-nautilus-22nb9 is created but not running
Dec 30 03:21:10.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-5845 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec 30 03:21:10.346: INFO: stderr: ""
Dec 30 03:21:10.346: INFO: stdout: "update-demo-nautilus-22nb9 update-demo-nautilus-gcckn "
Dec 30 03:21:10.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-5845 get pods update-demo-nautilus-22nb9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 30 03:21:10.441: INFO: stderr: ""
Dec 30 03:21:10.441: INFO: stdout: "true"
Dec 30 03:21:10.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-5845 get pods update-demo-nautilus-22nb9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec 30 03:21:10.538: INFO: stderr: ""
Dec 30 03:21:10.538: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Dec 30 03:21:10.538: INFO: validating pod update-demo-nautilus-22nb9
Dec 30 03:21:10.545: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 30 03:21:10.545: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 30 03:21:10.545: INFO: update-demo-nautilus-22nb9 is verified up and running
Dec 30 03:21:10.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-5845 get pods update-demo-nautilus-gcckn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 30 03:21:10.643: INFO: stderr: ""
Dec 30 03:21:10.643: INFO: stdout: "true"
Dec 30 03:21:10.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-5845 get pods update-demo-nautilus-gcckn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec 30 03:21:10.736: INFO: stderr: ""
Dec 30 03:21:10.736: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Dec 30 03:21:10.736: INFO: validating pod update-demo-nautilus-gcckn
Dec 30 03:21:10.742: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 30 03:21:10.742: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 30 03:21:10.742: INFO: update-demo-nautilus-gcckn is verified up and running
STEP: using delete to clean up resources 12/30/22 03:21:10.742
Dec 30 03:21:10.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-5845 delete --grace-period=0 --force -f -'
Dec 30 03:21:10.844: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 30 03:21:10.844: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Dec 30 03:21:10.844: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-5845 get rc,svc -l name=update-demo --no-headers'
Dec 30 03:21:10.950: INFO: stderr: "No resources found in kubectl-5845 namespace.\n"
Dec 30 03:21:10.950: INFO: stdout: ""
Dec 30 03:21:10.950: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-5845 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 30 03:21:11.054: INFO: stderr: ""
Dec 30 03:21:11.054: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Dec 30 03:21:11.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5845" for this suite. 12/30/22 03:21:11.061
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","completed":3,"skipped":54,"failed":0}
------------------------------
• [SLOW TEST] [12.452 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:322
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:337

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:20:58.615
    Dec 30 03:20:58.615: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename kubectl 12/30/22 03:20:58.616
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:20:58.63
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:20:58.632
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:324
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:337
    STEP: creating a replication controller 12/30/22 03:20:58.635
    Dec 30 03:20:58.636: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-5845 create -f -'
    Dec 30 03:20:59.880: INFO: stderr: ""
    Dec 30 03:20:59.880: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 12/30/22 03:20:59.88
    Dec 30 03:20:59.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-5845 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Dec 30 03:21:00.028: INFO: stderr: ""
    Dec 30 03:21:00.028: INFO: stdout: "update-demo-nautilus-gcckn "
    STEP: Replicas for name=update-demo: expected=2 actual=1 12/30/22 03:21:00.028
    Dec 30 03:21:05.033: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-5845 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Dec 30 03:21:05.139: INFO: stderr: ""
    Dec 30 03:21:05.140: INFO: stdout: "update-demo-nautilus-22nb9 update-demo-nautilus-gcckn "
    Dec 30 03:21:05.140: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-5845 get pods update-demo-nautilus-22nb9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Dec 30 03:21:05.237: INFO: stderr: ""
    Dec 30 03:21:05.237: INFO: stdout: ""
    Dec 30 03:21:05.237: INFO: update-demo-nautilus-22nb9 is created but not running
    Dec 30 03:21:10.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-5845 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Dec 30 03:21:10.346: INFO: stderr: ""
    Dec 30 03:21:10.346: INFO: stdout: "update-demo-nautilus-22nb9 update-demo-nautilus-gcckn "
    Dec 30 03:21:10.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-5845 get pods update-demo-nautilus-22nb9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Dec 30 03:21:10.441: INFO: stderr: ""
    Dec 30 03:21:10.441: INFO: stdout: "true"
    Dec 30 03:21:10.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-5845 get pods update-demo-nautilus-22nb9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Dec 30 03:21:10.538: INFO: stderr: ""
    Dec 30 03:21:10.538: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Dec 30 03:21:10.538: INFO: validating pod update-demo-nautilus-22nb9
    Dec 30 03:21:10.545: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Dec 30 03:21:10.545: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Dec 30 03:21:10.545: INFO: update-demo-nautilus-22nb9 is verified up and running
    Dec 30 03:21:10.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-5845 get pods update-demo-nautilus-gcckn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Dec 30 03:21:10.643: INFO: stderr: ""
    Dec 30 03:21:10.643: INFO: stdout: "true"
    Dec 30 03:21:10.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-5845 get pods update-demo-nautilus-gcckn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Dec 30 03:21:10.736: INFO: stderr: ""
    Dec 30 03:21:10.736: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Dec 30 03:21:10.736: INFO: validating pod update-demo-nautilus-gcckn
    Dec 30 03:21:10.742: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Dec 30 03:21:10.742: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Dec 30 03:21:10.742: INFO: update-demo-nautilus-gcckn is verified up and running
    STEP: using delete to clean up resources 12/30/22 03:21:10.742
    Dec 30 03:21:10.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-5845 delete --grace-period=0 --force -f -'
    Dec 30 03:21:10.844: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Dec 30 03:21:10.844: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Dec 30 03:21:10.844: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-5845 get rc,svc -l name=update-demo --no-headers'
    Dec 30 03:21:10.950: INFO: stderr: "No resources found in kubectl-5845 namespace.\n"
    Dec 30 03:21:10.950: INFO: stdout: ""
    Dec 30 03:21:10.950: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-5845 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Dec 30 03:21:11.054: INFO: stderr: ""
    Dec 30 03:21:11.054: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Dec 30 03:21:11.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-5845" for this suite. 12/30/22 03:21:11.061
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:21:11.068
Dec 30 03:21:11.068: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename projected 12/30/22 03:21:11.07
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:21:11.187
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:21:11.191
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192
STEP: Creating a pod to test downward API volume plugin 12/30/22 03:21:11.194
Dec 30 03:21:11.204: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1c04dece-1a78-4487-853f-5724679cf040" in namespace "projected-3429" to be "Succeeded or Failed"
Dec 30 03:21:11.208: INFO: Pod "downwardapi-volume-1c04dece-1a78-4487-853f-5724679cf040": Phase="Pending", Reason="", readiness=false. Elapsed: 3.571107ms
Dec 30 03:21:13.214: INFO: Pod "downwardapi-volume-1c04dece-1a78-4487-853f-5724679cf040": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009434354s
Dec 30 03:21:15.213: INFO: Pod "downwardapi-volume-1c04dece-1a78-4487-853f-5724679cf040": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008416616s
STEP: Saw pod success 12/30/22 03:21:15.213
Dec 30 03:21:15.213: INFO: Pod "downwardapi-volume-1c04dece-1a78-4487-853f-5724679cf040" satisfied condition "Succeeded or Failed"
Dec 30 03:21:15.607: INFO: Trying to get logs from node k8s-mgmt01 pod downwardapi-volume-1c04dece-1a78-4487-853f-5724679cf040 container client-container: <nil>
STEP: delete the pod 12/30/22 03:21:15.628
Dec 30 03:21:15.669: INFO: Waiting for pod downwardapi-volume-1c04dece-1a78-4487-853f-5724679cf040 to disappear
Dec 30 03:21:15.714: INFO: Pod downwardapi-volume-1c04dece-1a78-4487-853f-5724679cf040 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Dec 30 03:21:15.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3429" for this suite. 12/30/22 03:21:15.719
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","completed":4,"skipped":61,"failed":0}
------------------------------
• [4.699 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:21:11.068
    Dec 30 03:21:11.068: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename projected 12/30/22 03:21:11.07
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:21:11.187
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:21:11.191
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:192
    STEP: Creating a pod to test downward API volume plugin 12/30/22 03:21:11.194
    Dec 30 03:21:11.204: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1c04dece-1a78-4487-853f-5724679cf040" in namespace "projected-3429" to be "Succeeded or Failed"
    Dec 30 03:21:11.208: INFO: Pod "downwardapi-volume-1c04dece-1a78-4487-853f-5724679cf040": Phase="Pending", Reason="", readiness=false. Elapsed: 3.571107ms
    Dec 30 03:21:13.214: INFO: Pod "downwardapi-volume-1c04dece-1a78-4487-853f-5724679cf040": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009434354s
    Dec 30 03:21:15.213: INFO: Pod "downwardapi-volume-1c04dece-1a78-4487-853f-5724679cf040": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008416616s
    STEP: Saw pod success 12/30/22 03:21:15.213
    Dec 30 03:21:15.213: INFO: Pod "downwardapi-volume-1c04dece-1a78-4487-853f-5724679cf040" satisfied condition "Succeeded or Failed"
    Dec 30 03:21:15.607: INFO: Trying to get logs from node k8s-mgmt01 pod downwardapi-volume-1c04dece-1a78-4487-853f-5724679cf040 container client-container: <nil>
    STEP: delete the pod 12/30/22 03:21:15.628
    Dec 30 03:21:15.669: INFO: Waiting for pod downwardapi-volume-1c04dece-1a78-4487-853f-5724679cf040 to disappear
    Dec 30 03:21:15.714: INFO: Pod downwardapi-volume-1c04dece-1a78-4487-853f-5724679cf040 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Dec 30 03:21:15.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3429" for this suite. 12/30/22 03:21:15.719
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:21:15.771
Dec 30 03:21:15.771: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename webhook 12/30/22 03:21:15.773
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:21:15.822
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:21:15.825
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 12/30/22 03:21:16.176
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/30/22 03:21:16.564
STEP: Deploying the webhook pod 12/30/22 03:21:16.573
STEP: Wait for the deployment to be ready 12/30/22 03:21:16.583
Dec 30 03:21:16.589: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 12/30/22 03:21:18.603
STEP: Verifying the service has paired with the endpoint 12/30/22 03:21:18.62
Dec 30 03:21:19.620: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208
STEP: Registering the webhook via the AdmissionRegistration API 12/30/22 03:21:19.625
STEP: create a pod 12/30/22 03:21:19.646
Dec 30 03:21:19.651: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-3561" to be "running"
Dec 30 03:21:19.655: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.89149ms
Dec 30 03:21:21.661: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009860141s
Dec 30 03:21:21.661: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 12/30/22 03:21:21.661
Dec 30 03:21:21.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=webhook-3561 attach --namespace=webhook-3561 to-be-attached-pod -i -c=container1'
Dec 30 03:21:21.785: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Dec 30 03:21:21.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3561" for this suite. 12/30/22 03:21:21.797
STEP: Destroying namespace "webhook-3561-markers" for this suite. 12/30/22 03:21:21.808
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","completed":5,"skipped":111,"failed":0}
------------------------------
• [SLOW TEST] [6.076 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:21:15.771
    Dec 30 03:21:15.771: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename webhook 12/30/22 03:21:15.773
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:21:15.822
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:21:15.825
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 12/30/22 03:21:16.176
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/30/22 03:21:16.564
    STEP: Deploying the webhook pod 12/30/22 03:21:16.573
    STEP: Wait for the deployment to be ready 12/30/22 03:21:16.583
    Dec 30 03:21:16.589: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 12/30/22 03:21:18.603
    STEP: Verifying the service has paired with the endpoint 12/30/22 03:21:18.62
    Dec 30 03:21:19.620: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:208
    STEP: Registering the webhook via the AdmissionRegistration API 12/30/22 03:21:19.625
    STEP: create a pod 12/30/22 03:21:19.646
    Dec 30 03:21:19.651: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-3561" to be "running"
    Dec 30 03:21:19.655: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.89149ms
    Dec 30 03:21:21.661: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009860141s
    Dec 30 03:21:21.661: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 12/30/22 03:21:21.661
    Dec 30 03:21:21.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=webhook-3561 attach --namespace=webhook-3561 to-be-attached-pod -i -c=container1'
    Dec 30 03:21:21.785: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Dec 30 03:21:21.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-3561" for this suite. 12/30/22 03:21:21.797
    STEP: Destroying namespace "webhook-3561-markers" for this suite. 12/30/22 03:21:21.808
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:21:21.85
Dec 30 03:21:21.850: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename deployment 12/30/22 03:21:21.851
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:21:21.865
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:21:21.868
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
Dec 30 03:21:21.880: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Dec 30 03:21:26.884: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 12/30/22 03:21:26.884
Dec 30 03:21:26.885: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 12/30/22 03:21:26.896
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Dec 30 03:21:26.908: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-4896  bc9ee2a2-3fa8-4af4-ba48-a20e8e90fd48 420096 1 2022-12-30 03:21:26 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2022-12-30 03:21:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003827ed8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Dec 30 03:21:26.911: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
Dec 30 03:21:26.911: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Dec 30 03:21:26.911: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-4896  fe25fb24-8f81-42c1-b99b-db61764ea7ab 420099 1 2022-12-30 03:21:21 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment bc9ee2a2-3fa8-4af4-ba48-a20e8e90fd48 0xc002c69747 0xc002c69748}] [] [{e2e.test Update apps/v1 2022-12-30 03:21:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-30 03:21:26 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"bc9ee2a2-3fa8-4af4-ba48-a20e8e90fd48\"}":{}}}} } {kube-controller-manager Update apps/v1 2022-12-30 03:21:26 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002c69808 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Dec 30 03:21:26.916: INFO: Pod "test-cleanup-controller-kbzg4" is available:
&Pod{ObjectMeta:{test-cleanup-controller-kbzg4 test-cleanup-controller- deployment-4896  14b81d77-59e1-4361-90de-342415e04567 420090 0 2022-12-30 03:21:21 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:ed2472822b08b7dbf44eecf8293bb5d5e92d3a6eae80e53736d520bb44accf1d cni.projectcalico.org/podIP:10.233.125.197/32 cni.projectcalico.org/podIPs:10.233.125.197/32] [{apps/v1 ReplicaSet test-cleanup-controller fe25fb24-8f81-42c1-b99b-db61764ea7ab 0xc0023dc7b7 0xc0023dc7b8}] [] [{kube-controller-manager Update v1 2022-12-30 03:21:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fe25fb24-8f81-42c1-b99b-db61764ea7ab\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-12-30 03:21:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-30 03:21:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.125.197\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rhv7t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rhv7t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-mgmt02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:21:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:21:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:21:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:21:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.141,PodIP:10.233.125.197,StartTime:2022-12-30 03:21:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-30 03:21:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://ad7fa84aeeabf1554953ff929790e7e79750ec28b4693d3ae0ab23dc7859ccaa,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.125.197,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Dec 30 03:21:26.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4896" for this suite. 12/30/22 03:21:26.922
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","completed":6,"skipped":130,"failed":0}
------------------------------
• [SLOW TEST] [5.078 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:21:21.85
    Dec 30 03:21:21.850: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename deployment 12/30/22 03:21:21.851
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:21:21.865
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:21:21.868
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    Dec 30 03:21:21.880: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    Dec 30 03:21:26.884: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 12/30/22 03:21:26.884
    Dec 30 03:21:26.885: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 12/30/22 03:21:26.896
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Dec 30 03:21:26.908: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-4896  bc9ee2a2-3fa8-4af4-ba48-a20e8e90fd48 420096 1 2022-12-30 03:21:26 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2022-12-30 03:21:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003827ed8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

    Dec 30 03:21:26.911: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
    Dec 30 03:21:26.911: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
    Dec 30 03:21:26.911: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-4896  fe25fb24-8f81-42c1-b99b-db61764ea7ab 420099 1 2022-12-30 03:21:21 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment bc9ee2a2-3fa8-4af4-ba48-a20e8e90fd48 0xc002c69747 0xc002c69748}] [] [{e2e.test Update apps/v1 2022-12-30 03:21:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-30 03:21:26 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"bc9ee2a2-3fa8-4af4-ba48-a20e8e90fd48\"}":{}}}} } {kube-controller-manager Update apps/v1 2022-12-30 03:21:26 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002c69808 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Dec 30 03:21:26.916: INFO: Pod "test-cleanup-controller-kbzg4" is available:
    &Pod{ObjectMeta:{test-cleanup-controller-kbzg4 test-cleanup-controller- deployment-4896  14b81d77-59e1-4361-90de-342415e04567 420090 0 2022-12-30 03:21:21 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:ed2472822b08b7dbf44eecf8293bb5d5e92d3a6eae80e53736d520bb44accf1d cni.projectcalico.org/podIP:10.233.125.197/32 cni.projectcalico.org/podIPs:10.233.125.197/32] [{apps/v1 ReplicaSet test-cleanup-controller fe25fb24-8f81-42c1-b99b-db61764ea7ab 0xc0023dc7b7 0xc0023dc7b8}] [] [{kube-controller-manager Update v1 2022-12-30 03:21:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fe25fb24-8f81-42c1-b99b-db61764ea7ab\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-12-30 03:21:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-30 03:21:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.125.197\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rhv7t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rhv7t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-mgmt02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:21:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:21:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:21:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:21:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.141,PodIP:10.233.125.197,StartTime:2022-12-30 03:21:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-30 03:21:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://ad7fa84aeeabf1554953ff929790e7e79750ec28b4693d3ae0ab23dc7859ccaa,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.125.197,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Dec 30 03:21:26.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-4896" for this suite. 12/30/22 03:21:26.922
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:585
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:21:26.929
Dec 30 03:21:26.929: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename statefulset 12/30/22 03:21:26.931
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:21:26.942
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:21:26.945
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-9725 12/30/22 03:21:26.947
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:585
STEP: Initializing watcher for selector baz=blah,foo=bar 12/30/22 03:21:26.951
STEP: Creating stateful set ss in namespace statefulset-9725 12/30/22 03:21:26.954
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-9725 12/30/22 03:21:26.96
Dec 30 03:21:26.964: INFO: Found 0 stateful pods, waiting for 1
Dec 30 03:21:36.969: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 12/30/22 03:21:36.969
Dec 30 03:21:36.974: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=statefulset-9725 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 30 03:21:37.181: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 30 03:21:37.181: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 30 03:21:37.181: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 30 03:21:37.186: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Dec 30 03:21:47.192: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 30 03:21:47.192: INFO: Waiting for statefulset status.replicas updated to 0
Dec 30 03:21:47.209: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999582s
Dec 30 03:21:48.214: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.996045858s
Dec 30 03:21:49.219: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.99198767s
Dec 30 03:21:50.223: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.987055964s
Dec 30 03:21:51.228: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.982377605s
Dec 30 03:21:52.233: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.977701437s
Dec 30 03:21:53.238: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.972352192s
Dec 30 03:21:54.243: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.967383042s
Dec 30 03:21:55.247: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.963182062s
Dec 30 03:21:56.252: INFO: Verifying statefulset ss doesn't scale past 1 for another 958.615717ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-9725 12/30/22 03:21:57.253
Dec 30 03:21:57.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=statefulset-9725 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 30 03:21:57.465: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 30 03:21:57.465: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 30 03:21:57.465: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 30 03:21:57.469: INFO: Found 1 stateful pods, waiting for 3
Dec 30 03:22:07.478: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 30 03:22:07.478: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 30 03:22:07.478: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 12/30/22 03:22:07.478
STEP: Scale down will halt with unhealthy stateful pod 12/30/22 03:22:07.479
Dec 30 03:22:07.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=statefulset-9725 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 30 03:22:07.722: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 30 03:22:07.722: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 30 03:22:07.722: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 30 03:22:07.722: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=statefulset-9725 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 30 03:22:07.934: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 30 03:22:07.934: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 30 03:22:07.934: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 30 03:22:07.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=statefulset-9725 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 30 03:22:08.159: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 30 03:22:08.159: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 30 03:22:08.159: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 30 03:22:08.159: INFO: Waiting for statefulset status.replicas updated to 0
Dec 30 03:22:08.164: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Dec 30 03:22:18.175: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 30 03:22:18.175: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Dec 30 03:22:18.175: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Dec 30 03:22:18.190: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999472s
Dec 30 03:22:19.195: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995700216s
Dec 30 03:22:20.200: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.990219952s
Dec 30 03:22:21.206: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.984669783s
Dec 30 03:22:22.212: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.979026293s
Dec 30 03:22:23.219: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.972868515s
Dec 30 03:22:24.223: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.96661975s
Dec 30 03:22:25.229: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.96183665s
Dec 30 03:22:26.235: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.955620578s
Dec 30 03:22:27.241: INFO: Verifying statefulset ss doesn't scale past 3 for another 949.058002ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-9725 12/30/22 03:22:28.241
Dec 30 03:22:28.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=statefulset-9725 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 30 03:22:28.456: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 30 03:22:28.456: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 30 03:22:28.456: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 30 03:22:28.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=statefulset-9725 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 30 03:22:28.669: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 30 03:22:28.669: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 30 03:22:28.669: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 30 03:22:28.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=statefulset-9725 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 30 03:22:28.886: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 30 03:22:28.886: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 30 03:22:28.886: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 30 03:22:28.886: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 12/30/22 03:22:38.909
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Dec 30 03:22:38.909: INFO: Deleting all statefulset in ns statefulset-9725
Dec 30 03:22:38.913: INFO: Scaling statefulset ss to 0
Dec 30 03:22:38.926: INFO: Waiting for statefulset status.replicas updated to 0
Dec 30 03:22:38.929: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Dec 30 03:22:38.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9725" for this suite. 12/30/22 03:22:38.95
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","completed":7,"skipped":135,"failed":0}
------------------------------
• [SLOW TEST] [72.027 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:585

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:21:26.929
    Dec 30 03:21:26.929: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename statefulset 12/30/22 03:21:26.931
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:21:26.942
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:21:26.945
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-9725 12/30/22 03:21:26.947
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:585
    STEP: Initializing watcher for selector baz=blah,foo=bar 12/30/22 03:21:26.951
    STEP: Creating stateful set ss in namespace statefulset-9725 12/30/22 03:21:26.954
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-9725 12/30/22 03:21:26.96
    Dec 30 03:21:26.964: INFO: Found 0 stateful pods, waiting for 1
    Dec 30 03:21:36.969: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 12/30/22 03:21:36.969
    Dec 30 03:21:36.974: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=statefulset-9725 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Dec 30 03:21:37.181: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Dec 30 03:21:37.181: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Dec 30 03:21:37.181: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Dec 30 03:21:37.186: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Dec 30 03:21:47.192: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Dec 30 03:21:47.192: INFO: Waiting for statefulset status.replicas updated to 0
    Dec 30 03:21:47.209: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999582s
    Dec 30 03:21:48.214: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.996045858s
    Dec 30 03:21:49.219: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.99198767s
    Dec 30 03:21:50.223: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.987055964s
    Dec 30 03:21:51.228: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.982377605s
    Dec 30 03:21:52.233: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.977701437s
    Dec 30 03:21:53.238: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.972352192s
    Dec 30 03:21:54.243: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.967383042s
    Dec 30 03:21:55.247: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.963182062s
    Dec 30 03:21:56.252: INFO: Verifying statefulset ss doesn't scale past 1 for another 958.615717ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-9725 12/30/22 03:21:57.253
    Dec 30 03:21:57.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=statefulset-9725 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Dec 30 03:21:57.465: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Dec 30 03:21:57.465: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Dec 30 03:21:57.465: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Dec 30 03:21:57.469: INFO: Found 1 stateful pods, waiting for 3
    Dec 30 03:22:07.478: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Dec 30 03:22:07.478: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Dec 30 03:22:07.478: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 12/30/22 03:22:07.478
    STEP: Scale down will halt with unhealthy stateful pod 12/30/22 03:22:07.479
    Dec 30 03:22:07.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=statefulset-9725 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Dec 30 03:22:07.722: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Dec 30 03:22:07.722: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Dec 30 03:22:07.722: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Dec 30 03:22:07.722: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=statefulset-9725 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Dec 30 03:22:07.934: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Dec 30 03:22:07.934: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Dec 30 03:22:07.934: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Dec 30 03:22:07.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=statefulset-9725 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Dec 30 03:22:08.159: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Dec 30 03:22:08.159: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Dec 30 03:22:08.159: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Dec 30 03:22:08.159: INFO: Waiting for statefulset status.replicas updated to 0
    Dec 30 03:22:08.164: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Dec 30 03:22:18.175: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Dec 30 03:22:18.175: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Dec 30 03:22:18.175: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Dec 30 03:22:18.190: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999472s
    Dec 30 03:22:19.195: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995700216s
    Dec 30 03:22:20.200: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.990219952s
    Dec 30 03:22:21.206: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.984669783s
    Dec 30 03:22:22.212: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.979026293s
    Dec 30 03:22:23.219: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.972868515s
    Dec 30 03:22:24.223: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.96661975s
    Dec 30 03:22:25.229: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.96183665s
    Dec 30 03:22:26.235: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.955620578s
    Dec 30 03:22:27.241: INFO: Verifying statefulset ss doesn't scale past 3 for another 949.058002ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-9725 12/30/22 03:22:28.241
    Dec 30 03:22:28.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=statefulset-9725 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Dec 30 03:22:28.456: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Dec 30 03:22:28.456: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Dec 30 03:22:28.456: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Dec 30 03:22:28.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=statefulset-9725 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Dec 30 03:22:28.669: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Dec 30 03:22:28.669: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Dec 30 03:22:28.669: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Dec 30 03:22:28.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=statefulset-9725 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Dec 30 03:22:28.886: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Dec 30 03:22:28.886: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Dec 30 03:22:28.886: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Dec 30 03:22:28.886: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 12/30/22 03:22:38.909
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Dec 30 03:22:38.909: INFO: Deleting all statefulset in ns statefulset-9725
    Dec 30 03:22:38.913: INFO: Scaling statefulset ss to 0
    Dec 30 03:22:38.926: INFO: Waiting for statefulset status.replicas updated to 0
    Dec 30 03:22:38.929: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Dec 30 03:22:38.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-9725" for this suite. 12/30/22 03:22:38.95
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:22:38.957
Dec 30 03:22:38.958: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename pods 12/30/22 03:22:38.959
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:22:38.973
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:22:38.976
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535
Dec 30 03:22:38.980: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: creating the pod 12/30/22 03:22:38.981
STEP: submitting the pod to kubernetes 12/30/22 03:22:38.981
Dec 30 03:22:38.990: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-02ea3e0b-e748-4b4e-a440-aeb0625f8314" in namespace "pods-1615" to be "running and ready"
Dec 30 03:22:38.994: INFO: Pod "pod-exec-websocket-02ea3e0b-e748-4b4e-a440-aeb0625f8314": Phase="Pending", Reason="", readiness=false. Elapsed: 3.868179ms
Dec 30 03:22:38.994: INFO: The phase of Pod pod-exec-websocket-02ea3e0b-e748-4b4e-a440-aeb0625f8314 is Pending, waiting for it to be Running (with Ready = true)
Dec 30 03:22:40.999: INFO: Pod "pod-exec-websocket-02ea3e0b-e748-4b4e-a440-aeb0625f8314": Phase="Running", Reason="", readiness=true. Elapsed: 2.008456487s
Dec 30 03:22:40.999: INFO: The phase of Pod pod-exec-websocket-02ea3e0b-e748-4b4e-a440-aeb0625f8314 is Running (Ready = true)
Dec 30 03:22:40.999: INFO: Pod "pod-exec-websocket-02ea3e0b-e748-4b4e-a440-aeb0625f8314" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Dec 30 03:22:41.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1615" for this suite. 12/30/22 03:22:41.122
{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","completed":8,"skipped":140,"failed":0}
------------------------------
• [2.172 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:22:38.957
    Dec 30 03:22:38.958: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename pods 12/30/22 03:22:38.959
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:22:38.973
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:22:38.976
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:535
    Dec 30 03:22:38.980: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: creating the pod 12/30/22 03:22:38.981
    STEP: submitting the pod to kubernetes 12/30/22 03:22:38.981
    Dec 30 03:22:38.990: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-02ea3e0b-e748-4b4e-a440-aeb0625f8314" in namespace "pods-1615" to be "running and ready"
    Dec 30 03:22:38.994: INFO: Pod "pod-exec-websocket-02ea3e0b-e748-4b4e-a440-aeb0625f8314": Phase="Pending", Reason="", readiness=false. Elapsed: 3.868179ms
    Dec 30 03:22:38.994: INFO: The phase of Pod pod-exec-websocket-02ea3e0b-e748-4b4e-a440-aeb0625f8314 is Pending, waiting for it to be Running (with Ready = true)
    Dec 30 03:22:40.999: INFO: Pod "pod-exec-websocket-02ea3e0b-e748-4b4e-a440-aeb0625f8314": Phase="Running", Reason="", readiness=true. Elapsed: 2.008456487s
    Dec 30 03:22:40.999: INFO: The phase of Pod pod-exec-websocket-02ea3e0b-e748-4b4e-a440-aeb0625f8314 is Running (Ready = true)
    Dec 30 03:22:40.999: INFO: Pod "pod-exec-websocket-02ea3e0b-e748-4b4e-a440-aeb0625f8314" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Dec 30 03:22:41.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-1615" for this suite. 12/30/22 03:22:41.122
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:22:41.133
Dec 30 03:22:41.133: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename cronjob 12/30/22 03:22:41.134
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:22:41.147
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:22:41.151
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 12/30/22 03:22:41.154
STEP: Ensuring a job is scheduled 12/30/22 03:22:41.167
STEP: Ensuring exactly one is scheduled 12/30/22 03:23:01.172
STEP: Ensuring exactly one running job exists by listing jobs explicitly 12/30/22 03:23:01.175
STEP: Ensuring no more jobs are scheduled 12/30/22 03:23:01.179
STEP: Removing cronjob 12/30/22 03:28:01.188
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Dec 30 03:28:01.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-7890" for this suite. 12/30/22 03:28:01.2
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","completed":9,"skipped":194,"failed":0}
------------------------------
• [SLOW TEST] [320.074 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:22:41.133
    Dec 30 03:22:41.133: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename cronjob 12/30/22 03:22:41.134
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:22:41.147
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:22:41.151
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 12/30/22 03:22:41.154
    STEP: Ensuring a job is scheduled 12/30/22 03:22:41.167
    STEP: Ensuring exactly one is scheduled 12/30/22 03:23:01.172
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 12/30/22 03:23:01.175
    STEP: Ensuring no more jobs are scheduled 12/30/22 03:23:01.179
    STEP: Removing cronjob 12/30/22 03:28:01.188
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Dec 30 03:28:01.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-7890" for this suite. 12/30/22 03:28:01.2
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:28:01.209
Dec 30 03:28:01.209: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename events 12/30/22 03:28:01.21
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:28:01.224
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:28:01.226
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 12/30/22 03:28:01.23
Dec 30 03:28:01.235: INFO: created test-event-1
Dec 30 03:28:01.238: INFO: created test-event-2
Dec 30 03:28:01.242: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 12/30/22 03:28:01.242
STEP: delete collection of events 12/30/22 03:28:01.246
Dec 30 03:28:01.246: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 12/30/22 03:28:01.271
Dec 30 03:28:01.271: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
Dec 30 03:28:01.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-1733" for this suite. 12/30/22 03:28:01.28
{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","completed":10,"skipped":210,"failed":0}
------------------------------
• [0.078 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:28:01.209
    Dec 30 03:28:01.209: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename events 12/30/22 03:28:01.21
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:28:01.224
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:28:01.226
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 12/30/22 03:28:01.23
    Dec 30 03:28:01.235: INFO: created test-event-1
    Dec 30 03:28:01.238: INFO: created test-event-2
    Dec 30 03:28:01.242: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 12/30/22 03:28:01.242
    STEP: delete collection of events 12/30/22 03:28:01.246
    Dec 30 03:28:01.246: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 12/30/22 03:28:01.271
    Dec 30 03:28:01.271: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:187
    Dec 30 03:28:01.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-1733" for this suite. 12/30/22 03:28:01.28
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1507
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:28:01.29
Dec 30 03:28:01.290: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename kubectl 12/30/22 03:28:01.292
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:28:01.305
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:28:01.308
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1492
STEP: creating the pod 12/30/22 03:28:01.311
Dec 30 03:28:01.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-9883 create -f -'
Dec 30 03:28:02.378: INFO: stderr: ""
Dec 30 03:28:02.378: INFO: stdout: "pod/pause created\n"
Dec 30 03:28:02.378: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Dec 30 03:28:02.378: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-9883" to be "running and ready"
Dec 30 03:28:02.382: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 3.942423ms
Dec 30 03:28:02.383: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'k8s-mgmt01' to be 'Running' but was 'Pending'
Dec 30 03:28:04.388: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.009413396s
Dec 30 03:28:04.388: INFO: Pod "pause" satisfied condition "running and ready"
Dec 30 03:28:04.388: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1507
STEP: adding the label testing-label with value testing-label-value to a pod 12/30/22 03:28:04.388
Dec 30 03:28:04.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-9883 label pods pause testing-label=testing-label-value'
Dec 30 03:28:04.503: INFO: stderr: ""
Dec 30 03:28:04.503: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 12/30/22 03:28:04.503
Dec 30 03:28:04.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-9883 get pod pause -L testing-label'
Dec 30 03:28:04.601: INFO: stderr: ""
Dec 30 03:28:04.601: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod 12/30/22 03:28:04.601
Dec 30 03:28:04.602: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-9883 label pods pause testing-label-'
Dec 30 03:28:04.717: INFO: stderr: ""
Dec 30 03:28:04.717: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 12/30/22 03:28:04.717
Dec 30 03:28:04.717: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-9883 get pod pause -L testing-label'
Dec 30 03:28:04.815: INFO: stderr: ""
Dec 30 03:28:04.815: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1498
STEP: using delete to clean up resources 12/30/22 03:28:04.815
Dec 30 03:28:04.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-9883 delete --grace-period=0 --force -f -'
Dec 30 03:28:04.920: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 30 03:28:04.920: INFO: stdout: "pod \"pause\" force deleted\n"
Dec 30 03:28:04.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-9883 get rc,svc -l name=pause --no-headers'
Dec 30 03:28:05.032: INFO: stderr: "No resources found in kubectl-9883 namespace.\n"
Dec 30 03:28:05.032: INFO: stdout: ""
Dec 30 03:28:05.032: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-9883 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 30 03:28:05.127: INFO: stderr: ""
Dec 30 03:28:05.127: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Dec 30 03:28:05.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9883" for this suite. 12/30/22 03:28:05.133
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","completed":11,"skipped":250,"failed":0}
------------------------------
• [3.849 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1490
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:28:01.29
    Dec 30 03:28:01.290: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename kubectl 12/30/22 03:28:01.292
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:28:01.305
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:28:01.308
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1492
    STEP: creating the pod 12/30/22 03:28:01.311
    Dec 30 03:28:01.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-9883 create -f -'
    Dec 30 03:28:02.378: INFO: stderr: ""
    Dec 30 03:28:02.378: INFO: stdout: "pod/pause created\n"
    Dec 30 03:28:02.378: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    Dec 30 03:28:02.378: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-9883" to be "running and ready"
    Dec 30 03:28:02.382: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 3.942423ms
    Dec 30 03:28:02.383: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'k8s-mgmt01' to be 'Running' but was 'Pending'
    Dec 30 03:28:04.388: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.009413396s
    Dec 30 03:28:04.388: INFO: Pod "pause" satisfied condition "running and ready"
    Dec 30 03:28:04.388: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1507
    STEP: adding the label testing-label with value testing-label-value to a pod 12/30/22 03:28:04.388
    Dec 30 03:28:04.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-9883 label pods pause testing-label=testing-label-value'
    Dec 30 03:28:04.503: INFO: stderr: ""
    Dec 30 03:28:04.503: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 12/30/22 03:28:04.503
    Dec 30 03:28:04.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-9883 get pod pause -L testing-label'
    Dec 30 03:28:04.601: INFO: stderr: ""
    Dec 30 03:28:04.601: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 12/30/22 03:28:04.601
    Dec 30 03:28:04.602: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-9883 label pods pause testing-label-'
    Dec 30 03:28:04.717: INFO: stderr: ""
    Dec 30 03:28:04.717: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 12/30/22 03:28:04.717
    Dec 30 03:28:04.717: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-9883 get pod pause -L testing-label'
    Dec 30 03:28:04.815: INFO: stderr: ""
    Dec 30 03:28:04.815: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1498
    STEP: using delete to clean up resources 12/30/22 03:28:04.815
    Dec 30 03:28:04.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-9883 delete --grace-period=0 --force -f -'
    Dec 30 03:28:04.920: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Dec 30 03:28:04.920: INFO: stdout: "pod \"pause\" force deleted\n"
    Dec 30 03:28:04.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-9883 get rc,svc -l name=pause --no-headers'
    Dec 30 03:28:05.032: INFO: stderr: "No resources found in kubectl-9883 namespace.\n"
    Dec 30 03:28:05.032: INFO: stdout: ""
    Dec 30 03:28:05.032: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-9883 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Dec 30 03:28:05.127: INFO: stderr: ""
    Dec 30 03:28:05.127: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Dec 30 03:28:05.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-9883" for this suite. 12/30/22 03:28:05.133
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1590
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:28:05.143
Dec 30 03:28:05.143: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename kubectl 12/30/22 03:28:05.145
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:28:05.157
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:28:05.16
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1570
STEP: creating an pod 12/30/22 03:28:05.164
Dec 30 03:28:05.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-6807 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.40 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Dec 30 03:28:05.273: INFO: stderr: ""
Dec 30 03:28:05.273: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1590
STEP: Waiting for log generator to start. 12/30/22 03:28:05.273
Dec 30 03:28:05.273: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Dec 30 03:28:05.273: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-6807" to be "running and ready, or succeeded"
Dec 30 03:28:05.277: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.264629ms
Dec 30 03:28:05.277: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'k8s-mgmt01' to be 'Running' but was 'Pending'
Dec 30 03:28:07.282: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.008988169s
Dec 30 03:28:07.282: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Dec 30 03:28:07.282: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 12/30/22 03:28:07.282
Dec 30 03:28:07.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-6807 logs logs-generator logs-generator'
Dec 30 03:28:07.405: INFO: stderr: ""
Dec 30 03:28:07.406: INFO: stdout: "I1230 03:28:06.239700       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/kube-system/pods/x7r2 307\nI1230 03:28:06.439933       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/5mg 455\nI1230 03:28:06.640493       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/sngr 513\nI1230 03:28:06.839881       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/5l6 387\nI1230 03:28:07.040305       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/default/pods/bcps 434\nI1230 03:28:07.240767       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/zrvl 492\n"
STEP: limiting log lines 12/30/22 03:28:07.406
Dec 30 03:28:07.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-6807 logs logs-generator logs-generator --tail=1'
Dec 30 03:28:07.515: INFO: stderr: ""
Dec 30 03:28:07.515: INFO: stdout: "I1230 03:28:07.440158       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/cvbf 392\n"
Dec 30 03:28:07.515: INFO: got output "I1230 03:28:07.440158       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/cvbf 392\n"
STEP: limiting log bytes 12/30/22 03:28:07.515
Dec 30 03:28:07.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-6807 logs logs-generator logs-generator --limit-bytes=1'
Dec 30 03:28:07.630: INFO: stderr: ""
Dec 30 03:28:07.630: INFO: stdout: "I"
Dec 30 03:28:07.630: INFO: got output "I"
STEP: exposing timestamps 12/30/22 03:28:07.631
Dec 30 03:28:07.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-6807 logs logs-generator logs-generator --tail=1 --timestamps'
Dec 30 03:28:07.734: INFO: stderr: ""
Dec 30 03:28:07.734: INFO: stdout: "2022-12-30T03:28:07.640752810Z I1230 03:28:07.640615       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/c885 310\n"
Dec 30 03:28:07.734: INFO: got output "2022-12-30T03:28:07.640752810Z I1230 03:28:07.640615       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/c885 310\n"
STEP: restricting to a time range 12/30/22 03:28:07.734
Dec 30 03:28:10.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-6807 logs logs-generator logs-generator --since=1s'
Dec 30 03:28:10.344: INFO: stderr: ""
Dec 30 03:28:10.344: INFO: stdout: "I1230 03:28:09.440172       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/c9nc 263\nI1230 03:28:09.640642       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/nq8 210\nI1230 03:28:09.839974       1 logs_generator.go:76] 18 GET /api/v1/namespaces/default/pods/mgn 443\nI1230 03:28:10.040410       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/default/pods/8bs 429\nI1230 03:28:10.240820       1 logs_generator.go:76] 20 POST /api/v1/namespaces/default/pods/sfxw 285\n"
Dec 30 03:28:10.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-6807 logs logs-generator logs-generator --since=24h'
Dec 30 03:28:10.445: INFO: stderr: ""
Dec 30 03:28:10.445: INFO: stdout: "I1230 03:28:06.239700       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/kube-system/pods/x7r2 307\nI1230 03:28:06.439933       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/5mg 455\nI1230 03:28:06.640493       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/sngr 513\nI1230 03:28:06.839881       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/5l6 387\nI1230 03:28:07.040305       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/default/pods/bcps 434\nI1230 03:28:07.240767       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/zrvl 492\nI1230 03:28:07.440158       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/cvbf 392\nI1230 03:28:07.640615       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/c885 310\nI1230 03:28:07.839952       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/7dv 210\nI1230 03:28:08.040366       1 logs_generator.go:76] 9 GET /api/v1/namespaces/default/pods/jgfc 416\nI1230 03:28:08.240789       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/ns/pods/khtj 342\nI1230 03:28:08.440237       1 logs_generator.go:76] 11 POST /api/v1/namespaces/default/pods/sr7 251\nI1230 03:28:08.640692       1 logs_generator.go:76] 12 GET /api/v1/namespaces/ns/pods/lrt 472\nI1230 03:28:08.840046       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/ns/pods/4b5x 400\nI1230 03:28:09.040466       1 logs_generator.go:76] 14 GET /api/v1/namespaces/kube-system/pods/f68 237\nI1230 03:28:09.239795       1 logs_generator.go:76] 15 GET /api/v1/namespaces/ns/pods/29q9 379\nI1230 03:28:09.440172       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/c9nc 263\nI1230 03:28:09.640642       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/nq8 210\nI1230 03:28:09.839974       1 logs_generator.go:76] 18 GET /api/v1/namespaces/default/pods/mgn 443\nI1230 03:28:10.040410       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/default/pods/8bs 429\nI1230 03:28:10.240820       1 logs_generator.go:76] 20 POST /api/v1/namespaces/default/pods/sfxw 285\nI1230 03:28:10.440279       1 logs_generator.go:76] 21 POST /api/v1/namespaces/ns/pods/qd7g 482\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1575
Dec 30 03:28:10.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-6807 delete pod logs-generator'
Dec 30 03:28:11.766: INFO: stderr: ""
Dec 30 03:28:11.766: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Dec 30 03:28:11.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6807" for this suite. 12/30/22 03:28:11.772
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","completed":12,"skipped":282,"failed":0}
------------------------------
• [SLOW TEST] [6.635 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1567
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1590

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:28:05.143
    Dec 30 03:28:05.143: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename kubectl 12/30/22 03:28:05.145
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:28:05.157
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:28:05.16
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1570
    STEP: creating an pod 12/30/22 03:28:05.164
    Dec 30 03:28:05.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-6807 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.40 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    Dec 30 03:28:05.273: INFO: stderr: ""
    Dec 30 03:28:05.273: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1590
    STEP: Waiting for log generator to start. 12/30/22 03:28:05.273
    Dec 30 03:28:05.273: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    Dec 30 03:28:05.273: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-6807" to be "running and ready, or succeeded"
    Dec 30 03:28:05.277: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.264629ms
    Dec 30 03:28:05.277: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'k8s-mgmt01' to be 'Running' but was 'Pending'
    Dec 30 03:28:07.282: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.008988169s
    Dec 30 03:28:07.282: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    Dec 30 03:28:07.282: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 12/30/22 03:28:07.282
    Dec 30 03:28:07.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-6807 logs logs-generator logs-generator'
    Dec 30 03:28:07.405: INFO: stderr: ""
    Dec 30 03:28:07.406: INFO: stdout: "I1230 03:28:06.239700       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/kube-system/pods/x7r2 307\nI1230 03:28:06.439933       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/5mg 455\nI1230 03:28:06.640493       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/sngr 513\nI1230 03:28:06.839881       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/5l6 387\nI1230 03:28:07.040305       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/default/pods/bcps 434\nI1230 03:28:07.240767       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/zrvl 492\n"
    STEP: limiting log lines 12/30/22 03:28:07.406
    Dec 30 03:28:07.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-6807 logs logs-generator logs-generator --tail=1'
    Dec 30 03:28:07.515: INFO: stderr: ""
    Dec 30 03:28:07.515: INFO: stdout: "I1230 03:28:07.440158       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/cvbf 392\n"
    Dec 30 03:28:07.515: INFO: got output "I1230 03:28:07.440158       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/cvbf 392\n"
    STEP: limiting log bytes 12/30/22 03:28:07.515
    Dec 30 03:28:07.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-6807 logs logs-generator logs-generator --limit-bytes=1'
    Dec 30 03:28:07.630: INFO: stderr: ""
    Dec 30 03:28:07.630: INFO: stdout: "I"
    Dec 30 03:28:07.630: INFO: got output "I"
    STEP: exposing timestamps 12/30/22 03:28:07.631
    Dec 30 03:28:07.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-6807 logs logs-generator logs-generator --tail=1 --timestamps'
    Dec 30 03:28:07.734: INFO: stderr: ""
    Dec 30 03:28:07.734: INFO: stdout: "2022-12-30T03:28:07.640752810Z I1230 03:28:07.640615       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/c885 310\n"
    Dec 30 03:28:07.734: INFO: got output "2022-12-30T03:28:07.640752810Z I1230 03:28:07.640615       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/c885 310\n"
    STEP: restricting to a time range 12/30/22 03:28:07.734
    Dec 30 03:28:10.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-6807 logs logs-generator logs-generator --since=1s'
    Dec 30 03:28:10.344: INFO: stderr: ""
    Dec 30 03:28:10.344: INFO: stdout: "I1230 03:28:09.440172       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/c9nc 263\nI1230 03:28:09.640642       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/nq8 210\nI1230 03:28:09.839974       1 logs_generator.go:76] 18 GET /api/v1/namespaces/default/pods/mgn 443\nI1230 03:28:10.040410       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/default/pods/8bs 429\nI1230 03:28:10.240820       1 logs_generator.go:76] 20 POST /api/v1/namespaces/default/pods/sfxw 285\n"
    Dec 30 03:28:10.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-6807 logs logs-generator logs-generator --since=24h'
    Dec 30 03:28:10.445: INFO: stderr: ""
    Dec 30 03:28:10.445: INFO: stdout: "I1230 03:28:06.239700       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/kube-system/pods/x7r2 307\nI1230 03:28:06.439933       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/5mg 455\nI1230 03:28:06.640493       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/sngr 513\nI1230 03:28:06.839881       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/5l6 387\nI1230 03:28:07.040305       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/default/pods/bcps 434\nI1230 03:28:07.240767       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/zrvl 492\nI1230 03:28:07.440158       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/cvbf 392\nI1230 03:28:07.640615       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/c885 310\nI1230 03:28:07.839952       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/7dv 210\nI1230 03:28:08.040366       1 logs_generator.go:76] 9 GET /api/v1/namespaces/default/pods/jgfc 416\nI1230 03:28:08.240789       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/ns/pods/khtj 342\nI1230 03:28:08.440237       1 logs_generator.go:76] 11 POST /api/v1/namespaces/default/pods/sr7 251\nI1230 03:28:08.640692       1 logs_generator.go:76] 12 GET /api/v1/namespaces/ns/pods/lrt 472\nI1230 03:28:08.840046       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/ns/pods/4b5x 400\nI1230 03:28:09.040466       1 logs_generator.go:76] 14 GET /api/v1/namespaces/kube-system/pods/f68 237\nI1230 03:28:09.239795       1 logs_generator.go:76] 15 GET /api/v1/namespaces/ns/pods/29q9 379\nI1230 03:28:09.440172       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/c9nc 263\nI1230 03:28:09.640642       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/nq8 210\nI1230 03:28:09.839974       1 logs_generator.go:76] 18 GET /api/v1/namespaces/default/pods/mgn 443\nI1230 03:28:10.040410       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/default/pods/8bs 429\nI1230 03:28:10.240820       1 logs_generator.go:76] 20 POST /api/v1/namespaces/default/pods/sfxw 285\nI1230 03:28:10.440279       1 logs_generator.go:76] 21 POST /api/v1/namespaces/ns/pods/qd7g 482\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1575
    Dec 30 03:28:10.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-6807 delete pod logs-generator'
    Dec 30 03:28:11.766: INFO: stderr: ""
    Dec 30 03:28:11.766: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Dec 30 03:28:11.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-6807" for this suite. 12/30/22 03:28:11.772
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:28:11.785
Dec 30 03:28:11.785: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename projected 12/30/22 03:28:11.786
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:28:11.798
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:28:11.801
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234
STEP: Creating a pod to test downward API volume plugin 12/30/22 03:28:11.805
Dec 30 03:28:11.814: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3b21a62c-6452-49d6-b132-598bbd9f1d23" in namespace "projected-3419" to be "Succeeded or Failed"
Dec 30 03:28:11.818: INFO: Pod "downwardapi-volume-3b21a62c-6452-49d6-b132-598bbd9f1d23": Phase="Pending", Reason="", readiness=false. Elapsed: 3.62929ms
Dec 30 03:28:13.823: INFO: Pod "downwardapi-volume-3b21a62c-6452-49d6-b132-598bbd9f1d23": Phase="Running", Reason="", readiness=false. Elapsed: 2.008420224s
Dec 30 03:28:15.824: INFO: Pod "downwardapi-volume-3b21a62c-6452-49d6-b132-598bbd9f1d23": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009429891s
STEP: Saw pod success 12/30/22 03:28:15.824
Dec 30 03:28:15.824: INFO: Pod "downwardapi-volume-3b21a62c-6452-49d6-b132-598bbd9f1d23" satisfied condition "Succeeded or Failed"
Dec 30 03:28:15.828: INFO: Trying to get logs from node k8s-mgmt01 pod downwardapi-volume-3b21a62c-6452-49d6-b132-598bbd9f1d23 container client-container: <nil>
STEP: delete the pod 12/30/22 03:28:15.835
Dec 30 03:28:15.848: INFO: Waiting for pod downwardapi-volume-3b21a62c-6452-49d6-b132-598bbd9f1d23 to disappear
Dec 30 03:28:15.852: INFO: Pod downwardapi-volume-3b21a62c-6452-49d6-b132-598bbd9f1d23 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Dec 30 03:28:15.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3419" for this suite. 12/30/22 03:28:15.856
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","completed":13,"skipped":353,"failed":0}
------------------------------
• [4.078 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:28:11.785
    Dec 30 03:28:11.785: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename projected 12/30/22 03:28:11.786
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:28:11.798
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:28:11.801
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:234
    STEP: Creating a pod to test downward API volume plugin 12/30/22 03:28:11.805
    Dec 30 03:28:11.814: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3b21a62c-6452-49d6-b132-598bbd9f1d23" in namespace "projected-3419" to be "Succeeded or Failed"
    Dec 30 03:28:11.818: INFO: Pod "downwardapi-volume-3b21a62c-6452-49d6-b132-598bbd9f1d23": Phase="Pending", Reason="", readiness=false. Elapsed: 3.62929ms
    Dec 30 03:28:13.823: INFO: Pod "downwardapi-volume-3b21a62c-6452-49d6-b132-598bbd9f1d23": Phase="Running", Reason="", readiness=false. Elapsed: 2.008420224s
    Dec 30 03:28:15.824: INFO: Pod "downwardapi-volume-3b21a62c-6452-49d6-b132-598bbd9f1d23": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009429891s
    STEP: Saw pod success 12/30/22 03:28:15.824
    Dec 30 03:28:15.824: INFO: Pod "downwardapi-volume-3b21a62c-6452-49d6-b132-598bbd9f1d23" satisfied condition "Succeeded or Failed"
    Dec 30 03:28:15.828: INFO: Trying to get logs from node k8s-mgmt01 pod downwardapi-volume-3b21a62c-6452-49d6-b132-598bbd9f1d23 container client-container: <nil>
    STEP: delete the pod 12/30/22 03:28:15.835
    Dec 30 03:28:15.848: INFO: Waiting for pod downwardapi-volume-3b21a62c-6452-49d6-b132-598bbd9f1d23 to disappear
    Dec 30 03:28:15.852: INFO: Pod downwardapi-volume-3b21a62c-6452-49d6-b132-598bbd9f1d23 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Dec 30 03:28:15.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3419" for this suite. 12/30/22 03:28:15.856
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:28:15.866
Dec 30 03:28:15.866: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename namespaces 12/30/22 03:28:15.867
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:28:15.881
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:28:15.884
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250
STEP: Creating a test namespace 12/30/22 03:28:15.887
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:28:15.899
STEP: Creating a service in the namespace 12/30/22 03:28:15.902
STEP: Deleting the namespace 12/30/22 03:28:15.912
STEP: Waiting for the namespace to be removed. 12/30/22 03:28:15.919
STEP: Recreating the namespace 12/30/22 03:28:21.924
STEP: Verifying there is no service in the namespace 12/30/22 03:28:21.938
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Dec 30 03:28:21.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-4427" for this suite. 12/30/22 03:28:21.947
STEP: Destroying namespace "nsdeletetest-7685" for this suite. 12/30/22 03:28:21.957
Dec 30 03:28:21.962: INFO: Namespace nsdeletetest-7685 was already deleted
STEP: Destroying namespace "nsdeletetest-4647" for this suite. 12/30/22 03:28:21.962
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","completed":14,"skipped":389,"failed":0}
------------------------------
• [SLOW TEST] [6.102 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:28:15.866
    Dec 30 03:28:15.866: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename namespaces 12/30/22 03:28:15.867
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:28:15.881
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:28:15.884
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:250
    STEP: Creating a test namespace 12/30/22 03:28:15.887
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:28:15.899
    STEP: Creating a service in the namespace 12/30/22 03:28:15.902
    STEP: Deleting the namespace 12/30/22 03:28:15.912
    STEP: Waiting for the namespace to be removed. 12/30/22 03:28:15.919
    STEP: Recreating the namespace 12/30/22 03:28:21.924
    STEP: Verifying there is no service in the namespace 12/30/22 03:28:21.938
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Dec 30 03:28:21.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-4427" for this suite. 12/30/22 03:28:21.947
    STEP: Destroying namespace "nsdeletetest-7685" for this suite. 12/30/22 03:28:21.957
    Dec 30 03:28:21.962: INFO: Namespace nsdeletetest-7685 was already deleted
    STEP: Destroying namespace "nsdeletetest-4647" for this suite. 12/30/22 03:28:21.962
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:28:21.971
Dec 30 03:28:21.971: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename resourcequota 12/30/22 03:28:21.972
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:28:21.985
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:28:21.989
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793
STEP: Creating a ResourceQuota with best effort scope 12/30/22 03:28:21.992
STEP: Ensuring ResourceQuota status is calculated 12/30/22 03:28:21.997
STEP: Creating a ResourceQuota with not best effort scope 12/30/22 03:28:24.003
STEP: Ensuring ResourceQuota status is calculated 12/30/22 03:28:24.008
STEP: Creating a best-effort pod 12/30/22 03:28:26.014
STEP: Ensuring resource quota with best effort scope captures the pod usage 12/30/22 03:28:26.031
STEP: Ensuring resource quota with not best effort ignored the pod usage 12/30/22 03:28:28.036
STEP: Deleting the pod 12/30/22 03:28:30.041
STEP: Ensuring resource quota status released the pod usage 12/30/22 03:28:30.055
STEP: Creating a not best-effort pod 12/30/22 03:28:32.059
STEP: Ensuring resource quota with not best effort scope captures the pod usage 12/30/22 03:28:32.071
STEP: Ensuring resource quota with best effort scope ignored the pod usage 12/30/22 03:28:34.077
STEP: Deleting the pod 12/30/22 03:28:36.083
STEP: Ensuring resource quota status released the pod usage 12/30/22 03:28:36.096
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Dec 30 03:28:38.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4285" for this suite. 12/30/22 03:28:38.108
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","completed":15,"skipped":411,"failed":0}
------------------------------
• [SLOW TEST] [16.143 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:28:21.971
    Dec 30 03:28:21.971: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename resourcequota 12/30/22 03:28:21.972
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:28:21.985
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:28:21.989
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:793
    STEP: Creating a ResourceQuota with best effort scope 12/30/22 03:28:21.992
    STEP: Ensuring ResourceQuota status is calculated 12/30/22 03:28:21.997
    STEP: Creating a ResourceQuota with not best effort scope 12/30/22 03:28:24.003
    STEP: Ensuring ResourceQuota status is calculated 12/30/22 03:28:24.008
    STEP: Creating a best-effort pod 12/30/22 03:28:26.014
    STEP: Ensuring resource quota with best effort scope captures the pod usage 12/30/22 03:28:26.031
    STEP: Ensuring resource quota with not best effort ignored the pod usage 12/30/22 03:28:28.036
    STEP: Deleting the pod 12/30/22 03:28:30.041
    STEP: Ensuring resource quota status released the pod usage 12/30/22 03:28:30.055
    STEP: Creating a not best-effort pod 12/30/22 03:28:32.059
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 12/30/22 03:28:32.071
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 12/30/22 03:28:34.077
    STEP: Deleting the pod 12/30/22 03:28:36.083
    STEP: Ensuring resource quota status released the pod usage 12/30/22 03:28:36.096
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Dec 30 03:28:38.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-4285" for this suite. 12/30/22 03:28:38.108
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:28:38.116
Dec 30 03:28:38.117: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename init-container 12/30/22 03:28:38.118
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:28:38.133
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:28:38.137
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176
STEP: creating the pod 12/30/22 03:28:38.14
Dec 30 03:28:38.140: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Dec 30 03:28:43.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3410" for this suite. 12/30/22 03:28:43.861
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","completed":16,"skipped":430,"failed":0}
------------------------------
• [SLOW TEST] [5.755 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:28:38.116
    Dec 30 03:28:38.117: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename init-container 12/30/22 03:28:38.118
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:28:38.133
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:28:38.137
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:176
    STEP: creating the pod 12/30/22 03:28:38.14
    Dec 30 03:28:38.140: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Dec 30 03:28:43.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-3410" for this suite. 12/30/22 03:28:43.861
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:28:43.876
Dec 30 03:28:43.876: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename dns 12/30/22 03:28:43.878
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:28:43.89
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:28:43.894
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 12/30/22 03:28:43.897
Dec 30 03:28:43.906: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-4842  467ba856-d9ac-4d49-ac39-c1bb092422b6 421728 0 2022-12-30 03:28:43 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2022-12-30 03:28:43 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-px6zh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-px6zh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 30 03:28:43.907: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-4842" to be "running and ready"
Dec 30 03:28:43.911: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 3.815343ms
Dec 30 03:28:43.911: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Dec 30 03:28:45.916: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.008812806s
Dec 30 03:28:45.916: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
Dec 30 03:28:45.916: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 12/30/22 03:28:45.916
Dec 30 03:28:45.916: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-4842 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 30 03:28:45.916: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
Dec 30 03:28:45.917: INFO: ExecWithOptions: Clientset creation
Dec 30 03:28:45.917: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-4842/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 12/30/22 03:28:46.053
Dec 30 03:28:46.053: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-4842 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 30 03:28:46.053: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
Dec 30 03:28:46.054: INFO: ExecWithOptions: Clientset creation
Dec 30 03:28:46.054: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-4842/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Dec 30 03:28:46.181: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Dec 30 03:28:46.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4842" for this suite. 12/30/22 03:28:46.199
{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","completed":17,"skipped":483,"failed":0}
------------------------------
• [2.329 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:28:43.876
    Dec 30 03:28:43.876: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename dns 12/30/22 03:28:43.878
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:28:43.89
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:28:43.894
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 12/30/22 03:28:43.897
    Dec 30 03:28:43.906: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-4842  467ba856-d9ac-4d49-ac39-c1bb092422b6 421728 0 2022-12-30 03:28:43 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2022-12-30 03:28:43 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-px6zh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-px6zh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 30 03:28:43.907: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-4842" to be "running and ready"
    Dec 30 03:28:43.911: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 3.815343ms
    Dec 30 03:28:43.911: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Dec 30 03:28:45.916: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.008812806s
    Dec 30 03:28:45.916: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    Dec 30 03:28:45.916: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 12/30/22 03:28:45.916
    Dec 30 03:28:45.916: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-4842 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 30 03:28:45.916: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    Dec 30 03:28:45.917: INFO: ExecWithOptions: Clientset creation
    Dec 30 03:28:45.917: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-4842/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 12/30/22 03:28:46.053
    Dec 30 03:28:46.053: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-4842 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 30 03:28:46.053: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    Dec 30 03:28:46.054: INFO: ExecWithOptions: Clientset creation
    Dec 30 03:28:46.054: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-4842/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Dec 30 03:28:46.181: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Dec 30 03:28:46.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-4842" for this suite. 12/30/22 03:28:46.199
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:527
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:28:46.206
Dec 30 03:28:46.207: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename security-context-test 12/30/22 03:28:46.208
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:28:46.222
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:28:46.225
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:527
Dec 30 03:28:46.237: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-bf985439-d7d1-404f-8d9d-d96f941a0adf" in namespace "security-context-test-3929" to be "Succeeded or Failed"
Dec 30 03:28:46.242: INFO: Pod "busybox-privileged-false-bf985439-d7d1-404f-8d9d-d96f941a0adf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.726706ms
Dec 30 03:28:48.247: INFO: Pod "busybox-privileged-false-bf985439-d7d1-404f-8d9d-d96f941a0adf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00978s
Dec 30 03:28:50.248: INFO: Pod "busybox-privileged-false-bf985439-d7d1-404f-8d9d-d96f941a0adf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010444841s
Dec 30 03:28:50.248: INFO: Pod "busybox-privileged-false-bf985439-d7d1-404f-8d9d-d96f941a0adf" satisfied condition "Succeeded or Failed"
Dec 30 03:28:50.256: INFO: Got logs for pod "busybox-privileged-false-bf985439-d7d1-404f-8d9d-d96f941a0adf": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Dec 30 03:28:50.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3929" for this suite. 12/30/22 03:28:50.262
{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","completed":18,"skipped":493,"failed":0}
------------------------------
• [4.063 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:490
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:527

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:28:46.206
    Dec 30 03:28:46.207: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename security-context-test 12/30/22 03:28:46.208
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:28:46.222
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:28:46.225
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:527
    Dec 30 03:28:46.237: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-bf985439-d7d1-404f-8d9d-d96f941a0adf" in namespace "security-context-test-3929" to be "Succeeded or Failed"
    Dec 30 03:28:46.242: INFO: Pod "busybox-privileged-false-bf985439-d7d1-404f-8d9d-d96f941a0adf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.726706ms
    Dec 30 03:28:48.247: INFO: Pod "busybox-privileged-false-bf985439-d7d1-404f-8d9d-d96f941a0adf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00978s
    Dec 30 03:28:50.248: INFO: Pod "busybox-privileged-false-bf985439-d7d1-404f-8d9d-d96f941a0adf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010444841s
    Dec 30 03:28:50.248: INFO: Pod "busybox-privileged-false-bf985439-d7d1-404f-8d9d-d96f941a0adf" satisfied condition "Succeeded or Failed"
    Dec 30 03:28:50.256: INFO: Got logs for pod "busybox-privileged-false-bf985439-d7d1-404f-8d9d-d96f941a0adf": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Dec 30 03:28:50.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-3929" for this suite. 12/30/22 03:28:50.262
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:28:50.271
Dec 30 03:28:50.271: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename projected 12/30/22 03:28:50.272
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:28:50.286
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:28:50.289
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52
STEP: Creating a pod to test downward API volume plugin 12/30/22 03:28:50.292
Dec 30 03:28:50.302: INFO: Waiting up to 5m0s for pod "downwardapi-volume-15b2aeb2-451d-46fa-8477-f7a92039e8fb" in namespace "projected-484" to be "Succeeded or Failed"
Dec 30 03:28:50.305: INFO: Pod "downwardapi-volume-15b2aeb2-451d-46fa-8477-f7a92039e8fb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.013738ms
Dec 30 03:28:52.311: INFO: Pod "downwardapi-volume-15b2aeb2-451d-46fa-8477-f7a92039e8fb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008565679s
Dec 30 03:28:54.310: INFO: Pod "downwardapi-volume-15b2aeb2-451d-46fa-8477-f7a92039e8fb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007423213s
STEP: Saw pod success 12/30/22 03:28:54.31
Dec 30 03:28:54.310: INFO: Pod "downwardapi-volume-15b2aeb2-451d-46fa-8477-f7a92039e8fb" satisfied condition "Succeeded or Failed"
Dec 30 03:28:54.314: INFO: Trying to get logs from node k8s-mgmt01 pod downwardapi-volume-15b2aeb2-451d-46fa-8477-f7a92039e8fb container client-container: <nil>
STEP: delete the pod 12/30/22 03:28:54.322
Dec 30 03:28:54.334: INFO: Waiting for pod downwardapi-volume-15b2aeb2-451d-46fa-8477-f7a92039e8fb to disappear
Dec 30 03:28:54.338: INFO: Pod downwardapi-volume-15b2aeb2-451d-46fa-8477-f7a92039e8fb no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Dec 30 03:28:54.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-484" for this suite. 12/30/22 03:28:54.344
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","completed":19,"skipped":517,"failed":0}
------------------------------
• [4.080 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:28:50.271
    Dec 30 03:28:50.271: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename projected 12/30/22 03:28:50.272
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:28:50.286
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:28:50.289
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:52
    STEP: Creating a pod to test downward API volume plugin 12/30/22 03:28:50.292
    Dec 30 03:28:50.302: INFO: Waiting up to 5m0s for pod "downwardapi-volume-15b2aeb2-451d-46fa-8477-f7a92039e8fb" in namespace "projected-484" to be "Succeeded or Failed"
    Dec 30 03:28:50.305: INFO: Pod "downwardapi-volume-15b2aeb2-451d-46fa-8477-f7a92039e8fb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.013738ms
    Dec 30 03:28:52.311: INFO: Pod "downwardapi-volume-15b2aeb2-451d-46fa-8477-f7a92039e8fb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008565679s
    Dec 30 03:28:54.310: INFO: Pod "downwardapi-volume-15b2aeb2-451d-46fa-8477-f7a92039e8fb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007423213s
    STEP: Saw pod success 12/30/22 03:28:54.31
    Dec 30 03:28:54.310: INFO: Pod "downwardapi-volume-15b2aeb2-451d-46fa-8477-f7a92039e8fb" satisfied condition "Succeeded or Failed"
    Dec 30 03:28:54.314: INFO: Trying to get logs from node k8s-mgmt01 pod downwardapi-volume-15b2aeb2-451d-46fa-8477-f7a92039e8fb container client-container: <nil>
    STEP: delete the pod 12/30/22 03:28:54.322
    Dec 30 03:28:54.334: INFO: Waiting for pod downwardapi-volume-15b2aeb2-451d-46fa-8477-f7a92039e8fb to disappear
    Dec 30 03:28:54.338: INFO: Pod downwardapi-volume-15b2aeb2-451d-46fa-8477-f7a92039e8fb no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Dec 30 03:28:54.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-484" for this suite. 12/30/22 03:28:54.344
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:28:54.352
Dec 30 03:28:54.352: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename projected 12/30/22 03:28:54.353
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:28:54.366
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:28:54.369
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66
STEP: Creating projection with secret that has name projected-secret-test-08aa6533-a5b1-4179-b2c1-dc096ddb28bb 12/30/22 03:28:54.372
STEP: Creating a pod to test consume secrets 12/30/22 03:28:54.377
Dec 30 03:28:54.386: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4f2bdcf5-e507-408a-bb58-76c6cbe47065" in namespace "projected-7248" to be "Succeeded or Failed"
Dec 30 03:28:54.389: INFO: Pod "pod-projected-secrets-4f2bdcf5-e507-408a-bb58-76c6cbe47065": Phase="Pending", Reason="", readiness=false. Elapsed: 3.068608ms
Dec 30 03:28:56.394: INFO: Pod "pod-projected-secrets-4f2bdcf5-e507-408a-bb58-76c6cbe47065": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008352142s
Dec 30 03:28:58.394: INFO: Pod "pod-projected-secrets-4f2bdcf5-e507-408a-bb58-76c6cbe47065": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007941857s
STEP: Saw pod success 12/30/22 03:28:58.394
Dec 30 03:28:58.394: INFO: Pod "pod-projected-secrets-4f2bdcf5-e507-408a-bb58-76c6cbe47065" satisfied condition "Succeeded or Failed"
Dec 30 03:28:58.398: INFO: Trying to get logs from node k8s-mgmt01 pod pod-projected-secrets-4f2bdcf5-e507-408a-bb58-76c6cbe47065 container projected-secret-volume-test: <nil>
STEP: delete the pod 12/30/22 03:28:58.406
Dec 30 03:28:58.418: INFO: Waiting for pod pod-projected-secrets-4f2bdcf5-e507-408a-bb58-76c6cbe47065 to disappear
Dec 30 03:28:58.422: INFO: Pod pod-projected-secrets-4f2bdcf5-e507-408a-bb58-76c6cbe47065 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Dec 30 03:28:58.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7248" for this suite. 12/30/22 03:28:58.428
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","completed":20,"skipped":519,"failed":0}
------------------------------
• [4.083 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:28:54.352
    Dec 30 03:28:54.352: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename projected 12/30/22 03:28:54.353
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:28:54.366
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:28:54.369
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:66
    STEP: Creating projection with secret that has name projected-secret-test-08aa6533-a5b1-4179-b2c1-dc096ddb28bb 12/30/22 03:28:54.372
    STEP: Creating a pod to test consume secrets 12/30/22 03:28:54.377
    Dec 30 03:28:54.386: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4f2bdcf5-e507-408a-bb58-76c6cbe47065" in namespace "projected-7248" to be "Succeeded or Failed"
    Dec 30 03:28:54.389: INFO: Pod "pod-projected-secrets-4f2bdcf5-e507-408a-bb58-76c6cbe47065": Phase="Pending", Reason="", readiness=false. Elapsed: 3.068608ms
    Dec 30 03:28:56.394: INFO: Pod "pod-projected-secrets-4f2bdcf5-e507-408a-bb58-76c6cbe47065": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008352142s
    Dec 30 03:28:58.394: INFO: Pod "pod-projected-secrets-4f2bdcf5-e507-408a-bb58-76c6cbe47065": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007941857s
    STEP: Saw pod success 12/30/22 03:28:58.394
    Dec 30 03:28:58.394: INFO: Pod "pod-projected-secrets-4f2bdcf5-e507-408a-bb58-76c6cbe47065" satisfied condition "Succeeded or Failed"
    Dec 30 03:28:58.398: INFO: Trying to get logs from node k8s-mgmt01 pod pod-projected-secrets-4f2bdcf5-e507-408a-bb58-76c6cbe47065 container projected-secret-volume-test: <nil>
    STEP: delete the pod 12/30/22 03:28:58.406
    Dec 30 03:28:58.418: INFO: Waiting for pod pod-projected-secrets-4f2bdcf5-e507-408a-bb58-76c6cbe47065 to disappear
    Dec 30 03:28:58.422: INFO: Pod pod-projected-secrets-4f2bdcf5-e507-408a-bb58-76c6cbe47065 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Dec 30 03:28:58.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7248" for this suite. 12/30/22 03:28:58.428
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:28:58.436
Dec 30 03:28:58.436: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename emptydir 12/30/22 03:28:58.437
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:28:58.449
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:28:58.452
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196
STEP: Creating a pod to test emptydir 0644 on node default medium 12/30/22 03:28:58.455
Dec 30 03:28:58.464: INFO: Waiting up to 5m0s for pod "pod-ba343b62-74b0-4181-9c0b-ee8110952c75" in namespace "emptydir-1548" to be "Succeeded or Failed"
Dec 30 03:28:58.467: INFO: Pod "pod-ba343b62-74b0-4181-9c0b-ee8110952c75": Phase="Pending", Reason="", readiness=false. Elapsed: 3.142937ms
Dec 30 03:29:00.472: INFO: Pod "pod-ba343b62-74b0-4181-9c0b-ee8110952c75": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007836218s
Dec 30 03:29:02.473: INFO: Pod "pod-ba343b62-74b0-4181-9c0b-ee8110952c75": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008557729s
STEP: Saw pod success 12/30/22 03:29:02.473
Dec 30 03:29:02.473: INFO: Pod "pod-ba343b62-74b0-4181-9c0b-ee8110952c75" satisfied condition "Succeeded or Failed"
Dec 30 03:29:02.476: INFO: Trying to get logs from node k8s-mgmt01 pod pod-ba343b62-74b0-4181-9c0b-ee8110952c75 container test-container: <nil>
STEP: delete the pod 12/30/22 03:29:02.485
Dec 30 03:29:02.498: INFO: Waiting for pod pod-ba343b62-74b0-4181-9c0b-ee8110952c75 to disappear
Dec 30 03:29:02.501: INFO: Pod pod-ba343b62-74b0-4181-9c0b-ee8110952c75 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Dec 30 03:29:02.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1548" for this suite. 12/30/22 03:29:02.511
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":21,"skipped":524,"failed":0}
------------------------------
• [4.095 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:28:58.436
    Dec 30 03:28:58.436: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename emptydir 12/30/22 03:28:58.437
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:28:58.449
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:28:58.452
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:196
    STEP: Creating a pod to test emptydir 0644 on node default medium 12/30/22 03:28:58.455
    Dec 30 03:28:58.464: INFO: Waiting up to 5m0s for pod "pod-ba343b62-74b0-4181-9c0b-ee8110952c75" in namespace "emptydir-1548" to be "Succeeded or Failed"
    Dec 30 03:28:58.467: INFO: Pod "pod-ba343b62-74b0-4181-9c0b-ee8110952c75": Phase="Pending", Reason="", readiness=false. Elapsed: 3.142937ms
    Dec 30 03:29:00.472: INFO: Pod "pod-ba343b62-74b0-4181-9c0b-ee8110952c75": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007836218s
    Dec 30 03:29:02.473: INFO: Pod "pod-ba343b62-74b0-4181-9c0b-ee8110952c75": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008557729s
    STEP: Saw pod success 12/30/22 03:29:02.473
    Dec 30 03:29:02.473: INFO: Pod "pod-ba343b62-74b0-4181-9c0b-ee8110952c75" satisfied condition "Succeeded or Failed"
    Dec 30 03:29:02.476: INFO: Trying to get logs from node k8s-mgmt01 pod pod-ba343b62-74b0-4181-9c0b-ee8110952c75 container test-container: <nil>
    STEP: delete the pod 12/30/22 03:29:02.485
    Dec 30 03:29:02.498: INFO: Waiting for pod pod-ba343b62-74b0-4181-9c0b-ee8110952c75 to disappear
    Dec 30 03:29:02.501: INFO: Pod pod-ba343b62-74b0-4181-9c0b-ee8110952c75 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Dec 30 03:29:02.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-1548" for this suite. 12/30/22 03:29:02.511
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:29:02.532
Dec 30 03:29:02.532: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename ephemeral-containers-test 12/30/22 03:29:02.533
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:29:02.545
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:29:02.547
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 12/30/22 03:29:02.55
Dec 30 03:29:02.557: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-8493" to be "running and ready"
Dec 30 03:29:02.561: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.569841ms
Dec 30 03:29:02.561: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Dec 30 03:29:04.565: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007492144s
Dec 30 03:29:04.565: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
Dec 30 03:29:04.565: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 12/30/22 03:29:04.569
Dec 30 03:29:04.589: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-8493" to be "container debugger running"
Dec 30 03:29:04.592: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.968186ms
Dec 30 03:29:06.598: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008421563s
Dec 30 03:29:08.598: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.008097908s
Dec 30 03:29:08.598: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 12/30/22 03:29:08.598
Dec 30 03:29:08.598: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-8493 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 30 03:29:08.598: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
Dec 30 03:29:08.599: INFO: ExecWithOptions: Clientset creation
Dec 30 03:29:08.599: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/ephemeral-containers-test-8493/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
Dec 30 03:29:08.714: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/framework.go:187
Dec 30 03:29:08.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ephemeral-containers-test-8493" for this suite. 12/30/22 03:29:08.728
{"msg":"PASSED [sig-node] Ephemeral Containers [NodeConformance] will start an ephemeral container in an existing pod [Conformance]","completed":22,"skipped":542,"failed":0}
------------------------------
• [SLOW TEST] [6.203 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:29:02.532
    Dec 30 03:29:02.532: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename ephemeral-containers-test 12/30/22 03:29:02.533
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:29:02.545
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:29:02.547
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 12/30/22 03:29:02.55
    Dec 30 03:29:02.557: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-8493" to be "running and ready"
    Dec 30 03:29:02.561: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.569841ms
    Dec 30 03:29:02.561: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Dec 30 03:29:04.565: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007492144s
    Dec 30 03:29:04.565: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    Dec 30 03:29:04.565: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 12/30/22 03:29:04.569
    Dec 30 03:29:04.589: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-8493" to be "container debugger running"
    Dec 30 03:29:04.592: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.968186ms
    Dec 30 03:29:06.598: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008421563s
    Dec 30 03:29:08.598: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.008097908s
    Dec 30 03:29:08.598: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 12/30/22 03:29:08.598
    Dec 30 03:29:08.598: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-8493 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 30 03:29:08.598: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    Dec 30 03:29:08.599: INFO: ExecWithOptions: Clientset creation
    Dec 30 03:29:08.599: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/ephemeral-containers-test-8493/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    Dec 30 03:29:08.714: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/framework.go:187
    Dec 30 03:29:08.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ephemeral-containers-test-8493" for this suite. 12/30/22 03:29:08.728
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:29:08.736
Dec 30 03:29:08.736: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename proxy 12/30/22 03:29:08.737
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:29:08.75
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:29:08.753
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 12/30/22 03:29:08.766
STEP: creating replication controller proxy-service-qqsfq in namespace proxy-5754 12/30/22 03:29:08.766
I1230 03:29:08.773634      25 runners.go:193] Created replication controller with name: proxy-service-qqsfq, namespace: proxy-5754, replica count: 1
I1230 03:29:09.824688      25 runners.go:193] proxy-service-qqsfq Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1230 03:29:10.825359      25 runners.go:193] proxy-service-qqsfq Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 30 03:29:10.829: INFO: setup took 2.072950381s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 12/30/22 03:29:10.829
Dec 30 03:29:10.836: INFO: (0) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:162/proxy/: bar (200; 5.736962ms)
Dec 30 03:29:10.836: INFO: (0) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:160/proxy/: foo (200; 5.73226ms)
Dec 30 03:29:10.836: INFO: (0) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:1080/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:1080/proxy/rewriteme">test<... (200; 6.340139ms)
Dec 30 03:29:10.837: INFO: (0) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b/proxy/rewriteme">test</a> (200; 6.572456ms)
Dec 30 03:29:10.838: INFO: (0) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:162/proxy/: bar (200; 8.279687ms)
Dec 30 03:29:10.838: INFO: (0) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:1080/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:1080/proxy/rewriteme">... (200; 8.426259ms)
Dec 30 03:29:10.839: INFO: (0) /api/v1/namespaces/proxy-5754/services/proxy-service-qqsfq:portname1/proxy/: foo (200; 9.558322ms)
Dec 30 03:29:10.840: INFO: (0) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:160/proxy/: foo (200; 9.444805ms)
Dec 30 03:29:10.840: INFO: (0) /api/v1/namespaces/proxy-5754/services/proxy-service-qqsfq:portname2/proxy/: bar (200; 9.69705ms)
Dec 30 03:29:10.840: INFO: (0) /api/v1/namespaces/proxy-5754/services/http:proxy-service-qqsfq:portname1/proxy/: foo (200; 9.817716ms)
Dec 30 03:29:10.840: INFO: (0) /api/v1/namespaces/proxy-5754/services/http:proxy-service-qqsfq:portname2/proxy/: bar (200; 9.606381ms)
Dec 30 03:29:10.845: INFO: (0) /api/v1/namespaces/proxy-5754/services/https:proxy-service-qqsfq:tlsportname1/proxy/: tls baz (200; 15.59154ms)
Dec 30 03:29:10.845: INFO: (0) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:460/proxy/: tls baz (200; 15.527206ms)
Dec 30 03:29:10.846: INFO: (0) /api/v1/namespaces/proxy-5754/services/https:proxy-service-qqsfq:tlsportname2/proxy/: tls qux (200; 16.364499ms)
Dec 30 03:29:10.846: INFO: (0) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:462/proxy/: tls qux (200; 16.327115ms)
Dec 30 03:29:10.847: INFO: (0) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:443/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:443/proxy/tlsrewritem... (200; 16.553061ms)
Dec 30 03:29:10.851: INFO: (1) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:162/proxy/: bar (200; 4.063539ms)
Dec 30 03:29:10.851: INFO: (1) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:160/proxy/: foo (200; 4.088479ms)
Dec 30 03:29:10.851: INFO: (1) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:160/proxy/: foo (200; 4.037552ms)
Dec 30 03:29:10.851: INFO: (1) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:462/proxy/: tls qux (200; 4.148893ms)
Dec 30 03:29:10.851: INFO: (1) /api/v1/namespaces/proxy-5754/services/https:proxy-service-qqsfq:tlsportname1/proxy/: tls baz (200; 4.667898ms)
Dec 30 03:29:10.852: INFO: (1) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:1080/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:1080/proxy/rewriteme">test<... (200; 5.102456ms)
Dec 30 03:29:10.852: INFO: (1) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:1080/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:1080/proxy/rewriteme">... (200; 5.056871ms)
Dec 30 03:29:10.852: INFO: (1) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b/proxy/rewriteme">test</a> (200; 5.511487ms)
Dec 30 03:29:10.852: INFO: (1) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:443/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:443/proxy/tlsrewritem... (200; 5.508869ms)
Dec 30 03:29:10.852: INFO: (1) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:460/proxy/: tls baz (200; 5.570198ms)
Dec 30 03:29:10.852: INFO: (1) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:162/proxy/: bar (200; 5.515877ms)
Dec 30 03:29:10.853: INFO: (1) /api/v1/namespaces/proxy-5754/services/http:proxy-service-qqsfq:portname2/proxy/: bar (200; 5.987211ms)
Dec 30 03:29:10.853: INFO: (1) /api/v1/namespaces/proxy-5754/services/proxy-service-qqsfq:portname1/proxy/: foo (200; 6.015756ms)
Dec 30 03:29:10.853: INFO: (1) /api/v1/namespaces/proxy-5754/services/proxy-service-qqsfq:portname2/proxy/: bar (200; 6.155664ms)
Dec 30 03:29:10.854: INFO: (1) /api/v1/namespaces/proxy-5754/services/https:proxy-service-qqsfq:tlsportname2/proxy/: tls qux (200; 7.034456ms)
Dec 30 03:29:10.854: INFO: (1) /api/v1/namespaces/proxy-5754/services/http:proxy-service-qqsfq:portname1/proxy/: foo (200; 7.427496ms)
Dec 30 03:29:10.858: INFO: (2) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:460/proxy/: tls baz (200; 4.013156ms)
Dec 30 03:29:10.858: INFO: (2) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:1080/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:1080/proxy/rewriteme">test<... (200; 4.073972ms)
Dec 30 03:29:10.858: INFO: (2) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:160/proxy/: foo (200; 4.024608ms)
Dec 30 03:29:10.859: INFO: (2) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:160/proxy/: foo (200; 4.263317ms)
Dec 30 03:29:10.859: INFO: (2) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b/proxy/rewriteme">test</a> (200; 4.443175ms)
Dec 30 03:29:10.859: INFO: (2) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:162/proxy/: bar (200; 4.336464ms)
Dec 30 03:29:10.860: INFO: (2) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:462/proxy/: tls qux (200; 5.246426ms)
Dec 30 03:29:10.860: INFO: (2) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:1080/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:1080/proxy/rewriteme">... (200; 5.262877ms)
Dec 30 03:29:10.860: INFO: (2) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:162/proxy/: bar (200; 5.233707ms)
Dec 30 03:29:10.860: INFO: (2) /api/v1/namespaces/proxy-5754/services/http:proxy-service-qqsfq:portname1/proxy/: foo (200; 5.808316ms)
Dec 30 03:29:10.861: INFO: (2) /api/v1/namespaces/proxy-5754/services/https:proxy-service-qqsfq:tlsportname1/proxy/: tls baz (200; 6.188808ms)
Dec 30 03:29:10.861: INFO: (2) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:443/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:443/proxy/tlsrewritem... (200; 6.131628ms)
Dec 30 03:29:10.861: INFO: (2) /api/v1/namespaces/proxy-5754/services/http:proxy-service-qqsfq:portname2/proxy/: bar (200; 6.221616ms)
Dec 30 03:29:10.861: INFO: (2) /api/v1/namespaces/proxy-5754/services/proxy-service-qqsfq:portname1/proxy/: foo (200; 6.726798ms)
Dec 30 03:29:10.861: INFO: (2) /api/v1/namespaces/proxy-5754/services/https:proxy-service-qqsfq:tlsportname2/proxy/: tls qux (200; 6.805377ms)
Dec 30 03:29:10.861: INFO: (2) /api/v1/namespaces/proxy-5754/services/proxy-service-qqsfq:portname2/proxy/: bar (200; 6.975263ms)
Dec 30 03:29:10.864: INFO: (3) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:1080/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:1080/proxy/rewriteme">test<... (200; 2.951034ms)
Dec 30 03:29:10.865: INFO: (3) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:1080/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:1080/proxy/rewriteme">... (200; 3.358063ms)
Dec 30 03:29:10.865: INFO: (3) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:160/proxy/: foo (200; 3.49233ms)
Dec 30 03:29:10.865: INFO: (3) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b/proxy/rewriteme">test</a> (200; 3.523405ms)
Dec 30 03:29:10.865: INFO: (3) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:160/proxy/: foo (200; 3.685462ms)
Dec 30 03:29:10.866: INFO: (3) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:162/proxy/: bar (200; 4.518057ms)
Dec 30 03:29:10.866: INFO: (3) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:162/proxy/: bar (200; 4.667381ms)
Dec 30 03:29:10.866: INFO: (3) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:443/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:443/proxy/tlsrewritem... (200; 4.640995ms)
Dec 30 03:29:10.866: INFO: (3) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:460/proxy/: tls baz (200; 4.662486ms)
Dec 30 03:29:10.866: INFO: (3) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:462/proxy/: tls qux (200; 4.685329ms)
Dec 30 03:29:10.866: INFO: (3) /api/v1/namespaces/proxy-5754/services/proxy-service-qqsfq:portname1/proxy/: foo (200; 4.818412ms)
Dec 30 03:29:10.867: INFO: (3) /api/v1/namespaces/proxy-5754/services/http:proxy-service-qqsfq:portname2/proxy/: bar (200; 5.640944ms)
Dec 30 03:29:10.868: INFO: (3) /api/v1/namespaces/proxy-5754/services/https:proxy-service-qqsfq:tlsportname2/proxy/: tls qux (200; 6.111634ms)
Dec 30 03:29:10.868: INFO: (3) /api/v1/namespaces/proxy-5754/services/proxy-service-qqsfq:portname2/proxy/: bar (200; 6.670052ms)
Dec 30 03:29:10.868: INFO: (3) /api/v1/namespaces/proxy-5754/services/http:proxy-service-qqsfq:portname1/proxy/: foo (200; 6.707027ms)
Dec 30 03:29:10.869: INFO: (3) /api/v1/namespaces/proxy-5754/services/https:proxy-service-qqsfq:tlsportname1/proxy/: tls baz (200; 7.767994ms)
Dec 30 03:29:10.873: INFO: (4) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:443/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:443/proxy/tlsrewritem... (200; 3.25826ms)
Dec 30 03:29:10.873: INFO: (4) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:462/proxy/: tls qux (200; 3.520128ms)
Dec 30 03:29:10.873: INFO: (4) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:460/proxy/: tls baz (200; 3.588111ms)
Dec 30 03:29:10.874: INFO: (4) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:1080/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:1080/proxy/rewriteme">... (200; 3.956173ms)
Dec 30 03:29:10.874: INFO: (4) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b/proxy/rewriteme">test</a> (200; 4.382659ms)
Dec 30 03:29:10.874: INFO: (4) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:160/proxy/: foo (200; 4.417223ms)
Dec 30 03:29:10.874: INFO: (4) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:1080/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:1080/proxy/rewriteme">test<... (200; 4.551263ms)
Dec 30 03:29:10.874: INFO: (4) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:162/proxy/: bar (200; 4.654363ms)
Dec 30 03:29:10.874: INFO: (4) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:160/proxy/: foo (200; 4.613182ms)
Dec 30 03:29:10.874: INFO: (4) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:162/proxy/: bar (200; 4.670833ms)
Dec 30 03:29:10.875: INFO: (4) /api/v1/namespaces/proxy-5754/services/https:proxy-service-qqsfq:tlsportname2/proxy/: tls qux (200; 5.40212ms)
Dec 30 03:29:10.875: INFO: (4) /api/v1/namespaces/proxy-5754/services/http:proxy-service-qqsfq:portname2/proxy/: bar (200; 5.566318ms)
Dec 30 03:29:10.875: INFO: (4) /api/v1/namespaces/proxy-5754/services/https:proxy-service-qqsfq:tlsportname1/proxy/: tls baz (200; 5.451642ms)
Dec 30 03:29:10.876: INFO: (4) /api/v1/namespaces/proxy-5754/services/proxy-service-qqsfq:portname2/proxy/: bar (200; 6.212937ms)
Dec 30 03:29:10.876: INFO: (4) /api/v1/namespaces/proxy-5754/services/proxy-service-qqsfq:portname1/proxy/: foo (200; 6.47234ms)
Dec 30 03:29:10.877: INFO: (4) /api/v1/namespaces/proxy-5754/services/http:proxy-service-qqsfq:portname1/proxy/: foo (200; 6.982354ms)
Dec 30 03:29:10.880: INFO: (5) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:160/proxy/: foo (200; 3.628682ms)
Dec 30 03:29:10.880: INFO: (5) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b/proxy/rewriteme">test</a> (200; 3.681452ms)
Dec 30 03:29:10.880: INFO: (5) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:162/proxy/: bar (200; 3.742936ms)
Dec 30 03:29:10.881: INFO: (5) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:443/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:443/proxy/tlsrewritem... (200; 3.864931ms)
Dec 30 03:29:10.881: INFO: (5) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:462/proxy/: tls qux (200; 4.021358ms)
Dec 30 03:29:10.881: INFO: (5) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:160/proxy/: foo (200; 3.893261ms)
Dec 30 03:29:10.882: INFO: (5) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:460/proxy/: tls baz (200; 4.858112ms)
Dec 30 03:29:10.882: INFO: (5) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:162/proxy/: bar (200; 4.826518ms)
Dec 30 03:29:10.882: INFO: (5) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:1080/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:1080/proxy/rewriteme">test<... (200; 4.853745ms)
Dec 30 03:29:10.882: INFO: (5) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:1080/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:1080/proxy/rewriteme">... (200; 4.852909ms)
Dec 30 03:29:10.883: INFO: (5) /api/v1/namespaces/proxy-5754/services/http:proxy-service-qqsfq:portname1/proxy/: foo (200; 5.790142ms)
Dec 30 03:29:10.883: INFO: (5) /api/v1/namespaces/proxy-5754/services/proxy-service-qqsfq:portname1/proxy/: foo (200; 5.931226ms)
Dec 30 03:29:10.883: INFO: (5) /api/v1/namespaces/proxy-5754/services/proxy-service-qqsfq:portname2/proxy/: bar (200; 5.851524ms)
Dec 30 03:29:10.883: INFO: (5) /api/v1/namespaces/proxy-5754/services/https:proxy-service-qqsfq:tlsportname1/proxy/: tls baz (200; 6.615804ms)
Dec 30 03:29:10.884: INFO: (5) /api/v1/namespaces/proxy-5754/services/http:proxy-service-qqsfq:portname2/proxy/: bar (200; 6.825459ms)
Dec 30 03:29:10.884: INFO: (5) /api/v1/namespaces/proxy-5754/services/https:proxy-service-qqsfq:tlsportname2/proxy/: tls qux (200; 6.956252ms)
Dec 30 03:29:10.887: INFO: (6) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b/proxy/rewriteme">test</a> (200; 3.377371ms)
Dec 30 03:29:10.887: INFO: (6) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:162/proxy/: bar (200; 3.687168ms)
Dec 30 03:29:10.888: INFO: (6) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:160/proxy/: foo (200; 3.691792ms)
Dec 30 03:29:10.888: INFO: (6) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:160/proxy/: foo (200; 3.901983ms)
Dec 30 03:29:10.888: INFO: (6) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:460/proxy/: tls baz (200; 4.046941ms)
Dec 30 03:29:10.889: INFO: (6) /api/v1/namespaces/proxy-5754/services/https:proxy-service-qqsfq:tlsportname2/proxy/: tls qux (200; 5.024719ms)
Dec 30 03:29:10.889: INFO: (6) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:1080/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:1080/proxy/rewriteme">... (200; 5.018397ms)
Dec 30 03:29:10.889: INFO: (6) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:462/proxy/: tls qux (200; 5.109732ms)
Dec 30 03:29:10.889: INFO: (6) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:1080/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:1080/proxy/rewriteme">test<... (200; 5.116256ms)
Dec 30 03:29:10.889: INFO: (6) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:443/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:443/proxy/tlsrewritem... (200; 5.106933ms)
Dec 30 03:29:10.889: INFO: (6) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:162/proxy/: bar (200; 5.144825ms)
Dec 30 03:29:10.890: INFO: (6) /api/v1/namespaces/proxy-5754/services/http:proxy-service-qqsfq:portname2/proxy/: bar (200; 5.873517ms)
Dec 30 03:29:10.890: INFO: (6) /api/v1/namespaces/proxy-5754/services/proxy-service-qqsfq:portname2/proxy/: bar (200; 5.969572ms)
Dec 30 03:29:10.890: INFO: (6) /api/v1/namespaces/proxy-5754/services/http:proxy-service-qqsfq:portname1/proxy/: foo (200; 6.387144ms)
Dec 30 03:29:10.890: INFO: (6) /api/v1/namespaces/proxy-5754/services/proxy-service-qqsfq:portname1/proxy/: foo (200; 6.341725ms)
Dec 30 03:29:10.892: INFO: (6) /api/v1/namespaces/proxy-5754/services/https:proxy-service-qqsfq:tlsportname1/proxy/: tls baz (200; 7.554736ms)
Dec 30 03:29:10.895: INFO: (7) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:1080/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:1080/proxy/rewriteme">... (200; 3.341407ms)
Dec 30 03:29:10.895: INFO: (7) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b/proxy/rewriteme">test</a> (200; 3.270735ms)
Dec 30 03:29:10.895: INFO: (7) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:462/proxy/: tls qux (200; 3.375822ms)
Dec 30 03:29:10.896: INFO: (7) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:162/proxy/: bar (200; 3.792781ms)
Dec 30 03:29:10.896: INFO: (7) /api/v1/namespaces/proxy-5754/services/proxy-service-qqsfq:portname1/proxy/: foo (200; 4.323909ms)
Dec 30 03:29:10.896: INFO: (7) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:1080/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:1080/proxy/rewriteme">test<... (200; 4.668835ms)
Dec 30 03:29:10.896: INFO: (7) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:162/proxy/: bar (200; 4.769821ms)
Dec 30 03:29:10.896: INFO: (7) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:443/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:443/proxy/tlsrewritem... (200; 4.737434ms)
Dec 30 03:29:10.897: INFO: (7) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:160/proxy/: foo (200; 4.869148ms)
Dec 30 03:29:10.897: INFO: (7) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:160/proxy/: foo (200; 5.07446ms)
Dec 30 03:29:10.897: INFO: (7) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:460/proxy/: tls baz (200; 5.060097ms)
Dec 30 03:29:10.897: INFO: (7) /api/v1/namespaces/proxy-5754/services/proxy-service-qqsfq:portname2/proxy/: bar (200; 5.480052ms)
Dec 30 03:29:10.897: INFO: (7) /api/v1/namespaces/proxy-5754/services/http:proxy-service-qqsfq:portname1/proxy/: foo (200; 5.595632ms)
Dec 30 03:29:10.898: INFO: (7) /api/v1/namespaces/proxy-5754/services/https:proxy-service-qqsfq:tlsportname2/proxy/: tls qux (200; 5.878788ms)
Dec 30 03:29:10.899: INFO: (7) /api/v1/namespaces/proxy-5754/services/http:proxy-service-qqsfq:portname2/proxy/: bar (200; 6.824682ms)
Dec 30 03:29:10.899: INFO: (7) /api/v1/namespaces/proxy-5754/services/https:proxy-service-qqsfq:tlsportname1/proxy/: tls baz (200; 6.86721ms)
Dec 30 03:29:10.902: INFO: (8) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:160/proxy/: foo (200; 3.036732ms)
Dec 30 03:29:10.902: INFO: (8) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:160/proxy/: foo (200; 3.157912ms)
Dec 30 03:29:10.903: INFO: (8) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:1080/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:1080/proxy/rewriteme">test<... (200; 3.981044ms)
Dec 30 03:29:10.903: INFO: (8) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:162/proxy/: bar (200; 3.931466ms)
Dec 30 03:29:10.903: INFO: (8) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:443/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:443/proxy/tlsrewritem... (200; 4.089635ms)
Dec 30 03:29:10.903: INFO: (8) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:462/proxy/: tls qux (200; 4.147674ms)
Dec 30 03:29:10.904: INFO: (8) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:460/proxy/: tls baz (200; 5.062883ms)
Dec 30 03:29:10.904: INFO: (8) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:1080/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:1080/proxy/rewriteme">... (200; 5.081161ms)
Dec 30 03:29:10.904: INFO: (8) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b/proxy/rewriteme">test</a> (200; 5.14007ms)
Dec 30 03:29:10.904: INFO: (8) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:162/proxy/: bar (200; 5.138389ms)
Dec 30 03:29:10.905: INFO: (8) /api/v1/namespaces/proxy-5754/services/http:proxy-service-qqsfq:portname2/proxy/: bar (200; 6.029023ms)
Dec 30 03:29:10.905: INFO: (8) /api/v1/namespaces/proxy-5754/services/http:proxy-service-qqsfq:portname1/proxy/: foo (200; 5.987142ms)
Dec 30 03:29:10.905: INFO: (8) /api/v1/namespaces/proxy-5754/services/proxy-service-qqsfq:portname1/proxy/: foo (200; 6.262237ms)
Dec 30 03:29:10.905: INFO: (8) /api/v1/namespaces/proxy-5754/services/https:proxy-service-qqsfq:tlsportname1/proxy/: tls baz (200; 6.704959ms)
Dec 30 03:29:10.906: INFO: (8) /api/v1/namespaces/proxy-5754/services/proxy-service-qqsfq:portname2/proxy/: bar (200; 6.830678ms)
Dec 30 03:29:10.906: INFO: (8) /api/v1/namespaces/proxy-5754/services/https:proxy-service-qqsfq:tlsportname2/proxy/: tls qux (200; 7.22943ms)
Dec 30 03:29:10.913: INFO: (9) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:462/proxy/: tls qux (200; 7.084891ms)
Dec 30 03:29:10.913: INFO: (9) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:443/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:443/proxy/tlsrewritem... (200; 7.112576ms)
Dec 30 03:29:10.914: INFO: (9) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:160/proxy/: foo (200; 7.779048ms)
Dec 30 03:29:10.914: INFO: (9) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:162/proxy/: bar (200; 7.822461ms)
Dec 30 03:29:10.914: INFO: (9) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:1080/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:1080/proxy/rewriteme">test<... (200; 8.403278ms)
Dec 30 03:29:10.914: INFO: (9) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:160/proxy/: foo (200; 8.386974ms)
Dec 30 03:29:10.915: INFO: (9) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:460/proxy/: tls baz (200; 8.462799ms)
Dec 30 03:29:10.915: INFO: (9) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:162/proxy/: bar (200; 8.690168ms)
Dec 30 03:29:10.915: INFO: (9) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:1080/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:1080/proxy/rewriteme">... (200; 8.572618ms)
Dec 30 03:29:10.915: INFO: (9) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b/proxy/rewriteme">test</a> (200; 8.694318ms)
Dec 30 03:29:10.915: INFO: (9) /api/v1/namespaces/proxy-5754/services/http:proxy-service-qqsfq:portname2/proxy/: bar (200; 9.08385ms)
Dec 30 03:29:10.916: INFO: (9) /api/v1/namespaces/proxy-5754/services/http:proxy-service-qqsfq:portname1/proxy/: foo (200; 9.907095ms)
Dec 30 03:29:10.916: INFO: (9) /api/v1/namespaces/proxy-5754/services/proxy-service-qqsfq:portname2/proxy/: bar (200; 9.939601ms)
Dec 30 03:29:10.916: INFO: (9) /api/v1/namespaces/proxy-5754/services/https:proxy-service-qqsfq:tlsportname1/proxy/: tls baz (200; 10.06666ms)
Dec 30 03:29:10.917: INFO: (9) /api/v1/namespaces/proxy-5754/services/proxy-service-qqsfq:portname1/proxy/: foo (200; 10.712335ms)
Dec 30 03:29:10.917: INFO: (9) /api/v1/namespaces/proxy-5754/services/https:proxy-service-qqsfq:tlsportname2/proxy/: tls qux (200; 11.325902ms)
Dec 30 03:29:10.921: INFO: (10) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:1080/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:1080/proxy/rewriteme">test<... (200; 3.06328ms)
Dec 30 03:29:10.921: INFO: (10) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:162/proxy/: bar (200; 3.126337ms)
Dec 30 03:29:10.921: INFO: (10) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:160/proxy/: foo (200; 3.30895ms)
Dec 30 03:29:10.921: INFO: (10) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b/proxy/rewriteme">test</a> (200; 3.245747ms)
Dec 30 03:29:10.921: INFO: (10) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:1080/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:1080/proxy/rewriteme">... (200; 3.529603ms)
Dec 30 03:29:10.921: INFO: (10) /api/v1/namespaces/proxy-5754/services/proxy-service-qqsfq:portname2/proxy/: bar (200; 3.739701ms)
Dec 30 03:29:10.922: INFO: (10) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:162/proxy/: bar (200; 4.234445ms)
Dec 30 03:29:10.922: INFO: (10) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:460/proxy/: tls baz (200; 4.434471ms)
Dec 30 03:29:10.922: INFO: (10) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:462/proxy/: tls qux (200; 4.469919ms)
Dec 30 03:29:10.922: INFO: (10) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:443/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:443/proxy/tlsrewritem... (200; 4.518192ms)
Dec 30 03:29:10.922: INFO: (10) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:160/proxy/: foo (200; 4.449385ms)
Dec 30 03:29:10.923: INFO: (10) /api/v1/namespaces/proxy-5754/services/http:proxy-service-qqsfq:portname1/proxy/: foo (200; 5.061777ms)
Dec 30 03:29:10.923: INFO: (10) /api/v1/namespaces/proxy-5754/services/https:proxy-service-qqsfq:tlsportname1/proxy/: tls baz (200; 5.296887ms)
Dec 30 03:29:10.923: INFO: (10) /api/v1/namespaces/proxy-5754/services/proxy-service-qqsfq:portname1/proxy/: foo (200; 5.404522ms)
Dec 30 03:29:10.924: INFO: (10) /api/v1/namespaces/proxy-5754/services/https:proxy-service-qqsfq:tlsportname2/proxy/: tls qux (200; 6.075733ms)
Dec 30 03:29:10.924: INFO: (10) /api/v1/namespaces/proxy-5754/services/http:proxy-service-qqsfq:portname2/proxy/: bar (200; 6.134032ms)
Dec 30 03:29:10.926: INFO: (11) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:1080/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:1080/proxy/rewriteme">... (200; 2.528327ms)
Dec 30 03:29:10.927: INFO: (11) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:162/proxy/: bar (200; 2.850331ms)
Dec 30 03:29:10.927: INFO: (11) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:443/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:443/proxy/tlsrewritem... (200; 3.102379ms)
Dec 30 03:29:10.927: INFO: (11) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:162/proxy/: bar (200; 3.077988ms)
Dec 30 03:29:10.927: INFO: (11) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:160/proxy/: foo (200; 3.024302ms)
Dec 30 03:29:10.927: INFO: (11) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:1080/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:1080/proxy/rewriteme">test<... (200; 3.514961ms)
Dec 30 03:29:10.928: INFO: (11) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:160/proxy/: foo (200; 3.65878ms)
Dec 30 03:29:10.928: INFO: (11) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b/proxy/rewriteme">test</a> (200; 3.755514ms)
Dec 30 03:29:10.928: INFO: (11) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:462/proxy/: tls qux (200; 4.449409ms)
Dec 30 03:29:10.928: INFO: (11) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:460/proxy/: tls baz (200; 4.51714ms)
Dec 30 03:29:10.929: INFO: (11) /api/v1/namespaces/proxy-5754/services/proxy-service-qqsfq:portname1/proxy/: foo (200; 4.796838ms)
Dec 30 03:29:10.929: INFO: (11) /api/v1/namespaces/proxy-5754/services/https:proxy-service-qqsfq:tlsportname2/proxy/: tls qux (200; 5.038625ms)
Dec 30 03:29:10.929: INFO: (11) /api/v1/namespaces/proxy-5754/services/proxy-service-qqsfq:portname2/proxy/: bar (200; 5.491478ms)
Dec 30 03:29:10.930: INFO: (11) /api/v1/namespaces/proxy-5754/services/http:proxy-service-qqsfq:portname1/proxy/: foo (200; 5.861543ms)
Dec 30 03:29:10.930: INFO: (11) /api/v1/namespaces/proxy-5754/services/https:proxy-service-qqsfq:tlsportname1/proxy/: tls baz (200; 5.816492ms)
Dec 30 03:29:10.930: INFO: (11) /api/v1/namespaces/proxy-5754/services/http:proxy-service-qqsfq:portname2/proxy/: bar (200; 6.135517ms)
Dec 30 03:29:10.933: INFO: (12) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:160/proxy/: foo (200; 2.922285ms)
Dec 30 03:29:10.933: INFO: (12) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:443/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:443/proxy/tlsrewritem... (200; 3.034241ms)
Dec 30 03:29:10.933: INFO: (12) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:462/proxy/: tls qux (200; 3.11514ms)
Dec 30 03:29:10.933: INFO: (12) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:1080/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:1080/proxy/rewriteme">... (200; 3.287976ms)
Dec 30 03:29:10.934: INFO: (12) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:162/proxy/: bar (200; 3.317266ms)
Dec 30 03:29:10.934: INFO: (12) /api/v1/namespaces/proxy-5754/services/http:proxy-service-qqsfq:portname1/proxy/: foo (200; 4.099522ms)
Dec 30 03:29:10.934: INFO: (12) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:160/proxy/: foo (200; 4.292775ms)
Dec 30 03:29:10.934: INFO: (12) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:460/proxy/: tls baz (200; 4.33383ms)
Dec 30 03:29:10.934: INFO: (12) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b/proxy/rewriteme">test</a> (200; 4.256841ms)
Dec 30 03:29:10.934: INFO: (12) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:162/proxy/: bar (200; 4.349325ms)
Dec 30 03:29:10.935: INFO: (12) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:1080/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:1080/proxy/rewriteme">test<... (200; 4.311588ms)
Dec 30 03:29:10.935: INFO: (12) /api/v1/namespaces/proxy-5754/services/http:proxy-service-qqsfq:portname2/proxy/: bar (200; 5.098834ms)
Dec 30 03:29:10.936: INFO: (12) /api/v1/namespaces/proxy-5754/services/https:proxy-service-qqsfq:tlsportname2/proxy/: tls qux (200; 5.631178ms)
Dec 30 03:29:10.936: INFO: (12) /api/v1/namespaces/proxy-5754/services/proxy-service-qqsfq:portname1/proxy/: foo (200; 5.759185ms)
Dec 30 03:29:10.936: INFO: (12) /api/v1/namespaces/proxy-5754/services/https:proxy-service-qqsfq:tlsportname1/proxy/: tls baz (200; 5.818907ms)
Dec 30 03:29:10.937: INFO: (12) /api/v1/namespaces/proxy-5754/services/proxy-service-qqsfq:portname2/proxy/: bar (200; 6.682687ms)
Dec 30 03:29:10.940: INFO: (13) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:160/proxy/: foo (200; 2.859652ms)
Dec 30 03:29:10.940: INFO: (13) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:162/proxy/: bar (200; 3.143485ms)
Dec 30 03:29:10.940: INFO: (13) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:460/proxy/: tls baz (200; 3.556665ms)
Dec 30 03:29:10.940: INFO: (13) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b/proxy/rewriteme">test</a> (200; 3.430585ms)
Dec 30 03:29:10.942: INFO: (13) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:160/proxy/: foo (200; 4.500309ms)
Dec 30 03:29:10.942: INFO: (13) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:1080/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:1080/proxy/rewriteme">... (200; 4.732094ms)
Dec 30 03:29:10.942: INFO: (13) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:1080/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:1080/proxy/rewriteme">test<... (200; 4.751251ms)
Dec 30 03:29:10.942: INFO: (13) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:162/proxy/: bar (200; 4.94072ms)
Dec 30 03:29:10.942: INFO: (13) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:462/proxy/: tls qux (200; 4.971864ms)
Dec 30 03:29:10.942: INFO: (13) /api/v1/namespaces/proxy-5754/services/https:proxy-service-qqsfq:tlsportname1/proxy/: tls baz (200; 4.91691ms)
Dec 30 03:29:10.942: INFO: (13) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:443/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:443/proxy/tlsrewritem... (200; 4.908419ms)
Dec 30 03:29:10.943: INFO: (13) /api/v1/namespaces/proxy-5754/services/proxy-service-qqsfq:portname1/proxy/: foo (200; 5.600767ms)
Dec 30 03:29:10.943: INFO: (13) /api/v1/namespaces/proxy-5754/services/http:proxy-service-qqsfq:portname2/proxy/: bar (200; 5.627772ms)
Dec 30 03:29:10.943: INFO: (13) /api/v1/namespaces/proxy-5754/services/https:proxy-service-qqsfq:tlsportname2/proxy/: tls qux (200; 5.660486ms)
Dec 30 03:29:10.943: INFO: (13) /api/v1/namespaces/proxy-5754/services/http:proxy-service-qqsfq:portname1/proxy/: foo (200; 6.341924ms)
Dec 30 03:29:10.944: INFO: (13) /api/v1/namespaces/proxy-5754/services/proxy-service-qqsfq:portname2/proxy/: bar (200; 6.616865ms)
Dec 30 03:29:10.947: INFO: (14) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b/proxy/rewriteme">test</a> (200; 3.237384ms)
Dec 30 03:29:10.947: INFO: (14) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:1080/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:1080/proxy/rewriteme">test<... (200; 3.339205ms)
Dec 30 03:29:10.947: INFO: (14) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:160/proxy/: foo (200; 3.634674ms)
Dec 30 03:29:10.947: INFO: (14) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:160/proxy/: foo (200; 3.590364ms)
Dec 30 03:29:10.948: INFO: (14) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:162/proxy/: bar (200; 3.822485ms)
Dec 30 03:29:10.948: INFO: (14) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:1080/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:1080/proxy/rewriteme">... (200; 3.939058ms)
Dec 30 03:29:10.948: INFO: (14) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:460/proxy/: tls baz (200; 4.652519ms)
Dec 30 03:29:10.949: INFO: (14) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:443/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:443/proxy/tlsrewritem... (200; 4.917815ms)
Dec 30 03:29:10.949: INFO: (14) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:462/proxy/: tls qux (200; 4.947245ms)
Dec 30 03:29:10.949: INFO: (14) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:162/proxy/: bar (200; 5.036634ms)
Dec 30 03:29:10.949: INFO: (14) /api/v1/namespaces/proxy-5754/services/https:proxy-service-qqsfq:tlsportname1/proxy/: tls baz (200; 5.150775ms)
Dec 30 03:29:10.949: INFO: (14) /api/v1/namespaces/proxy-5754/services/https:proxy-service-qqsfq:tlsportname2/proxy/: tls qux (200; 5.745158ms)
Dec 30 03:29:10.950: INFO: (14) /api/v1/namespaces/proxy-5754/services/proxy-service-qqsfq:portname2/proxy/: bar (200; 5.876828ms)
Dec 30 03:29:10.950: INFO: (14) /api/v1/namespaces/proxy-5754/services/http:proxy-service-qqsfq:portname1/proxy/: foo (200; 6.099394ms)
Dec 30 03:29:10.950: INFO: (14) /api/v1/namespaces/proxy-5754/services/proxy-service-qqsfq:portname1/proxy/: foo (200; 6.282467ms)
Dec 30 03:29:10.950: INFO: (14) /api/v1/namespaces/proxy-5754/services/http:proxy-service-qqsfq:portname2/proxy/: bar (200; 6.352915ms)
Dec 30 03:29:10.953: INFO: (15) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:162/proxy/: bar (200; 3.018325ms)
Dec 30 03:29:10.953: INFO: (15) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:1080/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:1080/proxy/rewriteme">... (200; 3.300717ms)
Dec 30 03:29:10.954: INFO: (15) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b/proxy/rewriteme">test</a> (200; 3.36817ms)
Dec 30 03:29:10.954: INFO: (15) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:462/proxy/: tls qux (200; 3.718914ms)
Dec 30 03:29:10.954: INFO: (15) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:443/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:443/proxy/tlsrewritem... (200; 3.924642ms)
Dec 30 03:29:10.954: INFO: (15) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:460/proxy/: tls baz (200; 3.994876ms)
Dec 30 03:29:10.954: INFO: (15) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:1080/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:1080/proxy/rewriteme">test<... (200; 4.013032ms)
Dec 30 03:29:10.955: INFO: (15) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:160/proxy/: foo (200; 4.328195ms)
Dec 30 03:29:10.955: INFO: (15) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:160/proxy/: foo (200; 4.538014ms)
Dec 30 03:29:10.955: INFO: (15) /api/v1/namespaces/proxy-5754/services/http:proxy-service-qqsfq:portname2/proxy/: bar (200; 4.512057ms)
Dec 30 03:29:10.955: INFO: (15) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:162/proxy/: bar (200; 4.787205ms)
Dec 30 03:29:10.955: INFO: (15) /api/v1/namespaces/proxy-5754/services/proxy-service-qqsfq:portname1/proxy/: foo (200; 5.331518ms)
Dec 30 03:29:10.956: INFO: (15) /api/v1/namespaces/proxy-5754/services/proxy-service-qqsfq:portname2/proxy/: bar (200; 5.598689ms)
Dec 30 03:29:10.956: INFO: (15) /api/v1/namespaces/proxy-5754/services/https:proxy-service-qqsfq:tlsportname2/proxy/: tls qux (200; 6.038218ms)
Dec 30 03:29:10.957: INFO: (15) /api/v1/namespaces/proxy-5754/services/https:proxy-service-qqsfq:tlsportname1/proxy/: tls baz (200; 6.465271ms)
Dec 30 03:29:10.957: INFO: (15) /api/v1/namespaces/proxy-5754/services/http:proxy-service-qqsfq:portname1/proxy/: foo (200; 6.518995ms)
Dec 30 03:29:10.960: INFO: (16) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:1080/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:1080/proxy/rewriteme">... (200; 3.067359ms)
Dec 30 03:29:10.960: INFO: (16) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:160/proxy/: foo (200; 3.147691ms)
Dec 30 03:29:10.961: INFO: (16) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:1080/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:1080/proxy/rewriteme">test<... (200; 3.613072ms)
Dec 30 03:29:10.961: INFO: (16) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:443/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:443/proxy/tlsrewritem... (200; 3.876383ms)
Dec 30 03:29:10.961: INFO: (16) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:462/proxy/: tls qux (200; 3.826022ms)
Dec 30 03:29:10.961: INFO: (16) /api/v1/namespaces/proxy-5754/services/proxy-service-qqsfq:portname1/proxy/: foo (200; 3.929748ms)
Dec 30 03:29:10.961: INFO: (16) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:460/proxy/: tls baz (200; 4.205951ms)
Dec 30 03:29:10.961: INFO: (16) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:162/proxy/: bar (200; 4.343605ms)
Dec 30 03:29:10.961: INFO: (16) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:160/proxy/: foo (200; 4.336869ms)
Dec 30 03:29:10.962: INFO: (16) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b/proxy/rewriteme">test</a> (200; 4.669462ms)
Dec 30 03:29:10.962: INFO: (16) /api/v1/namespaces/proxy-5754/services/http:proxy-service-qqsfq:portname2/proxy/: bar (200; 4.705141ms)
Dec 30 03:29:10.962: INFO: (16) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:162/proxy/: bar (200; 4.651228ms)
Dec 30 03:29:10.962: INFO: (16) /api/v1/namespaces/proxy-5754/services/https:proxy-service-qqsfq:tlsportname2/proxy/: tls qux (200; 5.300715ms)
Dec 30 03:29:10.962: INFO: (16) /api/v1/namespaces/proxy-5754/services/http:proxy-service-qqsfq:portname1/proxy/: foo (200; 5.479495ms)
Dec 30 03:29:10.963: INFO: (16) /api/v1/namespaces/proxy-5754/services/proxy-service-qqsfq:portname2/proxy/: bar (200; 6.063399ms)
Dec 30 03:29:10.963: INFO: (16) /api/v1/namespaces/proxy-5754/services/https:proxy-service-qqsfq:tlsportname1/proxy/: tls baz (200; 6.207415ms)
Dec 30 03:29:10.967: INFO: (17) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:162/proxy/: bar (200; 3.326433ms)
Dec 30 03:29:10.967: INFO: (17) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:160/proxy/: foo (200; 3.572337ms)
Dec 30 03:29:10.967: INFO: (17) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:462/proxy/: tls qux (200; 4.003604ms)
Dec 30 03:29:10.967: INFO: (17) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:160/proxy/: foo (200; 4.020677ms)
Dec 30 03:29:10.967: INFO: (17) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:162/proxy/: bar (200; 4.068902ms)
Dec 30 03:29:10.968: INFO: (17) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b/proxy/rewriteme">test</a> (200; 4.079092ms)
Dec 30 03:29:10.968: INFO: (17) /api/v1/namespaces/proxy-5754/services/proxy-service-qqsfq:portname2/proxy/: bar (200; 4.732423ms)
Dec 30 03:29:10.969: INFO: (17) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:1080/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:1080/proxy/rewriteme">... (200; 5.167708ms)
Dec 30 03:29:10.969: INFO: (17) /api/v1/namespaces/proxy-5754/services/https:proxy-service-qqsfq:tlsportname1/proxy/: tls baz (200; 5.260468ms)
Dec 30 03:29:10.969: INFO: (17) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:1080/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:1080/proxy/rewriteme">test<... (200; 5.4015ms)
Dec 30 03:29:10.969: INFO: (17) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:443/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:443/proxy/tlsrewritem... (200; 5.368031ms)
Dec 30 03:29:10.969: INFO: (17) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:460/proxy/: tls baz (200; 5.403777ms)
Dec 30 03:29:10.969: INFO: (17) /api/v1/namespaces/proxy-5754/services/http:proxy-service-qqsfq:portname1/proxy/: foo (200; 6.05112ms)
Dec 30 03:29:10.969: INFO: (17) /api/v1/namespaces/proxy-5754/services/https:proxy-service-qqsfq:tlsportname2/proxy/: tls qux (200; 6.009893ms)
Dec 30 03:29:10.970: INFO: (17) /api/v1/namespaces/proxy-5754/services/http:proxy-service-qqsfq:portname2/proxy/: bar (200; 6.400173ms)
Dec 30 03:29:10.970: INFO: (17) /api/v1/namespaces/proxy-5754/services/proxy-service-qqsfq:portname1/proxy/: foo (200; 6.888395ms)
Dec 30 03:29:10.974: INFO: (18) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:460/proxy/: tls baz (200; 3.148727ms)
Dec 30 03:29:10.974: INFO: (18) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:162/proxy/: bar (200; 3.283223ms)
Dec 30 03:29:10.974: INFO: (18) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b/proxy/rewriteme">test</a> (200; 3.490803ms)
Dec 30 03:29:10.974: INFO: (18) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:443/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:443/proxy/tlsrewritem... (200; 3.516026ms)
Dec 30 03:29:10.974: INFO: (18) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:162/proxy/: bar (200; 3.645072ms)
Dec 30 03:29:10.975: INFO: (18) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:160/proxy/: foo (200; 4.419406ms)
Dec 30 03:29:10.975: INFO: (18) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:462/proxy/: tls qux (200; 4.455825ms)
Dec 30 03:29:10.975: INFO: (18) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:160/proxy/: foo (200; 4.345092ms)
Dec 30 03:29:10.975: INFO: (18) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:1080/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:1080/proxy/rewriteme">... (200; 4.585743ms)
Dec 30 03:29:10.975: INFO: (18) /api/v1/namespaces/proxy-5754/services/http:proxy-service-qqsfq:portname1/proxy/: foo (200; 4.692025ms)
Dec 30 03:29:10.975: INFO: (18) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:1080/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:1080/proxy/rewriteme">test<... (200; 4.635164ms)
Dec 30 03:29:10.976: INFO: (18) /api/v1/namespaces/proxy-5754/services/https:proxy-service-qqsfq:tlsportname2/proxy/: tls qux (200; 5.380943ms)
Dec 30 03:29:10.977: INFO: (18) /api/v1/namespaces/proxy-5754/services/proxy-service-qqsfq:portname2/proxy/: bar (200; 5.922782ms)
Dec 30 03:29:10.977: INFO: (18) /api/v1/namespaces/proxy-5754/services/proxy-service-qqsfq:portname1/proxy/: foo (200; 6.134443ms)
Dec 30 03:29:10.977: INFO: (18) /api/v1/namespaces/proxy-5754/services/http:proxy-service-qqsfq:portname2/proxy/: bar (200; 6.463619ms)
Dec 30 03:29:10.978: INFO: (18) /api/v1/namespaces/proxy-5754/services/https:proxy-service-qqsfq:tlsportname1/proxy/: tls baz (200; 7.043447ms)
Dec 30 03:29:10.981: INFO: (19) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:443/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:443/proxy/tlsrewritem... (200; 3.052325ms)
Dec 30 03:29:10.981: INFO: (19) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:162/proxy/: bar (200; 3.183653ms)
Dec 30 03:29:10.981: INFO: (19) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b/proxy/rewriteme">test</a> (200; 3.57858ms)
Dec 30 03:29:10.982: INFO: (19) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:460/proxy/: tls baz (200; 3.816272ms)
Dec 30 03:29:10.982: INFO: (19) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:462/proxy/: tls qux (200; 3.73968ms)
Dec 30 03:29:10.982: INFO: (19) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:160/proxy/: foo (200; 4.158234ms)
Dec 30 03:29:10.982: INFO: (19) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:1080/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:1080/proxy/rewriteme">... (200; 4.175737ms)
Dec 30 03:29:10.982: INFO: (19) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:160/proxy/: foo (200; 4.246267ms)
Dec 30 03:29:10.982: INFO: (19) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:1080/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:1080/proxy/rewriteme">test<... (200; 4.404864ms)
Dec 30 03:29:10.983: INFO: (19) /api/v1/namespaces/proxy-5754/services/https:proxy-service-qqsfq:tlsportname2/proxy/: tls qux (200; 4.900641ms)
Dec 30 03:29:10.983: INFO: (19) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:162/proxy/: bar (200; 5.031081ms)
Dec 30 03:29:10.983: INFO: (19) /api/v1/namespaces/proxy-5754/services/http:proxy-service-qqsfq:portname2/proxy/: bar (200; 5.10498ms)
Dec 30 03:29:10.983: INFO: (19) /api/v1/namespaces/proxy-5754/services/proxy-service-qqsfq:portname1/proxy/: foo (200; 5.431551ms)
Dec 30 03:29:10.984: INFO: (19) /api/v1/namespaces/proxy-5754/services/proxy-service-qqsfq:portname2/proxy/: bar (200; 5.941271ms)
Dec 30 03:29:10.984: INFO: (19) /api/v1/namespaces/proxy-5754/services/https:proxy-service-qqsfq:tlsportname1/proxy/: tls baz (200; 6.395103ms)
Dec 30 03:29:10.985: INFO: (19) /api/v1/namespaces/proxy-5754/services/http:proxy-service-qqsfq:portname1/proxy/: foo (200; 6.773327ms)
STEP: deleting ReplicationController proxy-service-qqsfq in namespace proxy-5754, will wait for the garbage collector to delete the pods 12/30/22 03:29:10.985
Dec 30 03:29:11.046: INFO: Deleting ReplicationController proxy-service-qqsfq took: 6.919289ms
Dec 30 03:29:11.147: INFO: Terminating ReplicationController proxy-service-qqsfq pods took: 101.116495ms
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Dec 30 03:29:13.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-5754" for this suite. 12/30/22 03:29:13.454
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","completed":23,"skipped":543,"failed":0}
------------------------------
• [4.726 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:29:08.736
    Dec 30 03:29:08.736: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename proxy 12/30/22 03:29:08.737
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:29:08.75
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:29:08.753
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 12/30/22 03:29:08.766
    STEP: creating replication controller proxy-service-qqsfq in namespace proxy-5754 12/30/22 03:29:08.766
    I1230 03:29:08.773634      25 runners.go:193] Created replication controller with name: proxy-service-qqsfq, namespace: proxy-5754, replica count: 1
    I1230 03:29:09.824688      25 runners.go:193] proxy-service-qqsfq Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I1230 03:29:10.825359      25 runners.go:193] proxy-service-qqsfq Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Dec 30 03:29:10.829: INFO: setup took 2.072950381s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 12/30/22 03:29:10.829
    Dec 30 03:29:10.836: INFO: (0) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:162/proxy/: bar (200; 5.736962ms)
    Dec 30 03:29:10.836: INFO: (0) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:160/proxy/: foo (200; 5.73226ms)
    Dec 30 03:29:10.836: INFO: (0) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:1080/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:1080/proxy/rewriteme">test<... (200; 6.340139ms)
    Dec 30 03:29:10.837: INFO: (0) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b/proxy/rewriteme">test</a> (200; 6.572456ms)
    Dec 30 03:29:10.838: INFO: (0) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:162/proxy/: bar (200; 8.279687ms)
    Dec 30 03:29:10.838: INFO: (0) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:1080/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:1080/proxy/rewriteme">... (200; 8.426259ms)
    Dec 30 03:29:10.839: INFO: (0) /api/v1/namespaces/proxy-5754/services/proxy-service-qqsfq:portname1/proxy/: foo (200; 9.558322ms)
    Dec 30 03:29:10.840: INFO: (0) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:160/proxy/: foo (200; 9.444805ms)
    Dec 30 03:29:10.840: INFO: (0) /api/v1/namespaces/proxy-5754/services/proxy-service-qqsfq:portname2/proxy/: bar (200; 9.69705ms)
    Dec 30 03:29:10.840: INFO: (0) /api/v1/namespaces/proxy-5754/services/http:proxy-service-qqsfq:portname1/proxy/: foo (200; 9.817716ms)
    Dec 30 03:29:10.840: INFO: (0) /api/v1/namespaces/proxy-5754/services/http:proxy-service-qqsfq:portname2/proxy/: bar (200; 9.606381ms)
    Dec 30 03:29:10.845: INFO: (0) /api/v1/namespaces/proxy-5754/services/https:proxy-service-qqsfq:tlsportname1/proxy/: tls baz (200; 15.59154ms)
    Dec 30 03:29:10.845: INFO: (0) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:460/proxy/: tls baz (200; 15.527206ms)
    Dec 30 03:29:10.846: INFO: (0) /api/v1/namespaces/proxy-5754/services/https:proxy-service-qqsfq:tlsportname2/proxy/: tls qux (200; 16.364499ms)
    Dec 30 03:29:10.846: INFO: (0) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:462/proxy/: tls qux (200; 16.327115ms)
    Dec 30 03:29:10.847: INFO: (0) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:443/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:443/proxy/tlsrewritem... (200; 16.553061ms)
    Dec 30 03:29:10.851: INFO: (1) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:162/proxy/: bar (200; 4.063539ms)
    Dec 30 03:29:10.851: INFO: (1) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:160/proxy/: foo (200; 4.088479ms)
    Dec 30 03:29:10.851: INFO: (1) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:160/proxy/: foo (200; 4.037552ms)
    Dec 30 03:29:10.851: INFO: (1) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:462/proxy/: tls qux (200; 4.148893ms)
    Dec 30 03:29:10.851: INFO: (1) /api/v1/namespaces/proxy-5754/services/https:proxy-service-qqsfq:tlsportname1/proxy/: tls baz (200; 4.667898ms)
    Dec 30 03:29:10.852: INFO: (1) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:1080/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:1080/proxy/rewriteme">test<... (200; 5.102456ms)
    Dec 30 03:29:10.852: INFO: (1) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:1080/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:1080/proxy/rewriteme">... (200; 5.056871ms)
    Dec 30 03:29:10.852: INFO: (1) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b/proxy/rewriteme">test</a> (200; 5.511487ms)
    Dec 30 03:29:10.852: INFO: (1) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:443/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:443/proxy/tlsrewritem... (200; 5.508869ms)
    Dec 30 03:29:10.852: INFO: (1) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:460/proxy/: tls baz (200; 5.570198ms)
    Dec 30 03:29:10.852: INFO: (1) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:162/proxy/: bar (200; 5.515877ms)
    Dec 30 03:29:10.853: INFO: (1) /api/v1/namespaces/proxy-5754/services/http:proxy-service-qqsfq:portname2/proxy/: bar (200; 5.987211ms)
    Dec 30 03:29:10.853: INFO: (1) /api/v1/namespaces/proxy-5754/services/proxy-service-qqsfq:portname1/proxy/: foo (200; 6.015756ms)
    Dec 30 03:29:10.853: INFO: (1) /api/v1/namespaces/proxy-5754/services/proxy-service-qqsfq:portname2/proxy/: bar (200; 6.155664ms)
    Dec 30 03:29:10.854: INFO: (1) /api/v1/namespaces/proxy-5754/services/https:proxy-service-qqsfq:tlsportname2/proxy/: tls qux (200; 7.034456ms)
    Dec 30 03:29:10.854: INFO: (1) /api/v1/namespaces/proxy-5754/services/http:proxy-service-qqsfq:portname1/proxy/: foo (200; 7.427496ms)
    Dec 30 03:29:10.858: INFO: (2) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:460/proxy/: tls baz (200; 4.013156ms)
    Dec 30 03:29:10.858: INFO: (2) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:1080/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:1080/proxy/rewriteme">test<... (200; 4.073972ms)
    Dec 30 03:29:10.858: INFO: (2) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:160/proxy/: foo (200; 4.024608ms)
    Dec 30 03:29:10.859: INFO: (2) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:160/proxy/: foo (200; 4.263317ms)
    Dec 30 03:29:10.859: INFO: (2) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b/proxy/rewriteme">test</a> (200; 4.443175ms)
    Dec 30 03:29:10.859: INFO: (2) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:162/proxy/: bar (200; 4.336464ms)
    Dec 30 03:29:10.860: INFO: (2) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:462/proxy/: tls qux (200; 5.246426ms)
    Dec 30 03:29:10.860: INFO: (2) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:1080/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:1080/proxy/rewriteme">... (200; 5.262877ms)
    Dec 30 03:29:10.860: INFO: (2) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:162/proxy/: bar (200; 5.233707ms)
    Dec 30 03:29:10.860: INFO: (2) /api/v1/namespaces/proxy-5754/services/http:proxy-service-qqsfq:portname1/proxy/: foo (200; 5.808316ms)
    Dec 30 03:29:10.861: INFO: (2) /api/v1/namespaces/proxy-5754/services/https:proxy-service-qqsfq:tlsportname1/proxy/: tls baz (200; 6.188808ms)
    Dec 30 03:29:10.861: INFO: (2) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:443/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:443/proxy/tlsrewritem... (200; 6.131628ms)
    Dec 30 03:29:10.861: INFO: (2) /api/v1/namespaces/proxy-5754/services/http:proxy-service-qqsfq:portname2/proxy/: bar (200; 6.221616ms)
    Dec 30 03:29:10.861: INFO: (2) /api/v1/namespaces/proxy-5754/services/proxy-service-qqsfq:portname1/proxy/: foo (200; 6.726798ms)
    Dec 30 03:29:10.861: INFO: (2) /api/v1/namespaces/proxy-5754/services/https:proxy-service-qqsfq:tlsportname2/proxy/: tls qux (200; 6.805377ms)
    Dec 30 03:29:10.861: INFO: (2) /api/v1/namespaces/proxy-5754/services/proxy-service-qqsfq:portname2/proxy/: bar (200; 6.975263ms)
    Dec 30 03:29:10.864: INFO: (3) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:1080/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:1080/proxy/rewriteme">test<... (200; 2.951034ms)
    Dec 30 03:29:10.865: INFO: (3) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:1080/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:1080/proxy/rewriteme">... (200; 3.358063ms)
    Dec 30 03:29:10.865: INFO: (3) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:160/proxy/: foo (200; 3.49233ms)
    Dec 30 03:29:10.865: INFO: (3) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b/proxy/rewriteme">test</a> (200; 3.523405ms)
    Dec 30 03:29:10.865: INFO: (3) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:160/proxy/: foo (200; 3.685462ms)
    Dec 30 03:29:10.866: INFO: (3) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:162/proxy/: bar (200; 4.518057ms)
    Dec 30 03:29:10.866: INFO: (3) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:162/proxy/: bar (200; 4.667381ms)
    Dec 30 03:29:10.866: INFO: (3) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:443/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:443/proxy/tlsrewritem... (200; 4.640995ms)
    Dec 30 03:29:10.866: INFO: (3) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:460/proxy/: tls baz (200; 4.662486ms)
    Dec 30 03:29:10.866: INFO: (3) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:462/proxy/: tls qux (200; 4.685329ms)
    Dec 30 03:29:10.866: INFO: (3) /api/v1/namespaces/proxy-5754/services/proxy-service-qqsfq:portname1/proxy/: foo (200; 4.818412ms)
    Dec 30 03:29:10.867: INFO: (3) /api/v1/namespaces/proxy-5754/services/http:proxy-service-qqsfq:portname2/proxy/: bar (200; 5.640944ms)
    Dec 30 03:29:10.868: INFO: (3) /api/v1/namespaces/proxy-5754/services/https:proxy-service-qqsfq:tlsportname2/proxy/: tls qux (200; 6.111634ms)
    Dec 30 03:29:10.868: INFO: (3) /api/v1/namespaces/proxy-5754/services/proxy-service-qqsfq:portname2/proxy/: bar (200; 6.670052ms)
    Dec 30 03:29:10.868: INFO: (3) /api/v1/namespaces/proxy-5754/services/http:proxy-service-qqsfq:portname1/proxy/: foo (200; 6.707027ms)
    Dec 30 03:29:10.869: INFO: (3) /api/v1/namespaces/proxy-5754/services/https:proxy-service-qqsfq:tlsportname1/proxy/: tls baz (200; 7.767994ms)
    Dec 30 03:29:10.873: INFO: (4) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:443/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:443/proxy/tlsrewritem... (200; 3.25826ms)
    Dec 30 03:29:10.873: INFO: (4) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:462/proxy/: tls qux (200; 3.520128ms)
    Dec 30 03:29:10.873: INFO: (4) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:460/proxy/: tls baz (200; 3.588111ms)
    Dec 30 03:29:10.874: INFO: (4) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:1080/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:1080/proxy/rewriteme">... (200; 3.956173ms)
    Dec 30 03:29:10.874: INFO: (4) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b/proxy/rewriteme">test</a> (200; 4.382659ms)
    Dec 30 03:29:10.874: INFO: (4) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:160/proxy/: foo (200; 4.417223ms)
    Dec 30 03:29:10.874: INFO: (4) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:1080/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:1080/proxy/rewriteme">test<... (200; 4.551263ms)
    Dec 30 03:29:10.874: INFO: (4) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:162/proxy/: bar (200; 4.654363ms)
    Dec 30 03:29:10.874: INFO: (4) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:160/proxy/: foo (200; 4.613182ms)
    Dec 30 03:29:10.874: INFO: (4) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:162/proxy/: bar (200; 4.670833ms)
    Dec 30 03:29:10.875: INFO: (4) /api/v1/namespaces/proxy-5754/services/https:proxy-service-qqsfq:tlsportname2/proxy/: tls qux (200; 5.40212ms)
    Dec 30 03:29:10.875: INFO: (4) /api/v1/namespaces/proxy-5754/services/http:proxy-service-qqsfq:portname2/proxy/: bar (200; 5.566318ms)
    Dec 30 03:29:10.875: INFO: (4) /api/v1/namespaces/proxy-5754/services/https:proxy-service-qqsfq:tlsportname1/proxy/: tls baz (200; 5.451642ms)
    Dec 30 03:29:10.876: INFO: (4) /api/v1/namespaces/proxy-5754/services/proxy-service-qqsfq:portname2/proxy/: bar (200; 6.212937ms)
    Dec 30 03:29:10.876: INFO: (4) /api/v1/namespaces/proxy-5754/services/proxy-service-qqsfq:portname1/proxy/: foo (200; 6.47234ms)
    Dec 30 03:29:10.877: INFO: (4) /api/v1/namespaces/proxy-5754/services/http:proxy-service-qqsfq:portname1/proxy/: foo (200; 6.982354ms)
    Dec 30 03:29:10.880: INFO: (5) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:160/proxy/: foo (200; 3.628682ms)
    Dec 30 03:29:10.880: INFO: (5) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b/proxy/rewriteme">test</a> (200; 3.681452ms)
    Dec 30 03:29:10.880: INFO: (5) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:162/proxy/: bar (200; 3.742936ms)
    Dec 30 03:29:10.881: INFO: (5) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:443/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:443/proxy/tlsrewritem... (200; 3.864931ms)
    Dec 30 03:29:10.881: INFO: (5) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:462/proxy/: tls qux (200; 4.021358ms)
    Dec 30 03:29:10.881: INFO: (5) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:160/proxy/: foo (200; 3.893261ms)
    Dec 30 03:29:10.882: INFO: (5) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:460/proxy/: tls baz (200; 4.858112ms)
    Dec 30 03:29:10.882: INFO: (5) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:162/proxy/: bar (200; 4.826518ms)
    Dec 30 03:29:10.882: INFO: (5) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:1080/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:1080/proxy/rewriteme">test<... (200; 4.853745ms)
    Dec 30 03:29:10.882: INFO: (5) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:1080/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:1080/proxy/rewriteme">... (200; 4.852909ms)
    Dec 30 03:29:10.883: INFO: (5) /api/v1/namespaces/proxy-5754/services/http:proxy-service-qqsfq:portname1/proxy/: foo (200; 5.790142ms)
    Dec 30 03:29:10.883: INFO: (5) /api/v1/namespaces/proxy-5754/services/proxy-service-qqsfq:portname1/proxy/: foo (200; 5.931226ms)
    Dec 30 03:29:10.883: INFO: (5) /api/v1/namespaces/proxy-5754/services/proxy-service-qqsfq:portname2/proxy/: bar (200; 5.851524ms)
    Dec 30 03:29:10.883: INFO: (5) /api/v1/namespaces/proxy-5754/services/https:proxy-service-qqsfq:tlsportname1/proxy/: tls baz (200; 6.615804ms)
    Dec 30 03:29:10.884: INFO: (5) /api/v1/namespaces/proxy-5754/services/http:proxy-service-qqsfq:portname2/proxy/: bar (200; 6.825459ms)
    Dec 30 03:29:10.884: INFO: (5) /api/v1/namespaces/proxy-5754/services/https:proxy-service-qqsfq:tlsportname2/proxy/: tls qux (200; 6.956252ms)
    Dec 30 03:29:10.887: INFO: (6) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b/proxy/rewriteme">test</a> (200; 3.377371ms)
    Dec 30 03:29:10.887: INFO: (6) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:162/proxy/: bar (200; 3.687168ms)
    Dec 30 03:29:10.888: INFO: (6) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:160/proxy/: foo (200; 3.691792ms)
    Dec 30 03:29:10.888: INFO: (6) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:160/proxy/: foo (200; 3.901983ms)
    Dec 30 03:29:10.888: INFO: (6) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:460/proxy/: tls baz (200; 4.046941ms)
    Dec 30 03:29:10.889: INFO: (6) /api/v1/namespaces/proxy-5754/services/https:proxy-service-qqsfq:tlsportname2/proxy/: tls qux (200; 5.024719ms)
    Dec 30 03:29:10.889: INFO: (6) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:1080/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:1080/proxy/rewriteme">... (200; 5.018397ms)
    Dec 30 03:29:10.889: INFO: (6) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:462/proxy/: tls qux (200; 5.109732ms)
    Dec 30 03:29:10.889: INFO: (6) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:1080/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:1080/proxy/rewriteme">test<... (200; 5.116256ms)
    Dec 30 03:29:10.889: INFO: (6) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:443/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:443/proxy/tlsrewritem... (200; 5.106933ms)
    Dec 30 03:29:10.889: INFO: (6) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:162/proxy/: bar (200; 5.144825ms)
    Dec 30 03:29:10.890: INFO: (6) /api/v1/namespaces/proxy-5754/services/http:proxy-service-qqsfq:portname2/proxy/: bar (200; 5.873517ms)
    Dec 30 03:29:10.890: INFO: (6) /api/v1/namespaces/proxy-5754/services/proxy-service-qqsfq:portname2/proxy/: bar (200; 5.969572ms)
    Dec 30 03:29:10.890: INFO: (6) /api/v1/namespaces/proxy-5754/services/http:proxy-service-qqsfq:portname1/proxy/: foo (200; 6.387144ms)
    Dec 30 03:29:10.890: INFO: (6) /api/v1/namespaces/proxy-5754/services/proxy-service-qqsfq:portname1/proxy/: foo (200; 6.341725ms)
    Dec 30 03:29:10.892: INFO: (6) /api/v1/namespaces/proxy-5754/services/https:proxy-service-qqsfq:tlsportname1/proxy/: tls baz (200; 7.554736ms)
    Dec 30 03:29:10.895: INFO: (7) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:1080/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:1080/proxy/rewriteme">... (200; 3.341407ms)
    Dec 30 03:29:10.895: INFO: (7) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b/proxy/rewriteme">test</a> (200; 3.270735ms)
    Dec 30 03:29:10.895: INFO: (7) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:462/proxy/: tls qux (200; 3.375822ms)
    Dec 30 03:29:10.896: INFO: (7) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:162/proxy/: bar (200; 3.792781ms)
    Dec 30 03:29:10.896: INFO: (7) /api/v1/namespaces/proxy-5754/services/proxy-service-qqsfq:portname1/proxy/: foo (200; 4.323909ms)
    Dec 30 03:29:10.896: INFO: (7) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:1080/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:1080/proxy/rewriteme">test<... (200; 4.668835ms)
    Dec 30 03:29:10.896: INFO: (7) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:162/proxy/: bar (200; 4.769821ms)
    Dec 30 03:29:10.896: INFO: (7) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:443/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:443/proxy/tlsrewritem... (200; 4.737434ms)
    Dec 30 03:29:10.897: INFO: (7) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:160/proxy/: foo (200; 4.869148ms)
    Dec 30 03:29:10.897: INFO: (7) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:160/proxy/: foo (200; 5.07446ms)
    Dec 30 03:29:10.897: INFO: (7) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:460/proxy/: tls baz (200; 5.060097ms)
    Dec 30 03:29:10.897: INFO: (7) /api/v1/namespaces/proxy-5754/services/proxy-service-qqsfq:portname2/proxy/: bar (200; 5.480052ms)
    Dec 30 03:29:10.897: INFO: (7) /api/v1/namespaces/proxy-5754/services/http:proxy-service-qqsfq:portname1/proxy/: foo (200; 5.595632ms)
    Dec 30 03:29:10.898: INFO: (7) /api/v1/namespaces/proxy-5754/services/https:proxy-service-qqsfq:tlsportname2/proxy/: tls qux (200; 5.878788ms)
    Dec 30 03:29:10.899: INFO: (7) /api/v1/namespaces/proxy-5754/services/http:proxy-service-qqsfq:portname2/proxy/: bar (200; 6.824682ms)
    Dec 30 03:29:10.899: INFO: (7) /api/v1/namespaces/proxy-5754/services/https:proxy-service-qqsfq:tlsportname1/proxy/: tls baz (200; 6.86721ms)
    Dec 30 03:29:10.902: INFO: (8) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:160/proxy/: foo (200; 3.036732ms)
    Dec 30 03:29:10.902: INFO: (8) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:160/proxy/: foo (200; 3.157912ms)
    Dec 30 03:29:10.903: INFO: (8) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:1080/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:1080/proxy/rewriteme">test<... (200; 3.981044ms)
    Dec 30 03:29:10.903: INFO: (8) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:162/proxy/: bar (200; 3.931466ms)
    Dec 30 03:29:10.903: INFO: (8) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:443/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:443/proxy/tlsrewritem... (200; 4.089635ms)
    Dec 30 03:29:10.903: INFO: (8) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:462/proxy/: tls qux (200; 4.147674ms)
    Dec 30 03:29:10.904: INFO: (8) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:460/proxy/: tls baz (200; 5.062883ms)
    Dec 30 03:29:10.904: INFO: (8) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:1080/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:1080/proxy/rewriteme">... (200; 5.081161ms)
    Dec 30 03:29:10.904: INFO: (8) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b/proxy/rewriteme">test</a> (200; 5.14007ms)
    Dec 30 03:29:10.904: INFO: (8) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:162/proxy/: bar (200; 5.138389ms)
    Dec 30 03:29:10.905: INFO: (8) /api/v1/namespaces/proxy-5754/services/http:proxy-service-qqsfq:portname2/proxy/: bar (200; 6.029023ms)
    Dec 30 03:29:10.905: INFO: (8) /api/v1/namespaces/proxy-5754/services/http:proxy-service-qqsfq:portname1/proxy/: foo (200; 5.987142ms)
    Dec 30 03:29:10.905: INFO: (8) /api/v1/namespaces/proxy-5754/services/proxy-service-qqsfq:portname1/proxy/: foo (200; 6.262237ms)
    Dec 30 03:29:10.905: INFO: (8) /api/v1/namespaces/proxy-5754/services/https:proxy-service-qqsfq:tlsportname1/proxy/: tls baz (200; 6.704959ms)
    Dec 30 03:29:10.906: INFO: (8) /api/v1/namespaces/proxy-5754/services/proxy-service-qqsfq:portname2/proxy/: bar (200; 6.830678ms)
    Dec 30 03:29:10.906: INFO: (8) /api/v1/namespaces/proxy-5754/services/https:proxy-service-qqsfq:tlsportname2/proxy/: tls qux (200; 7.22943ms)
    Dec 30 03:29:10.913: INFO: (9) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:462/proxy/: tls qux (200; 7.084891ms)
    Dec 30 03:29:10.913: INFO: (9) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:443/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:443/proxy/tlsrewritem... (200; 7.112576ms)
    Dec 30 03:29:10.914: INFO: (9) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:160/proxy/: foo (200; 7.779048ms)
    Dec 30 03:29:10.914: INFO: (9) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:162/proxy/: bar (200; 7.822461ms)
    Dec 30 03:29:10.914: INFO: (9) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:1080/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:1080/proxy/rewriteme">test<... (200; 8.403278ms)
    Dec 30 03:29:10.914: INFO: (9) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:160/proxy/: foo (200; 8.386974ms)
    Dec 30 03:29:10.915: INFO: (9) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:460/proxy/: tls baz (200; 8.462799ms)
    Dec 30 03:29:10.915: INFO: (9) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:162/proxy/: bar (200; 8.690168ms)
    Dec 30 03:29:10.915: INFO: (9) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:1080/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:1080/proxy/rewriteme">... (200; 8.572618ms)
    Dec 30 03:29:10.915: INFO: (9) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b/proxy/rewriteme">test</a> (200; 8.694318ms)
    Dec 30 03:29:10.915: INFO: (9) /api/v1/namespaces/proxy-5754/services/http:proxy-service-qqsfq:portname2/proxy/: bar (200; 9.08385ms)
    Dec 30 03:29:10.916: INFO: (9) /api/v1/namespaces/proxy-5754/services/http:proxy-service-qqsfq:portname1/proxy/: foo (200; 9.907095ms)
    Dec 30 03:29:10.916: INFO: (9) /api/v1/namespaces/proxy-5754/services/proxy-service-qqsfq:portname2/proxy/: bar (200; 9.939601ms)
    Dec 30 03:29:10.916: INFO: (9) /api/v1/namespaces/proxy-5754/services/https:proxy-service-qqsfq:tlsportname1/proxy/: tls baz (200; 10.06666ms)
    Dec 30 03:29:10.917: INFO: (9) /api/v1/namespaces/proxy-5754/services/proxy-service-qqsfq:portname1/proxy/: foo (200; 10.712335ms)
    Dec 30 03:29:10.917: INFO: (9) /api/v1/namespaces/proxy-5754/services/https:proxy-service-qqsfq:tlsportname2/proxy/: tls qux (200; 11.325902ms)
    Dec 30 03:29:10.921: INFO: (10) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:1080/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:1080/proxy/rewriteme">test<... (200; 3.06328ms)
    Dec 30 03:29:10.921: INFO: (10) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:162/proxy/: bar (200; 3.126337ms)
    Dec 30 03:29:10.921: INFO: (10) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:160/proxy/: foo (200; 3.30895ms)
    Dec 30 03:29:10.921: INFO: (10) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b/proxy/rewriteme">test</a> (200; 3.245747ms)
    Dec 30 03:29:10.921: INFO: (10) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:1080/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:1080/proxy/rewriteme">... (200; 3.529603ms)
    Dec 30 03:29:10.921: INFO: (10) /api/v1/namespaces/proxy-5754/services/proxy-service-qqsfq:portname2/proxy/: bar (200; 3.739701ms)
    Dec 30 03:29:10.922: INFO: (10) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:162/proxy/: bar (200; 4.234445ms)
    Dec 30 03:29:10.922: INFO: (10) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:460/proxy/: tls baz (200; 4.434471ms)
    Dec 30 03:29:10.922: INFO: (10) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:462/proxy/: tls qux (200; 4.469919ms)
    Dec 30 03:29:10.922: INFO: (10) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:443/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:443/proxy/tlsrewritem... (200; 4.518192ms)
    Dec 30 03:29:10.922: INFO: (10) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:160/proxy/: foo (200; 4.449385ms)
    Dec 30 03:29:10.923: INFO: (10) /api/v1/namespaces/proxy-5754/services/http:proxy-service-qqsfq:portname1/proxy/: foo (200; 5.061777ms)
    Dec 30 03:29:10.923: INFO: (10) /api/v1/namespaces/proxy-5754/services/https:proxy-service-qqsfq:tlsportname1/proxy/: tls baz (200; 5.296887ms)
    Dec 30 03:29:10.923: INFO: (10) /api/v1/namespaces/proxy-5754/services/proxy-service-qqsfq:portname1/proxy/: foo (200; 5.404522ms)
    Dec 30 03:29:10.924: INFO: (10) /api/v1/namespaces/proxy-5754/services/https:proxy-service-qqsfq:tlsportname2/proxy/: tls qux (200; 6.075733ms)
    Dec 30 03:29:10.924: INFO: (10) /api/v1/namespaces/proxy-5754/services/http:proxy-service-qqsfq:portname2/proxy/: bar (200; 6.134032ms)
    Dec 30 03:29:10.926: INFO: (11) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:1080/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:1080/proxy/rewriteme">... (200; 2.528327ms)
    Dec 30 03:29:10.927: INFO: (11) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:162/proxy/: bar (200; 2.850331ms)
    Dec 30 03:29:10.927: INFO: (11) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:443/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:443/proxy/tlsrewritem... (200; 3.102379ms)
    Dec 30 03:29:10.927: INFO: (11) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:162/proxy/: bar (200; 3.077988ms)
    Dec 30 03:29:10.927: INFO: (11) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:160/proxy/: foo (200; 3.024302ms)
    Dec 30 03:29:10.927: INFO: (11) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:1080/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:1080/proxy/rewriteme">test<... (200; 3.514961ms)
    Dec 30 03:29:10.928: INFO: (11) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:160/proxy/: foo (200; 3.65878ms)
    Dec 30 03:29:10.928: INFO: (11) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b/proxy/rewriteme">test</a> (200; 3.755514ms)
    Dec 30 03:29:10.928: INFO: (11) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:462/proxy/: tls qux (200; 4.449409ms)
    Dec 30 03:29:10.928: INFO: (11) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:460/proxy/: tls baz (200; 4.51714ms)
    Dec 30 03:29:10.929: INFO: (11) /api/v1/namespaces/proxy-5754/services/proxy-service-qqsfq:portname1/proxy/: foo (200; 4.796838ms)
    Dec 30 03:29:10.929: INFO: (11) /api/v1/namespaces/proxy-5754/services/https:proxy-service-qqsfq:tlsportname2/proxy/: tls qux (200; 5.038625ms)
    Dec 30 03:29:10.929: INFO: (11) /api/v1/namespaces/proxy-5754/services/proxy-service-qqsfq:portname2/proxy/: bar (200; 5.491478ms)
    Dec 30 03:29:10.930: INFO: (11) /api/v1/namespaces/proxy-5754/services/http:proxy-service-qqsfq:portname1/proxy/: foo (200; 5.861543ms)
    Dec 30 03:29:10.930: INFO: (11) /api/v1/namespaces/proxy-5754/services/https:proxy-service-qqsfq:tlsportname1/proxy/: tls baz (200; 5.816492ms)
    Dec 30 03:29:10.930: INFO: (11) /api/v1/namespaces/proxy-5754/services/http:proxy-service-qqsfq:portname2/proxy/: bar (200; 6.135517ms)
    Dec 30 03:29:10.933: INFO: (12) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:160/proxy/: foo (200; 2.922285ms)
    Dec 30 03:29:10.933: INFO: (12) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:443/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:443/proxy/tlsrewritem... (200; 3.034241ms)
    Dec 30 03:29:10.933: INFO: (12) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:462/proxy/: tls qux (200; 3.11514ms)
    Dec 30 03:29:10.933: INFO: (12) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:1080/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:1080/proxy/rewriteme">... (200; 3.287976ms)
    Dec 30 03:29:10.934: INFO: (12) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:162/proxy/: bar (200; 3.317266ms)
    Dec 30 03:29:10.934: INFO: (12) /api/v1/namespaces/proxy-5754/services/http:proxy-service-qqsfq:portname1/proxy/: foo (200; 4.099522ms)
    Dec 30 03:29:10.934: INFO: (12) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:160/proxy/: foo (200; 4.292775ms)
    Dec 30 03:29:10.934: INFO: (12) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:460/proxy/: tls baz (200; 4.33383ms)
    Dec 30 03:29:10.934: INFO: (12) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b/proxy/rewriteme">test</a> (200; 4.256841ms)
    Dec 30 03:29:10.934: INFO: (12) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:162/proxy/: bar (200; 4.349325ms)
    Dec 30 03:29:10.935: INFO: (12) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:1080/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:1080/proxy/rewriteme">test<... (200; 4.311588ms)
    Dec 30 03:29:10.935: INFO: (12) /api/v1/namespaces/proxy-5754/services/http:proxy-service-qqsfq:portname2/proxy/: bar (200; 5.098834ms)
    Dec 30 03:29:10.936: INFO: (12) /api/v1/namespaces/proxy-5754/services/https:proxy-service-qqsfq:tlsportname2/proxy/: tls qux (200; 5.631178ms)
    Dec 30 03:29:10.936: INFO: (12) /api/v1/namespaces/proxy-5754/services/proxy-service-qqsfq:portname1/proxy/: foo (200; 5.759185ms)
    Dec 30 03:29:10.936: INFO: (12) /api/v1/namespaces/proxy-5754/services/https:proxy-service-qqsfq:tlsportname1/proxy/: tls baz (200; 5.818907ms)
    Dec 30 03:29:10.937: INFO: (12) /api/v1/namespaces/proxy-5754/services/proxy-service-qqsfq:portname2/proxy/: bar (200; 6.682687ms)
    Dec 30 03:29:10.940: INFO: (13) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:160/proxy/: foo (200; 2.859652ms)
    Dec 30 03:29:10.940: INFO: (13) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:162/proxy/: bar (200; 3.143485ms)
    Dec 30 03:29:10.940: INFO: (13) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:460/proxy/: tls baz (200; 3.556665ms)
    Dec 30 03:29:10.940: INFO: (13) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b/proxy/rewriteme">test</a> (200; 3.430585ms)
    Dec 30 03:29:10.942: INFO: (13) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:160/proxy/: foo (200; 4.500309ms)
    Dec 30 03:29:10.942: INFO: (13) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:1080/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:1080/proxy/rewriteme">... (200; 4.732094ms)
    Dec 30 03:29:10.942: INFO: (13) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:1080/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:1080/proxy/rewriteme">test<... (200; 4.751251ms)
    Dec 30 03:29:10.942: INFO: (13) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:162/proxy/: bar (200; 4.94072ms)
    Dec 30 03:29:10.942: INFO: (13) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:462/proxy/: tls qux (200; 4.971864ms)
    Dec 30 03:29:10.942: INFO: (13) /api/v1/namespaces/proxy-5754/services/https:proxy-service-qqsfq:tlsportname1/proxy/: tls baz (200; 4.91691ms)
    Dec 30 03:29:10.942: INFO: (13) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:443/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:443/proxy/tlsrewritem... (200; 4.908419ms)
    Dec 30 03:29:10.943: INFO: (13) /api/v1/namespaces/proxy-5754/services/proxy-service-qqsfq:portname1/proxy/: foo (200; 5.600767ms)
    Dec 30 03:29:10.943: INFO: (13) /api/v1/namespaces/proxy-5754/services/http:proxy-service-qqsfq:portname2/proxy/: bar (200; 5.627772ms)
    Dec 30 03:29:10.943: INFO: (13) /api/v1/namespaces/proxy-5754/services/https:proxy-service-qqsfq:tlsportname2/proxy/: tls qux (200; 5.660486ms)
    Dec 30 03:29:10.943: INFO: (13) /api/v1/namespaces/proxy-5754/services/http:proxy-service-qqsfq:portname1/proxy/: foo (200; 6.341924ms)
    Dec 30 03:29:10.944: INFO: (13) /api/v1/namespaces/proxy-5754/services/proxy-service-qqsfq:portname2/proxy/: bar (200; 6.616865ms)
    Dec 30 03:29:10.947: INFO: (14) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b/proxy/rewriteme">test</a> (200; 3.237384ms)
    Dec 30 03:29:10.947: INFO: (14) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:1080/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:1080/proxy/rewriteme">test<... (200; 3.339205ms)
    Dec 30 03:29:10.947: INFO: (14) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:160/proxy/: foo (200; 3.634674ms)
    Dec 30 03:29:10.947: INFO: (14) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:160/proxy/: foo (200; 3.590364ms)
    Dec 30 03:29:10.948: INFO: (14) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:162/proxy/: bar (200; 3.822485ms)
    Dec 30 03:29:10.948: INFO: (14) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:1080/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:1080/proxy/rewriteme">... (200; 3.939058ms)
    Dec 30 03:29:10.948: INFO: (14) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:460/proxy/: tls baz (200; 4.652519ms)
    Dec 30 03:29:10.949: INFO: (14) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:443/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:443/proxy/tlsrewritem... (200; 4.917815ms)
    Dec 30 03:29:10.949: INFO: (14) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:462/proxy/: tls qux (200; 4.947245ms)
    Dec 30 03:29:10.949: INFO: (14) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:162/proxy/: bar (200; 5.036634ms)
    Dec 30 03:29:10.949: INFO: (14) /api/v1/namespaces/proxy-5754/services/https:proxy-service-qqsfq:tlsportname1/proxy/: tls baz (200; 5.150775ms)
    Dec 30 03:29:10.949: INFO: (14) /api/v1/namespaces/proxy-5754/services/https:proxy-service-qqsfq:tlsportname2/proxy/: tls qux (200; 5.745158ms)
    Dec 30 03:29:10.950: INFO: (14) /api/v1/namespaces/proxy-5754/services/proxy-service-qqsfq:portname2/proxy/: bar (200; 5.876828ms)
    Dec 30 03:29:10.950: INFO: (14) /api/v1/namespaces/proxy-5754/services/http:proxy-service-qqsfq:portname1/proxy/: foo (200; 6.099394ms)
    Dec 30 03:29:10.950: INFO: (14) /api/v1/namespaces/proxy-5754/services/proxy-service-qqsfq:portname1/proxy/: foo (200; 6.282467ms)
    Dec 30 03:29:10.950: INFO: (14) /api/v1/namespaces/proxy-5754/services/http:proxy-service-qqsfq:portname2/proxy/: bar (200; 6.352915ms)
    Dec 30 03:29:10.953: INFO: (15) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:162/proxy/: bar (200; 3.018325ms)
    Dec 30 03:29:10.953: INFO: (15) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:1080/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:1080/proxy/rewriteme">... (200; 3.300717ms)
    Dec 30 03:29:10.954: INFO: (15) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b/proxy/rewriteme">test</a> (200; 3.36817ms)
    Dec 30 03:29:10.954: INFO: (15) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:462/proxy/: tls qux (200; 3.718914ms)
    Dec 30 03:29:10.954: INFO: (15) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:443/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:443/proxy/tlsrewritem... (200; 3.924642ms)
    Dec 30 03:29:10.954: INFO: (15) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:460/proxy/: tls baz (200; 3.994876ms)
    Dec 30 03:29:10.954: INFO: (15) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:1080/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:1080/proxy/rewriteme">test<... (200; 4.013032ms)
    Dec 30 03:29:10.955: INFO: (15) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:160/proxy/: foo (200; 4.328195ms)
    Dec 30 03:29:10.955: INFO: (15) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:160/proxy/: foo (200; 4.538014ms)
    Dec 30 03:29:10.955: INFO: (15) /api/v1/namespaces/proxy-5754/services/http:proxy-service-qqsfq:portname2/proxy/: bar (200; 4.512057ms)
    Dec 30 03:29:10.955: INFO: (15) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:162/proxy/: bar (200; 4.787205ms)
    Dec 30 03:29:10.955: INFO: (15) /api/v1/namespaces/proxy-5754/services/proxy-service-qqsfq:portname1/proxy/: foo (200; 5.331518ms)
    Dec 30 03:29:10.956: INFO: (15) /api/v1/namespaces/proxy-5754/services/proxy-service-qqsfq:portname2/proxy/: bar (200; 5.598689ms)
    Dec 30 03:29:10.956: INFO: (15) /api/v1/namespaces/proxy-5754/services/https:proxy-service-qqsfq:tlsportname2/proxy/: tls qux (200; 6.038218ms)
    Dec 30 03:29:10.957: INFO: (15) /api/v1/namespaces/proxy-5754/services/https:proxy-service-qqsfq:tlsportname1/proxy/: tls baz (200; 6.465271ms)
    Dec 30 03:29:10.957: INFO: (15) /api/v1/namespaces/proxy-5754/services/http:proxy-service-qqsfq:portname1/proxy/: foo (200; 6.518995ms)
    Dec 30 03:29:10.960: INFO: (16) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:1080/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:1080/proxy/rewriteme">... (200; 3.067359ms)
    Dec 30 03:29:10.960: INFO: (16) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:160/proxy/: foo (200; 3.147691ms)
    Dec 30 03:29:10.961: INFO: (16) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:1080/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:1080/proxy/rewriteme">test<... (200; 3.613072ms)
    Dec 30 03:29:10.961: INFO: (16) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:443/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:443/proxy/tlsrewritem... (200; 3.876383ms)
    Dec 30 03:29:10.961: INFO: (16) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:462/proxy/: tls qux (200; 3.826022ms)
    Dec 30 03:29:10.961: INFO: (16) /api/v1/namespaces/proxy-5754/services/proxy-service-qqsfq:portname1/proxy/: foo (200; 3.929748ms)
    Dec 30 03:29:10.961: INFO: (16) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:460/proxy/: tls baz (200; 4.205951ms)
    Dec 30 03:29:10.961: INFO: (16) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:162/proxy/: bar (200; 4.343605ms)
    Dec 30 03:29:10.961: INFO: (16) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:160/proxy/: foo (200; 4.336869ms)
    Dec 30 03:29:10.962: INFO: (16) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b/proxy/rewriteme">test</a> (200; 4.669462ms)
    Dec 30 03:29:10.962: INFO: (16) /api/v1/namespaces/proxy-5754/services/http:proxy-service-qqsfq:portname2/proxy/: bar (200; 4.705141ms)
    Dec 30 03:29:10.962: INFO: (16) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:162/proxy/: bar (200; 4.651228ms)
    Dec 30 03:29:10.962: INFO: (16) /api/v1/namespaces/proxy-5754/services/https:proxy-service-qqsfq:tlsportname2/proxy/: tls qux (200; 5.300715ms)
    Dec 30 03:29:10.962: INFO: (16) /api/v1/namespaces/proxy-5754/services/http:proxy-service-qqsfq:portname1/proxy/: foo (200; 5.479495ms)
    Dec 30 03:29:10.963: INFO: (16) /api/v1/namespaces/proxy-5754/services/proxy-service-qqsfq:portname2/proxy/: bar (200; 6.063399ms)
    Dec 30 03:29:10.963: INFO: (16) /api/v1/namespaces/proxy-5754/services/https:proxy-service-qqsfq:tlsportname1/proxy/: tls baz (200; 6.207415ms)
    Dec 30 03:29:10.967: INFO: (17) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:162/proxy/: bar (200; 3.326433ms)
    Dec 30 03:29:10.967: INFO: (17) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:160/proxy/: foo (200; 3.572337ms)
    Dec 30 03:29:10.967: INFO: (17) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:462/proxy/: tls qux (200; 4.003604ms)
    Dec 30 03:29:10.967: INFO: (17) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:160/proxy/: foo (200; 4.020677ms)
    Dec 30 03:29:10.967: INFO: (17) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:162/proxy/: bar (200; 4.068902ms)
    Dec 30 03:29:10.968: INFO: (17) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b/proxy/rewriteme">test</a> (200; 4.079092ms)
    Dec 30 03:29:10.968: INFO: (17) /api/v1/namespaces/proxy-5754/services/proxy-service-qqsfq:portname2/proxy/: bar (200; 4.732423ms)
    Dec 30 03:29:10.969: INFO: (17) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:1080/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:1080/proxy/rewriteme">... (200; 5.167708ms)
    Dec 30 03:29:10.969: INFO: (17) /api/v1/namespaces/proxy-5754/services/https:proxy-service-qqsfq:tlsportname1/proxy/: tls baz (200; 5.260468ms)
    Dec 30 03:29:10.969: INFO: (17) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:1080/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:1080/proxy/rewriteme">test<... (200; 5.4015ms)
    Dec 30 03:29:10.969: INFO: (17) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:443/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:443/proxy/tlsrewritem... (200; 5.368031ms)
    Dec 30 03:29:10.969: INFO: (17) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:460/proxy/: tls baz (200; 5.403777ms)
    Dec 30 03:29:10.969: INFO: (17) /api/v1/namespaces/proxy-5754/services/http:proxy-service-qqsfq:portname1/proxy/: foo (200; 6.05112ms)
    Dec 30 03:29:10.969: INFO: (17) /api/v1/namespaces/proxy-5754/services/https:proxy-service-qqsfq:tlsportname2/proxy/: tls qux (200; 6.009893ms)
    Dec 30 03:29:10.970: INFO: (17) /api/v1/namespaces/proxy-5754/services/http:proxy-service-qqsfq:portname2/proxy/: bar (200; 6.400173ms)
    Dec 30 03:29:10.970: INFO: (17) /api/v1/namespaces/proxy-5754/services/proxy-service-qqsfq:portname1/proxy/: foo (200; 6.888395ms)
    Dec 30 03:29:10.974: INFO: (18) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:460/proxy/: tls baz (200; 3.148727ms)
    Dec 30 03:29:10.974: INFO: (18) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:162/proxy/: bar (200; 3.283223ms)
    Dec 30 03:29:10.974: INFO: (18) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b/proxy/rewriteme">test</a> (200; 3.490803ms)
    Dec 30 03:29:10.974: INFO: (18) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:443/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:443/proxy/tlsrewritem... (200; 3.516026ms)
    Dec 30 03:29:10.974: INFO: (18) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:162/proxy/: bar (200; 3.645072ms)
    Dec 30 03:29:10.975: INFO: (18) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:160/proxy/: foo (200; 4.419406ms)
    Dec 30 03:29:10.975: INFO: (18) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:462/proxy/: tls qux (200; 4.455825ms)
    Dec 30 03:29:10.975: INFO: (18) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:160/proxy/: foo (200; 4.345092ms)
    Dec 30 03:29:10.975: INFO: (18) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:1080/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:1080/proxy/rewriteme">... (200; 4.585743ms)
    Dec 30 03:29:10.975: INFO: (18) /api/v1/namespaces/proxy-5754/services/http:proxy-service-qqsfq:portname1/proxy/: foo (200; 4.692025ms)
    Dec 30 03:29:10.975: INFO: (18) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:1080/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:1080/proxy/rewriteme">test<... (200; 4.635164ms)
    Dec 30 03:29:10.976: INFO: (18) /api/v1/namespaces/proxy-5754/services/https:proxy-service-qqsfq:tlsportname2/proxy/: tls qux (200; 5.380943ms)
    Dec 30 03:29:10.977: INFO: (18) /api/v1/namespaces/proxy-5754/services/proxy-service-qqsfq:portname2/proxy/: bar (200; 5.922782ms)
    Dec 30 03:29:10.977: INFO: (18) /api/v1/namespaces/proxy-5754/services/proxy-service-qqsfq:portname1/proxy/: foo (200; 6.134443ms)
    Dec 30 03:29:10.977: INFO: (18) /api/v1/namespaces/proxy-5754/services/http:proxy-service-qqsfq:portname2/proxy/: bar (200; 6.463619ms)
    Dec 30 03:29:10.978: INFO: (18) /api/v1/namespaces/proxy-5754/services/https:proxy-service-qqsfq:tlsportname1/proxy/: tls baz (200; 7.043447ms)
    Dec 30 03:29:10.981: INFO: (19) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:443/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:443/proxy/tlsrewritem... (200; 3.052325ms)
    Dec 30 03:29:10.981: INFO: (19) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:162/proxy/: bar (200; 3.183653ms)
    Dec 30 03:29:10.981: INFO: (19) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b/proxy/rewriteme">test</a> (200; 3.57858ms)
    Dec 30 03:29:10.982: INFO: (19) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:460/proxy/: tls baz (200; 3.816272ms)
    Dec 30 03:29:10.982: INFO: (19) /api/v1/namespaces/proxy-5754/pods/https:proxy-service-qqsfq-6qc6b:462/proxy/: tls qux (200; 3.73968ms)
    Dec 30 03:29:10.982: INFO: (19) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:160/proxy/: foo (200; 4.158234ms)
    Dec 30 03:29:10.982: INFO: (19) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:1080/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:1080/proxy/rewriteme">... (200; 4.175737ms)
    Dec 30 03:29:10.982: INFO: (19) /api/v1/namespaces/proxy-5754/pods/http:proxy-service-qqsfq-6qc6b:160/proxy/: foo (200; 4.246267ms)
    Dec 30 03:29:10.982: INFO: (19) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:1080/proxy/: <a href="/api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:1080/proxy/rewriteme">test<... (200; 4.404864ms)
    Dec 30 03:29:10.983: INFO: (19) /api/v1/namespaces/proxy-5754/services/https:proxy-service-qqsfq:tlsportname2/proxy/: tls qux (200; 4.900641ms)
    Dec 30 03:29:10.983: INFO: (19) /api/v1/namespaces/proxy-5754/pods/proxy-service-qqsfq-6qc6b:162/proxy/: bar (200; 5.031081ms)
    Dec 30 03:29:10.983: INFO: (19) /api/v1/namespaces/proxy-5754/services/http:proxy-service-qqsfq:portname2/proxy/: bar (200; 5.10498ms)
    Dec 30 03:29:10.983: INFO: (19) /api/v1/namespaces/proxy-5754/services/proxy-service-qqsfq:portname1/proxy/: foo (200; 5.431551ms)
    Dec 30 03:29:10.984: INFO: (19) /api/v1/namespaces/proxy-5754/services/proxy-service-qqsfq:portname2/proxy/: bar (200; 5.941271ms)
    Dec 30 03:29:10.984: INFO: (19) /api/v1/namespaces/proxy-5754/services/https:proxy-service-qqsfq:tlsportname1/proxy/: tls baz (200; 6.395103ms)
    Dec 30 03:29:10.985: INFO: (19) /api/v1/namespaces/proxy-5754/services/http:proxy-service-qqsfq:portname1/proxy/: foo (200; 6.773327ms)
    STEP: deleting ReplicationController proxy-service-qqsfq in namespace proxy-5754, will wait for the garbage collector to delete the pods 12/30/22 03:29:10.985
    Dec 30 03:29:11.046: INFO: Deleting ReplicationController proxy-service-qqsfq took: 6.919289ms
    Dec 30 03:29:11.147: INFO: Terminating ReplicationController proxy-service-qqsfq pods took: 101.116495ms
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Dec 30 03:29:13.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-5754" for this suite. 12/30/22 03:29:13.454
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:695
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:29:13.463
Dec 30 03:29:13.463: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename statefulset 12/30/22 03:29:13.464
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:29:13.478
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:29:13.481
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-2000 12/30/22 03:29:13.484
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:695
STEP: Creating stateful set ss in namespace statefulset-2000 12/30/22 03:29:13.489
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2000 12/30/22 03:29:13.495
Dec 30 03:29:13.499: INFO: Found 0 stateful pods, waiting for 1
Dec 30 03:29:23.505: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 12/30/22 03:29:23.505
Dec 30 03:29:23.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=statefulset-2000 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 30 03:29:23.702: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 30 03:29:23.702: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 30 03:29:23.702: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 30 03:29:23.706: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Dec 30 03:29:33.713: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 30 03:29:33.713: INFO: Waiting for statefulset status.replicas updated to 0
Dec 30 03:29:33.731: INFO: POD   NODE        PHASE    GRACE  CONDITIONS
Dec 30 03:29:33.731: INFO: ss-0  k8s-mgmt02  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-30 03:29:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-30 03:29:23 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-30 03:29:23 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-30 03:29:13 +0000 UTC  }]
Dec 30 03:29:33.731: INFO: 
Dec 30 03:29:33.731: INFO: StatefulSet ss has not reached scale 3, at 1
Dec 30 03:29:34.738: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995562661s
Dec 30 03:29:35.744: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.989254236s
Dec 30 03:29:36.750: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.982757792s
Dec 30 03:29:37.755: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.977470347s
Dec 30 03:29:38.761: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.971756808s
Dec 30 03:29:39.766: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.965968143s
Dec 30 03:29:40.772: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.960657565s
Dec 30 03:29:41.778: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.954846376s
Dec 30 03:29:42.783: INFO: Verifying statefulset ss doesn't scale past 3 for another 949.229457ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2000 12/30/22 03:29:43.783
Dec 30 03:29:43.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=statefulset-2000 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 30 03:29:43.991: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 30 03:29:43.991: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 30 03:29:43.991: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 30 03:29:43.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=statefulset-2000 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 30 03:29:44.182: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Dec 30 03:29:44.182: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 30 03:29:44.182: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 30 03:29:44.182: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=statefulset-2000 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 30 03:29:44.386: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Dec 30 03:29:44.386: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 30 03:29:44.386: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 30 03:29:44.391: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Dec 30 03:29:54.401: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 30 03:29:54.401: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 30 03:29:54.401: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 12/30/22 03:29:54.401
Dec 30 03:29:54.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=statefulset-2000 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 30 03:29:54.633: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 30 03:29:54.633: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 30 03:29:54.633: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 30 03:29:54.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=statefulset-2000 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 30 03:29:54.837: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 30 03:29:54.837: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 30 03:29:54.837: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 30 03:29:54.837: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=statefulset-2000 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 30 03:29:55.055: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 30 03:29:55.055: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 30 03:29:55.055: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 30 03:29:55.055: INFO: Waiting for statefulset status.replicas updated to 0
Dec 30 03:29:55.060: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Dec 30 03:30:05.074: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 30 03:30:05.074: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Dec 30 03:30:05.074: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Dec 30 03:30:05.090: INFO: POD   NODE        PHASE    GRACE  CONDITIONS
Dec 30 03:30:05.090: INFO: ss-0  k8s-mgmt02  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-30 03:29:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-30 03:29:54 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-30 03:29:54 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-30 03:29:13 +0000 UTC  }]
Dec 30 03:30:05.090: INFO: ss-1  k8s-mgmt01  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-30 03:29:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-30 03:29:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-30 03:29:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-30 03:29:33 +0000 UTC  }]
Dec 30 03:30:05.090: INFO: ss-2  k8s-mgmt03  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-30 03:29:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-30 03:29:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-30 03:29:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-30 03:29:33 +0000 UTC  }]
Dec 30 03:30:05.090: INFO: 
Dec 30 03:30:05.090: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 30 03:30:06.095: INFO: POD   NODE        PHASE    GRACE  CONDITIONS
Dec 30 03:30:06.095: INFO: ss-0  k8s-mgmt02  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-30 03:29:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-30 03:29:54 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-30 03:29:54 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-30 03:29:13 +0000 UTC  }]
Dec 30 03:30:06.095: INFO: ss-1  k8s-mgmt01  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-30 03:29:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-30 03:29:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-30 03:29:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-30 03:29:33 +0000 UTC  }]
Dec 30 03:30:06.095: INFO: ss-2  k8s-mgmt03  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-30 03:29:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-30 03:29:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-30 03:29:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-30 03:29:33 +0000 UTC  }]
Dec 30 03:30:06.095: INFO: 
Dec 30 03:30:06.095: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 30 03:30:07.100: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.989079091s
Dec 30 03:30:08.105: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.984448318s
Dec 30 03:30:09.109: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.979825451s
Dec 30 03:30:10.114: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.974741634s
Dec 30 03:30:11.119: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.969910061s
Dec 30 03:30:12.124: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.965158254s
Dec 30 03:30:13.129: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.960453796s
Dec 30 03:30:14.134: INFO: Verifying statefulset ss doesn't scale past 0 for another 954.684916ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2000 12/30/22 03:30:15.134
Dec 30 03:30:15.139: INFO: Scaling statefulset ss to 0
Dec 30 03:30:15.153: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Dec 30 03:30:15.158: INFO: Deleting all statefulset in ns statefulset-2000
Dec 30 03:30:15.163: INFO: Scaling statefulset ss to 0
Dec 30 03:30:15.175: INFO: Waiting for statefulset status.replicas updated to 0
Dec 30 03:30:15.178: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Dec 30 03:30:15.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2000" for this suite. 12/30/22 03:30:15.196
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","completed":24,"skipped":546,"failed":0}
------------------------------
• [SLOW TEST] [61.739 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:695

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:29:13.463
    Dec 30 03:29:13.463: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename statefulset 12/30/22 03:29:13.464
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:29:13.478
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:29:13.481
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-2000 12/30/22 03:29:13.484
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:695
    STEP: Creating stateful set ss in namespace statefulset-2000 12/30/22 03:29:13.489
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2000 12/30/22 03:29:13.495
    Dec 30 03:29:13.499: INFO: Found 0 stateful pods, waiting for 1
    Dec 30 03:29:23.505: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 12/30/22 03:29:23.505
    Dec 30 03:29:23.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=statefulset-2000 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Dec 30 03:29:23.702: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Dec 30 03:29:23.702: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Dec 30 03:29:23.702: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Dec 30 03:29:23.706: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Dec 30 03:29:33.713: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Dec 30 03:29:33.713: INFO: Waiting for statefulset status.replicas updated to 0
    Dec 30 03:29:33.731: INFO: POD   NODE        PHASE    GRACE  CONDITIONS
    Dec 30 03:29:33.731: INFO: ss-0  k8s-mgmt02  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-30 03:29:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-30 03:29:23 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-30 03:29:23 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-30 03:29:13 +0000 UTC  }]
    Dec 30 03:29:33.731: INFO: 
    Dec 30 03:29:33.731: INFO: StatefulSet ss has not reached scale 3, at 1
    Dec 30 03:29:34.738: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995562661s
    Dec 30 03:29:35.744: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.989254236s
    Dec 30 03:29:36.750: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.982757792s
    Dec 30 03:29:37.755: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.977470347s
    Dec 30 03:29:38.761: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.971756808s
    Dec 30 03:29:39.766: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.965968143s
    Dec 30 03:29:40.772: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.960657565s
    Dec 30 03:29:41.778: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.954846376s
    Dec 30 03:29:42.783: INFO: Verifying statefulset ss doesn't scale past 3 for another 949.229457ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2000 12/30/22 03:29:43.783
    Dec 30 03:29:43.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=statefulset-2000 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Dec 30 03:29:43.991: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Dec 30 03:29:43.991: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Dec 30 03:29:43.991: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Dec 30 03:29:43.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=statefulset-2000 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Dec 30 03:29:44.182: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Dec 30 03:29:44.182: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Dec 30 03:29:44.182: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Dec 30 03:29:44.182: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=statefulset-2000 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Dec 30 03:29:44.386: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Dec 30 03:29:44.386: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Dec 30 03:29:44.386: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Dec 30 03:29:44.391: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
    Dec 30 03:29:54.401: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Dec 30 03:29:54.401: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Dec 30 03:29:54.401: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 12/30/22 03:29:54.401
    Dec 30 03:29:54.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=statefulset-2000 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Dec 30 03:29:54.633: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Dec 30 03:29:54.633: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Dec 30 03:29:54.633: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Dec 30 03:29:54.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=statefulset-2000 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Dec 30 03:29:54.837: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Dec 30 03:29:54.837: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Dec 30 03:29:54.837: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Dec 30 03:29:54.837: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=statefulset-2000 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Dec 30 03:29:55.055: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Dec 30 03:29:55.055: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Dec 30 03:29:55.055: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Dec 30 03:29:55.055: INFO: Waiting for statefulset status.replicas updated to 0
    Dec 30 03:29:55.060: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
    Dec 30 03:30:05.074: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Dec 30 03:30:05.074: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Dec 30 03:30:05.074: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Dec 30 03:30:05.090: INFO: POD   NODE        PHASE    GRACE  CONDITIONS
    Dec 30 03:30:05.090: INFO: ss-0  k8s-mgmt02  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-30 03:29:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-30 03:29:54 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-30 03:29:54 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-30 03:29:13 +0000 UTC  }]
    Dec 30 03:30:05.090: INFO: ss-1  k8s-mgmt01  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-30 03:29:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-30 03:29:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-30 03:29:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-30 03:29:33 +0000 UTC  }]
    Dec 30 03:30:05.090: INFO: ss-2  k8s-mgmt03  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-30 03:29:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-30 03:29:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-30 03:29:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-30 03:29:33 +0000 UTC  }]
    Dec 30 03:30:05.090: INFO: 
    Dec 30 03:30:05.090: INFO: StatefulSet ss has not reached scale 0, at 3
    Dec 30 03:30:06.095: INFO: POD   NODE        PHASE    GRACE  CONDITIONS
    Dec 30 03:30:06.095: INFO: ss-0  k8s-mgmt02  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-30 03:29:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-30 03:29:54 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-30 03:29:54 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-30 03:29:13 +0000 UTC  }]
    Dec 30 03:30:06.095: INFO: ss-1  k8s-mgmt01  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-30 03:29:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-30 03:29:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-30 03:29:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-30 03:29:33 +0000 UTC  }]
    Dec 30 03:30:06.095: INFO: ss-2  k8s-mgmt03  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-30 03:29:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-30 03:29:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-30 03:29:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-30 03:29:33 +0000 UTC  }]
    Dec 30 03:30:06.095: INFO: 
    Dec 30 03:30:06.095: INFO: StatefulSet ss has not reached scale 0, at 3
    Dec 30 03:30:07.100: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.989079091s
    Dec 30 03:30:08.105: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.984448318s
    Dec 30 03:30:09.109: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.979825451s
    Dec 30 03:30:10.114: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.974741634s
    Dec 30 03:30:11.119: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.969910061s
    Dec 30 03:30:12.124: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.965158254s
    Dec 30 03:30:13.129: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.960453796s
    Dec 30 03:30:14.134: INFO: Verifying statefulset ss doesn't scale past 0 for another 954.684916ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2000 12/30/22 03:30:15.134
    Dec 30 03:30:15.139: INFO: Scaling statefulset ss to 0
    Dec 30 03:30:15.153: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Dec 30 03:30:15.158: INFO: Deleting all statefulset in ns statefulset-2000
    Dec 30 03:30:15.163: INFO: Scaling statefulset ss to 0
    Dec 30 03:30:15.175: INFO: Waiting for statefulset status.replicas updated to 0
    Dec 30 03:30:15.178: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Dec 30 03:30:15.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-2000" for this suite. 12/30/22 03:30:15.196
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:194
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:30:15.203
Dec 30 03:30:15.203: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename container-runtime 12/30/22 03:30:15.205
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:30:15.218
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:30:15.221
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:194
STEP: create the container 12/30/22 03:30:15.225
STEP: wait for the container to reach Succeeded 12/30/22 03:30:15.232
STEP: get the container status 12/30/22 03:30:19.254
STEP: the container should be terminated 12/30/22 03:30:19.258
STEP: the termination message should be set 12/30/22 03:30:19.259
Dec 30 03:30:19.259: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 12/30/22 03:30:19.259
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Dec 30 03:30:19.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-828" for this suite. 12/30/22 03:30:19.28
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","completed":25,"skipped":554,"failed":0}
------------------------------
• [4.084 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:30:15.203
    Dec 30 03:30:15.203: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename container-runtime 12/30/22 03:30:15.205
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:30:15.218
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:30:15.221
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:194
    STEP: create the container 12/30/22 03:30:15.225
    STEP: wait for the container to reach Succeeded 12/30/22 03:30:15.232
    STEP: get the container status 12/30/22 03:30:19.254
    STEP: the container should be terminated 12/30/22 03:30:19.258
    STEP: the termination message should be set 12/30/22 03:30:19.259
    Dec 30 03:30:19.259: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 12/30/22 03:30:19.259
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Dec 30 03:30:19.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-828" for this suite. 12/30/22 03:30:19.28
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:960
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:30:19.288
Dec 30 03:30:19.288: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename kubectl 12/30/22 03:30:19.29
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:30:19.303
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:30:19.306
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:960
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 12/30/22 03:30:19.309
Dec 30 03:30:19.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-5528 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Dec 30 03:30:19.414: INFO: stderr: ""
Dec 30 03:30:19.414: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 12/30/22 03:30:19.414
Dec 30 03:30:19.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-5528 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
Dec 30 03:30:19.696: INFO: stderr: ""
Dec 30 03:30:19.696: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 12/30/22 03:30:19.696
Dec 30 03:30:19.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-5528 delete pods e2e-test-httpd-pod'
Dec 30 03:30:22.179: INFO: stderr: ""
Dec 30 03:30:22.179: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Dec 30 03:30:22.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5528" for this suite. 12/30/22 03:30:22.186
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","completed":26,"skipped":565,"failed":0}
------------------------------
• [2.905 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:954
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:960

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:30:19.288
    Dec 30 03:30:19.288: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename kubectl 12/30/22 03:30:19.29
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:30:19.303
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:30:19.306
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:960
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 12/30/22 03:30:19.309
    Dec 30 03:30:19.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-5528 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Dec 30 03:30:19.414: INFO: stderr: ""
    Dec 30 03:30:19.414: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 12/30/22 03:30:19.414
    Dec 30 03:30:19.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-5528 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
    Dec 30 03:30:19.696: INFO: stderr: ""
    Dec 30 03:30:19.696: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 12/30/22 03:30:19.696
    Dec 30 03:30:19.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-5528 delete pods e2e-test-httpd-pod'
    Dec 30 03:30:22.179: INFO: stderr: ""
    Dec 30 03:30:22.179: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Dec 30 03:30:22.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-5528" for this suite. 12/30/22 03:30:22.186
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:30:22.194
Dec 30 03:30:22.195: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename pods 12/30/22 03:30:22.196
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:30:22.209
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:30:22.212
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617
Dec 30 03:30:22.216: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: creating the pod 12/30/22 03:30:22.217
STEP: submitting the pod to kubernetes 12/30/22 03:30:22.217
Dec 30 03:30:22.226: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-cc8434a7-3544-485c-8228-2a8c982a2006" in namespace "pods-3482" to be "running and ready"
Dec 30 03:30:22.230: INFO: Pod "pod-logs-websocket-cc8434a7-3544-485c-8228-2a8c982a2006": Phase="Pending", Reason="", readiness=false. Elapsed: 3.954383ms
Dec 30 03:30:22.230: INFO: The phase of Pod pod-logs-websocket-cc8434a7-3544-485c-8228-2a8c982a2006 is Pending, waiting for it to be Running (with Ready = true)
Dec 30 03:30:24.236: INFO: Pod "pod-logs-websocket-cc8434a7-3544-485c-8228-2a8c982a2006": Phase="Running", Reason="", readiness=true. Elapsed: 2.009651544s
Dec 30 03:30:24.236: INFO: The phase of Pod pod-logs-websocket-cc8434a7-3544-485c-8228-2a8c982a2006 is Running (Ready = true)
Dec 30 03:30:24.236: INFO: Pod "pod-logs-websocket-cc8434a7-3544-485c-8228-2a8c982a2006" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Dec 30 03:30:24.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3482" for this suite. 12/30/22 03:30:24.316
{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","completed":27,"skipped":569,"failed":0}
------------------------------
• [2.128 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:30:22.194
    Dec 30 03:30:22.195: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename pods 12/30/22 03:30:22.196
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:30:22.209
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:30:22.212
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:617
    Dec 30 03:30:22.216: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: creating the pod 12/30/22 03:30:22.217
    STEP: submitting the pod to kubernetes 12/30/22 03:30:22.217
    Dec 30 03:30:22.226: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-cc8434a7-3544-485c-8228-2a8c982a2006" in namespace "pods-3482" to be "running and ready"
    Dec 30 03:30:22.230: INFO: Pod "pod-logs-websocket-cc8434a7-3544-485c-8228-2a8c982a2006": Phase="Pending", Reason="", readiness=false. Elapsed: 3.954383ms
    Dec 30 03:30:22.230: INFO: The phase of Pod pod-logs-websocket-cc8434a7-3544-485c-8228-2a8c982a2006 is Pending, waiting for it to be Running (with Ready = true)
    Dec 30 03:30:24.236: INFO: Pod "pod-logs-websocket-cc8434a7-3544-485c-8228-2a8c982a2006": Phase="Running", Reason="", readiness=true. Elapsed: 2.009651544s
    Dec 30 03:30:24.236: INFO: The phase of Pod pod-logs-websocket-cc8434a7-3544-485c-8228-2a8c982a2006 is Running (Ready = true)
    Dec 30 03:30:24.236: INFO: Pod "pod-logs-websocket-cc8434a7-3544-485c-8228-2a8c982a2006" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Dec 30 03:30:24.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-3482" for this suite. 12/30/22 03:30:24.316
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:30:24.324
Dec 30 03:30:24.324: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename svcaccounts 12/30/22 03:30:24.325
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:30:24.431
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:30:24.434
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75
Dec 30 03:30:24.451: INFO: Waiting up to 5m0s for pod "pod-service-account-88dbf07e-6fd6-4584-958f-a42903c7946f" in namespace "svcaccounts-6617" to be "running"
Dec 30 03:30:24.455: INFO: Pod "pod-service-account-88dbf07e-6fd6-4584-958f-a42903c7946f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.954029ms
Dec 30 03:30:26.461: INFO: Pod "pod-service-account-88dbf07e-6fd6-4584-958f-a42903c7946f": Phase="Running", Reason="", readiness=true. Elapsed: 2.009045898s
Dec 30 03:30:26.461: INFO: Pod "pod-service-account-88dbf07e-6fd6-4584-958f-a42903c7946f" satisfied condition "running"
STEP: reading a file in the container 12/30/22 03:30:26.461
Dec 30 03:30:26.461: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6617 pod-service-account-88dbf07e-6fd6-4584-958f-a42903c7946f -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 12/30/22 03:30:26.682
Dec 30 03:30:26.682: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6617 pod-service-account-88dbf07e-6fd6-4584-958f-a42903c7946f -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 12/30/22 03:30:26.89
Dec 30 03:30:26.890: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6617 pod-service-account-88dbf07e-6fd6-4584-958f-a42903c7946f -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Dec 30 03:30:27.085: INFO: Got root ca configmap in namespace "svcaccounts-6617"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Dec 30 03:30:27.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6617" for this suite. 12/30/22 03:30:27.094
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","completed":28,"skipped":584,"failed":0}
------------------------------
• [2.778 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:30:24.324
    Dec 30 03:30:24.324: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename svcaccounts 12/30/22 03:30:24.325
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:30:24.431
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:30:24.434
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:75
    Dec 30 03:30:24.451: INFO: Waiting up to 5m0s for pod "pod-service-account-88dbf07e-6fd6-4584-958f-a42903c7946f" in namespace "svcaccounts-6617" to be "running"
    Dec 30 03:30:24.455: INFO: Pod "pod-service-account-88dbf07e-6fd6-4584-958f-a42903c7946f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.954029ms
    Dec 30 03:30:26.461: INFO: Pod "pod-service-account-88dbf07e-6fd6-4584-958f-a42903c7946f": Phase="Running", Reason="", readiness=true. Elapsed: 2.009045898s
    Dec 30 03:30:26.461: INFO: Pod "pod-service-account-88dbf07e-6fd6-4584-958f-a42903c7946f" satisfied condition "running"
    STEP: reading a file in the container 12/30/22 03:30:26.461
    Dec 30 03:30:26.461: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6617 pod-service-account-88dbf07e-6fd6-4584-958f-a42903c7946f -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 12/30/22 03:30:26.682
    Dec 30 03:30:26.682: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6617 pod-service-account-88dbf07e-6fd6-4584-958f-a42903c7946f -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 12/30/22 03:30:26.89
    Dec 30 03:30:26.890: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6617 pod-service-account-88dbf07e-6fd6-4584-958f-a42903c7946f -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    Dec 30 03:30:27.085: INFO: Got root ca configmap in namespace "svcaccounts-6617"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Dec 30 03:30:27.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-6617" for this suite. 12/30/22 03:30:27.094
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:30:27.109
Dec 30 03:30:27.110: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename projected 12/30/22 03:30:27.111
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:30:27.124
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:30:27.128
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161
STEP: Creating the pod 12/30/22 03:30:27.131
Dec 30 03:30:27.140: INFO: Waiting up to 5m0s for pod "annotationupdate67ab14ae-818a-459f-8fa4-e9e5b53631ff" in namespace "projected-9847" to be "running and ready"
Dec 30 03:30:27.144: INFO: Pod "annotationupdate67ab14ae-818a-459f-8fa4-e9e5b53631ff": Phase="Pending", Reason="", readiness=false. Elapsed: 3.935102ms
Dec 30 03:30:27.144: INFO: The phase of Pod annotationupdate67ab14ae-818a-459f-8fa4-e9e5b53631ff is Pending, waiting for it to be Running (with Ready = true)
Dec 30 03:30:29.149: INFO: Pod "annotationupdate67ab14ae-818a-459f-8fa4-e9e5b53631ff": Phase="Running", Reason="", readiness=true. Elapsed: 2.008242116s
Dec 30 03:30:29.149: INFO: The phase of Pod annotationupdate67ab14ae-818a-459f-8fa4-e9e5b53631ff is Running (Ready = true)
Dec 30 03:30:29.149: INFO: Pod "annotationupdate67ab14ae-818a-459f-8fa4-e9e5b53631ff" satisfied condition "running and ready"
Dec 30 03:30:29.696: INFO: Successfully updated pod "annotationupdate67ab14ae-818a-459f-8fa4-e9e5b53631ff"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Dec 30 03:30:33.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9847" for this suite. 12/30/22 03:30:33.729
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","completed":29,"skipped":657,"failed":0}
------------------------------
• [SLOW TEST] [6.627 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:30:27.109
    Dec 30 03:30:27.110: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename projected 12/30/22 03:30:27.111
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:30:27.124
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:30:27.128
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:161
    STEP: Creating the pod 12/30/22 03:30:27.131
    Dec 30 03:30:27.140: INFO: Waiting up to 5m0s for pod "annotationupdate67ab14ae-818a-459f-8fa4-e9e5b53631ff" in namespace "projected-9847" to be "running and ready"
    Dec 30 03:30:27.144: INFO: Pod "annotationupdate67ab14ae-818a-459f-8fa4-e9e5b53631ff": Phase="Pending", Reason="", readiness=false. Elapsed: 3.935102ms
    Dec 30 03:30:27.144: INFO: The phase of Pod annotationupdate67ab14ae-818a-459f-8fa4-e9e5b53631ff is Pending, waiting for it to be Running (with Ready = true)
    Dec 30 03:30:29.149: INFO: Pod "annotationupdate67ab14ae-818a-459f-8fa4-e9e5b53631ff": Phase="Running", Reason="", readiness=true. Elapsed: 2.008242116s
    Dec 30 03:30:29.149: INFO: The phase of Pod annotationupdate67ab14ae-818a-459f-8fa4-e9e5b53631ff is Running (Ready = true)
    Dec 30 03:30:29.149: INFO: Pod "annotationupdate67ab14ae-818a-459f-8fa4-e9e5b53631ff" satisfied condition "running and ready"
    Dec 30 03:30:29.696: INFO: Successfully updated pod "annotationupdate67ab14ae-818a-459f-8fa4-e9e5b53631ff"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Dec 30 03:30:33.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9847" for this suite. 12/30/22 03:30:33.729
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:30:33.737
Dec 30 03:30:33.737: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename sched-preemption 12/30/22 03:30:33.739
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:30:33.752
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:30:33.755
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Dec 30 03:30:33.772: INFO: Waiting up to 1m0s for all nodes to be ready
Dec 30 03:31:33.830: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218
STEP: Create pods that use 4/5 of node resources. 12/30/22 03:31:33.833
Dec 30 03:31:33.857: INFO: Created pod: pod0-0-sched-preemption-low-priority
Dec 30 03:31:33.862: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Dec 30 03:31:33.904: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Dec 30 03:31:33.912: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Dec 30 03:31:33.926: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Dec 30 03:31:33.931: INFO: Created pod: pod2-1-sched-preemption-medium-priority
Dec 30 03:31:33.947: INFO: Created pod: pod3-0-sched-preemption-medium-priority
Dec 30 03:31:33.952: INFO: Created pod: pod3-1-sched-preemption-medium-priority
Dec 30 03:31:33.969: INFO: Created pod: pod4-0-sched-preemption-medium-priority
Dec 30 03:31:33.975: INFO: Created pod: pod4-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 12/30/22 03:31:33.975
Dec 30 03:31:33.975: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-7945" to be "running"
Dec 30 03:31:33.978: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.980792ms
Dec 30 03:31:35.984: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008779826s
Dec 30 03:31:37.984: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008658775s
Dec 30 03:31:39.984: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.009471125s
Dec 30 03:31:39.984: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Dec 30 03:31:39.984: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-7945" to be "running"
Dec 30 03:31:39.988: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.645283ms
Dec 30 03:31:39.988: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Dec 30 03:31:39.988: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-7945" to be "running"
Dec 30 03:31:39.993: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.34058ms
Dec 30 03:31:41.998: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00987238s
Dec 30 03:31:43.997: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.009146235s
Dec 30 03:31:43.997: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Dec 30 03:31:43.997: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-7945" to be "running"
Dec 30 03:31:44.001: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.804523ms
Dec 30 03:31:44.001: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Dec 30 03:31:44.001: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-7945" to be "running"
Dec 30 03:31:44.005: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.204918ms
Dec 30 03:31:44.006: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Dec 30 03:31:44.006: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-7945" to be "running"
Dec 30 03:31:44.009: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.729504ms
Dec 30 03:31:44.009: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
Dec 30 03:31:44.009: INFO: Waiting up to 5m0s for pod "pod3-0-sched-preemption-medium-priority" in namespace "sched-preemption-7945" to be "running"
Dec 30 03:31:44.013: INFO: Pod "pod3-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 3.318327ms
Dec 30 03:31:46.019: INFO: Pod "pod3-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009171676s
Dec 30 03:31:48.018: INFO: Pod "pod3-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.009139288s
Dec 30 03:31:48.019: INFO: Pod "pod3-0-sched-preemption-medium-priority" satisfied condition "running"
Dec 30 03:31:48.019: INFO: Waiting up to 5m0s for pod "pod3-1-sched-preemption-medium-priority" in namespace "sched-preemption-7945" to be "running"
Dec 30 03:31:48.023: INFO: Pod "pod3-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.18435ms
Dec 30 03:31:48.023: INFO: Pod "pod3-1-sched-preemption-medium-priority" satisfied condition "running"
Dec 30 03:31:48.023: INFO: Waiting up to 5m0s for pod "pod4-0-sched-preemption-medium-priority" in namespace "sched-preemption-7945" to be "running"
Dec 30 03:31:48.027: INFO: Pod "pod4-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.442629ms
Dec 30 03:31:48.027: INFO: Pod "pod4-0-sched-preemption-medium-priority" satisfied condition "running"
Dec 30 03:31:48.027: INFO: Waiting up to 5m0s for pod "pod4-1-sched-preemption-medium-priority" in namespace "sched-preemption-7945" to be "running"
Dec 30 03:31:48.031: INFO: Pod "pod4-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.413921ms
Dec 30 03:31:48.031: INFO: Pod "pod4-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 12/30/22 03:31:48.031
Dec 30 03:31:48.046: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Dec 30 03:31:48.050: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.958213ms
Dec 30 03:31:50.056: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009836825s
Dec 30 03:31:52.056: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.009732618s
Dec 30 03:31:52.056: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Dec 30 03:31:52.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-7945" for this suite. 12/30/22 03:31:52.121
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","completed":30,"skipped":658,"failed":0}
------------------------------
• [SLOW TEST] [78.464 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:30:33.737
    Dec 30 03:30:33.737: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename sched-preemption 12/30/22 03:30:33.739
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:30:33.752
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:30:33.755
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Dec 30 03:30:33.772: INFO: Waiting up to 1m0s for all nodes to be ready
    Dec 30 03:31:33.830: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:218
    STEP: Create pods that use 4/5 of node resources. 12/30/22 03:31:33.833
    Dec 30 03:31:33.857: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Dec 30 03:31:33.862: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Dec 30 03:31:33.904: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Dec 30 03:31:33.912: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Dec 30 03:31:33.926: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Dec 30 03:31:33.931: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    Dec 30 03:31:33.947: INFO: Created pod: pod3-0-sched-preemption-medium-priority
    Dec 30 03:31:33.952: INFO: Created pod: pod3-1-sched-preemption-medium-priority
    Dec 30 03:31:33.969: INFO: Created pod: pod4-0-sched-preemption-medium-priority
    Dec 30 03:31:33.975: INFO: Created pod: pod4-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 12/30/22 03:31:33.975
    Dec 30 03:31:33.975: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-7945" to be "running"
    Dec 30 03:31:33.978: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.980792ms
    Dec 30 03:31:35.984: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008779826s
    Dec 30 03:31:37.984: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008658775s
    Dec 30 03:31:39.984: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.009471125s
    Dec 30 03:31:39.984: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Dec 30 03:31:39.984: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-7945" to be "running"
    Dec 30 03:31:39.988: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.645283ms
    Dec 30 03:31:39.988: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Dec 30 03:31:39.988: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-7945" to be "running"
    Dec 30 03:31:39.993: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.34058ms
    Dec 30 03:31:41.998: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00987238s
    Dec 30 03:31:43.997: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.009146235s
    Dec 30 03:31:43.997: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Dec 30 03:31:43.997: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-7945" to be "running"
    Dec 30 03:31:44.001: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.804523ms
    Dec 30 03:31:44.001: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Dec 30 03:31:44.001: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-7945" to be "running"
    Dec 30 03:31:44.005: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.204918ms
    Dec 30 03:31:44.006: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Dec 30 03:31:44.006: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-7945" to be "running"
    Dec 30 03:31:44.009: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.729504ms
    Dec 30 03:31:44.009: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    Dec 30 03:31:44.009: INFO: Waiting up to 5m0s for pod "pod3-0-sched-preemption-medium-priority" in namespace "sched-preemption-7945" to be "running"
    Dec 30 03:31:44.013: INFO: Pod "pod3-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 3.318327ms
    Dec 30 03:31:46.019: INFO: Pod "pod3-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009171676s
    Dec 30 03:31:48.018: INFO: Pod "pod3-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.009139288s
    Dec 30 03:31:48.019: INFO: Pod "pod3-0-sched-preemption-medium-priority" satisfied condition "running"
    Dec 30 03:31:48.019: INFO: Waiting up to 5m0s for pod "pod3-1-sched-preemption-medium-priority" in namespace "sched-preemption-7945" to be "running"
    Dec 30 03:31:48.023: INFO: Pod "pod3-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.18435ms
    Dec 30 03:31:48.023: INFO: Pod "pod3-1-sched-preemption-medium-priority" satisfied condition "running"
    Dec 30 03:31:48.023: INFO: Waiting up to 5m0s for pod "pod4-0-sched-preemption-medium-priority" in namespace "sched-preemption-7945" to be "running"
    Dec 30 03:31:48.027: INFO: Pod "pod4-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.442629ms
    Dec 30 03:31:48.027: INFO: Pod "pod4-0-sched-preemption-medium-priority" satisfied condition "running"
    Dec 30 03:31:48.027: INFO: Waiting up to 5m0s for pod "pod4-1-sched-preemption-medium-priority" in namespace "sched-preemption-7945" to be "running"
    Dec 30 03:31:48.031: INFO: Pod "pod4-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.413921ms
    Dec 30 03:31:48.031: INFO: Pod "pod4-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 12/30/22 03:31:48.031
    Dec 30 03:31:48.046: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Dec 30 03:31:48.050: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.958213ms
    Dec 30 03:31:50.056: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009836825s
    Dec 30 03:31:52.056: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.009732618s
    Dec 30 03:31:52.056: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Dec 30 03:31:52.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-7945" for this suite. 12/30/22 03:31:52.121
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:31:52.203
Dec 30 03:31:52.203: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename secrets 12/30/22 03:31:52.204
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:31:52.219
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:31:52.222
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78
STEP: Creating secret with name secret-test-map-3a70ff1f-5ef2-456c-8b72-84744b7d79e9 12/30/22 03:31:52.225
STEP: Creating a pod to test consume secrets 12/30/22 03:31:52.23
Dec 30 03:31:52.238: INFO: Waiting up to 5m0s for pod "pod-secrets-762a7bab-a53c-4df8-91b2-5acea0e359eb" in namespace "secrets-7927" to be "Succeeded or Failed"
Dec 30 03:31:52.242: INFO: Pod "pod-secrets-762a7bab-a53c-4df8-91b2-5acea0e359eb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.884918ms
Dec 30 03:31:54.247: INFO: Pod "pod-secrets-762a7bab-a53c-4df8-91b2-5acea0e359eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008485377s
Dec 30 03:31:56.247: INFO: Pod "pod-secrets-762a7bab-a53c-4df8-91b2-5acea0e359eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00864044s
STEP: Saw pod success 12/30/22 03:31:56.247
Dec 30 03:31:56.247: INFO: Pod "pod-secrets-762a7bab-a53c-4df8-91b2-5acea0e359eb" satisfied condition "Succeeded or Failed"
Dec 30 03:31:56.251: INFO: Trying to get logs from node k8s-mgmt01 pod pod-secrets-762a7bab-a53c-4df8-91b2-5acea0e359eb container secret-volume-test: <nil>
STEP: delete the pod 12/30/22 03:31:56.272
Dec 30 03:31:56.285: INFO: Waiting for pod pod-secrets-762a7bab-a53c-4df8-91b2-5acea0e359eb to disappear
Dec 30 03:31:56.289: INFO: Pod pod-secrets-762a7bab-a53c-4df8-91b2-5acea0e359eb no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Dec 30 03:31:56.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7927" for this suite. 12/30/22 03:31:56.294
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":31,"skipped":670,"failed":0}
------------------------------
• [4.098 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:31:52.203
    Dec 30 03:31:52.203: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename secrets 12/30/22 03:31:52.204
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:31:52.219
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:31:52.222
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:78
    STEP: Creating secret with name secret-test-map-3a70ff1f-5ef2-456c-8b72-84744b7d79e9 12/30/22 03:31:52.225
    STEP: Creating a pod to test consume secrets 12/30/22 03:31:52.23
    Dec 30 03:31:52.238: INFO: Waiting up to 5m0s for pod "pod-secrets-762a7bab-a53c-4df8-91b2-5acea0e359eb" in namespace "secrets-7927" to be "Succeeded or Failed"
    Dec 30 03:31:52.242: INFO: Pod "pod-secrets-762a7bab-a53c-4df8-91b2-5acea0e359eb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.884918ms
    Dec 30 03:31:54.247: INFO: Pod "pod-secrets-762a7bab-a53c-4df8-91b2-5acea0e359eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008485377s
    Dec 30 03:31:56.247: INFO: Pod "pod-secrets-762a7bab-a53c-4df8-91b2-5acea0e359eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00864044s
    STEP: Saw pod success 12/30/22 03:31:56.247
    Dec 30 03:31:56.247: INFO: Pod "pod-secrets-762a7bab-a53c-4df8-91b2-5acea0e359eb" satisfied condition "Succeeded or Failed"
    Dec 30 03:31:56.251: INFO: Trying to get logs from node k8s-mgmt01 pod pod-secrets-762a7bab-a53c-4df8-91b2-5acea0e359eb container secret-volume-test: <nil>
    STEP: delete the pod 12/30/22 03:31:56.272
    Dec 30 03:31:56.285: INFO: Waiting for pod pod-secrets-762a7bab-a53c-4df8-91b2-5acea0e359eb to disappear
    Dec 30 03:31:56.289: INFO: Pod pod-secrets-762a7bab-a53c-4df8-91b2-5acea0e359eb no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Dec 30 03:31:56.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-7927" for this suite. 12/30/22 03:31:56.294
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:31:56.302
Dec 30 03:31:56.302: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename pod-network-test 12/30/22 03:31:56.304
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:31:56.316
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:31:56.319
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-2996 12/30/22 03:31:56.322
STEP: creating a selector 12/30/22 03:31:56.322
STEP: Creating the service pods in kubernetes 12/30/22 03:31:56.322
Dec 30 03:31:56.323: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Dec 30 03:31:56.364: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-2996" to be "running and ready"
Dec 30 03:31:56.368: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054002ms
Dec 30 03:31:56.368: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec 30 03:31:58.374: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.009858185s
Dec 30 03:31:58.374: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 30 03:32:00.374: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.010254629s
Dec 30 03:32:00.374: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 30 03:32:02.374: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.010002429s
Dec 30 03:32:02.374: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 30 03:32:04.374: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.009800029s
Dec 30 03:32:04.374: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 30 03:32:06.373: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.009646192s
Dec 30 03:32:06.373: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 30 03:32:08.374: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.010088423s
Dec 30 03:32:08.374: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 30 03:32:10.374: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.010014133s
Dec 30 03:32:10.374: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 30 03:32:12.374: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.010047313s
Dec 30 03:32:12.374: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 30 03:32:14.375: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.011148824s
Dec 30 03:32:14.375: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 30 03:32:16.374: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.009855363s
Dec 30 03:32:16.374: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 30 03:32:18.374: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.01035244s
Dec 30 03:32:18.374: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Dec 30 03:32:18.374: INFO: Pod "netserver-0" satisfied condition "running and ready"
Dec 30 03:32:18.378: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-2996" to be "running and ready"
Dec 30 03:32:18.382: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.042154ms
Dec 30 03:32:18.382: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Dec 30 03:32:18.382: INFO: Pod "netserver-1" satisfied condition "running and ready"
Dec 30 03:32:18.386: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-2996" to be "running and ready"
Dec 30 03:32:18.389: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.254655ms
Dec 30 03:32:18.389: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Dec 30 03:32:18.389: INFO: Pod "netserver-2" satisfied condition "running and ready"
Dec 30 03:32:18.393: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-2996" to be "running and ready"
Dec 30 03:32:18.397: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=false. Elapsed: 4.001873ms
Dec 30 03:32:18.397: INFO: The phase of Pod netserver-3 is Running (Ready = false)
Dec 30 03:32:20.402: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=false. Elapsed: 2.009124457s
Dec 30 03:32:20.402: INFO: The phase of Pod netserver-3 is Running (Ready = false)
Dec 30 03:32:22.403: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=false. Elapsed: 4.010226794s
Dec 30 03:32:22.403: INFO: The phase of Pod netserver-3 is Running (Ready = false)
Dec 30 03:32:24.403: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=false. Elapsed: 6.010322347s
Dec 30 03:32:24.403: INFO: The phase of Pod netserver-3 is Running (Ready = false)
Dec 30 03:32:26.402: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=false. Elapsed: 8.008481599s
Dec 30 03:32:26.402: INFO: The phase of Pod netserver-3 is Running (Ready = false)
Dec 30 03:32:28.402: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 10.009323548s
Dec 30 03:32:28.402: INFO: The phase of Pod netserver-3 is Running (Ready = true)
Dec 30 03:32:28.402: INFO: Pod "netserver-3" satisfied condition "running and ready"
Dec 30 03:32:28.406: INFO: Waiting up to 5m0s for pod "netserver-4" in namespace "pod-network-test-2996" to be "running and ready"
Dec 30 03:32:28.411: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=true. Elapsed: 4.151227ms
Dec 30 03:32:28.411: INFO: The phase of Pod netserver-4 is Running (Ready = true)
Dec 30 03:32:28.411: INFO: Pod "netserver-4" satisfied condition "running and ready"
STEP: Creating test pods 12/30/22 03:32:28.414
Dec 30 03:32:28.427: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-2996" to be "running"
Dec 30 03:32:28.430: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.75444ms
Dec 30 03:32:30.435: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008264539s
Dec 30 03:32:30.435: INFO: Pod "test-container-pod" satisfied condition "running"
Dec 30 03:32:30.439: INFO: Setting MaxTries for pod polling to 55 for networking test based on endpoint count 5
Dec 30 03:32:30.439: INFO: Breadth first check of 10.233.112.156 on host 10.78.26.140...
Dec 30 03:32:30.442: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.112.157:9080/dial?request=hostname&protocol=udp&host=10.233.112.156&port=8081&tries=1'] Namespace:pod-network-test-2996 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 30 03:32:30.442: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
Dec 30 03:32:30.443: INFO: ExecWithOptions: Clientset creation
Dec 30 03:32:30.443: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-2996/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.112.157%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.112.156%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Dec 30 03:32:30.568: INFO: Waiting for responses: map[]
Dec 30 03:32:30.568: INFO: reached 10.233.112.156 after 0/1 tries
Dec 30 03:32:30.568: INFO: Breadth first check of 10.233.125.203 on host 10.78.26.141...
Dec 30 03:32:30.571: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.112.157:9080/dial?request=hostname&protocol=udp&host=10.233.125.203&port=8081&tries=1'] Namespace:pod-network-test-2996 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 30 03:32:30.572: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
Dec 30 03:32:30.572: INFO: ExecWithOptions: Clientset creation
Dec 30 03:32:30.572: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-2996/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.112.157%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.125.203%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Dec 30 03:32:30.673: INFO: Waiting for responses: map[]
Dec 30 03:32:30.673: INFO: reached 10.233.125.203 after 0/1 tries
Dec 30 03:32:30.673: INFO: Breadth first check of 10.233.78.140 on host 10.78.26.142...
Dec 30 03:32:30.676: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.112.157:9080/dial?request=hostname&protocol=udp&host=10.233.78.140&port=8081&tries=1'] Namespace:pod-network-test-2996 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 30 03:32:30.676: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
Dec 30 03:32:30.677: INFO: ExecWithOptions: Clientset creation
Dec 30 03:32:30.677: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-2996/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.112.157%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.78.140%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Dec 30 03:32:30.787: INFO: Waiting for responses: map[]
Dec 30 03:32:30.787: INFO: reached 10.233.78.140 after 0/1 tries
Dec 30 03:32:30.787: INFO: Breadth first check of 10.233.79.67 on host 10.78.26.194...
Dec 30 03:32:30.791: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.112.157:9080/dial?request=hostname&protocol=udp&host=10.233.79.67&port=8081&tries=1'] Namespace:pod-network-test-2996 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 30 03:32:30.791: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
Dec 30 03:32:30.792: INFO: ExecWithOptions: Clientset creation
Dec 30 03:32:30.792: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-2996/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.112.157%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.79.67%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Dec 30 03:32:30.899: INFO: Waiting for responses: map[]
Dec 30 03:32:30.899: INFO: reached 10.233.79.67 after 0/1 tries
Dec 30 03:32:30.899: INFO: Breadth first check of 10.233.109.67 on host 10.78.26.195...
Dec 30 03:32:30.903: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.112.157:9080/dial?request=hostname&protocol=udp&host=10.233.109.67&port=8081&tries=1'] Namespace:pod-network-test-2996 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 30 03:32:30.903: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
Dec 30 03:32:30.903: INFO: ExecWithOptions: Clientset creation
Dec 30 03:32:30.903: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-2996/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.112.157%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.109.67%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Dec 30 03:32:30.997: INFO: Waiting for responses: map[]
Dec 30 03:32:30.997: INFO: reached 10.233.109.67 after 0/1 tries
Dec 30 03:32:30.997: INFO: Going to retry 0 out of 5 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Dec 30 03:32:30.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2996" for this suite. 12/30/22 03:32:31.002
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","completed":32,"skipped":694,"failed":0}
------------------------------
• [SLOW TEST] [34.707 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:31:56.302
    Dec 30 03:31:56.302: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename pod-network-test 12/30/22 03:31:56.304
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:31:56.316
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:31:56.319
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-2996 12/30/22 03:31:56.322
    STEP: creating a selector 12/30/22 03:31:56.322
    STEP: Creating the service pods in kubernetes 12/30/22 03:31:56.322
    Dec 30 03:31:56.323: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Dec 30 03:31:56.364: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-2996" to be "running and ready"
    Dec 30 03:31:56.368: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054002ms
    Dec 30 03:31:56.368: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Dec 30 03:31:58.374: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.009858185s
    Dec 30 03:31:58.374: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 30 03:32:00.374: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.010254629s
    Dec 30 03:32:00.374: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 30 03:32:02.374: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.010002429s
    Dec 30 03:32:02.374: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 30 03:32:04.374: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.009800029s
    Dec 30 03:32:04.374: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 30 03:32:06.373: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.009646192s
    Dec 30 03:32:06.373: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 30 03:32:08.374: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.010088423s
    Dec 30 03:32:08.374: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 30 03:32:10.374: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.010014133s
    Dec 30 03:32:10.374: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 30 03:32:12.374: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.010047313s
    Dec 30 03:32:12.374: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 30 03:32:14.375: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.011148824s
    Dec 30 03:32:14.375: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 30 03:32:16.374: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.009855363s
    Dec 30 03:32:16.374: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 30 03:32:18.374: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.01035244s
    Dec 30 03:32:18.374: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Dec 30 03:32:18.374: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Dec 30 03:32:18.378: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-2996" to be "running and ready"
    Dec 30 03:32:18.382: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.042154ms
    Dec 30 03:32:18.382: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Dec 30 03:32:18.382: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Dec 30 03:32:18.386: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-2996" to be "running and ready"
    Dec 30 03:32:18.389: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.254655ms
    Dec 30 03:32:18.389: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Dec 30 03:32:18.389: INFO: Pod "netserver-2" satisfied condition "running and ready"
    Dec 30 03:32:18.393: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-2996" to be "running and ready"
    Dec 30 03:32:18.397: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=false. Elapsed: 4.001873ms
    Dec 30 03:32:18.397: INFO: The phase of Pod netserver-3 is Running (Ready = false)
    Dec 30 03:32:20.402: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=false. Elapsed: 2.009124457s
    Dec 30 03:32:20.402: INFO: The phase of Pod netserver-3 is Running (Ready = false)
    Dec 30 03:32:22.403: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=false. Elapsed: 4.010226794s
    Dec 30 03:32:22.403: INFO: The phase of Pod netserver-3 is Running (Ready = false)
    Dec 30 03:32:24.403: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=false. Elapsed: 6.010322347s
    Dec 30 03:32:24.403: INFO: The phase of Pod netserver-3 is Running (Ready = false)
    Dec 30 03:32:26.402: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=false. Elapsed: 8.008481599s
    Dec 30 03:32:26.402: INFO: The phase of Pod netserver-3 is Running (Ready = false)
    Dec 30 03:32:28.402: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 10.009323548s
    Dec 30 03:32:28.402: INFO: The phase of Pod netserver-3 is Running (Ready = true)
    Dec 30 03:32:28.402: INFO: Pod "netserver-3" satisfied condition "running and ready"
    Dec 30 03:32:28.406: INFO: Waiting up to 5m0s for pod "netserver-4" in namespace "pod-network-test-2996" to be "running and ready"
    Dec 30 03:32:28.411: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=true. Elapsed: 4.151227ms
    Dec 30 03:32:28.411: INFO: The phase of Pod netserver-4 is Running (Ready = true)
    Dec 30 03:32:28.411: INFO: Pod "netserver-4" satisfied condition "running and ready"
    STEP: Creating test pods 12/30/22 03:32:28.414
    Dec 30 03:32:28.427: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-2996" to be "running"
    Dec 30 03:32:28.430: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.75444ms
    Dec 30 03:32:30.435: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008264539s
    Dec 30 03:32:30.435: INFO: Pod "test-container-pod" satisfied condition "running"
    Dec 30 03:32:30.439: INFO: Setting MaxTries for pod polling to 55 for networking test based on endpoint count 5
    Dec 30 03:32:30.439: INFO: Breadth first check of 10.233.112.156 on host 10.78.26.140...
    Dec 30 03:32:30.442: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.112.157:9080/dial?request=hostname&protocol=udp&host=10.233.112.156&port=8081&tries=1'] Namespace:pod-network-test-2996 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 30 03:32:30.442: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    Dec 30 03:32:30.443: INFO: ExecWithOptions: Clientset creation
    Dec 30 03:32:30.443: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-2996/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.112.157%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.112.156%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Dec 30 03:32:30.568: INFO: Waiting for responses: map[]
    Dec 30 03:32:30.568: INFO: reached 10.233.112.156 after 0/1 tries
    Dec 30 03:32:30.568: INFO: Breadth first check of 10.233.125.203 on host 10.78.26.141...
    Dec 30 03:32:30.571: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.112.157:9080/dial?request=hostname&protocol=udp&host=10.233.125.203&port=8081&tries=1'] Namespace:pod-network-test-2996 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 30 03:32:30.572: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    Dec 30 03:32:30.572: INFO: ExecWithOptions: Clientset creation
    Dec 30 03:32:30.572: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-2996/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.112.157%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.125.203%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Dec 30 03:32:30.673: INFO: Waiting for responses: map[]
    Dec 30 03:32:30.673: INFO: reached 10.233.125.203 after 0/1 tries
    Dec 30 03:32:30.673: INFO: Breadth first check of 10.233.78.140 on host 10.78.26.142...
    Dec 30 03:32:30.676: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.112.157:9080/dial?request=hostname&protocol=udp&host=10.233.78.140&port=8081&tries=1'] Namespace:pod-network-test-2996 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 30 03:32:30.676: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    Dec 30 03:32:30.677: INFO: ExecWithOptions: Clientset creation
    Dec 30 03:32:30.677: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-2996/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.112.157%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.78.140%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Dec 30 03:32:30.787: INFO: Waiting for responses: map[]
    Dec 30 03:32:30.787: INFO: reached 10.233.78.140 after 0/1 tries
    Dec 30 03:32:30.787: INFO: Breadth first check of 10.233.79.67 on host 10.78.26.194...
    Dec 30 03:32:30.791: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.112.157:9080/dial?request=hostname&protocol=udp&host=10.233.79.67&port=8081&tries=1'] Namespace:pod-network-test-2996 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 30 03:32:30.791: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    Dec 30 03:32:30.792: INFO: ExecWithOptions: Clientset creation
    Dec 30 03:32:30.792: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-2996/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.112.157%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.79.67%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Dec 30 03:32:30.899: INFO: Waiting for responses: map[]
    Dec 30 03:32:30.899: INFO: reached 10.233.79.67 after 0/1 tries
    Dec 30 03:32:30.899: INFO: Breadth first check of 10.233.109.67 on host 10.78.26.195...
    Dec 30 03:32:30.903: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.112.157:9080/dial?request=hostname&protocol=udp&host=10.233.109.67&port=8081&tries=1'] Namespace:pod-network-test-2996 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 30 03:32:30.903: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    Dec 30 03:32:30.903: INFO: ExecWithOptions: Clientset creation
    Dec 30 03:32:30.903: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-2996/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.112.157%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.109.67%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Dec 30 03:32:30.997: INFO: Waiting for responses: map[]
    Dec 30 03:32:30.997: INFO: reached 10.233.109.67 after 0/1 tries
    Dec 30 03:32:30.997: INFO: Going to retry 0 out of 5 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Dec 30 03:32:30.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-2996" for this suite. 12/30/22 03:32:31.002
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:32:31.014
Dec 30 03:32:31.014: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename job 12/30/22 03:32:31.015
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:32:31.028
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:32:31.031
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531
STEP: Creating a suspended job 12/30/22 03:32:31.039
STEP: Patching the Job 12/30/22 03:32:31.045
STEP: Watching for Job to be patched 12/30/22 03:32:31.068
Dec 30 03:32:31.070: INFO: Event ADDED observed for Job e2e-xrdw6 in namespace job-9073 with labels: map[e2e-job-label:e2e-xrdw6] and annotations: map[batch.kubernetes.io/job-tracking:]
Dec 30 03:32:31.070: INFO: Event MODIFIED observed for Job e2e-xrdw6 in namespace job-9073 with labels: map[e2e-job-label:e2e-xrdw6] and annotations: map[batch.kubernetes.io/job-tracking:]
Dec 30 03:32:31.070: INFO: Event MODIFIED found for Job e2e-xrdw6 in namespace job-9073 with labels: map[e2e-job-label:e2e-xrdw6 e2e-xrdw6:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 12/30/22 03:32:31.071
STEP: Watching for Job to be updated 12/30/22 03:32:31.081
Dec 30 03:32:31.083: INFO: Event MODIFIED found for Job e2e-xrdw6 in namespace job-9073 with labels: map[e2e-job-label:e2e-xrdw6 e2e-xrdw6:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Dec 30 03:32:31.083: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 12/30/22 03:32:31.083
Dec 30 03:32:31.088: INFO: Job: e2e-xrdw6 as labels: map[e2e-job-label:e2e-xrdw6 e2e-xrdw6:patched]
STEP: Waiting for job to complete 12/30/22 03:32:31.088
STEP: Delete a job collection with a labelselector 12/30/22 03:32:41.092
STEP: Watching for Job to be deleted 12/30/22 03:32:41.102
Dec 30 03:32:41.104: INFO: Event MODIFIED observed for Job e2e-xrdw6 in namespace job-9073 with labels: map[e2e-job-label:e2e-xrdw6 e2e-xrdw6:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Dec 30 03:32:41.104: INFO: Event MODIFIED observed for Job e2e-xrdw6 in namespace job-9073 with labels: map[e2e-job-label:e2e-xrdw6 e2e-xrdw6:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Dec 30 03:32:41.104: INFO: Event MODIFIED observed for Job e2e-xrdw6 in namespace job-9073 with labels: map[e2e-job-label:e2e-xrdw6 e2e-xrdw6:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Dec 30 03:32:41.104: INFO: Event MODIFIED observed for Job e2e-xrdw6 in namespace job-9073 with labels: map[e2e-job-label:e2e-xrdw6 e2e-xrdw6:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Dec 30 03:32:41.104: INFO: Event MODIFIED observed for Job e2e-xrdw6 in namespace job-9073 with labels: map[e2e-job-label:e2e-xrdw6 e2e-xrdw6:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Dec 30 03:32:41.104: INFO: Event MODIFIED observed for Job e2e-xrdw6 in namespace job-9073 with labels: map[e2e-job-label:e2e-xrdw6 e2e-xrdw6:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Dec 30 03:32:41.104: INFO: Event MODIFIED observed for Job e2e-xrdw6 in namespace job-9073 with labels: map[e2e-job-label:e2e-xrdw6 e2e-xrdw6:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Dec 30 03:32:41.104: INFO: Event MODIFIED observed for Job e2e-xrdw6 in namespace job-9073 with labels: map[e2e-job-label:e2e-xrdw6 e2e-xrdw6:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Dec 30 03:32:41.105: INFO: Event MODIFIED observed for Job e2e-xrdw6 in namespace job-9073 with labels: map[e2e-job-label:e2e-xrdw6 e2e-xrdw6:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Dec 30 03:32:41.105: INFO: Event DELETED found for Job e2e-xrdw6 in namespace job-9073 with labels: map[e2e-job-label:e2e-xrdw6 e2e-xrdw6:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 12/30/22 03:32:41.105
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Dec 30 03:32:41.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-9073" for this suite. 12/30/22 03:32:41.114
{"msg":"PASSED [sig-apps] Job should manage the lifecycle of a job [Conformance]","completed":33,"skipped":750,"failed":0}
------------------------------
• [SLOW TEST] [10.107 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:32:31.014
    Dec 30 03:32:31.014: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename job 12/30/22 03:32:31.015
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:32:31.028
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:32:31.031
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:531
    STEP: Creating a suspended job 12/30/22 03:32:31.039
    STEP: Patching the Job 12/30/22 03:32:31.045
    STEP: Watching for Job to be patched 12/30/22 03:32:31.068
    Dec 30 03:32:31.070: INFO: Event ADDED observed for Job e2e-xrdw6 in namespace job-9073 with labels: map[e2e-job-label:e2e-xrdw6] and annotations: map[batch.kubernetes.io/job-tracking:]
    Dec 30 03:32:31.070: INFO: Event MODIFIED observed for Job e2e-xrdw6 in namespace job-9073 with labels: map[e2e-job-label:e2e-xrdw6] and annotations: map[batch.kubernetes.io/job-tracking:]
    Dec 30 03:32:31.070: INFO: Event MODIFIED found for Job e2e-xrdw6 in namespace job-9073 with labels: map[e2e-job-label:e2e-xrdw6 e2e-xrdw6:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 12/30/22 03:32:31.071
    STEP: Watching for Job to be updated 12/30/22 03:32:31.081
    Dec 30 03:32:31.083: INFO: Event MODIFIED found for Job e2e-xrdw6 in namespace job-9073 with labels: map[e2e-job-label:e2e-xrdw6 e2e-xrdw6:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Dec 30 03:32:31.083: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 12/30/22 03:32:31.083
    Dec 30 03:32:31.088: INFO: Job: e2e-xrdw6 as labels: map[e2e-job-label:e2e-xrdw6 e2e-xrdw6:patched]
    STEP: Waiting for job to complete 12/30/22 03:32:31.088
    STEP: Delete a job collection with a labelselector 12/30/22 03:32:41.092
    STEP: Watching for Job to be deleted 12/30/22 03:32:41.102
    Dec 30 03:32:41.104: INFO: Event MODIFIED observed for Job e2e-xrdw6 in namespace job-9073 with labels: map[e2e-job-label:e2e-xrdw6 e2e-xrdw6:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Dec 30 03:32:41.104: INFO: Event MODIFIED observed for Job e2e-xrdw6 in namespace job-9073 with labels: map[e2e-job-label:e2e-xrdw6 e2e-xrdw6:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Dec 30 03:32:41.104: INFO: Event MODIFIED observed for Job e2e-xrdw6 in namespace job-9073 with labels: map[e2e-job-label:e2e-xrdw6 e2e-xrdw6:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Dec 30 03:32:41.104: INFO: Event MODIFIED observed for Job e2e-xrdw6 in namespace job-9073 with labels: map[e2e-job-label:e2e-xrdw6 e2e-xrdw6:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Dec 30 03:32:41.104: INFO: Event MODIFIED observed for Job e2e-xrdw6 in namespace job-9073 with labels: map[e2e-job-label:e2e-xrdw6 e2e-xrdw6:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Dec 30 03:32:41.104: INFO: Event MODIFIED observed for Job e2e-xrdw6 in namespace job-9073 with labels: map[e2e-job-label:e2e-xrdw6 e2e-xrdw6:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Dec 30 03:32:41.104: INFO: Event MODIFIED observed for Job e2e-xrdw6 in namespace job-9073 with labels: map[e2e-job-label:e2e-xrdw6 e2e-xrdw6:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Dec 30 03:32:41.104: INFO: Event MODIFIED observed for Job e2e-xrdw6 in namespace job-9073 with labels: map[e2e-job-label:e2e-xrdw6 e2e-xrdw6:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Dec 30 03:32:41.105: INFO: Event MODIFIED observed for Job e2e-xrdw6 in namespace job-9073 with labels: map[e2e-job-label:e2e-xrdw6 e2e-xrdw6:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Dec 30 03:32:41.105: INFO: Event DELETED found for Job e2e-xrdw6 in namespace job-9073 with labels: map[e2e-job-label:e2e-xrdw6 e2e-xrdw6:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 12/30/22 03:32:41.105
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Dec 30 03:32:41.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-9073" for this suite. 12/30/22 03:32:41.114
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:32:41.122
Dec 30 03:32:41.122: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename endpointslice 12/30/22 03:32:41.123
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:32:41.138
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:32:41.143
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65
Dec 30 03:32:41.168: INFO: Endpoints addresses: [10.78.26.140 10.78.26.141 10.78.26.142] , ports: [6443]
Dec 30 03:32:41.168: INFO: EndpointSlices addresses: [10.78.26.140 10.78.26.141 10.78.26.142] , ports: [6443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Dec 30 03:32:41.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-2716" for this suite. 12/30/22 03:32:41.173
{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","completed":34,"skipped":753,"failed":0}
------------------------------
• [0.058 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:32:41.122
    Dec 30 03:32:41.122: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename endpointslice 12/30/22 03:32:41.123
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:32:41.138
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:32:41.143
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:65
    Dec 30 03:32:41.168: INFO: Endpoints addresses: [10.78.26.140 10.78.26.141 10.78.26.142] , ports: [6443]
    Dec 30 03:32:41.168: INFO: EndpointSlices addresses: [10.78.26.140 10.78.26.141 10.78.26.142] , ports: [6443]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Dec 30 03:32:41.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-2716" for this suite. 12/30/22 03:32:41.173
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:32:41.18
Dec 30 03:32:41.181: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename init-container 12/30/22 03:32:41.182
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:32:41.193
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:32:41.196
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457
STEP: creating the pod 12/30/22 03:32:41.2
Dec 30 03:32:41.200: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Dec 30 03:32:45.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9715" for this suite. 12/30/22 03:32:45.593
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","completed":35,"skipped":754,"failed":0}
------------------------------
• [4.420 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:32:41.18
    Dec 30 03:32:41.181: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename init-container 12/30/22 03:32:41.182
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:32:41.193
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:32:41.196
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:457
    STEP: creating the pod 12/30/22 03:32:41.2
    Dec 30 03:32:41.200: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Dec 30 03:32:45.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-9715" for this suite. 12/30/22 03:32:45.593
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:32:45.601
Dec 30 03:32:45.601: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename container-probe 12/30/22 03:32:45.602
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:32:45.614
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:32:45.619
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68
Dec 30 03:32:45.630: INFO: Waiting up to 5m0s for pod "test-webserver-d7898134-aede-407a-9542-e8dda12dee39" in namespace "container-probe-7148" to be "running and ready"
Dec 30 03:32:45.634: INFO: Pod "test-webserver-d7898134-aede-407a-9542-e8dda12dee39": Phase="Pending", Reason="", readiness=false. Elapsed: 3.79871ms
Dec 30 03:32:45.634: INFO: The phase of Pod test-webserver-d7898134-aede-407a-9542-e8dda12dee39 is Pending, waiting for it to be Running (with Ready = true)
Dec 30 03:32:47.639: INFO: Pod "test-webserver-d7898134-aede-407a-9542-e8dda12dee39": Phase="Running", Reason="", readiness=false. Elapsed: 2.008765455s
Dec 30 03:32:47.639: INFO: The phase of Pod test-webserver-d7898134-aede-407a-9542-e8dda12dee39 is Running (Ready = false)
Dec 30 03:32:49.640: INFO: Pod "test-webserver-d7898134-aede-407a-9542-e8dda12dee39": Phase="Running", Reason="", readiness=false. Elapsed: 4.010063867s
Dec 30 03:32:49.640: INFO: The phase of Pod test-webserver-d7898134-aede-407a-9542-e8dda12dee39 is Running (Ready = false)
Dec 30 03:32:51.639: INFO: Pod "test-webserver-d7898134-aede-407a-9542-e8dda12dee39": Phase="Running", Reason="", readiness=false. Elapsed: 6.009239901s
Dec 30 03:32:51.639: INFO: The phase of Pod test-webserver-d7898134-aede-407a-9542-e8dda12dee39 is Running (Ready = false)
Dec 30 03:32:53.640: INFO: Pod "test-webserver-d7898134-aede-407a-9542-e8dda12dee39": Phase="Running", Reason="", readiness=false. Elapsed: 8.009514993s
Dec 30 03:32:53.640: INFO: The phase of Pod test-webserver-d7898134-aede-407a-9542-e8dda12dee39 is Running (Ready = false)
Dec 30 03:32:55.638: INFO: Pod "test-webserver-d7898134-aede-407a-9542-e8dda12dee39": Phase="Running", Reason="", readiness=false. Elapsed: 10.008246849s
Dec 30 03:32:55.638: INFO: The phase of Pod test-webserver-d7898134-aede-407a-9542-e8dda12dee39 is Running (Ready = false)
Dec 30 03:32:57.640: INFO: Pod "test-webserver-d7898134-aede-407a-9542-e8dda12dee39": Phase="Running", Reason="", readiness=false. Elapsed: 12.009930403s
Dec 30 03:32:57.640: INFO: The phase of Pod test-webserver-d7898134-aede-407a-9542-e8dda12dee39 is Running (Ready = false)
Dec 30 03:32:59.640: INFO: Pod "test-webserver-d7898134-aede-407a-9542-e8dda12dee39": Phase="Running", Reason="", readiness=false. Elapsed: 14.009824465s
Dec 30 03:32:59.640: INFO: The phase of Pod test-webserver-d7898134-aede-407a-9542-e8dda12dee39 is Running (Ready = false)
Dec 30 03:33:01.639: INFO: Pod "test-webserver-d7898134-aede-407a-9542-e8dda12dee39": Phase="Running", Reason="", readiness=false. Elapsed: 16.008479762s
Dec 30 03:33:01.639: INFO: The phase of Pod test-webserver-d7898134-aede-407a-9542-e8dda12dee39 is Running (Ready = false)
Dec 30 03:33:03.640: INFO: Pod "test-webserver-d7898134-aede-407a-9542-e8dda12dee39": Phase="Running", Reason="", readiness=false. Elapsed: 18.010074748s
Dec 30 03:33:03.640: INFO: The phase of Pod test-webserver-d7898134-aede-407a-9542-e8dda12dee39 is Running (Ready = false)
Dec 30 03:33:05.640: INFO: Pod "test-webserver-d7898134-aede-407a-9542-e8dda12dee39": Phase="Running", Reason="", readiness=false. Elapsed: 20.010225848s
Dec 30 03:33:05.640: INFO: The phase of Pod test-webserver-d7898134-aede-407a-9542-e8dda12dee39 is Running (Ready = false)
Dec 30 03:33:07.640: INFO: Pod "test-webserver-d7898134-aede-407a-9542-e8dda12dee39": Phase="Running", Reason="", readiness=true. Elapsed: 22.009472016s
Dec 30 03:33:07.640: INFO: The phase of Pod test-webserver-d7898134-aede-407a-9542-e8dda12dee39 is Running (Ready = true)
Dec 30 03:33:07.640: INFO: Pod "test-webserver-d7898134-aede-407a-9542-e8dda12dee39" satisfied condition "running and ready"
Dec 30 03:33:07.643: INFO: Container started at 2022-12-30 03:32:46 +0000 UTC, pod became ready at 2022-12-30 03:33:05 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Dec 30 03:33:07.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7148" for this suite. 12/30/22 03:33:07.649
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","completed":36,"skipped":754,"failed":0}
------------------------------
• [SLOW TEST] [22.055 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:32:45.601
    Dec 30 03:32:45.601: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename container-probe 12/30/22 03:32:45.602
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:32:45.614
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:32:45.619
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:68
    Dec 30 03:32:45.630: INFO: Waiting up to 5m0s for pod "test-webserver-d7898134-aede-407a-9542-e8dda12dee39" in namespace "container-probe-7148" to be "running and ready"
    Dec 30 03:32:45.634: INFO: Pod "test-webserver-d7898134-aede-407a-9542-e8dda12dee39": Phase="Pending", Reason="", readiness=false. Elapsed: 3.79871ms
    Dec 30 03:32:45.634: INFO: The phase of Pod test-webserver-d7898134-aede-407a-9542-e8dda12dee39 is Pending, waiting for it to be Running (with Ready = true)
    Dec 30 03:32:47.639: INFO: Pod "test-webserver-d7898134-aede-407a-9542-e8dda12dee39": Phase="Running", Reason="", readiness=false. Elapsed: 2.008765455s
    Dec 30 03:32:47.639: INFO: The phase of Pod test-webserver-d7898134-aede-407a-9542-e8dda12dee39 is Running (Ready = false)
    Dec 30 03:32:49.640: INFO: Pod "test-webserver-d7898134-aede-407a-9542-e8dda12dee39": Phase="Running", Reason="", readiness=false. Elapsed: 4.010063867s
    Dec 30 03:32:49.640: INFO: The phase of Pod test-webserver-d7898134-aede-407a-9542-e8dda12dee39 is Running (Ready = false)
    Dec 30 03:32:51.639: INFO: Pod "test-webserver-d7898134-aede-407a-9542-e8dda12dee39": Phase="Running", Reason="", readiness=false. Elapsed: 6.009239901s
    Dec 30 03:32:51.639: INFO: The phase of Pod test-webserver-d7898134-aede-407a-9542-e8dda12dee39 is Running (Ready = false)
    Dec 30 03:32:53.640: INFO: Pod "test-webserver-d7898134-aede-407a-9542-e8dda12dee39": Phase="Running", Reason="", readiness=false. Elapsed: 8.009514993s
    Dec 30 03:32:53.640: INFO: The phase of Pod test-webserver-d7898134-aede-407a-9542-e8dda12dee39 is Running (Ready = false)
    Dec 30 03:32:55.638: INFO: Pod "test-webserver-d7898134-aede-407a-9542-e8dda12dee39": Phase="Running", Reason="", readiness=false. Elapsed: 10.008246849s
    Dec 30 03:32:55.638: INFO: The phase of Pod test-webserver-d7898134-aede-407a-9542-e8dda12dee39 is Running (Ready = false)
    Dec 30 03:32:57.640: INFO: Pod "test-webserver-d7898134-aede-407a-9542-e8dda12dee39": Phase="Running", Reason="", readiness=false. Elapsed: 12.009930403s
    Dec 30 03:32:57.640: INFO: The phase of Pod test-webserver-d7898134-aede-407a-9542-e8dda12dee39 is Running (Ready = false)
    Dec 30 03:32:59.640: INFO: Pod "test-webserver-d7898134-aede-407a-9542-e8dda12dee39": Phase="Running", Reason="", readiness=false. Elapsed: 14.009824465s
    Dec 30 03:32:59.640: INFO: The phase of Pod test-webserver-d7898134-aede-407a-9542-e8dda12dee39 is Running (Ready = false)
    Dec 30 03:33:01.639: INFO: Pod "test-webserver-d7898134-aede-407a-9542-e8dda12dee39": Phase="Running", Reason="", readiness=false. Elapsed: 16.008479762s
    Dec 30 03:33:01.639: INFO: The phase of Pod test-webserver-d7898134-aede-407a-9542-e8dda12dee39 is Running (Ready = false)
    Dec 30 03:33:03.640: INFO: Pod "test-webserver-d7898134-aede-407a-9542-e8dda12dee39": Phase="Running", Reason="", readiness=false. Elapsed: 18.010074748s
    Dec 30 03:33:03.640: INFO: The phase of Pod test-webserver-d7898134-aede-407a-9542-e8dda12dee39 is Running (Ready = false)
    Dec 30 03:33:05.640: INFO: Pod "test-webserver-d7898134-aede-407a-9542-e8dda12dee39": Phase="Running", Reason="", readiness=false. Elapsed: 20.010225848s
    Dec 30 03:33:05.640: INFO: The phase of Pod test-webserver-d7898134-aede-407a-9542-e8dda12dee39 is Running (Ready = false)
    Dec 30 03:33:07.640: INFO: Pod "test-webserver-d7898134-aede-407a-9542-e8dda12dee39": Phase="Running", Reason="", readiness=true. Elapsed: 22.009472016s
    Dec 30 03:33:07.640: INFO: The phase of Pod test-webserver-d7898134-aede-407a-9542-e8dda12dee39 is Running (Ready = true)
    Dec 30 03:33:07.640: INFO: Pod "test-webserver-d7898134-aede-407a-9542-e8dda12dee39" satisfied condition "running and ready"
    Dec 30 03:33:07.643: INFO: Container started at 2022-12-30 03:32:46 +0000 UTC, pod became ready at 2022-12-30 03:33:05 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Dec 30 03:33:07.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-7148" for this suite. 12/30/22 03:33:07.649
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:33:07.656
Dec 30 03:33:07.656: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename tables 12/30/22 03:33:07.657
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:33:07.672
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:33:07.675
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:187
Dec 30 03:33:07.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-4293" for this suite. 12/30/22 03:33:07.687
{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","completed":37,"skipped":754,"failed":0}
------------------------------
• [0.037 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:33:07.656
    Dec 30 03:33:07.656: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename tables 12/30/22 03:33:07.657
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:33:07.672
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:33:07.675
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/framework.go:187
    Dec 30 03:33:07.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "tables-4293" for this suite. 12/30/22 03:33:07.687
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:33:07.694
Dec 30 03:33:07.694: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename webhook 12/30/22 03:33:07.695
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:33:07.708
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:33:07.711
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 12/30/22 03:33:07.727
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/30/22 03:33:08.124
STEP: Deploying the webhook pod 12/30/22 03:33:08.133
STEP: Wait for the deployment to be ready 12/30/22 03:33:08.142
Dec 30 03:33:08.150: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 12/30/22 03:33:10.164
STEP: Verifying the service has paired with the endpoint 12/30/22 03:33:10.175
Dec 30 03:33:11.175: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507
STEP: Creating a mutating webhook configuration 12/30/22 03:33:11.179
STEP: Updating a mutating webhook configuration's rules to not include the create operation 12/30/22 03:33:11.203
STEP: Creating a configMap that should not be mutated 12/30/22 03:33:11.21
STEP: Patching a mutating webhook configuration's rules to include the create operation 12/30/22 03:33:11.221
STEP: Creating a configMap that should be mutated 12/30/22 03:33:11.23
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Dec 30 03:33:11.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6565" for this suite. 12/30/22 03:33:11.265
STEP: Destroying namespace "webhook-6565-markers" for this suite. 12/30/22 03:33:11.272
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","completed":38,"skipped":757,"failed":0}
------------------------------
• [3.623 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:33:07.694
    Dec 30 03:33:07.694: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename webhook 12/30/22 03:33:07.695
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:33:07.708
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:33:07.711
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 12/30/22 03:33:07.727
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/30/22 03:33:08.124
    STEP: Deploying the webhook pod 12/30/22 03:33:08.133
    STEP: Wait for the deployment to be ready 12/30/22 03:33:08.142
    Dec 30 03:33:08.150: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 12/30/22 03:33:10.164
    STEP: Verifying the service has paired with the endpoint 12/30/22 03:33:10.175
    Dec 30 03:33:11.175: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:507
    STEP: Creating a mutating webhook configuration 12/30/22 03:33:11.179
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 12/30/22 03:33:11.203
    STEP: Creating a configMap that should not be mutated 12/30/22 03:33:11.21
    STEP: Patching a mutating webhook configuration's rules to include the create operation 12/30/22 03:33:11.221
    STEP: Creating a configMap that should be mutated 12/30/22 03:33:11.23
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Dec 30 03:33:11.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-6565" for this suite. 12/30/22 03:33:11.265
    STEP: Destroying namespace "webhook-6565-markers" for this suite. 12/30/22 03:33:11.272
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:33:11.322
Dec 30 03:33:11.322: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename sysctl 12/30/22 03:33:11.323
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:33:11.337
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:33:11.34
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 12/30/22 03:33:11.343
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
Dec 30 03:33:11.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-9191" for this suite. 12/30/22 03:33:11.355
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","completed":39,"skipped":814,"failed":0}
------------------------------
• [0.040 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:33:11.322
    Dec 30 03:33:11.322: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename sysctl 12/30/22 03:33:11.323
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:33:11.337
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:33:11.34
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 12/30/22 03:33:11.343
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:187
    Dec 30 03:33:11.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sysctl-9191" for this suite. 12/30/22 03:33:11.355
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:33:11.365
Dec 30 03:33:11.365: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename services 12/30/22 03:33:11.366
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:33:11.378
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:33:11.381
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-3238 12/30/22 03:33:11.384
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 12/30/22 03:33:11.394
STEP: creating service externalsvc in namespace services-3238 12/30/22 03:33:11.394
STEP: creating replication controller externalsvc in namespace services-3238 12/30/22 03:33:11.404
I1230 03:33:11.410130      25 runners.go:193] Created replication controller with name: externalsvc, namespace: services-3238, replica count: 2
I1230 03:33:14.462208      25 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 12/30/22 03:33:14.466
Dec 30 03:33:14.481: INFO: Creating new exec pod
Dec 30 03:33:14.487: INFO: Waiting up to 5m0s for pod "execpodfdgft" in namespace "services-3238" to be "running"
Dec 30 03:33:14.491: INFO: Pod "execpodfdgft": Phase="Pending", Reason="", readiness=false. Elapsed: 3.796335ms
Dec 30 03:33:16.496: INFO: Pod "execpodfdgft": Phase="Running", Reason="", readiness=true. Elapsed: 2.008267644s
Dec 30 03:33:16.496: INFO: Pod "execpodfdgft" satisfied condition "running"
Dec 30 03:33:16.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-3238 exec execpodfdgft -- /bin/sh -x -c nslookup clusterip-service.services-3238.svc.cluster.local'
Dec 30 03:33:16.743: INFO: stderr: "+ nslookup clusterip-service.services-3238.svc.cluster.local\n"
Dec 30 03:33:16.743: INFO: stdout: "Server:\t\t169.254.25.10\nAddress:\t169.254.25.10#53\n\nclusterip-service.services-3238.svc.cluster.local\tcanonical name = externalsvc.services-3238.svc.cluster.local.\nName:\texternalsvc.services-3238.svc.cluster.local\nAddress: 10.233.60.16\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-3238, will wait for the garbage collector to delete the pods 12/30/22 03:33:16.744
Dec 30 03:33:16.805: INFO: Deleting ReplicationController externalsvc took: 6.687551ms
Dec 30 03:33:16.906: INFO: Terminating ReplicationController externalsvc pods took: 100.45546ms
Dec 30 03:33:19.123: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Dec 30 03:33:19.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3238" for this suite. 12/30/22 03:33:19.14
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","completed":40,"skipped":842,"failed":0}
------------------------------
• [SLOW TEST] [7.781 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:33:11.365
    Dec 30 03:33:11.365: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename services 12/30/22 03:33:11.366
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:33:11.378
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:33:11.381
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1481
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-3238 12/30/22 03:33:11.384
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 12/30/22 03:33:11.394
    STEP: creating service externalsvc in namespace services-3238 12/30/22 03:33:11.394
    STEP: creating replication controller externalsvc in namespace services-3238 12/30/22 03:33:11.404
    I1230 03:33:11.410130      25 runners.go:193] Created replication controller with name: externalsvc, namespace: services-3238, replica count: 2
    I1230 03:33:14.462208      25 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 12/30/22 03:33:14.466
    Dec 30 03:33:14.481: INFO: Creating new exec pod
    Dec 30 03:33:14.487: INFO: Waiting up to 5m0s for pod "execpodfdgft" in namespace "services-3238" to be "running"
    Dec 30 03:33:14.491: INFO: Pod "execpodfdgft": Phase="Pending", Reason="", readiness=false. Elapsed: 3.796335ms
    Dec 30 03:33:16.496: INFO: Pod "execpodfdgft": Phase="Running", Reason="", readiness=true. Elapsed: 2.008267644s
    Dec 30 03:33:16.496: INFO: Pod "execpodfdgft" satisfied condition "running"
    Dec 30 03:33:16.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-3238 exec execpodfdgft -- /bin/sh -x -c nslookup clusterip-service.services-3238.svc.cluster.local'
    Dec 30 03:33:16.743: INFO: stderr: "+ nslookup clusterip-service.services-3238.svc.cluster.local\n"
    Dec 30 03:33:16.743: INFO: stdout: "Server:\t\t169.254.25.10\nAddress:\t169.254.25.10#53\n\nclusterip-service.services-3238.svc.cluster.local\tcanonical name = externalsvc.services-3238.svc.cluster.local.\nName:\texternalsvc.services-3238.svc.cluster.local\nAddress: 10.233.60.16\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-3238, will wait for the garbage collector to delete the pods 12/30/22 03:33:16.744
    Dec 30 03:33:16.805: INFO: Deleting ReplicationController externalsvc took: 6.687551ms
    Dec 30 03:33:16.906: INFO: Terminating ReplicationController externalsvc pods took: 100.45546ms
    Dec 30 03:33:19.123: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Dec 30 03:33:19.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-3238" for this suite. 12/30/22 03:33:19.14
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:33:19.146
Dec 30 03:33:19.146: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename events 12/30/22 03:33:19.147
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:33:19.16
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:33:19.163
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 12/30/22 03:33:19.166
STEP: listing events in all namespaces 12/30/22 03:33:19.174
STEP: listing events in test namespace 12/30/22 03:33:19.181
STEP: listing events with field selection filtering on source 12/30/22 03:33:19.185
STEP: listing events with field selection filtering on reportingController 12/30/22 03:33:19.188
STEP: getting the test event 12/30/22 03:33:19.192
STEP: patching the test event 12/30/22 03:33:19.195
STEP: getting the test event 12/30/22 03:33:19.209
STEP: updating the test event 12/30/22 03:33:19.212
STEP: getting the test event 12/30/22 03:33:19.22
STEP: deleting the test event 12/30/22 03:33:19.224
STEP: listing events in all namespaces 12/30/22 03:33:19.232
STEP: listing events in test namespace 12/30/22 03:33:19.241
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
Dec 30 03:33:19.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5212" for this suite. 12/30/22 03:33:19.249
{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","completed":41,"skipped":845,"failed":0}
------------------------------
• [0.110 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:33:19.146
    Dec 30 03:33:19.146: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename events 12/30/22 03:33:19.147
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:33:19.16
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:33:19.163
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 12/30/22 03:33:19.166
    STEP: listing events in all namespaces 12/30/22 03:33:19.174
    STEP: listing events in test namespace 12/30/22 03:33:19.181
    STEP: listing events with field selection filtering on source 12/30/22 03:33:19.185
    STEP: listing events with field selection filtering on reportingController 12/30/22 03:33:19.188
    STEP: getting the test event 12/30/22 03:33:19.192
    STEP: patching the test event 12/30/22 03:33:19.195
    STEP: getting the test event 12/30/22 03:33:19.209
    STEP: updating the test event 12/30/22 03:33:19.212
    STEP: getting the test event 12/30/22 03:33:19.22
    STEP: deleting the test event 12/30/22 03:33:19.224
    STEP: listing events in all namespaces 12/30/22 03:33:19.232
    STEP: listing events in test namespace 12/30/22 03:33:19.241
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:187
    Dec 30 03:33:19.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-5212" for this suite. 12/30/22 03:33:19.249
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:33:19.256
Dec 30 03:33:19.256: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename downward-api 12/30/22 03:33:19.258
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:33:19.272
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:33:19.275
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248
STEP: Creating a pod to test downward API volume plugin 12/30/22 03:33:19.279
Dec 30 03:33:19.288: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bf131341-2fd7-4e21-8266-0beb13796456" in namespace "downward-api-2510" to be "Succeeded or Failed"
Dec 30 03:33:19.292: INFO: Pod "downwardapi-volume-bf131341-2fd7-4e21-8266-0beb13796456": Phase="Pending", Reason="", readiness=false. Elapsed: 3.436481ms
Dec 30 03:33:21.297: INFO: Pod "downwardapi-volume-bf131341-2fd7-4e21-8266-0beb13796456": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008319182s
Dec 30 03:33:23.298: INFO: Pod "downwardapi-volume-bf131341-2fd7-4e21-8266-0beb13796456": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009177648s
STEP: Saw pod success 12/30/22 03:33:23.298
Dec 30 03:33:23.298: INFO: Pod "downwardapi-volume-bf131341-2fd7-4e21-8266-0beb13796456" satisfied condition "Succeeded or Failed"
Dec 30 03:33:23.302: INFO: Trying to get logs from node k8s-mgmt03 pod downwardapi-volume-bf131341-2fd7-4e21-8266-0beb13796456 container client-container: <nil>
STEP: delete the pod 12/30/22 03:33:23.321
Dec 30 03:33:23.335: INFO: Waiting for pod downwardapi-volume-bf131341-2fd7-4e21-8266-0beb13796456 to disappear
Dec 30 03:33:23.339: INFO: Pod downwardapi-volume-bf131341-2fd7-4e21-8266-0beb13796456 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Dec 30 03:33:23.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2510" for this suite. 12/30/22 03:33:23.345
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","completed":42,"skipped":848,"failed":0}
------------------------------
• [4.096 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:33:19.256
    Dec 30 03:33:19.256: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename downward-api 12/30/22 03:33:19.258
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:33:19.272
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:33:19.275
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:248
    STEP: Creating a pod to test downward API volume plugin 12/30/22 03:33:19.279
    Dec 30 03:33:19.288: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bf131341-2fd7-4e21-8266-0beb13796456" in namespace "downward-api-2510" to be "Succeeded or Failed"
    Dec 30 03:33:19.292: INFO: Pod "downwardapi-volume-bf131341-2fd7-4e21-8266-0beb13796456": Phase="Pending", Reason="", readiness=false. Elapsed: 3.436481ms
    Dec 30 03:33:21.297: INFO: Pod "downwardapi-volume-bf131341-2fd7-4e21-8266-0beb13796456": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008319182s
    Dec 30 03:33:23.298: INFO: Pod "downwardapi-volume-bf131341-2fd7-4e21-8266-0beb13796456": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009177648s
    STEP: Saw pod success 12/30/22 03:33:23.298
    Dec 30 03:33:23.298: INFO: Pod "downwardapi-volume-bf131341-2fd7-4e21-8266-0beb13796456" satisfied condition "Succeeded or Failed"
    Dec 30 03:33:23.302: INFO: Trying to get logs from node k8s-mgmt03 pod downwardapi-volume-bf131341-2fd7-4e21-8266-0beb13796456 container client-container: <nil>
    STEP: delete the pod 12/30/22 03:33:23.321
    Dec 30 03:33:23.335: INFO: Waiting for pod downwardapi-volume-bf131341-2fd7-4e21-8266-0beb13796456 to disappear
    Dec 30 03:33:23.339: INFO: Pod downwardapi-volume-bf131341-2fd7-4e21-8266-0beb13796456 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Dec 30 03:33:23.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-2510" for this suite. 12/30/22 03:33:23.345
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:33:23.357
Dec 30 03:33:23.358: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename proxy 12/30/22 03:33:23.359
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:33:23.371
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:33:23.374
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
Dec 30 03:33:23.378: INFO: Creating pod...
Dec 30 03:33:23.386: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-5428" to be "running"
Dec 30 03:33:23.390: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 3.604675ms
Dec 30 03:33:25.395: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.009098134s
Dec 30 03:33:25.395: INFO: Pod "agnhost" satisfied condition "running"
Dec 30 03:33:25.395: INFO: Creating service...
Dec 30 03:33:25.406: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5428/pods/agnhost/proxy?method=DELETE
Dec 30 03:33:25.411: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Dec 30 03:33:25.411: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5428/pods/agnhost/proxy?method=OPTIONS
Dec 30 03:33:25.416: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Dec 30 03:33:25.416: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5428/pods/agnhost/proxy?method=PATCH
Dec 30 03:33:25.420: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Dec 30 03:33:25.421: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5428/pods/agnhost/proxy?method=POST
Dec 30 03:33:25.424: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Dec 30 03:33:25.424: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5428/pods/agnhost/proxy?method=PUT
Dec 30 03:33:25.429: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Dec 30 03:33:25.429: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5428/services/e2e-proxy-test-service/proxy?method=DELETE
Dec 30 03:33:25.435: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Dec 30 03:33:25.435: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5428/services/e2e-proxy-test-service/proxy?method=OPTIONS
Dec 30 03:33:25.441: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Dec 30 03:33:25.441: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5428/services/e2e-proxy-test-service/proxy?method=PATCH
Dec 30 03:33:25.447: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Dec 30 03:33:25.447: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5428/services/e2e-proxy-test-service/proxy?method=POST
Dec 30 03:33:25.453: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Dec 30 03:33:25.453: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5428/services/e2e-proxy-test-service/proxy?method=PUT
Dec 30 03:33:25.459: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Dec 30 03:33:25.459: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5428/pods/agnhost/proxy?method=GET
Dec 30 03:33:25.462: INFO: http.Client request:GET StatusCode:301
Dec 30 03:33:25.462: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5428/services/e2e-proxy-test-service/proxy?method=GET
Dec 30 03:33:25.468: INFO: http.Client request:GET StatusCode:301
Dec 30 03:33:25.468: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5428/pods/agnhost/proxy?method=HEAD
Dec 30 03:33:25.470: INFO: http.Client request:HEAD StatusCode:301
Dec 30 03:33:25.470: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5428/services/e2e-proxy-test-service/proxy?method=HEAD
Dec 30 03:33:25.476: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Dec 30 03:33:25.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-5428" for this suite. 12/30/22 03:33:25.482
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]","completed":43,"skipped":913,"failed":0}
------------------------------
• [2.131 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:33:23.357
    Dec 30 03:33:23.358: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename proxy 12/30/22 03:33:23.359
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:33:23.371
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:33:23.374
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    Dec 30 03:33:23.378: INFO: Creating pod...
    Dec 30 03:33:23.386: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-5428" to be "running"
    Dec 30 03:33:23.390: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 3.604675ms
    Dec 30 03:33:25.395: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.009098134s
    Dec 30 03:33:25.395: INFO: Pod "agnhost" satisfied condition "running"
    Dec 30 03:33:25.395: INFO: Creating service...
    Dec 30 03:33:25.406: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5428/pods/agnhost/proxy?method=DELETE
    Dec 30 03:33:25.411: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Dec 30 03:33:25.411: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5428/pods/agnhost/proxy?method=OPTIONS
    Dec 30 03:33:25.416: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Dec 30 03:33:25.416: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5428/pods/agnhost/proxy?method=PATCH
    Dec 30 03:33:25.420: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Dec 30 03:33:25.421: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5428/pods/agnhost/proxy?method=POST
    Dec 30 03:33:25.424: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Dec 30 03:33:25.424: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5428/pods/agnhost/proxy?method=PUT
    Dec 30 03:33:25.429: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Dec 30 03:33:25.429: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5428/services/e2e-proxy-test-service/proxy?method=DELETE
    Dec 30 03:33:25.435: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Dec 30 03:33:25.435: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5428/services/e2e-proxy-test-service/proxy?method=OPTIONS
    Dec 30 03:33:25.441: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Dec 30 03:33:25.441: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5428/services/e2e-proxy-test-service/proxy?method=PATCH
    Dec 30 03:33:25.447: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Dec 30 03:33:25.447: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5428/services/e2e-proxy-test-service/proxy?method=POST
    Dec 30 03:33:25.453: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Dec 30 03:33:25.453: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5428/services/e2e-proxy-test-service/proxy?method=PUT
    Dec 30 03:33:25.459: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Dec 30 03:33:25.459: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5428/pods/agnhost/proxy?method=GET
    Dec 30 03:33:25.462: INFO: http.Client request:GET StatusCode:301
    Dec 30 03:33:25.462: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5428/services/e2e-proxy-test-service/proxy?method=GET
    Dec 30 03:33:25.468: INFO: http.Client request:GET StatusCode:301
    Dec 30 03:33:25.468: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5428/pods/agnhost/proxy?method=HEAD
    Dec 30 03:33:25.470: INFO: http.Client request:HEAD StatusCode:301
    Dec 30 03:33:25.470: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5428/services/e2e-proxy-test-service/proxy?method=HEAD
    Dec 30 03:33:25.476: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Dec 30 03:33:25.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-5428" for this suite. 12/30/22 03:33:25.482
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:33:25.49
Dec 30 03:33:25.490: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename var-expansion 12/30/22 03:33:25.492
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:33:25.504
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:33:25.507
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185
Dec 30 03:33:25.519: INFO: Waiting up to 2m0s for pod "var-expansion-26e24cf2-48c6-4eef-a24c-928d551dbea1" in namespace "var-expansion-3606" to be "container 0 failed with reason CreateContainerConfigError"
Dec 30 03:33:25.522: INFO: Pod "var-expansion-26e24cf2-48c6-4eef-a24c-928d551dbea1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.351442ms
Dec 30 03:33:27.528: INFO: Pod "var-expansion-26e24cf2-48c6-4eef-a24c-928d551dbea1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009061157s
Dec 30 03:33:27.528: INFO: Pod "var-expansion-26e24cf2-48c6-4eef-a24c-928d551dbea1" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Dec 30 03:33:27.528: INFO: Deleting pod "var-expansion-26e24cf2-48c6-4eef-a24c-928d551dbea1" in namespace "var-expansion-3606"
Dec 30 03:33:27.535: INFO: Wait up to 5m0s for pod "var-expansion-26e24cf2-48c6-4eef-a24c-928d551dbea1" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Dec 30 03:33:31.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3606" for this suite. 12/30/22 03:33:31.55
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","completed":44,"skipped":919,"failed":0}
------------------------------
• [SLOW TEST] [6.067 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:33:25.49
    Dec 30 03:33:25.490: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename var-expansion 12/30/22 03:33:25.492
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:33:25.504
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:33:25.507
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:185
    Dec 30 03:33:25.519: INFO: Waiting up to 2m0s for pod "var-expansion-26e24cf2-48c6-4eef-a24c-928d551dbea1" in namespace "var-expansion-3606" to be "container 0 failed with reason CreateContainerConfigError"
    Dec 30 03:33:25.522: INFO: Pod "var-expansion-26e24cf2-48c6-4eef-a24c-928d551dbea1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.351442ms
    Dec 30 03:33:27.528: INFO: Pod "var-expansion-26e24cf2-48c6-4eef-a24c-928d551dbea1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009061157s
    Dec 30 03:33:27.528: INFO: Pod "var-expansion-26e24cf2-48c6-4eef-a24c-928d551dbea1" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Dec 30 03:33:27.528: INFO: Deleting pod "var-expansion-26e24cf2-48c6-4eef-a24c-928d551dbea1" in namespace "var-expansion-3606"
    Dec 30 03:33:27.535: INFO: Wait up to 5m0s for pod "var-expansion-26e24cf2-48c6-4eef-a24c-928d551dbea1" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Dec 30 03:33:31.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-3606" for this suite. 12/30/22 03:33:31.55
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1810
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:33:31.559
Dec 30 03:33:31.559: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename kubectl 12/30/22 03:33:31.56
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:33:31.573
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:33:31.577
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1810
STEP: Starting the proxy 12/30/22 03:33:31.58
Dec 30 03:33:31.581: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-4891 proxy --unix-socket=/tmp/kubectl-proxy-unix1890206885/test'
STEP: retrieving proxy /api/ output 12/30/22 03:33:31.652
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Dec 30 03:33:31.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4891" for this suite. 12/30/22 03:33:31.66
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","completed":45,"skipped":940,"failed":0}
------------------------------
• [0.108 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1778
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:33:31.559
    Dec 30 03:33:31.559: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename kubectl 12/30/22 03:33:31.56
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:33:31.573
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:33:31.577
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1810
    STEP: Starting the proxy 12/30/22 03:33:31.58
    Dec 30 03:33:31.581: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-4891 proxy --unix-socket=/tmp/kubectl-proxy-unix1890206885/test'
    STEP: retrieving proxy /api/ output 12/30/22 03:33:31.652
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Dec 30 03:33:31.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-4891" for this suite. 12/30/22 03:33:31.66
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:33:31.667
Dec 30 03:33:31.667: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename secrets 12/30/22 03:33:31.669
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:33:31.682
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:33:31.684
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88
STEP: Creating secret with name secret-test-map-de4d2225-de0c-48f0-979c-e35dda787da7 12/30/22 03:33:31.687
STEP: Creating a pod to test consume secrets 12/30/22 03:33:31.692
Dec 30 03:33:31.701: INFO: Waiting up to 5m0s for pod "pod-secrets-822c1ead-6ffe-49fe-b0ae-32e9a662ec3a" in namespace "secrets-5016" to be "Succeeded or Failed"
Dec 30 03:33:31.704: INFO: Pod "pod-secrets-822c1ead-6ffe-49fe-b0ae-32e9a662ec3a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.129803ms
Dec 30 03:33:33.709: INFO: Pod "pod-secrets-822c1ead-6ffe-49fe-b0ae-32e9a662ec3a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00744855s
Dec 30 03:33:35.709: INFO: Pod "pod-secrets-822c1ead-6ffe-49fe-b0ae-32e9a662ec3a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007989857s
STEP: Saw pod success 12/30/22 03:33:35.709
Dec 30 03:33:35.709: INFO: Pod "pod-secrets-822c1ead-6ffe-49fe-b0ae-32e9a662ec3a" satisfied condition "Succeeded or Failed"
Dec 30 03:33:35.713: INFO: Trying to get logs from node k8s-mgmt03 pod pod-secrets-822c1ead-6ffe-49fe-b0ae-32e9a662ec3a container secret-volume-test: <nil>
STEP: delete the pod 12/30/22 03:33:35.721
Dec 30 03:33:35.734: INFO: Waiting for pod pod-secrets-822c1ead-6ffe-49fe-b0ae-32e9a662ec3a to disappear
Dec 30 03:33:35.737: INFO: Pod pod-secrets-822c1ead-6ffe-49fe-b0ae-32e9a662ec3a no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Dec 30 03:33:35.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5016" for this suite. 12/30/22 03:33:35.742
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":46,"skipped":947,"failed":0}
------------------------------
• [4.082 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:33:31.667
    Dec 30 03:33:31.667: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename secrets 12/30/22 03:33:31.669
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:33:31.682
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:33:31.684
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:88
    STEP: Creating secret with name secret-test-map-de4d2225-de0c-48f0-979c-e35dda787da7 12/30/22 03:33:31.687
    STEP: Creating a pod to test consume secrets 12/30/22 03:33:31.692
    Dec 30 03:33:31.701: INFO: Waiting up to 5m0s for pod "pod-secrets-822c1ead-6ffe-49fe-b0ae-32e9a662ec3a" in namespace "secrets-5016" to be "Succeeded or Failed"
    Dec 30 03:33:31.704: INFO: Pod "pod-secrets-822c1ead-6ffe-49fe-b0ae-32e9a662ec3a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.129803ms
    Dec 30 03:33:33.709: INFO: Pod "pod-secrets-822c1ead-6ffe-49fe-b0ae-32e9a662ec3a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00744855s
    Dec 30 03:33:35.709: INFO: Pod "pod-secrets-822c1ead-6ffe-49fe-b0ae-32e9a662ec3a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007989857s
    STEP: Saw pod success 12/30/22 03:33:35.709
    Dec 30 03:33:35.709: INFO: Pod "pod-secrets-822c1ead-6ffe-49fe-b0ae-32e9a662ec3a" satisfied condition "Succeeded or Failed"
    Dec 30 03:33:35.713: INFO: Trying to get logs from node k8s-mgmt03 pod pod-secrets-822c1ead-6ffe-49fe-b0ae-32e9a662ec3a container secret-volume-test: <nil>
    STEP: delete the pod 12/30/22 03:33:35.721
    Dec 30 03:33:35.734: INFO: Waiting for pod pod-secrets-822c1ead-6ffe-49fe-b0ae-32e9a662ec3a to disappear
    Dec 30 03:33:35.737: INFO: Pod pod-secrets-822c1ead-6ffe-49fe-b0ae-32e9a662ec3a no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Dec 30 03:33:35.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-5016" for this suite. 12/30/22 03:33:35.742
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:737
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:33:35.752
Dec 30 03:33:35.752: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename statefulset 12/30/22 03:33:35.753
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:33:35.766
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:33:35.769
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-8182 12/30/22 03:33:35.772
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:737
STEP: Looking for a node to schedule stateful set and pod 12/30/22 03:33:35.777
STEP: Creating pod with conflicting port in namespace statefulset-8182 12/30/22 03:33:35.783
STEP: Waiting until pod test-pod will start running in namespace statefulset-8182 12/30/22 03:33:35.792
Dec 30 03:33:35.792: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-8182" to be "running"
Dec 30 03:33:35.796: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.316168ms
Dec 30 03:33:37.803: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01077611s
Dec 30 03:33:39.802: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010081441s
Dec 30 03:33:41.801: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009002651s
Dec 30 03:33:43.803: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.010652521s
Dec 30 03:33:45.801: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 10.00871439s
Dec 30 03:33:45.801: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-8182 12/30/22 03:33:45.801
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-8182 12/30/22 03:33:45.807
Dec 30 03:33:45.821: INFO: Observed stateful pod in namespace: statefulset-8182, name: ss-0, uid: 7c03dfa3-4265-47fa-9aa8-7b171e64b1a5, status phase: Pending. Waiting for statefulset controller to delete.
Dec 30 03:33:45.834: INFO: Observed stateful pod in namespace: statefulset-8182, name: ss-0, uid: 7c03dfa3-4265-47fa-9aa8-7b171e64b1a5, status phase: Failed. Waiting for statefulset controller to delete.
Dec 30 03:33:45.843: INFO: Observed stateful pod in namespace: statefulset-8182, name: ss-0, uid: 7c03dfa3-4265-47fa-9aa8-7b171e64b1a5, status phase: Failed. Waiting for statefulset controller to delete.
Dec 30 03:33:45.848: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-8182
STEP: Removing pod with conflicting port in namespace statefulset-8182 12/30/22 03:33:45.848
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-8182 and will be in running state 12/30/22 03:33:45.862
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Dec 30 03:33:51.880: INFO: Deleting all statefulset in ns statefulset-8182
Dec 30 03:33:51.884: INFO: Scaling statefulset ss to 0
Dec 30 03:34:01.904: INFO: Waiting for statefulset status.replicas updated to 0
Dec 30 03:34:01.908: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Dec 30 03:34:01.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8182" for this suite. 12/30/22 03:34:01.927
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","completed":47,"skipped":981,"failed":0}
------------------------------
• [SLOW TEST] [26.181 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:737

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:33:35.752
    Dec 30 03:33:35.752: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename statefulset 12/30/22 03:33:35.753
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:33:35.766
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:33:35.769
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-8182 12/30/22 03:33:35.772
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:737
    STEP: Looking for a node to schedule stateful set and pod 12/30/22 03:33:35.777
    STEP: Creating pod with conflicting port in namespace statefulset-8182 12/30/22 03:33:35.783
    STEP: Waiting until pod test-pod will start running in namespace statefulset-8182 12/30/22 03:33:35.792
    Dec 30 03:33:35.792: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-8182" to be "running"
    Dec 30 03:33:35.796: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.316168ms
    Dec 30 03:33:37.803: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01077611s
    Dec 30 03:33:39.802: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010081441s
    Dec 30 03:33:41.801: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009002651s
    Dec 30 03:33:43.803: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.010652521s
    Dec 30 03:33:45.801: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 10.00871439s
    Dec 30 03:33:45.801: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-8182 12/30/22 03:33:45.801
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-8182 12/30/22 03:33:45.807
    Dec 30 03:33:45.821: INFO: Observed stateful pod in namespace: statefulset-8182, name: ss-0, uid: 7c03dfa3-4265-47fa-9aa8-7b171e64b1a5, status phase: Pending. Waiting for statefulset controller to delete.
    Dec 30 03:33:45.834: INFO: Observed stateful pod in namespace: statefulset-8182, name: ss-0, uid: 7c03dfa3-4265-47fa-9aa8-7b171e64b1a5, status phase: Failed. Waiting for statefulset controller to delete.
    Dec 30 03:33:45.843: INFO: Observed stateful pod in namespace: statefulset-8182, name: ss-0, uid: 7c03dfa3-4265-47fa-9aa8-7b171e64b1a5, status phase: Failed. Waiting for statefulset controller to delete.
    Dec 30 03:33:45.848: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-8182
    STEP: Removing pod with conflicting port in namespace statefulset-8182 12/30/22 03:33:45.848
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-8182 and will be in running state 12/30/22 03:33:45.862
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Dec 30 03:33:51.880: INFO: Deleting all statefulset in ns statefulset-8182
    Dec 30 03:33:51.884: INFO: Scaling statefulset ss to 0
    Dec 30 03:34:01.904: INFO: Waiting for statefulset status.replicas updated to 0
    Dec 30 03:34:01.908: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Dec 30 03:34:01.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-8182" for this suite. 12/30/22 03:34:01.927
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:34:01.936
Dec 30 03:34:01.937: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename endpointslice 12/30/22 03:34:01.938
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:34:01.954
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:34:01.957
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Dec 30 03:34:04.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-3621" for this suite. 12/30/22 03:34:04.01
{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","completed":48,"skipped":1014,"failed":0}
------------------------------
• [2.080 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:34:01.936
    Dec 30 03:34:01.937: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename endpointslice 12/30/22 03:34:01.938
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:34:01.954
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:34:01.957
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:101
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Dec 30 03:34:04.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-3621" for this suite. 12/30/22 03:34:04.01
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:34:04.02
Dec 30 03:34:04.020: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename projected 12/30/22 03:34:04.021
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:34:04.035
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:34:04.038
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206
STEP: Creating a pod to test downward API volume plugin 12/30/22 03:34:04.041
Dec 30 03:34:04.052: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dd46f1e1-836c-45a2-9b87-3040e0741e5c" in namespace "projected-7124" to be "Succeeded or Failed"
Dec 30 03:34:04.055: INFO: Pod "downwardapi-volume-dd46f1e1-836c-45a2-9b87-3040e0741e5c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.604994ms
Dec 30 03:34:06.060: INFO: Pod "downwardapi-volume-dd46f1e1-836c-45a2-9b87-3040e0741e5c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00839674s
Dec 30 03:34:08.062: INFO: Pod "downwardapi-volume-dd46f1e1-836c-45a2-9b87-3040e0741e5c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010061428s
STEP: Saw pod success 12/30/22 03:34:08.062
Dec 30 03:34:08.062: INFO: Pod "downwardapi-volume-dd46f1e1-836c-45a2-9b87-3040e0741e5c" satisfied condition "Succeeded or Failed"
Dec 30 03:34:08.066: INFO: Trying to get logs from node k8s-mgmt01 pod downwardapi-volume-dd46f1e1-836c-45a2-9b87-3040e0741e5c container client-container: <nil>
STEP: delete the pod 12/30/22 03:34:08.086
Dec 30 03:34:08.097: INFO: Waiting for pod downwardapi-volume-dd46f1e1-836c-45a2-9b87-3040e0741e5c to disappear
Dec 30 03:34:08.101: INFO: Pod downwardapi-volume-dd46f1e1-836c-45a2-9b87-3040e0741e5c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Dec 30 03:34:08.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7124" for this suite. 12/30/22 03:34:08.106
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","completed":49,"skipped":1047,"failed":0}
------------------------------
• [4.093 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:34:04.02
    Dec 30 03:34:04.020: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename projected 12/30/22 03:34:04.021
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:34:04.035
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:34:04.038
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:206
    STEP: Creating a pod to test downward API volume plugin 12/30/22 03:34:04.041
    Dec 30 03:34:04.052: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dd46f1e1-836c-45a2-9b87-3040e0741e5c" in namespace "projected-7124" to be "Succeeded or Failed"
    Dec 30 03:34:04.055: INFO: Pod "downwardapi-volume-dd46f1e1-836c-45a2-9b87-3040e0741e5c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.604994ms
    Dec 30 03:34:06.060: INFO: Pod "downwardapi-volume-dd46f1e1-836c-45a2-9b87-3040e0741e5c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00839674s
    Dec 30 03:34:08.062: INFO: Pod "downwardapi-volume-dd46f1e1-836c-45a2-9b87-3040e0741e5c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010061428s
    STEP: Saw pod success 12/30/22 03:34:08.062
    Dec 30 03:34:08.062: INFO: Pod "downwardapi-volume-dd46f1e1-836c-45a2-9b87-3040e0741e5c" satisfied condition "Succeeded or Failed"
    Dec 30 03:34:08.066: INFO: Trying to get logs from node k8s-mgmt01 pod downwardapi-volume-dd46f1e1-836c-45a2-9b87-3040e0741e5c container client-container: <nil>
    STEP: delete the pod 12/30/22 03:34:08.086
    Dec 30 03:34:08.097: INFO: Waiting for pod downwardapi-volume-dd46f1e1-836c-45a2-9b87-3040e0741e5c to disappear
    Dec 30 03:34:08.101: INFO: Pod downwardapi-volume-dd46f1e1-836c-45a2-9b87-3040e0741e5c no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Dec 30 03:34:08.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7124" for this suite. 12/30/22 03:34:08.106
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:34:08.114
Dec 30 03:34:08.115: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename var-expansion 12/30/22 03:34:08.118
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:34:08.131
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:34:08.134
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72
STEP: Creating a pod to test substitution in container's command 12/30/22 03:34:08.137
Dec 30 03:34:08.146: INFO: Waiting up to 5m0s for pod "var-expansion-ab1f414d-ccd5-448f-b835-b91700bdf047" in namespace "var-expansion-1592" to be "Succeeded or Failed"
Dec 30 03:34:08.149: INFO: Pod "var-expansion-ab1f414d-ccd5-448f-b835-b91700bdf047": Phase="Pending", Reason="", readiness=false. Elapsed: 3.044718ms
Dec 30 03:34:10.153: INFO: Pod "var-expansion-ab1f414d-ccd5-448f-b835-b91700bdf047": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007875839s
Dec 30 03:34:12.154: INFO: Pod "var-expansion-ab1f414d-ccd5-448f-b835-b91700bdf047": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008758722s
STEP: Saw pod success 12/30/22 03:34:12.154
Dec 30 03:34:12.155: INFO: Pod "var-expansion-ab1f414d-ccd5-448f-b835-b91700bdf047" satisfied condition "Succeeded or Failed"
Dec 30 03:34:12.158: INFO: Trying to get logs from node k8s-mgmt01 pod var-expansion-ab1f414d-ccd5-448f-b835-b91700bdf047 container dapi-container: <nil>
STEP: delete the pod 12/30/22 03:34:12.167
Dec 30 03:34:12.181: INFO: Waiting for pod var-expansion-ab1f414d-ccd5-448f-b835-b91700bdf047 to disappear
Dec 30 03:34:12.184: INFO: Pod var-expansion-ab1f414d-ccd5-448f-b835-b91700bdf047 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Dec 30 03:34:12.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1592" for this suite. 12/30/22 03:34:12.189
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","completed":50,"skipped":1050,"failed":0}
------------------------------
• [4.082 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:34:08.114
    Dec 30 03:34:08.115: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename var-expansion 12/30/22 03:34:08.118
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:34:08.131
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:34:08.134
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:72
    STEP: Creating a pod to test substitution in container's command 12/30/22 03:34:08.137
    Dec 30 03:34:08.146: INFO: Waiting up to 5m0s for pod "var-expansion-ab1f414d-ccd5-448f-b835-b91700bdf047" in namespace "var-expansion-1592" to be "Succeeded or Failed"
    Dec 30 03:34:08.149: INFO: Pod "var-expansion-ab1f414d-ccd5-448f-b835-b91700bdf047": Phase="Pending", Reason="", readiness=false. Elapsed: 3.044718ms
    Dec 30 03:34:10.153: INFO: Pod "var-expansion-ab1f414d-ccd5-448f-b835-b91700bdf047": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007875839s
    Dec 30 03:34:12.154: INFO: Pod "var-expansion-ab1f414d-ccd5-448f-b835-b91700bdf047": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008758722s
    STEP: Saw pod success 12/30/22 03:34:12.154
    Dec 30 03:34:12.155: INFO: Pod "var-expansion-ab1f414d-ccd5-448f-b835-b91700bdf047" satisfied condition "Succeeded or Failed"
    Dec 30 03:34:12.158: INFO: Trying to get logs from node k8s-mgmt01 pod var-expansion-ab1f414d-ccd5-448f-b835-b91700bdf047 container dapi-container: <nil>
    STEP: delete the pod 12/30/22 03:34:12.167
    Dec 30 03:34:12.181: INFO: Waiting for pod var-expansion-ab1f414d-ccd5-448f-b835-b91700bdf047 to disappear
    Dec 30 03:34:12.184: INFO: Pod var-expansion-ab1f414d-ccd5-448f-b835-b91700bdf047 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Dec 30 03:34:12.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-1592" for this suite. 12/30/22 03:34:12.189
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:34:12.196
Dec 30 03:34:12.196: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename prestop 12/30/22 03:34:12.198
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:34:12.211
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:34:12.215
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-9922 12/30/22 03:34:12.218
STEP: Waiting for pods to come up. 12/30/22 03:34:12.227
Dec 30 03:34:12.227: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-9922" to be "running"
Dec 30 03:34:12.231: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 3.573984ms
Dec 30 03:34:14.235: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.007491803s
Dec 30 03:34:14.235: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-9922 12/30/22 03:34:14.239
Dec 30 03:34:14.245: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-9922" to be "running"
Dec 30 03:34:14.248: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 3.034706ms
Dec 30 03:34:16.253: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.007852484s
Dec 30 03:34:16.253: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 12/30/22 03:34:16.253
Dec 30 03:34:21.267: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 12/30/22 03:34:21.267
[AfterEach] [sig-node] PreStop
  test/e2e/framework/framework.go:187
Dec 30 03:34:21.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-9922" for this suite. 12/30/22 03:34:21.285
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","completed":51,"skipped":1051,"failed":0}
------------------------------
• [SLOW TEST] [9.095 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:34:12.196
    Dec 30 03:34:12.196: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename prestop 12/30/22 03:34:12.198
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:34:12.211
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:34:12.215
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-9922 12/30/22 03:34:12.218
    STEP: Waiting for pods to come up. 12/30/22 03:34:12.227
    Dec 30 03:34:12.227: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-9922" to be "running"
    Dec 30 03:34:12.231: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 3.573984ms
    Dec 30 03:34:14.235: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.007491803s
    Dec 30 03:34:14.235: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-9922 12/30/22 03:34:14.239
    Dec 30 03:34:14.245: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-9922" to be "running"
    Dec 30 03:34:14.248: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 3.034706ms
    Dec 30 03:34:16.253: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.007852484s
    Dec 30 03:34:16.253: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 12/30/22 03:34:16.253
    Dec 30 03:34:21.267: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 12/30/22 03:34:21.267
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/framework.go:187
    Dec 30 03:34:21.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "prestop-9922" for this suite. 12/30/22 03:34:21.285
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:34:21.294
Dec 30 03:34:21.295: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename replicaset 12/30/22 03:34:21.296
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:34:21.309
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:34:21.312
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 12/30/22 03:34:21.319
STEP: Verify that the required pods have come up. 12/30/22 03:34:21.325
Dec 30 03:34:21.329: INFO: Pod name sample-pod: Found 0 pods out of 1
Dec 30 03:34:26.334: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 12/30/22 03:34:26.334
STEP: Getting /status 12/30/22 03:34:26.334
Dec 30 03:34:26.339: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 12/30/22 03:34:26.339
Dec 30 03:34:26.350: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 12/30/22 03:34:26.35
Dec 30 03:34:26.352: INFO: Observed &ReplicaSet event: ADDED
Dec 30 03:34:26.352: INFO: Observed &ReplicaSet event: MODIFIED
Dec 30 03:34:26.352: INFO: Observed &ReplicaSet event: MODIFIED
Dec 30 03:34:26.352: INFO: Observed &ReplicaSet event: MODIFIED
Dec 30 03:34:26.352: INFO: Found replicaset test-rs in namespace replicaset-2639 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Dec 30 03:34:26.352: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 12/30/22 03:34:26.352
Dec 30 03:34:26.353: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Dec 30 03:34:26.360: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 12/30/22 03:34:26.36
Dec 30 03:34:26.362: INFO: Observed &ReplicaSet event: ADDED
Dec 30 03:34:26.363: INFO: Observed &ReplicaSet event: MODIFIED
Dec 30 03:34:26.363: INFO: Observed &ReplicaSet event: MODIFIED
Dec 30 03:34:26.363: INFO: Observed &ReplicaSet event: MODIFIED
Dec 30 03:34:26.363: INFO: Observed replicaset test-rs in namespace replicaset-2639 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Dec 30 03:34:26.363: INFO: Observed &ReplicaSet event: MODIFIED
Dec 30 03:34:26.363: INFO: Found replicaset test-rs in namespace replicaset-2639 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Dec 30 03:34:26.363: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Dec 30 03:34:26.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2639" for this suite. 12/30/22 03:34:26.367
{"msg":"PASSED [sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]","completed":52,"skipped":1099,"failed":0}
------------------------------
• [SLOW TEST] [5.079 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:34:21.294
    Dec 30 03:34:21.295: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename replicaset 12/30/22 03:34:21.296
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:34:21.309
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:34:21.312
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 12/30/22 03:34:21.319
    STEP: Verify that the required pods have come up. 12/30/22 03:34:21.325
    Dec 30 03:34:21.329: INFO: Pod name sample-pod: Found 0 pods out of 1
    Dec 30 03:34:26.334: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 12/30/22 03:34:26.334
    STEP: Getting /status 12/30/22 03:34:26.334
    Dec 30 03:34:26.339: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 12/30/22 03:34:26.339
    Dec 30 03:34:26.350: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 12/30/22 03:34:26.35
    Dec 30 03:34:26.352: INFO: Observed &ReplicaSet event: ADDED
    Dec 30 03:34:26.352: INFO: Observed &ReplicaSet event: MODIFIED
    Dec 30 03:34:26.352: INFO: Observed &ReplicaSet event: MODIFIED
    Dec 30 03:34:26.352: INFO: Observed &ReplicaSet event: MODIFIED
    Dec 30 03:34:26.352: INFO: Found replicaset test-rs in namespace replicaset-2639 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Dec 30 03:34:26.352: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 12/30/22 03:34:26.352
    Dec 30 03:34:26.353: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Dec 30 03:34:26.360: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 12/30/22 03:34:26.36
    Dec 30 03:34:26.362: INFO: Observed &ReplicaSet event: ADDED
    Dec 30 03:34:26.363: INFO: Observed &ReplicaSet event: MODIFIED
    Dec 30 03:34:26.363: INFO: Observed &ReplicaSet event: MODIFIED
    Dec 30 03:34:26.363: INFO: Observed &ReplicaSet event: MODIFIED
    Dec 30 03:34:26.363: INFO: Observed replicaset test-rs in namespace replicaset-2639 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Dec 30 03:34:26.363: INFO: Observed &ReplicaSet event: MODIFIED
    Dec 30 03:34:26.363: INFO: Found replicaset test-rs in namespace replicaset-2639 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    Dec 30 03:34:26.363: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Dec 30 03:34:26.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-2639" for this suite. 12/30/22 03:34:26.367
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:346
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:34:26.376
Dec 30 03:34:26.376: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename security-context-test 12/30/22 03:34:26.378
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:34:26.391
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:34:26.395
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:346
Dec 30 03:34:26.407: INFO: Waiting up to 5m0s for pod "busybox-user-65534-9852cf7b-b39d-4bda-b305-ad091fb5e131" in namespace "security-context-test-1577" to be "Succeeded or Failed"
Dec 30 03:34:26.410: INFO: Pod "busybox-user-65534-9852cf7b-b39d-4bda-b305-ad091fb5e131": Phase="Pending", Reason="", readiness=false. Elapsed: 3.434262ms
Dec 30 03:34:28.415: INFO: Pod "busybox-user-65534-9852cf7b-b39d-4bda-b305-ad091fb5e131": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008613356s
Dec 30 03:34:30.417: INFO: Pod "busybox-user-65534-9852cf7b-b39d-4bda-b305-ad091fb5e131": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010169671s
Dec 30 03:34:30.417: INFO: Pod "busybox-user-65534-9852cf7b-b39d-4bda-b305-ad091fb5e131" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Dec 30 03:34:30.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-1577" for this suite. 12/30/22 03:34:30.423
{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","completed":53,"skipped":1120,"failed":0}
------------------------------
• [4.054 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:308
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:346

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:34:26.376
    Dec 30 03:34:26.376: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename security-context-test 12/30/22 03:34:26.378
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:34:26.391
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:34:26.395
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:346
    Dec 30 03:34:26.407: INFO: Waiting up to 5m0s for pod "busybox-user-65534-9852cf7b-b39d-4bda-b305-ad091fb5e131" in namespace "security-context-test-1577" to be "Succeeded or Failed"
    Dec 30 03:34:26.410: INFO: Pod "busybox-user-65534-9852cf7b-b39d-4bda-b305-ad091fb5e131": Phase="Pending", Reason="", readiness=false. Elapsed: 3.434262ms
    Dec 30 03:34:28.415: INFO: Pod "busybox-user-65534-9852cf7b-b39d-4bda-b305-ad091fb5e131": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008613356s
    Dec 30 03:34:30.417: INFO: Pod "busybox-user-65534-9852cf7b-b39d-4bda-b305-ad091fb5e131": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010169671s
    Dec 30 03:34:30.417: INFO: Pod "busybox-user-65534-9852cf7b-b39d-4bda-b305-ad091fb5e131" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Dec 30 03:34:30.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-1577" for this suite. 12/30/22 03:34:30.423
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:34:30.432
Dec 30 03:34:30.433: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename emptydir 12/30/22 03:34:30.434
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:34:30.448
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:34:30.451
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86
STEP: Creating a pod to test emptydir volume type on tmpfs 12/30/22 03:34:30.454
Dec 30 03:34:30.464: INFO: Waiting up to 5m0s for pod "pod-910e2fe0-a878-4977-9e0e-6b371890c9db" in namespace "emptydir-5867" to be "Succeeded or Failed"
Dec 30 03:34:30.467: INFO: Pod "pod-910e2fe0-a878-4977-9e0e-6b371890c9db": Phase="Pending", Reason="", readiness=false. Elapsed: 3.48581ms
Dec 30 03:34:32.473: INFO: Pod "pod-910e2fe0-a878-4977-9e0e-6b371890c9db": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009401066s
Dec 30 03:34:34.473: INFO: Pod "pod-910e2fe0-a878-4977-9e0e-6b371890c9db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009896167s
STEP: Saw pod success 12/30/22 03:34:34.474
Dec 30 03:34:34.474: INFO: Pod "pod-910e2fe0-a878-4977-9e0e-6b371890c9db" satisfied condition "Succeeded or Failed"
Dec 30 03:34:34.477: INFO: Trying to get logs from node k8s-mgmt01 pod pod-910e2fe0-a878-4977-9e0e-6b371890c9db container test-container: <nil>
STEP: delete the pod 12/30/22 03:34:34.486
Dec 30 03:34:34.499: INFO: Waiting for pod pod-910e2fe0-a878-4977-9e0e-6b371890c9db to disappear
Dec 30 03:34:34.502: INFO: Pod pod-910e2fe0-a878-4977-9e0e-6b371890c9db no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Dec 30 03:34:34.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5867" for this suite. 12/30/22 03:34:34.508
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","completed":54,"skipped":1143,"failed":0}
------------------------------
• [4.082 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:34:30.432
    Dec 30 03:34:30.433: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename emptydir 12/30/22 03:34:30.434
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:34:30.448
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:34:30.451
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:86
    STEP: Creating a pod to test emptydir volume type on tmpfs 12/30/22 03:34:30.454
    Dec 30 03:34:30.464: INFO: Waiting up to 5m0s for pod "pod-910e2fe0-a878-4977-9e0e-6b371890c9db" in namespace "emptydir-5867" to be "Succeeded or Failed"
    Dec 30 03:34:30.467: INFO: Pod "pod-910e2fe0-a878-4977-9e0e-6b371890c9db": Phase="Pending", Reason="", readiness=false. Elapsed: 3.48581ms
    Dec 30 03:34:32.473: INFO: Pod "pod-910e2fe0-a878-4977-9e0e-6b371890c9db": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009401066s
    Dec 30 03:34:34.473: INFO: Pod "pod-910e2fe0-a878-4977-9e0e-6b371890c9db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009896167s
    STEP: Saw pod success 12/30/22 03:34:34.474
    Dec 30 03:34:34.474: INFO: Pod "pod-910e2fe0-a878-4977-9e0e-6b371890c9db" satisfied condition "Succeeded or Failed"
    Dec 30 03:34:34.477: INFO: Trying to get logs from node k8s-mgmt01 pod pod-910e2fe0-a878-4977-9e0e-6b371890c9db container test-container: <nil>
    STEP: delete the pod 12/30/22 03:34:34.486
    Dec 30 03:34:34.499: INFO: Waiting for pod pod-910e2fe0-a878-4977-9e0e-6b371890c9db to disappear
    Dec 30 03:34:34.502: INFO: Pod pod-910e2fe0-a878-4977-9e0e-6b371890c9db no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Dec 30 03:34:34.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-5867" for this suite. 12/30/22 03:34:34.508
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:34:34.516
Dec 30 03:34:34.516: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename pod-network-test 12/30/22 03:34:34.518
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:34:34.531
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:34:34.534
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-6843 12/30/22 03:34:34.538
STEP: creating a selector 12/30/22 03:34:34.538
STEP: Creating the service pods in kubernetes 12/30/22 03:34:34.538
Dec 30 03:34:34.538: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Dec 30 03:34:34.579: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-6843" to be "running and ready"
Dec 30 03:34:34.583: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.394877ms
Dec 30 03:34:34.583: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec 30 03:34:36.589: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.009879549s
Dec 30 03:34:36.589: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 30 03:34:38.588: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.008776794s
Dec 30 03:34:38.588: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 30 03:34:40.588: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.008534637s
Dec 30 03:34:40.588: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 30 03:34:42.589: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.009417757s
Dec 30 03:34:42.589: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 30 03:34:44.587: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.007263737s
Dec 30 03:34:44.587: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 30 03:34:46.588: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.008765898s
Dec 30 03:34:46.588: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 30 03:34:48.590: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.01049343s
Dec 30 03:34:48.590: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 30 03:34:50.590: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.010276741s
Dec 30 03:34:50.590: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 30 03:34:52.589: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.009184319s
Dec 30 03:34:52.589: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 30 03:34:54.589: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.009893817s
Dec 30 03:34:54.589: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 30 03:34:56.588: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.008238925s
Dec 30 03:34:56.588: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Dec 30 03:34:56.588: INFO: Pod "netserver-0" satisfied condition "running and ready"
Dec 30 03:34:56.591: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-6843" to be "running and ready"
Dec 30 03:34:56.595: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.983384ms
Dec 30 03:34:56.595: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Dec 30 03:34:56.595: INFO: Pod "netserver-1" satisfied condition "running and ready"
Dec 30 03:34:56.599: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-6843" to be "running and ready"
Dec 30 03:34:56.602: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.210639ms
Dec 30 03:34:56.602: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Dec 30 03:34:56.602: INFO: Pod "netserver-2" satisfied condition "running and ready"
Dec 30 03:34:56.606: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-6843" to be "running and ready"
Dec 30 03:34:56.611: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 4.137419ms
Dec 30 03:34:56.611: INFO: The phase of Pod netserver-3 is Running (Ready = true)
Dec 30 03:34:56.611: INFO: Pod "netserver-3" satisfied condition "running and ready"
Dec 30 03:34:56.614: INFO: Waiting up to 5m0s for pod "netserver-4" in namespace "pod-network-test-6843" to be "running and ready"
Dec 30 03:34:56.618: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=true. Elapsed: 3.901796ms
Dec 30 03:34:56.618: INFO: The phase of Pod netserver-4 is Running (Ready = true)
Dec 30 03:34:56.618: INFO: Pod "netserver-4" satisfied condition "running and ready"
STEP: Creating test pods 12/30/22 03:34:56.622
Dec 30 03:34:56.633: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-6843" to be "running"
Dec 30 03:34:56.637: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.978586ms
Dec 30 03:34:58.648: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.014616142s
Dec 30 03:34:58.648: INFO: Pod "test-container-pod" satisfied condition "running"
Dec 30 03:34:58.652: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-6843" to be "running"
Dec 30 03:34:58.656: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.775972ms
Dec 30 03:34:58.656: INFO: Pod "host-test-container-pod" satisfied condition "running"
Dec 30 03:34:58.659: INFO: Setting MaxTries for pod polling to 55 for networking test based on endpoint count 5
Dec 30 03:34:58.659: INFO: Going to poll 10.233.112.170 on port 8081 at least 0 times, with a maximum of 55 tries before failing
Dec 30 03:34:58.663: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.112.170 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6843 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 30 03:34:58.663: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
Dec 30 03:34:58.664: INFO: ExecWithOptions: Clientset creation
Dec 30 03:34:58.664: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-6843/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.112.170+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Dec 30 03:34:59.784: INFO: Found all 1 expected endpoints: [netserver-0]
Dec 30 03:34:59.784: INFO: Going to poll 10.233.125.209 on port 8081 at least 0 times, with a maximum of 55 tries before failing
Dec 30 03:34:59.788: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.125.209 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6843 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 30 03:34:59.788: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
Dec 30 03:34:59.789: INFO: ExecWithOptions: Clientset creation
Dec 30 03:34:59.789: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-6843/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.125.209+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Dec 30 03:35:00.884: INFO: Found all 1 expected endpoints: [netserver-1]
Dec 30 03:35:00.884: INFO: Going to poll 10.233.78.145 on port 8081 at least 0 times, with a maximum of 55 tries before failing
Dec 30 03:35:00.888: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.78.145 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6843 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 30 03:35:00.888: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
Dec 30 03:35:00.889: INFO: ExecWithOptions: Clientset creation
Dec 30 03:35:00.889: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-6843/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.78.145+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Dec 30 03:35:02.007: INFO: Found all 1 expected endpoints: [netserver-2]
Dec 30 03:35:02.007: INFO: Going to poll 10.233.79.68 on port 8081 at least 0 times, with a maximum of 55 tries before failing
Dec 30 03:35:02.011: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.79.68 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6843 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 30 03:35:02.011: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
Dec 30 03:35:02.012: INFO: ExecWithOptions: Clientset creation
Dec 30 03:35:02.012: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-6843/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.79.68+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Dec 30 03:35:03.108: INFO: Found all 1 expected endpoints: [netserver-3]
Dec 30 03:35:03.108: INFO: Going to poll 10.233.109.70 on port 8081 at least 0 times, with a maximum of 55 tries before failing
Dec 30 03:35:03.113: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.109.70 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6843 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 30 03:35:03.113: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
Dec 30 03:35:03.114: INFO: ExecWithOptions: Clientset creation
Dec 30 03:35:03.114: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-6843/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.109.70+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Dec 30 03:35:04.211: INFO: Found all 1 expected endpoints: [netserver-4]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Dec 30 03:35:04.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6843" for this suite. 12/30/22 03:35:04.216
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","completed":55,"skipped":1164,"failed":0}
------------------------------
• [SLOW TEST] [29.707 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:34:34.516
    Dec 30 03:34:34.516: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename pod-network-test 12/30/22 03:34:34.518
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:34:34.531
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:34:34.534
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-6843 12/30/22 03:34:34.538
    STEP: creating a selector 12/30/22 03:34:34.538
    STEP: Creating the service pods in kubernetes 12/30/22 03:34:34.538
    Dec 30 03:34:34.538: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Dec 30 03:34:34.579: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-6843" to be "running and ready"
    Dec 30 03:34:34.583: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.394877ms
    Dec 30 03:34:34.583: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Dec 30 03:34:36.589: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.009879549s
    Dec 30 03:34:36.589: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 30 03:34:38.588: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.008776794s
    Dec 30 03:34:38.588: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 30 03:34:40.588: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.008534637s
    Dec 30 03:34:40.588: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 30 03:34:42.589: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.009417757s
    Dec 30 03:34:42.589: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 30 03:34:44.587: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.007263737s
    Dec 30 03:34:44.587: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 30 03:34:46.588: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.008765898s
    Dec 30 03:34:46.588: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 30 03:34:48.590: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.01049343s
    Dec 30 03:34:48.590: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 30 03:34:50.590: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.010276741s
    Dec 30 03:34:50.590: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 30 03:34:52.589: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.009184319s
    Dec 30 03:34:52.589: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 30 03:34:54.589: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.009893817s
    Dec 30 03:34:54.589: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 30 03:34:56.588: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.008238925s
    Dec 30 03:34:56.588: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Dec 30 03:34:56.588: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Dec 30 03:34:56.591: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-6843" to be "running and ready"
    Dec 30 03:34:56.595: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.983384ms
    Dec 30 03:34:56.595: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Dec 30 03:34:56.595: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Dec 30 03:34:56.599: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-6843" to be "running and ready"
    Dec 30 03:34:56.602: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.210639ms
    Dec 30 03:34:56.602: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Dec 30 03:34:56.602: INFO: Pod "netserver-2" satisfied condition "running and ready"
    Dec 30 03:34:56.606: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-6843" to be "running and ready"
    Dec 30 03:34:56.611: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 4.137419ms
    Dec 30 03:34:56.611: INFO: The phase of Pod netserver-3 is Running (Ready = true)
    Dec 30 03:34:56.611: INFO: Pod "netserver-3" satisfied condition "running and ready"
    Dec 30 03:34:56.614: INFO: Waiting up to 5m0s for pod "netserver-4" in namespace "pod-network-test-6843" to be "running and ready"
    Dec 30 03:34:56.618: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=true. Elapsed: 3.901796ms
    Dec 30 03:34:56.618: INFO: The phase of Pod netserver-4 is Running (Ready = true)
    Dec 30 03:34:56.618: INFO: Pod "netserver-4" satisfied condition "running and ready"
    STEP: Creating test pods 12/30/22 03:34:56.622
    Dec 30 03:34:56.633: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-6843" to be "running"
    Dec 30 03:34:56.637: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.978586ms
    Dec 30 03:34:58.648: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.014616142s
    Dec 30 03:34:58.648: INFO: Pod "test-container-pod" satisfied condition "running"
    Dec 30 03:34:58.652: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-6843" to be "running"
    Dec 30 03:34:58.656: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.775972ms
    Dec 30 03:34:58.656: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Dec 30 03:34:58.659: INFO: Setting MaxTries for pod polling to 55 for networking test based on endpoint count 5
    Dec 30 03:34:58.659: INFO: Going to poll 10.233.112.170 on port 8081 at least 0 times, with a maximum of 55 tries before failing
    Dec 30 03:34:58.663: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.112.170 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6843 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 30 03:34:58.663: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    Dec 30 03:34:58.664: INFO: ExecWithOptions: Clientset creation
    Dec 30 03:34:58.664: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-6843/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.112.170+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Dec 30 03:34:59.784: INFO: Found all 1 expected endpoints: [netserver-0]
    Dec 30 03:34:59.784: INFO: Going to poll 10.233.125.209 on port 8081 at least 0 times, with a maximum of 55 tries before failing
    Dec 30 03:34:59.788: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.125.209 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6843 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 30 03:34:59.788: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    Dec 30 03:34:59.789: INFO: ExecWithOptions: Clientset creation
    Dec 30 03:34:59.789: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-6843/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.125.209+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Dec 30 03:35:00.884: INFO: Found all 1 expected endpoints: [netserver-1]
    Dec 30 03:35:00.884: INFO: Going to poll 10.233.78.145 on port 8081 at least 0 times, with a maximum of 55 tries before failing
    Dec 30 03:35:00.888: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.78.145 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6843 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 30 03:35:00.888: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    Dec 30 03:35:00.889: INFO: ExecWithOptions: Clientset creation
    Dec 30 03:35:00.889: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-6843/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.78.145+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Dec 30 03:35:02.007: INFO: Found all 1 expected endpoints: [netserver-2]
    Dec 30 03:35:02.007: INFO: Going to poll 10.233.79.68 on port 8081 at least 0 times, with a maximum of 55 tries before failing
    Dec 30 03:35:02.011: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.79.68 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6843 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 30 03:35:02.011: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    Dec 30 03:35:02.012: INFO: ExecWithOptions: Clientset creation
    Dec 30 03:35:02.012: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-6843/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.79.68+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Dec 30 03:35:03.108: INFO: Found all 1 expected endpoints: [netserver-3]
    Dec 30 03:35:03.108: INFO: Going to poll 10.233.109.70 on port 8081 at least 0 times, with a maximum of 55 tries before failing
    Dec 30 03:35:03.113: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.109.70 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6843 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 30 03:35:03.113: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    Dec 30 03:35:03.114: INFO: ExecWithOptions: Clientset creation
    Dec 30 03:35:03.114: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-6843/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.109.70+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Dec 30 03:35:04.211: INFO: Found all 1 expected endpoints: [netserver-4]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Dec 30 03:35:04.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-6843" for this suite. 12/30/22 03:35:04.216
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:35:04.224
Dec 30 03:35:04.224: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename replicaset 12/30/22 03:35:04.226
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:35:04.24
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:35:04.243
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 12/30/22 03:35:04.246
STEP: Verify that the required pods have come up 12/30/22 03:35:04.252
Dec 30 03:35:04.256: INFO: Pod name sample-pod: Found 0 pods out of 3
Dec 30 03:35:09.263: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 12/30/22 03:35:09.263
Dec 30 03:35:09.267: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 12/30/22 03:35:09.267
STEP: DeleteCollection of the ReplicaSets 12/30/22 03:35:09.272
STEP: After DeleteCollection verify that ReplicaSets have been deleted 12/30/22 03:35:09.281
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Dec 30 03:35:09.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6658" for this suite. 12/30/22 03:35:09.29
{"msg":"PASSED [sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]","completed":56,"skipped":1181,"failed":0}
------------------------------
• [SLOW TEST] [5.073 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:35:04.224
    Dec 30 03:35:04.224: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename replicaset 12/30/22 03:35:04.226
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:35:04.24
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:35:04.243
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 12/30/22 03:35:04.246
    STEP: Verify that the required pods have come up 12/30/22 03:35:04.252
    Dec 30 03:35:04.256: INFO: Pod name sample-pod: Found 0 pods out of 3
    Dec 30 03:35:09.263: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 12/30/22 03:35:09.263
    Dec 30 03:35:09.267: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 12/30/22 03:35:09.267
    STEP: DeleteCollection of the ReplicaSets 12/30/22 03:35:09.272
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 12/30/22 03:35:09.281
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Dec 30 03:35:09.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-6658" for this suite. 12/30/22 03:35:09.29
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:35:09.299
Dec 30 03:35:09.299: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename configmap 12/30/22 03:35:09.301
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:35:09.315
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:35:09.318
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92
STEP: Creating configMap configmap-325/configmap-test-d4d3cf2f-8a2c-4e48-9a92-83e4a7c5b3b9 12/30/22 03:35:09.32
STEP: Creating a pod to test consume configMaps 12/30/22 03:35:09.323
Dec 30 03:35:09.331: INFO: Waiting up to 5m0s for pod "pod-configmaps-b38a8167-aa25-4b74-99c8-e12873e0905f" in namespace "configmap-325" to be "Succeeded or Failed"
Dec 30 03:35:09.334: INFO: Pod "pod-configmaps-b38a8167-aa25-4b74-99c8-e12873e0905f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.047128ms
Dec 30 03:35:11.339: INFO: Pod "pod-configmaps-b38a8167-aa25-4b74-99c8-e12873e0905f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008688708s
Dec 30 03:35:13.340: INFO: Pod "pod-configmaps-b38a8167-aa25-4b74-99c8-e12873e0905f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009391064s
STEP: Saw pod success 12/30/22 03:35:13.34
Dec 30 03:35:13.340: INFO: Pod "pod-configmaps-b38a8167-aa25-4b74-99c8-e12873e0905f" satisfied condition "Succeeded or Failed"
Dec 30 03:35:13.344: INFO: Trying to get logs from node k8s-mgmt02 pod pod-configmaps-b38a8167-aa25-4b74-99c8-e12873e0905f container env-test: <nil>
STEP: delete the pod 12/30/22 03:35:13.366
Dec 30 03:35:13.381: INFO: Waiting for pod pod-configmaps-b38a8167-aa25-4b74-99c8-e12873e0905f to disappear
Dec 30 03:35:13.384: INFO: Pod pod-configmaps-b38a8167-aa25-4b74-99c8-e12873e0905f no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Dec 30 03:35:13.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-325" for this suite. 12/30/22 03:35:13.39
{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","completed":57,"skipped":1192,"failed":0}
------------------------------
• [4.097 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:35:09.299
    Dec 30 03:35:09.299: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename configmap 12/30/22 03:35:09.301
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:35:09.315
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:35:09.318
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:92
    STEP: Creating configMap configmap-325/configmap-test-d4d3cf2f-8a2c-4e48-9a92-83e4a7c5b3b9 12/30/22 03:35:09.32
    STEP: Creating a pod to test consume configMaps 12/30/22 03:35:09.323
    Dec 30 03:35:09.331: INFO: Waiting up to 5m0s for pod "pod-configmaps-b38a8167-aa25-4b74-99c8-e12873e0905f" in namespace "configmap-325" to be "Succeeded or Failed"
    Dec 30 03:35:09.334: INFO: Pod "pod-configmaps-b38a8167-aa25-4b74-99c8-e12873e0905f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.047128ms
    Dec 30 03:35:11.339: INFO: Pod "pod-configmaps-b38a8167-aa25-4b74-99c8-e12873e0905f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008688708s
    Dec 30 03:35:13.340: INFO: Pod "pod-configmaps-b38a8167-aa25-4b74-99c8-e12873e0905f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009391064s
    STEP: Saw pod success 12/30/22 03:35:13.34
    Dec 30 03:35:13.340: INFO: Pod "pod-configmaps-b38a8167-aa25-4b74-99c8-e12873e0905f" satisfied condition "Succeeded or Failed"
    Dec 30 03:35:13.344: INFO: Trying to get logs from node k8s-mgmt02 pod pod-configmaps-b38a8167-aa25-4b74-99c8-e12873e0905f container env-test: <nil>
    STEP: delete the pod 12/30/22 03:35:13.366
    Dec 30 03:35:13.381: INFO: Waiting for pod pod-configmaps-b38a8167-aa25-4b74-99c8-e12873e0905f to disappear
    Dec 30 03:35:13.384: INFO: Pod pod-configmaps-b38a8167-aa25-4b74-99c8-e12873e0905f no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Dec 30 03:35:13.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-325" for this suite. 12/30/22 03:35:13.39
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:35:13.399
Dec 30 03:35:13.399: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename deployment 12/30/22 03:35:13.4
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:35:13.413
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:35:13.416
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
Dec 30 03:35:13.419: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Dec 30 03:35:13.428: INFO: Pod name sample-pod: Found 0 pods out of 1
Dec 30 03:35:18.435: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 12/30/22 03:35:18.435
Dec 30 03:35:18.436: INFO: Creating deployment "test-rolling-update-deployment"
Dec 30 03:35:18.442: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Dec 30 03:35:18.449: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Dec 30 03:35:20.459: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Dec 30 03:35:20.463: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Dec 30 03:35:20.475: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-5333  67fde940-6cb1-49f4-9439-0dc477fb96c7 425062 1 2022-12-30 03:35:18 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2022-12-30 03:35:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-30 03:35:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0035aed48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-12-30 03:35:18 +0000 UTC,LastTransitionTime:2022-12-30 03:35:18 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-78f575d8ff" has successfully progressed.,LastUpdateTime:2022-12-30 03:35:19 +0000 UTC,LastTransitionTime:2022-12-30 03:35:18 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Dec 30 03:35:20.479: INFO: New ReplicaSet "test-rolling-update-deployment-78f575d8ff" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-78f575d8ff  deployment-5333  f793617c-2eb4-41e1-8017-dc37745d3557 425052 1 2022-12-30 03:35:18 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 67fde940-6cb1-49f4-9439-0dc477fb96c7 0xc0044b8a67 0xc0044b8a68}] [] [{kube-controller-manager Update apps/v1 2022-12-30 03:35:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"67fde940-6cb1-49f4-9439-0dc477fb96c7\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-30 03:35:19 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 78f575d8ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0044b8b18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Dec 30 03:35:20.479: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Dec 30 03:35:20.480: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-5333  de96366a-b700-4e0e-91e6-53cd328ae274 425061 2 2022-12-30 03:35:13 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 67fde940-6cb1-49f4-9439-0dc477fb96c7 0xc0044b8937 0xc0044b8938}] [] [{e2e.test Update apps/v1 2022-12-30 03:35:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-30 03:35:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"67fde940-6cb1-49f4-9439-0dc477fb96c7\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-12-30 03:35:19 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0044b89f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 30 03:35:20.484: INFO: Pod "test-rolling-update-deployment-78f575d8ff-tz28p" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-78f575d8ff-tz28p test-rolling-update-deployment-78f575d8ff- deployment-5333  ebb384bd-f9f2-4eff-b8a3-8dc27d05c035 425051 0 2022-12-30 03:35:18 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[cni.projectcalico.org/containerID:738b0ba5103bd99e7a09ca0f25917240bcd223031ec1f351659bed89a07a2cd7 cni.projectcalico.org/podIP:10.233.78.147/32 cni.projectcalico.org/podIPs:10.233.78.147/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-78f575d8ff f793617c-2eb4-41e1-8017-dc37745d3557 0xc0044b8fb7 0xc0044b8fb8}] [] [{kube-controller-manager Update v1 2022-12-30 03:35:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f793617c-2eb4-41e1-8017-dc37745d3557\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-12-30 03:35:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-30 03:35:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.78.147\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mqgz6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mqgz6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-mgmt03,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:35:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:35:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:35:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:35:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.142,PodIP:10.233.78.147,StartTime:2022-12-30 03:35:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-30 03:35:19 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://69d017ae6732b118d4867324ccf210441031283669c04dc3f5e9afe27b52da5c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.78.147,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Dec 30 03:35:20.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5333" for this suite. 12/30/22 03:35:20.49
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","completed":58,"skipped":1216,"failed":0}
------------------------------
• [SLOW TEST] [7.098 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:35:13.399
    Dec 30 03:35:13.399: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename deployment 12/30/22 03:35:13.4
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:35:13.413
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:35:13.416
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    Dec 30 03:35:13.419: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    Dec 30 03:35:13.428: INFO: Pod name sample-pod: Found 0 pods out of 1
    Dec 30 03:35:18.435: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 12/30/22 03:35:18.435
    Dec 30 03:35:18.436: INFO: Creating deployment "test-rolling-update-deployment"
    Dec 30 03:35:18.442: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    Dec 30 03:35:18.449: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    Dec 30 03:35:20.459: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    Dec 30 03:35:20.463: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Dec 30 03:35:20.475: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-5333  67fde940-6cb1-49f4-9439-0dc477fb96c7 425062 1 2022-12-30 03:35:18 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2022-12-30 03:35:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-30 03:35:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0035aed48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-12-30 03:35:18 +0000 UTC,LastTransitionTime:2022-12-30 03:35:18 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-78f575d8ff" has successfully progressed.,LastUpdateTime:2022-12-30 03:35:19 +0000 UTC,LastTransitionTime:2022-12-30 03:35:18 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Dec 30 03:35:20.479: INFO: New ReplicaSet "test-rolling-update-deployment-78f575d8ff" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-78f575d8ff  deployment-5333  f793617c-2eb4-41e1-8017-dc37745d3557 425052 1 2022-12-30 03:35:18 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 67fde940-6cb1-49f4-9439-0dc477fb96c7 0xc0044b8a67 0xc0044b8a68}] [] [{kube-controller-manager Update apps/v1 2022-12-30 03:35:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"67fde940-6cb1-49f4-9439-0dc477fb96c7\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-30 03:35:19 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 78f575d8ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0044b8b18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Dec 30 03:35:20.479: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    Dec 30 03:35:20.480: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-5333  de96366a-b700-4e0e-91e6-53cd328ae274 425061 2 2022-12-30 03:35:13 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 67fde940-6cb1-49f4-9439-0dc477fb96c7 0xc0044b8937 0xc0044b8938}] [] [{e2e.test Update apps/v1 2022-12-30 03:35:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-30 03:35:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"67fde940-6cb1-49f4-9439-0dc477fb96c7\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-12-30 03:35:19 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0044b89f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Dec 30 03:35:20.484: INFO: Pod "test-rolling-update-deployment-78f575d8ff-tz28p" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-78f575d8ff-tz28p test-rolling-update-deployment-78f575d8ff- deployment-5333  ebb384bd-f9f2-4eff-b8a3-8dc27d05c035 425051 0 2022-12-30 03:35:18 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[cni.projectcalico.org/containerID:738b0ba5103bd99e7a09ca0f25917240bcd223031ec1f351659bed89a07a2cd7 cni.projectcalico.org/podIP:10.233.78.147/32 cni.projectcalico.org/podIPs:10.233.78.147/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-78f575d8ff f793617c-2eb4-41e1-8017-dc37745d3557 0xc0044b8fb7 0xc0044b8fb8}] [] [{kube-controller-manager Update v1 2022-12-30 03:35:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f793617c-2eb4-41e1-8017-dc37745d3557\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-12-30 03:35:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-30 03:35:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.78.147\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mqgz6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mqgz6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-mgmt03,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:35:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:35:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:35:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:35:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.142,PodIP:10.233.78.147,StartTime:2022-12-30 03:35:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-30 03:35:19 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://69d017ae6732b118d4867324ccf210441031283669c04dc3f5e9afe27b52da5c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.78.147,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Dec 30 03:35:20.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-5333" for this suite. 12/30/22 03:35:20.49
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:929
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:35:20.502
Dec 30 03:35:20.502: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename kubectl 12/30/22 03:35:20.503
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:35:20.517
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:35:20.52
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:929
STEP: create deployment with httpd image 12/30/22 03:35:20.523
Dec 30 03:35:20.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-3315 create -f -'
Dec 30 03:35:20.802: INFO: stderr: ""
Dec 30 03:35:20.802: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 12/30/22 03:35:20.802
Dec 30 03:35:20.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-3315 diff -f -'
Dec 30 03:35:21.922: INFO: rc: 1
Dec 30 03:35:21.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-3315 delete -f -'
Dec 30 03:35:22.017: INFO: stderr: ""
Dec 30 03:35:22.017: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Dec 30 03:35:22.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3315" for this suite. 12/30/22 03:35:22.023
{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","completed":59,"skipped":1274,"failed":0}
------------------------------
• [1.528 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:923
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:929

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:35:20.502
    Dec 30 03:35:20.502: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename kubectl 12/30/22 03:35:20.503
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:35:20.517
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:35:20.52
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:929
    STEP: create deployment with httpd image 12/30/22 03:35:20.523
    Dec 30 03:35:20.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-3315 create -f -'
    Dec 30 03:35:20.802: INFO: stderr: ""
    Dec 30 03:35:20.802: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 12/30/22 03:35:20.802
    Dec 30 03:35:20.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-3315 diff -f -'
    Dec 30 03:35:21.922: INFO: rc: 1
    Dec 30 03:35:21.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-3315 delete -f -'
    Dec 30 03:35:22.017: INFO: stderr: ""
    Dec 30 03:35:22.017: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Dec 30 03:35:22.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-3315" for this suite. 12/30/22 03:35:22.023
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:35:22.031
Dec 30 03:35:22.031: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename configmap 12/30/22 03:35:22.032
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:35:22.045
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:35:22.049
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108
STEP: Creating configMap with name configmap-test-volume-map-506bdb5f-d6c1-4ec6-ba3a-f65eba767ed9 12/30/22 03:35:22.052
STEP: Creating a pod to test consume configMaps 12/30/22 03:35:22.057
Dec 30 03:35:22.066: INFO: Waiting up to 5m0s for pod "pod-configmaps-76c2d62d-249d-4be5-9f31-b31570a7ecf1" in namespace "configmap-7940" to be "Succeeded or Failed"
Dec 30 03:35:22.070: INFO: Pod "pod-configmaps-76c2d62d-249d-4be5-9f31-b31570a7ecf1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.659087ms
Dec 30 03:35:24.074: INFO: Pod "pod-configmaps-76c2d62d-249d-4be5-9f31-b31570a7ecf1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007787321s
Dec 30 03:35:26.075: INFO: Pod "pod-configmaps-76c2d62d-249d-4be5-9f31-b31570a7ecf1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00818974s
STEP: Saw pod success 12/30/22 03:35:26.075
Dec 30 03:35:26.075: INFO: Pod "pod-configmaps-76c2d62d-249d-4be5-9f31-b31570a7ecf1" satisfied condition "Succeeded or Failed"
Dec 30 03:35:26.079: INFO: Trying to get logs from node k8s-mgmt02 pod pod-configmaps-76c2d62d-249d-4be5-9f31-b31570a7ecf1 container agnhost-container: <nil>
STEP: delete the pod 12/30/22 03:35:26.087
Dec 30 03:35:26.100: INFO: Waiting for pod pod-configmaps-76c2d62d-249d-4be5-9f31-b31570a7ecf1 to disappear
Dec 30 03:35:26.103: INFO: Pod pod-configmaps-76c2d62d-249d-4be5-9f31-b31570a7ecf1 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Dec 30 03:35:26.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7940" for this suite. 12/30/22 03:35:26.109
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","completed":60,"skipped":1281,"failed":0}
------------------------------
• [4.085 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:35:22.031
    Dec 30 03:35:22.031: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename configmap 12/30/22 03:35:22.032
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:35:22.045
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:35:22.049
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:108
    STEP: Creating configMap with name configmap-test-volume-map-506bdb5f-d6c1-4ec6-ba3a-f65eba767ed9 12/30/22 03:35:22.052
    STEP: Creating a pod to test consume configMaps 12/30/22 03:35:22.057
    Dec 30 03:35:22.066: INFO: Waiting up to 5m0s for pod "pod-configmaps-76c2d62d-249d-4be5-9f31-b31570a7ecf1" in namespace "configmap-7940" to be "Succeeded or Failed"
    Dec 30 03:35:22.070: INFO: Pod "pod-configmaps-76c2d62d-249d-4be5-9f31-b31570a7ecf1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.659087ms
    Dec 30 03:35:24.074: INFO: Pod "pod-configmaps-76c2d62d-249d-4be5-9f31-b31570a7ecf1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007787321s
    Dec 30 03:35:26.075: INFO: Pod "pod-configmaps-76c2d62d-249d-4be5-9f31-b31570a7ecf1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00818974s
    STEP: Saw pod success 12/30/22 03:35:26.075
    Dec 30 03:35:26.075: INFO: Pod "pod-configmaps-76c2d62d-249d-4be5-9f31-b31570a7ecf1" satisfied condition "Succeeded or Failed"
    Dec 30 03:35:26.079: INFO: Trying to get logs from node k8s-mgmt02 pod pod-configmaps-76c2d62d-249d-4be5-9f31-b31570a7ecf1 container agnhost-container: <nil>
    STEP: delete the pod 12/30/22 03:35:26.087
    Dec 30 03:35:26.100: INFO: Waiting for pod pod-configmaps-76c2d62d-249d-4be5-9f31-b31570a7ecf1 to disappear
    Dec 30 03:35:26.103: INFO: Pod pod-configmaps-76c2d62d-249d-4be5-9f31-b31570a7ecf1 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Dec 30 03:35:26.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-7940" for this suite. 12/30/22 03:35:26.109
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:35:26.116
Dec 30 03:35:26.116: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename watch 12/30/22 03:35:26.117
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:35:26.13
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:35:26.133
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 12/30/22 03:35:26.137
STEP: creating a new configmap 12/30/22 03:35:26.138
STEP: modifying the configmap once 12/30/22 03:35:26.143
STEP: closing the watch once it receives two notifications 12/30/22 03:35:26.151
Dec 30 03:35:26.151: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9171  34a16f22-4308-4c0d-b399-1ce05582d001 425170 0 2022-12-30 03:35:26 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2022-12-30 03:35:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 30 03:35:26.152: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9171  34a16f22-4308-4c0d-b399-1ce05582d001 425171 0 2022-12-30 03:35:26 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2022-12-30 03:35:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 12/30/22 03:35:26.152
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 12/30/22 03:35:26.16
STEP: deleting the configmap 12/30/22 03:35:26.161
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 12/30/22 03:35:26.168
Dec 30 03:35:26.169: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9171  34a16f22-4308-4c0d-b399-1ce05582d001 425172 0 2022-12-30 03:35:26 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2022-12-30 03:35:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 30 03:35:26.169: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9171  34a16f22-4308-4c0d-b399-1ce05582d001 425173 0 2022-12-30 03:35:26 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2022-12-30 03:35:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Dec 30 03:35:26.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9171" for this suite. 12/30/22 03:35:26.174
{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","completed":61,"skipped":1283,"failed":0}
------------------------------
• [0.065 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:35:26.116
    Dec 30 03:35:26.116: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename watch 12/30/22 03:35:26.117
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:35:26.13
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:35:26.133
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 12/30/22 03:35:26.137
    STEP: creating a new configmap 12/30/22 03:35:26.138
    STEP: modifying the configmap once 12/30/22 03:35:26.143
    STEP: closing the watch once it receives two notifications 12/30/22 03:35:26.151
    Dec 30 03:35:26.151: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9171  34a16f22-4308-4c0d-b399-1ce05582d001 425170 0 2022-12-30 03:35:26 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2022-12-30 03:35:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Dec 30 03:35:26.152: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9171  34a16f22-4308-4c0d-b399-1ce05582d001 425171 0 2022-12-30 03:35:26 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2022-12-30 03:35:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 12/30/22 03:35:26.152
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 12/30/22 03:35:26.16
    STEP: deleting the configmap 12/30/22 03:35:26.161
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 12/30/22 03:35:26.168
    Dec 30 03:35:26.169: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9171  34a16f22-4308-4c0d-b399-1ce05582d001 425172 0 2022-12-30 03:35:26 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2022-12-30 03:35:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Dec 30 03:35:26.169: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9171  34a16f22-4308-4c0d-b399-1ce05582d001 425173 0 2022-12-30 03:35:26 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2022-12-30 03:35:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Dec 30 03:35:26.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-9171" for this suite. 12/30/22 03:35:26.174
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:152
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:35:26.181
Dec 30 03:35:26.181: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename container-lifecycle-hook 12/30/22 03:35:26.183
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:35:26.196
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:35:26.199
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 12/30/22 03:35:26.208
Dec 30 03:35:26.217: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-8242" to be "running and ready"
Dec 30 03:35:26.221: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.898115ms
Dec 30 03:35:26.221: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Dec 30 03:35:28.227: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009498003s
Dec 30 03:35:28.227: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Dec 30 03:35:30.226: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.008591376s
Dec 30 03:35:30.226: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Dec 30 03:35:30.226: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:152
STEP: create the pod with lifecycle hook 12/30/22 03:35:30.229
Dec 30 03:35:30.236: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-8242" to be "running and ready"
Dec 30 03:35:30.240: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013431ms
Dec 30 03:35:30.240: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Dec 30 03:35:32.245: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009568749s
Dec 30 03:35:32.245: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Dec 30 03:35:34.244: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.008629654s
Dec 30 03:35:34.244: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
Dec 30 03:35:34.244: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 12/30/22 03:35:34.248
Dec 30 03:35:34.256: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 30 03:35:34.261: INFO: Pod pod-with-prestop-http-hook still exists
Dec 30 03:35:36.262: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 30 03:35:36.267: INFO: Pod pod-with-prestop-http-hook still exists
Dec 30 03:35:38.262: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 30 03:35:38.267: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 12/30/22 03:35:38.267
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Dec 30 03:35:38.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8242" for this suite. 12/30/22 03:35:38.293
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","completed":62,"skipped":1287,"failed":0}
------------------------------
• [SLOW TEST] [12.119 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:35:26.181
    Dec 30 03:35:26.181: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename container-lifecycle-hook 12/30/22 03:35:26.183
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:35:26.196
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:35:26.199
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 12/30/22 03:35:26.208
    Dec 30 03:35:26.217: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-8242" to be "running and ready"
    Dec 30 03:35:26.221: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.898115ms
    Dec 30 03:35:26.221: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Dec 30 03:35:28.227: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009498003s
    Dec 30 03:35:28.227: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Dec 30 03:35:30.226: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.008591376s
    Dec 30 03:35:30.226: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Dec 30 03:35:30.226: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:152
    STEP: create the pod with lifecycle hook 12/30/22 03:35:30.229
    Dec 30 03:35:30.236: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-8242" to be "running and ready"
    Dec 30 03:35:30.240: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013431ms
    Dec 30 03:35:30.240: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Dec 30 03:35:32.245: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009568749s
    Dec 30 03:35:32.245: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Dec 30 03:35:34.244: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.008629654s
    Dec 30 03:35:34.244: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    Dec 30 03:35:34.244: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 12/30/22 03:35:34.248
    Dec 30 03:35:34.256: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Dec 30 03:35:34.261: INFO: Pod pod-with-prestop-http-hook still exists
    Dec 30 03:35:36.262: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Dec 30 03:35:36.267: INFO: Pod pod-with-prestop-http-hook still exists
    Dec 30 03:35:38.262: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Dec 30 03:35:38.267: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 12/30/22 03:35:38.267
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Dec 30 03:35:38.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-8242" for this suite. 12/30/22 03:35:38.293
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:35:38.303
Dec 30 03:35:38.303: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename job 12/30/22 03:35:38.306
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:35:38.32
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:35:38.322
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194
STEP: Creating Indexed job 12/30/22 03:35:38.325
STEP: Ensuring job reaches completions 12/30/22 03:35:38.331
STEP: Ensuring pods with index for job exist 12/30/22 03:35:46.335
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Dec 30 03:35:46.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-219" for this suite. 12/30/22 03:35:46.346
{"msg":"PASSED [sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]","completed":63,"skipped":1290,"failed":0}
------------------------------
• [SLOW TEST] [8.050 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:35:38.303
    Dec 30 03:35:38.303: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename job 12/30/22 03:35:38.306
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:35:38.32
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:35:38.322
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:194
    STEP: Creating Indexed job 12/30/22 03:35:38.325
    STEP: Ensuring job reaches completions 12/30/22 03:35:38.331
    STEP: Ensuring pods with index for job exist 12/30/22 03:35:46.335
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Dec 30 03:35:46.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-219" for this suite. 12/30/22 03:35:46.346
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:35:46.353
Dec 30 03:35:46.354: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename svcaccounts 12/30/22 03:35:46.355
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:35:46.368
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:35:46.371
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528
Dec 30 03:35:46.388: INFO: created pod
Dec 30 03:35:46.388: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-6896" to be "Succeeded or Failed"
Dec 30 03:35:46.392: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 3.925642ms
Dec 30 03:35:48.396: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008526533s
Dec 30 03:35:50.397: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009126488s
STEP: Saw pod success 12/30/22 03:35:50.397
Dec 30 03:35:50.397: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Dec 30 03:36:20.401: INFO: polling logs
Dec 30 03:36:20.423: INFO: Pod logs: 
I1230 03:35:47.375311       1 log.go:195] OK: Got token
I1230 03:35:47.375352       1 log.go:195] validating with in-cluster discovery
I1230 03:35:47.375705       1 log.go:195] OK: got issuer https://kubernetes.default.svc.cluster.local
I1230 03:35:47.375737       1 log.go:195] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-6896:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1672371946, NotBefore:1672371346, IssuedAt:1672371346, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-6896", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"c520c1d8-dbf1-4e1a-a8af-9a038f917835"}}}
I1230 03:35:47.391321       1 log.go:195] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
I1230 03:35:47.401395       1 log.go:195] OK: Validated signature on JWT
I1230 03:35:47.401513       1 log.go:195] OK: Got valid claims from token!
I1230 03:35:47.401557       1 log.go:195] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-6896:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1672371946, NotBefore:1672371346, IssuedAt:1672371346, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-6896", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"c520c1d8-dbf1-4e1a-a8af-9a038f917835"}}}

Dec 30 03:36:20.423: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Dec 30 03:36:20.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6896" for this suite. 12/30/22 03:36:20.435
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","completed":64,"skipped":1299,"failed":0}
------------------------------
• [SLOW TEST] [34.088 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:35:46.353
    Dec 30 03:35:46.354: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename svcaccounts 12/30/22 03:35:46.355
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:35:46.368
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:35:46.371
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:528
    Dec 30 03:35:46.388: INFO: created pod
    Dec 30 03:35:46.388: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-6896" to be "Succeeded or Failed"
    Dec 30 03:35:46.392: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 3.925642ms
    Dec 30 03:35:48.396: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008526533s
    Dec 30 03:35:50.397: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009126488s
    STEP: Saw pod success 12/30/22 03:35:50.397
    Dec 30 03:35:50.397: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    Dec 30 03:36:20.401: INFO: polling logs
    Dec 30 03:36:20.423: INFO: Pod logs: 
    I1230 03:35:47.375311       1 log.go:195] OK: Got token
    I1230 03:35:47.375352       1 log.go:195] validating with in-cluster discovery
    I1230 03:35:47.375705       1 log.go:195] OK: got issuer https://kubernetes.default.svc.cluster.local
    I1230 03:35:47.375737       1 log.go:195] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-6896:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1672371946, NotBefore:1672371346, IssuedAt:1672371346, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-6896", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"c520c1d8-dbf1-4e1a-a8af-9a038f917835"}}}
    I1230 03:35:47.391321       1 log.go:195] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
    I1230 03:35:47.401395       1 log.go:195] OK: Validated signature on JWT
    I1230 03:35:47.401513       1 log.go:195] OK: Got valid claims from token!
    I1230 03:35:47.401557       1 log.go:195] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-6896:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1672371946, NotBefore:1672371346, IssuedAt:1672371346, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-6896", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"c520c1d8-dbf1-4e1a-a8af-9a038f917835"}}}

    Dec 30 03:36:20.423: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Dec 30 03:36:20.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-6896" for this suite. 12/30/22 03:36:20.435
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:36:20.443
Dec 30 03:36:20.443: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename projected 12/30/22 03:36:20.445
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:36:20.458
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:36:20.461
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87
STEP: Creating projection with secret that has name projected-secret-test-map-1a9b07e6-5beb-4c04-878d-2339b40f4010 12/30/22 03:36:20.465
STEP: Creating a pod to test consume secrets 12/30/22 03:36:20.469
Dec 30 03:36:20.479: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4a820bd3-d534-4789-9e0b-b1197202b4bd" in namespace "projected-7125" to be "Succeeded or Failed"
Dec 30 03:36:20.483: INFO: Pod "pod-projected-secrets-4a820bd3-d534-4789-9e0b-b1197202b4bd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.88005ms
Dec 30 03:36:22.488: INFO: Pod "pod-projected-secrets-4a820bd3-d534-4789-9e0b-b1197202b4bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009388393s
Dec 30 03:36:24.488: INFO: Pod "pod-projected-secrets-4a820bd3-d534-4789-9e0b-b1197202b4bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009772949s
STEP: Saw pod success 12/30/22 03:36:24.488
Dec 30 03:36:24.489: INFO: Pod "pod-projected-secrets-4a820bd3-d534-4789-9e0b-b1197202b4bd" satisfied condition "Succeeded or Failed"
Dec 30 03:36:24.493: INFO: Trying to get logs from node k8s-mgmt01 pod pod-projected-secrets-4a820bd3-d534-4789-9e0b-b1197202b4bd container projected-secret-volume-test: <nil>
STEP: delete the pod 12/30/22 03:36:24.5
Dec 30 03:36:24.511: INFO: Waiting for pod pod-projected-secrets-4a820bd3-d534-4789-9e0b-b1197202b4bd to disappear
Dec 30 03:36:24.515: INFO: Pod pod-projected-secrets-4a820bd3-d534-4789-9e0b-b1197202b4bd no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Dec 30 03:36:24.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7125" for this suite. 12/30/22 03:36:24.521
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":65,"skipped":1313,"failed":0}
------------------------------
• [4.085 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:36:20.443
    Dec 30 03:36:20.443: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename projected 12/30/22 03:36:20.445
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:36:20.458
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:36:20.461
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:87
    STEP: Creating projection with secret that has name projected-secret-test-map-1a9b07e6-5beb-4c04-878d-2339b40f4010 12/30/22 03:36:20.465
    STEP: Creating a pod to test consume secrets 12/30/22 03:36:20.469
    Dec 30 03:36:20.479: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4a820bd3-d534-4789-9e0b-b1197202b4bd" in namespace "projected-7125" to be "Succeeded or Failed"
    Dec 30 03:36:20.483: INFO: Pod "pod-projected-secrets-4a820bd3-d534-4789-9e0b-b1197202b4bd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.88005ms
    Dec 30 03:36:22.488: INFO: Pod "pod-projected-secrets-4a820bd3-d534-4789-9e0b-b1197202b4bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009388393s
    Dec 30 03:36:24.488: INFO: Pod "pod-projected-secrets-4a820bd3-d534-4789-9e0b-b1197202b4bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009772949s
    STEP: Saw pod success 12/30/22 03:36:24.488
    Dec 30 03:36:24.489: INFO: Pod "pod-projected-secrets-4a820bd3-d534-4789-9e0b-b1197202b4bd" satisfied condition "Succeeded or Failed"
    Dec 30 03:36:24.493: INFO: Trying to get logs from node k8s-mgmt01 pod pod-projected-secrets-4a820bd3-d534-4789-9e0b-b1197202b4bd container projected-secret-volume-test: <nil>
    STEP: delete the pod 12/30/22 03:36:24.5
    Dec 30 03:36:24.511: INFO: Waiting for pod pod-projected-secrets-4a820bd3-d534-4789-9e0b-b1197202b4bd to disappear
    Dec 30 03:36:24.515: INFO: Pod pod-projected-secrets-4a820bd3-d534-4789-9e0b-b1197202b4bd no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Dec 30 03:36:24.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7125" for this suite. 12/30/22 03:36:24.521
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:36:24.529
Dec 30 03:36:24.529: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename services 12/30/22 03:36:24.531
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:36:24.544
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:36:24.547
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443
STEP: creating a service externalname-service with the type=ExternalName in namespace services-7097 12/30/22 03:36:24.55
STEP: changing the ExternalName service to type=NodePort 12/30/22 03:36:24.556
STEP: creating replication controller externalname-service in namespace services-7097 12/30/22 03:36:24.575
I1230 03:36:24.580883      25 runners.go:193] Created replication controller with name: externalname-service, namespace: services-7097, replica count: 2
I1230 03:36:27.632166      25 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 30 03:36:27.632: INFO: Creating new exec pod
Dec 30 03:36:27.638: INFO: Waiting up to 5m0s for pod "execpoddg8kl" in namespace "services-7097" to be "running"
Dec 30 03:36:27.643: INFO: Pod "execpoddg8kl": Phase="Pending", Reason="", readiness=false. Elapsed: 5.173178ms
Dec 30 03:36:29.648: INFO: Pod "execpoddg8kl": Phase="Running", Reason="", readiness=true. Elapsed: 2.010033653s
Dec 30 03:36:29.648: INFO: Pod "execpoddg8kl" satisfied condition "running"
Dec 30 03:36:30.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-7097 exec execpoddg8kl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Dec 30 03:36:30.865: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Dec 30 03:36:30.865: INFO: stdout: "externalname-service-btcd6"
Dec 30 03:36:30.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-7097 exec execpoddg8kl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.53.197 80'
Dec 30 03:36:31.071: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.53.197 80\nConnection to 10.233.53.197 80 port [tcp/http] succeeded!\n"
Dec 30 03:36:31.071: INFO: stdout: "externalname-service-cz27r"
Dec 30 03:36:31.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-7097 exec execpoddg8kl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.78.26.194 30945'
Dec 30 03:36:31.267: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.78.26.194 30945\nConnection to 10.78.26.194 30945 port [tcp/*] succeeded!\n"
Dec 30 03:36:31.267: INFO: stdout: "externalname-service-btcd6"
Dec 30 03:36:31.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-7097 exec execpoddg8kl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.78.26.140 30945'
Dec 30 03:36:31.470: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.78.26.140 30945\nConnection to 10.78.26.140 30945 port [tcp/*] succeeded!\n"
Dec 30 03:36:31.470: INFO: stdout: "externalname-service-btcd6"
Dec 30 03:36:31.470: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Dec 30 03:36:31.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7097" for this suite. 12/30/22 03:36:31.502
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","completed":66,"skipped":1328,"failed":0}
------------------------------
• [SLOW TEST] [6.978 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:36:24.529
    Dec 30 03:36:24.529: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename services 12/30/22 03:36:24.531
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:36:24.544
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:36:24.547
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1443
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-7097 12/30/22 03:36:24.55
    STEP: changing the ExternalName service to type=NodePort 12/30/22 03:36:24.556
    STEP: creating replication controller externalname-service in namespace services-7097 12/30/22 03:36:24.575
    I1230 03:36:24.580883      25 runners.go:193] Created replication controller with name: externalname-service, namespace: services-7097, replica count: 2
    I1230 03:36:27.632166      25 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Dec 30 03:36:27.632: INFO: Creating new exec pod
    Dec 30 03:36:27.638: INFO: Waiting up to 5m0s for pod "execpoddg8kl" in namespace "services-7097" to be "running"
    Dec 30 03:36:27.643: INFO: Pod "execpoddg8kl": Phase="Pending", Reason="", readiness=false. Elapsed: 5.173178ms
    Dec 30 03:36:29.648: INFO: Pod "execpoddg8kl": Phase="Running", Reason="", readiness=true. Elapsed: 2.010033653s
    Dec 30 03:36:29.648: INFO: Pod "execpoddg8kl" satisfied condition "running"
    Dec 30 03:36:30.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-7097 exec execpoddg8kl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Dec 30 03:36:30.865: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Dec 30 03:36:30.865: INFO: stdout: "externalname-service-btcd6"
    Dec 30 03:36:30.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-7097 exec execpoddg8kl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.53.197 80'
    Dec 30 03:36:31.071: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.53.197 80\nConnection to 10.233.53.197 80 port [tcp/http] succeeded!\n"
    Dec 30 03:36:31.071: INFO: stdout: "externalname-service-cz27r"
    Dec 30 03:36:31.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-7097 exec execpoddg8kl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.78.26.194 30945'
    Dec 30 03:36:31.267: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.78.26.194 30945\nConnection to 10.78.26.194 30945 port [tcp/*] succeeded!\n"
    Dec 30 03:36:31.267: INFO: stdout: "externalname-service-btcd6"
    Dec 30 03:36:31.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-7097 exec execpoddg8kl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.78.26.140 30945'
    Dec 30 03:36:31.470: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.78.26.140 30945\nConnection to 10.78.26.140 30945 port [tcp/*] succeeded!\n"
    Dec 30 03:36:31.470: INFO: stdout: "externalname-service-btcd6"
    Dec 30 03:36:31.470: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Dec 30 03:36:31.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-7097" for this suite. 12/30/22 03:36:31.502
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:36:31.508
Dec 30 03:36:31.509: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename gc 12/30/22 03:36:31.51
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:36:31.524
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:36:31.528
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 12/30/22 03:36:31.532
STEP: Wait for the Deployment to create new ReplicaSet 12/30/22 03:36:31.537
STEP: delete the deployment 12/30/22 03:36:32.044
STEP: wait for all rs to be garbage collected 12/30/22 03:36:32.052
STEP: expected 0 rs, got 1 rs 12/30/22 03:36:32.058
STEP: expected 0 pods, got 2 pods 12/30/22 03:36:32.062
STEP: Gathering metrics 12/30/22 03:36:32.576
Dec 30 03:36:32.614: INFO: Waiting up to 5m0s for pod "kube-controller-manager-k8s-mgmt03" in namespace "kube-system" to be "running and ready"
Dec 30 03:36:32.618: INFO: Pod "kube-controller-manager-k8s-mgmt03": Phase="Running", Reason="", readiness=true. Elapsed: 3.801746ms
Dec 30 03:36:32.618: INFO: The phase of Pod kube-controller-manager-k8s-mgmt03 is Running (Ready = true)
Dec 30 03:36:32.618: INFO: Pod "kube-controller-manager-k8s-mgmt03" satisfied condition "running and ready"
Dec 30 03:36:32.721: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Dec 30 03:36:32.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2498" for this suite. 12/30/22 03:36:32.727
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","completed":67,"skipped":1337,"failed":0}
------------------------------
• [1.225 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:36:31.508
    Dec 30 03:36:31.509: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename gc 12/30/22 03:36:31.51
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:36:31.524
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:36:31.528
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 12/30/22 03:36:31.532
    STEP: Wait for the Deployment to create new ReplicaSet 12/30/22 03:36:31.537
    STEP: delete the deployment 12/30/22 03:36:32.044
    STEP: wait for all rs to be garbage collected 12/30/22 03:36:32.052
    STEP: expected 0 rs, got 1 rs 12/30/22 03:36:32.058
    STEP: expected 0 pods, got 2 pods 12/30/22 03:36:32.062
    STEP: Gathering metrics 12/30/22 03:36:32.576
    Dec 30 03:36:32.614: INFO: Waiting up to 5m0s for pod "kube-controller-manager-k8s-mgmt03" in namespace "kube-system" to be "running and ready"
    Dec 30 03:36:32.618: INFO: Pod "kube-controller-manager-k8s-mgmt03": Phase="Running", Reason="", readiness=true. Elapsed: 3.801746ms
    Dec 30 03:36:32.618: INFO: The phase of Pod kube-controller-manager-k8s-mgmt03 is Running (Ready = true)
    Dec 30 03:36:32.618: INFO: Pod "kube-controller-manager-k8s-mgmt03" satisfied condition "running and ready"
    Dec 30 03:36:32.721: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Dec 30 03:36:32.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-2498" for this suite. 12/30/22 03:36:32.727
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:36:32.736
Dec 30 03:36:32.736: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename replication-controller 12/30/22 03:36:32.737
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:36:32.751
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:36:32.754
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82
Dec 30 03:36:32.758: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 12/30/22 03:36:33.77
STEP: Checking rc "condition-test" has the desired failure condition set 12/30/22 03:36:33.775
STEP: Scaling down rc "condition-test" to satisfy pod quota 12/30/22 03:36:34.784
Dec 30 03:36:34.794: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 12/30/22 03:36:34.794
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Dec 30 03:36:35.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6056" for this suite. 12/30/22 03:36:35.808
{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","completed":68,"skipped":1358,"failed":0}
------------------------------
• [3.079 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:36:32.736
    Dec 30 03:36:32.736: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename replication-controller 12/30/22 03:36:32.737
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:36:32.751
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:36:32.754
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:82
    Dec 30 03:36:32.758: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 12/30/22 03:36:33.77
    STEP: Checking rc "condition-test" has the desired failure condition set 12/30/22 03:36:33.775
    STEP: Scaling down rc "condition-test" to satisfy pod quota 12/30/22 03:36:34.784
    Dec 30 03:36:34.794: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 12/30/22 03:36:34.794
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Dec 30 03:36:35.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-6056" for this suite. 12/30/22 03:36:35.808
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:36:35.816
Dec 30 03:36:35.816: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename events 12/30/22 03:36:35.817
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:36:35.83
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:36:35.833
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 12/30/22 03:36:35.836
STEP: get a list of Events with a label in the current namespace 12/30/22 03:36:35.854
STEP: delete a list of events 12/30/22 03:36:35.858
Dec 30 03:36:35.858: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 12/30/22 03:36:35.882
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
Dec 30 03:36:35.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9478" for this suite. 12/30/22 03:36:35.891
{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","completed":69,"skipped":1367,"failed":0}
------------------------------
• [0.082 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:36:35.816
    Dec 30 03:36:35.816: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename events 12/30/22 03:36:35.817
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:36:35.83
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:36:35.833
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 12/30/22 03:36:35.836
    STEP: get a list of Events with a label in the current namespace 12/30/22 03:36:35.854
    STEP: delete a list of events 12/30/22 03:36:35.858
    Dec 30 03:36:35.858: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 12/30/22 03:36:35.882
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:187
    Dec 30 03:36:35.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-9478" for this suite. 12/30/22 03:36:35.891
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:36:35.898
Dec 30 03:36:35.899: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename projected 12/30/22 03:36:35.901
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:36:35.92
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:36:35.923
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173
STEP: Creating configMap with name cm-test-opt-del-f7cf6a05-3431-4bb9-817e-6cc448729f32 12/30/22 03:36:35.932
STEP: Creating configMap with name cm-test-opt-upd-2a91b233-4d93-472d-88ae-dae4cd4abf16 12/30/22 03:36:35.937
STEP: Creating the pod 12/30/22 03:36:35.941
Dec 30 03:36:35.952: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9f3cab27-eb5a-4c67-b650-083e554a7d06" in namespace "projected-274" to be "running and ready"
Dec 30 03:36:35.956: INFO: Pod "pod-projected-configmaps-9f3cab27-eb5a-4c67-b650-083e554a7d06": Phase="Pending", Reason="", readiness=false. Elapsed: 3.989015ms
Dec 30 03:36:35.956: INFO: The phase of Pod pod-projected-configmaps-9f3cab27-eb5a-4c67-b650-083e554a7d06 is Pending, waiting for it to be Running (with Ready = true)
Dec 30 03:36:37.960: INFO: Pod "pod-projected-configmaps-9f3cab27-eb5a-4c67-b650-083e554a7d06": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008001557s
Dec 30 03:36:37.960: INFO: The phase of Pod pod-projected-configmaps-9f3cab27-eb5a-4c67-b650-083e554a7d06 is Pending, waiting for it to be Running (with Ready = true)
Dec 30 03:36:39.962: INFO: Pod "pod-projected-configmaps-9f3cab27-eb5a-4c67-b650-083e554a7d06": Phase="Running", Reason="", readiness=true. Elapsed: 4.00993298s
Dec 30 03:36:39.962: INFO: The phase of Pod pod-projected-configmaps-9f3cab27-eb5a-4c67-b650-083e554a7d06 is Running (Ready = true)
Dec 30 03:36:39.962: INFO: Pod "pod-projected-configmaps-9f3cab27-eb5a-4c67-b650-083e554a7d06" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-f7cf6a05-3431-4bb9-817e-6cc448729f32 12/30/22 03:36:39.99
STEP: Updating configmap cm-test-opt-upd-2a91b233-4d93-472d-88ae-dae4cd4abf16 12/30/22 03:36:39.997
STEP: Creating configMap with name cm-test-opt-create-2a42ba34-2964-4815-9ad6-22a996010b52 12/30/22 03:36:40.001
STEP: waiting to observe update in volume 12/30/22 03:36:40.006
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Dec 30 03:38:10.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-274" for this suite. 12/30/22 03:38:10.472
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":70,"skipped":1373,"failed":0}
------------------------------
• [SLOW TEST] [94.581 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:36:35.898
    Dec 30 03:36:35.899: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename projected 12/30/22 03:36:35.901
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:36:35.92
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:36:35.923
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:173
    STEP: Creating configMap with name cm-test-opt-del-f7cf6a05-3431-4bb9-817e-6cc448729f32 12/30/22 03:36:35.932
    STEP: Creating configMap with name cm-test-opt-upd-2a91b233-4d93-472d-88ae-dae4cd4abf16 12/30/22 03:36:35.937
    STEP: Creating the pod 12/30/22 03:36:35.941
    Dec 30 03:36:35.952: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9f3cab27-eb5a-4c67-b650-083e554a7d06" in namespace "projected-274" to be "running and ready"
    Dec 30 03:36:35.956: INFO: Pod "pod-projected-configmaps-9f3cab27-eb5a-4c67-b650-083e554a7d06": Phase="Pending", Reason="", readiness=false. Elapsed: 3.989015ms
    Dec 30 03:36:35.956: INFO: The phase of Pod pod-projected-configmaps-9f3cab27-eb5a-4c67-b650-083e554a7d06 is Pending, waiting for it to be Running (with Ready = true)
    Dec 30 03:36:37.960: INFO: Pod "pod-projected-configmaps-9f3cab27-eb5a-4c67-b650-083e554a7d06": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008001557s
    Dec 30 03:36:37.960: INFO: The phase of Pod pod-projected-configmaps-9f3cab27-eb5a-4c67-b650-083e554a7d06 is Pending, waiting for it to be Running (with Ready = true)
    Dec 30 03:36:39.962: INFO: Pod "pod-projected-configmaps-9f3cab27-eb5a-4c67-b650-083e554a7d06": Phase="Running", Reason="", readiness=true. Elapsed: 4.00993298s
    Dec 30 03:36:39.962: INFO: The phase of Pod pod-projected-configmaps-9f3cab27-eb5a-4c67-b650-083e554a7d06 is Running (Ready = true)
    Dec 30 03:36:39.962: INFO: Pod "pod-projected-configmaps-9f3cab27-eb5a-4c67-b650-083e554a7d06" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-f7cf6a05-3431-4bb9-817e-6cc448729f32 12/30/22 03:36:39.99
    STEP: Updating configmap cm-test-opt-upd-2a91b233-4d93-472d-88ae-dae4cd4abf16 12/30/22 03:36:39.997
    STEP: Creating configMap with name cm-test-opt-create-2a42ba34-2964-4815-9ad6-22a996010b52 12/30/22 03:36:40.001
    STEP: waiting to observe update in volume 12/30/22 03:36:40.006
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Dec 30 03:38:10.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-274" for this suite. 12/30/22 03:38:10.472
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:38:10.483
Dec 30 03:38:10.483: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename containers 12/30/22 03:38:10.484
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:38:10.497
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:38:10.5
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72
STEP: Creating a pod to test override command 12/30/22 03:38:10.504
Dec 30 03:38:10.512: INFO: Waiting up to 5m0s for pod "client-containers-6b08e626-0e7c-4b34-99c2-882f712f3693" in namespace "containers-2473" to be "Succeeded or Failed"
Dec 30 03:38:10.516: INFO: Pod "client-containers-6b08e626-0e7c-4b34-99c2-882f712f3693": Phase="Pending", Reason="", readiness=false. Elapsed: 4.241896ms
Dec 30 03:38:12.523: INFO: Pod "client-containers-6b08e626-0e7c-4b34-99c2-882f712f3693": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010560665s
Dec 30 03:38:14.522: INFO: Pod "client-containers-6b08e626-0e7c-4b34-99c2-882f712f3693": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010078987s
STEP: Saw pod success 12/30/22 03:38:14.522
Dec 30 03:38:14.523: INFO: Pod "client-containers-6b08e626-0e7c-4b34-99c2-882f712f3693" satisfied condition "Succeeded or Failed"
Dec 30 03:38:14.526: INFO: Trying to get logs from node k8s-mgmt01 pod client-containers-6b08e626-0e7c-4b34-99c2-882f712f3693 container agnhost-container: <nil>
STEP: delete the pod 12/30/22 03:38:14.545
Dec 30 03:38:14.558: INFO: Waiting for pod client-containers-6b08e626-0e7c-4b34-99c2-882f712f3693 to disappear
Dec 30 03:38:14.562: INFO: Pod client-containers-6b08e626-0e7c-4b34-99c2-882f712f3693 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Dec 30 03:38:14.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2473" for this suite. 12/30/22 03:38:14.567
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]","completed":71,"skipped":1407,"failed":0}
------------------------------
• [4.092 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:38:10.483
    Dec 30 03:38:10.483: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename containers 12/30/22 03:38:10.484
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:38:10.497
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:38:10.5
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:72
    STEP: Creating a pod to test override command 12/30/22 03:38:10.504
    Dec 30 03:38:10.512: INFO: Waiting up to 5m0s for pod "client-containers-6b08e626-0e7c-4b34-99c2-882f712f3693" in namespace "containers-2473" to be "Succeeded or Failed"
    Dec 30 03:38:10.516: INFO: Pod "client-containers-6b08e626-0e7c-4b34-99c2-882f712f3693": Phase="Pending", Reason="", readiness=false. Elapsed: 4.241896ms
    Dec 30 03:38:12.523: INFO: Pod "client-containers-6b08e626-0e7c-4b34-99c2-882f712f3693": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010560665s
    Dec 30 03:38:14.522: INFO: Pod "client-containers-6b08e626-0e7c-4b34-99c2-882f712f3693": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010078987s
    STEP: Saw pod success 12/30/22 03:38:14.522
    Dec 30 03:38:14.523: INFO: Pod "client-containers-6b08e626-0e7c-4b34-99c2-882f712f3693" satisfied condition "Succeeded or Failed"
    Dec 30 03:38:14.526: INFO: Trying to get logs from node k8s-mgmt01 pod client-containers-6b08e626-0e7c-4b34-99c2-882f712f3693 container agnhost-container: <nil>
    STEP: delete the pod 12/30/22 03:38:14.545
    Dec 30 03:38:14.558: INFO: Waiting for pod client-containers-6b08e626-0e7c-4b34-99c2-882f712f3693 to disappear
    Dec 30 03:38:14.562: INFO: Pod client-containers-6b08e626-0e7c-4b34-99c2-882f712f3693 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Dec 30 03:38:14.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-2473" for this suite. 12/30/22 03:38:14.567
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:38:14.575
Dec 30 03:38:14.575: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename replication-controller 12/30/22 03:38:14.576
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:38:14.591
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:38:14.595
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100
STEP: Given a ReplicationController is created 12/30/22 03:38:14.598
STEP: When the matched label of one of its pods change 12/30/22 03:38:14.604
Dec 30 03:38:14.607: INFO: Pod name pod-release: Found 0 pods out of 1
Dec 30 03:38:19.613: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 12/30/22 03:38:19.626
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Dec 30 03:38:20.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5018" for this suite. 12/30/22 03:38:20.641
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","completed":72,"skipped":1407,"failed":0}
------------------------------
• [SLOW TEST] [6.074 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:38:14.575
    Dec 30 03:38:14.575: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename replication-controller 12/30/22 03:38:14.576
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:38:14.591
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:38:14.595
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:100
    STEP: Given a ReplicationController is created 12/30/22 03:38:14.598
    STEP: When the matched label of one of its pods change 12/30/22 03:38:14.604
    Dec 30 03:38:14.607: INFO: Pod name pod-release: Found 0 pods out of 1
    Dec 30 03:38:19.613: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 12/30/22 03:38:19.626
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Dec 30 03:38:20.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-5018" for this suite. 12/30/22 03:38:20.641
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:38:20.65
Dec 30 03:38:20.650: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename custom-resource-definition 12/30/22 03:38:20.651
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:38:20.663
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:38:20.667
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
Dec 30 03:38:20.670: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Dec 30 03:38:28.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-752" for this suite. 12/30/22 03:38:28.862
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","completed":73,"skipped":1410,"failed":0}
------------------------------
• [SLOW TEST] [8.220 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:38:20.65
    Dec 30 03:38:20.650: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename custom-resource-definition 12/30/22 03:38:20.651
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:38:20.663
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:38:20.667
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    Dec 30 03:38:20.670: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Dec 30 03:38:28.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-752" for this suite. 12/30/22 03:38:28.862
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:38:28.871
Dec 30 03:38:28.872: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename daemonsets 12/30/22 03:38:28.873
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:38:28.886
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:38:28.889
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293
STEP: Creating a simple DaemonSet "daemon-set" 12/30/22 03:38:28.924
STEP: Check that daemon pods launch on every node of the cluster. 12/30/22 03:38:28.93
Dec 30 03:38:28.938: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 30 03:38:28.938: INFO: Node k8s-mgmt01 is running 0 daemon pod, expected 1
Dec 30 03:38:29.950: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Dec 30 03:38:29.950: INFO: Node k8s-mgmt01 is running 0 daemon pod, expected 1
Dec 30 03:38:30.951: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Dec 30 03:38:30.951: INFO: Node k8s-worker01 is running 0 daemon pod, expected 1
Dec 30 03:38:31.950: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Dec 30 03:38:31.950: INFO: Node k8s-worker01 is running 0 daemon pod, expected 1
Dec 30 03:38:32.951: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Dec 30 03:38:32.951: INFO: Node k8s-worker01 is running 0 daemon pod, expected 1
Dec 30 03:38:33.949: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Dec 30 03:38:33.950: INFO: Node k8s-worker01 is running 0 daemon pod, expected 1
Dec 30 03:38:34.950: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Dec 30 03:38:34.950: INFO: Node k8s-worker01 is running 0 daemon pod, expected 1
Dec 30 03:38:35.949: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Dec 30 03:38:35.949: INFO: Node k8s-worker01 is running 0 daemon pod, expected 1
Dec 30 03:38:36.949: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
Dec 30 03:38:36.949: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 12/30/22 03:38:36.952
Dec 30 03:38:36.978: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Dec 30 03:38:36.978: INFO: Node k8s-mgmt03 is running 0 daemon pod, expected 1
Dec 30 03:38:37.987: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
Dec 30 03:38:37.987: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 12/30/22 03:38:37.987
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 12/30/22 03:38:37.995
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8461, will wait for the garbage collector to delete the pods 12/30/22 03:38:37.995
Dec 30 03:38:38.057: INFO: Deleting DaemonSet.extensions daemon-set took: 7.268988ms
Dec 30 03:38:38.158: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.175721ms
Dec 30 03:38:41.863: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 30 03:38:41.863: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Dec 30 03:38:41.866: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"426479"},"items":null}

Dec 30 03:38:41.870: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"426479"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Dec 30 03:38:41.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8461" for this suite. 12/30/22 03:38:41.901
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","completed":74,"skipped":1421,"failed":0}
------------------------------
• [SLOW TEST] [13.036 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:38:28.871
    Dec 30 03:38:28.872: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename daemonsets 12/30/22 03:38:28.873
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:38:28.886
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:38:28.889
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:293
    STEP: Creating a simple DaemonSet "daemon-set" 12/30/22 03:38:28.924
    STEP: Check that daemon pods launch on every node of the cluster. 12/30/22 03:38:28.93
    Dec 30 03:38:28.938: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 30 03:38:28.938: INFO: Node k8s-mgmt01 is running 0 daemon pod, expected 1
    Dec 30 03:38:29.950: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Dec 30 03:38:29.950: INFO: Node k8s-mgmt01 is running 0 daemon pod, expected 1
    Dec 30 03:38:30.951: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Dec 30 03:38:30.951: INFO: Node k8s-worker01 is running 0 daemon pod, expected 1
    Dec 30 03:38:31.950: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Dec 30 03:38:31.950: INFO: Node k8s-worker01 is running 0 daemon pod, expected 1
    Dec 30 03:38:32.951: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Dec 30 03:38:32.951: INFO: Node k8s-worker01 is running 0 daemon pod, expected 1
    Dec 30 03:38:33.949: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Dec 30 03:38:33.950: INFO: Node k8s-worker01 is running 0 daemon pod, expected 1
    Dec 30 03:38:34.950: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Dec 30 03:38:34.950: INFO: Node k8s-worker01 is running 0 daemon pod, expected 1
    Dec 30 03:38:35.949: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Dec 30 03:38:35.949: INFO: Node k8s-worker01 is running 0 daemon pod, expected 1
    Dec 30 03:38:36.949: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
    Dec 30 03:38:36.949: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 12/30/22 03:38:36.952
    Dec 30 03:38:36.978: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Dec 30 03:38:36.978: INFO: Node k8s-mgmt03 is running 0 daemon pod, expected 1
    Dec 30 03:38:37.987: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
    Dec 30 03:38:37.987: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 12/30/22 03:38:37.987
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 12/30/22 03:38:37.995
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8461, will wait for the garbage collector to delete the pods 12/30/22 03:38:37.995
    Dec 30 03:38:38.057: INFO: Deleting DaemonSet.extensions daemon-set took: 7.268988ms
    Dec 30 03:38:38.158: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.175721ms
    Dec 30 03:38:41.863: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 30 03:38:41.863: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Dec 30 03:38:41.866: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"426479"},"items":null}

    Dec 30 03:38:41.870: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"426479"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Dec 30 03:38:41.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-8461" for this suite. 12/30/22 03:38:41.901
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:38:41.908
Dec 30 03:38:41.909: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename secrets 12/30/22 03:38:41.911
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:38:41.924
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:38:41.928
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45
STEP: Creating secret with name secret-test-3c374cc5-d9c9-4625-8457-f3268fe82951 12/30/22 03:38:41.931
STEP: Creating a pod to test consume secrets 12/30/22 03:38:41.936
Dec 30 03:38:41.944: INFO: Waiting up to 5m0s for pod "pod-secrets-a874da96-0a65-4ef8-97e5-8a638a6ed70c" in namespace "secrets-4388" to be "Succeeded or Failed"
Dec 30 03:38:41.948: INFO: Pod "pod-secrets-a874da96-0a65-4ef8-97e5-8a638a6ed70c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.862752ms
Dec 30 03:38:43.954: INFO: Pod "pod-secrets-a874da96-0a65-4ef8-97e5-8a638a6ed70c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009508893s
Dec 30 03:38:45.953: INFO: Pod "pod-secrets-a874da96-0a65-4ef8-97e5-8a638a6ed70c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008370667s
STEP: Saw pod success 12/30/22 03:38:45.953
Dec 30 03:38:45.953: INFO: Pod "pod-secrets-a874da96-0a65-4ef8-97e5-8a638a6ed70c" satisfied condition "Succeeded or Failed"
Dec 30 03:38:45.957: INFO: Trying to get logs from node k8s-mgmt01 pod pod-secrets-a874da96-0a65-4ef8-97e5-8a638a6ed70c container secret-env-test: <nil>
STEP: delete the pod 12/30/22 03:38:45.966
Dec 30 03:38:45.976: INFO: Waiting for pod pod-secrets-a874da96-0a65-4ef8-97e5-8a638a6ed70c to disappear
Dec 30 03:38:45.980: INFO: Pod pod-secrets-a874da96-0a65-4ef8-97e5-8a638a6ed70c no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Dec 30 03:38:45.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4388" for this suite. 12/30/22 03:38:45.984
{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","completed":75,"skipped":1425,"failed":0}
------------------------------
• [4.083 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:38:41.908
    Dec 30 03:38:41.909: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename secrets 12/30/22 03:38:41.911
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:38:41.924
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:38:41.928
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:45
    STEP: Creating secret with name secret-test-3c374cc5-d9c9-4625-8457-f3268fe82951 12/30/22 03:38:41.931
    STEP: Creating a pod to test consume secrets 12/30/22 03:38:41.936
    Dec 30 03:38:41.944: INFO: Waiting up to 5m0s for pod "pod-secrets-a874da96-0a65-4ef8-97e5-8a638a6ed70c" in namespace "secrets-4388" to be "Succeeded or Failed"
    Dec 30 03:38:41.948: INFO: Pod "pod-secrets-a874da96-0a65-4ef8-97e5-8a638a6ed70c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.862752ms
    Dec 30 03:38:43.954: INFO: Pod "pod-secrets-a874da96-0a65-4ef8-97e5-8a638a6ed70c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009508893s
    Dec 30 03:38:45.953: INFO: Pod "pod-secrets-a874da96-0a65-4ef8-97e5-8a638a6ed70c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008370667s
    STEP: Saw pod success 12/30/22 03:38:45.953
    Dec 30 03:38:45.953: INFO: Pod "pod-secrets-a874da96-0a65-4ef8-97e5-8a638a6ed70c" satisfied condition "Succeeded or Failed"
    Dec 30 03:38:45.957: INFO: Trying to get logs from node k8s-mgmt01 pod pod-secrets-a874da96-0a65-4ef8-97e5-8a638a6ed70c container secret-env-test: <nil>
    STEP: delete the pod 12/30/22 03:38:45.966
    Dec 30 03:38:45.976: INFO: Waiting for pod pod-secrets-a874da96-0a65-4ef8-97e5-8a638a6ed70c to disappear
    Dec 30 03:38:45.980: INFO: Pod pod-secrets-a874da96-0a65-4ef8-97e5-8a638a6ed70c no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Dec 30 03:38:45.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-4388" for this suite. 12/30/22 03:38:45.984
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:38:45.992
Dec 30 03:38:45.992: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename webhook 12/30/22 03:38:45.994
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:38:46.006
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:38:46.01
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 12/30/22 03:38:46.026
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/30/22 03:38:46.596
STEP: Deploying the webhook pod 12/30/22 03:38:46.604
STEP: Wait for the deployment to be ready 12/30/22 03:38:46.616
Dec 30 03:38:46.641: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 12/30/22 03:38:48.66
STEP: Verifying the service has paired with the endpoint 12/30/22 03:38:48.671
Dec 30 03:38:49.671: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 12/30/22 03:38:49.675
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 12/30/22 03:38:49.694
STEP: Creating a dummy validating-webhook-configuration object 12/30/22 03:38:49.711
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 12/30/22 03:38:49.723
STEP: Creating a dummy mutating-webhook-configuration object 12/30/22 03:38:49.729
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 12/30/22 03:38:49.737
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Dec 30 03:38:49.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9282" for this suite. 12/30/22 03:38:49.762
STEP: Destroying namespace "webhook-9282-markers" for this suite. 12/30/22 03:38:49.769
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","completed":76,"skipped":1430,"failed":0}
------------------------------
• [3.819 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:38:45.992
    Dec 30 03:38:45.992: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename webhook 12/30/22 03:38:45.994
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:38:46.006
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:38:46.01
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 12/30/22 03:38:46.026
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/30/22 03:38:46.596
    STEP: Deploying the webhook pod 12/30/22 03:38:46.604
    STEP: Wait for the deployment to be ready 12/30/22 03:38:46.616
    Dec 30 03:38:46.641: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 12/30/22 03:38:48.66
    STEP: Verifying the service has paired with the endpoint 12/30/22 03:38:48.671
    Dec 30 03:38:49.671: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:276
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 12/30/22 03:38:49.675
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 12/30/22 03:38:49.694
    STEP: Creating a dummy validating-webhook-configuration object 12/30/22 03:38:49.711
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 12/30/22 03:38:49.723
    STEP: Creating a dummy mutating-webhook-configuration object 12/30/22 03:38:49.729
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 12/30/22 03:38:49.737
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Dec 30 03:38:49.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-9282" for this suite. 12/30/22 03:38:49.762
    STEP: Destroying namespace "webhook-9282-markers" for this suite. 12/30/22 03:38:49.769
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:38:49.812
Dec 30 03:38:49.812: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename custom-resource-definition 12/30/22 03:38:49.814
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:38:49.827
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:38:49.83
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
Dec 30 03:38:49.833: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Dec 30 03:38:55.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4357" for this suite. 12/30/22 03:38:55.859
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","completed":77,"skipped":1439,"failed":0}
------------------------------
• [SLOW TEST] [6.054 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:38:49.812
    Dec 30 03:38:49.812: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename custom-resource-definition 12/30/22 03:38:49.814
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:38:49.827
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:38:49.83
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    Dec 30 03:38:49.833: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Dec 30 03:38:55.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-4357" for this suite. 12/30/22 03:38:55.859
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:846
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:38:55.868
Dec 30 03:38:55.868: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename statefulset 12/30/22 03:38:55.87
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:38:55.884
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:38:55.887
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-8565 12/30/22 03:38:55.89
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:846
STEP: Creating statefulset ss in namespace statefulset-8565 12/30/22 03:38:55.896
Dec 30 03:38:55.907: INFO: Found 0 stateful pods, waiting for 1
Dec 30 03:39:05.912: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 12/30/22 03:39:05.92
STEP: updating a scale subresource 12/30/22 03:39:05.924
STEP: verifying the statefulset Spec.Replicas was modified 12/30/22 03:39:05.93
STEP: Patch a scale subresource 12/30/22 03:39:05.934
STEP: verifying the statefulset Spec.Replicas was modified 12/30/22 03:39:05.944
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Dec 30 03:39:05.947: INFO: Deleting all statefulset in ns statefulset-8565
Dec 30 03:39:05.951: INFO: Scaling statefulset ss to 0
Dec 30 03:39:15.971: INFO: Waiting for statefulset status.replicas updated to 0
Dec 30 03:39:15.975: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Dec 30 03:39:15.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8565" for this suite. 12/30/22 03:39:15.995
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","completed":78,"skipped":1468,"failed":0}
------------------------------
• [SLOW TEST] [20.134 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:846

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:38:55.868
    Dec 30 03:38:55.868: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename statefulset 12/30/22 03:38:55.87
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:38:55.884
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:38:55.887
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-8565 12/30/22 03:38:55.89
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:846
    STEP: Creating statefulset ss in namespace statefulset-8565 12/30/22 03:38:55.896
    Dec 30 03:38:55.907: INFO: Found 0 stateful pods, waiting for 1
    Dec 30 03:39:05.912: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 12/30/22 03:39:05.92
    STEP: updating a scale subresource 12/30/22 03:39:05.924
    STEP: verifying the statefulset Spec.Replicas was modified 12/30/22 03:39:05.93
    STEP: Patch a scale subresource 12/30/22 03:39:05.934
    STEP: verifying the statefulset Spec.Replicas was modified 12/30/22 03:39:05.944
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Dec 30 03:39:05.947: INFO: Deleting all statefulset in ns statefulset-8565
    Dec 30 03:39:05.951: INFO: Scaling statefulset ss to 0
    Dec 30 03:39:15.971: INFO: Waiting for statefulset status.replicas updated to 0
    Dec 30 03:39:15.975: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Dec 30 03:39:15.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-8565" for this suite. 12/30/22 03:39:15.995
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:39:16.004
Dec 30 03:39:16.004: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename kubelet-test 12/30/22 03:39:16.005
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:39:16.019
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:39:16.021
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Dec 30 03:39:16.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8105" for this suite. 12/30/22 03:39:16.054
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","completed":79,"skipped":1473,"failed":0}
------------------------------
• [0.057 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:39:16.004
    Dec 30 03:39:16.004: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename kubelet-test 12/30/22 03:39:16.005
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:39:16.019
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:39:16.021
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Dec 30 03:39:16.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-8105" for this suite. 12/30/22 03:39:16.054
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:39:16.061
Dec 30 03:39:16.062: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename gc 12/30/22 03:39:16.064
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:39:16.076
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:39:16.079
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 12/30/22 03:39:16.088
STEP: delete the rc 12/30/22 03:39:21.099
STEP: wait for the rc to be deleted 12/30/22 03:39:21.106
Dec 30 03:39:22.127: INFO: 80 pods remaining
Dec 30 03:39:22.127: INFO: 80 pods has nil DeletionTimestamp
Dec 30 03:39:22.127: INFO: 
Dec 30 03:39:23.125: INFO: 71 pods remaining
Dec 30 03:39:23.125: INFO: 71 pods has nil DeletionTimestamp
Dec 30 03:39:23.125: INFO: 
Dec 30 03:39:24.125: INFO: 60 pods remaining
Dec 30 03:39:24.125: INFO: 60 pods has nil DeletionTimestamp
Dec 30 03:39:24.125: INFO: 
Dec 30 03:39:25.118: INFO: 40 pods remaining
Dec 30 03:39:25.118: INFO: 40 pods has nil DeletionTimestamp
Dec 30 03:39:25.118: INFO: 
Dec 30 03:39:26.121: INFO: 31 pods remaining
Dec 30 03:39:26.121: INFO: 30 pods has nil DeletionTimestamp
Dec 30 03:39:26.121: INFO: 
Dec 30 03:39:27.118: INFO: 20 pods remaining
Dec 30 03:39:27.118: INFO: 20 pods has nil DeletionTimestamp
Dec 30 03:39:27.118: INFO: 
STEP: Gathering metrics 12/30/22 03:39:28.115
Dec 30 03:39:28.156: INFO: Waiting up to 5m0s for pod "kube-controller-manager-k8s-mgmt03" in namespace "kube-system" to be "running and ready"
Dec 30 03:39:28.160: INFO: Pod "kube-controller-manager-k8s-mgmt03": Phase="Running", Reason="", readiness=true. Elapsed: 4.036142ms
Dec 30 03:39:28.160: INFO: The phase of Pod kube-controller-manager-k8s-mgmt03 is Running (Ready = true)
Dec 30 03:39:28.160: INFO: Pod "kube-controller-manager-k8s-mgmt03" satisfied condition "running and ready"
Dec 30 03:39:28.256: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Dec 30 03:39:28.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9961" for this suite. 12/30/22 03:39:28.261
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","completed":80,"skipped":1479,"failed":0}
------------------------------
• [SLOW TEST] [12.207 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:39:16.061
    Dec 30 03:39:16.062: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename gc 12/30/22 03:39:16.064
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:39:16.076
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:39:16.079
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 12/30/22 03:39:16.088
    STEP: delete the rc 12/30/22 03:39:21.099
    STEP: wait for the rc to be deleted 12/30/22 03:39:21.106
    Dec 30 03:39:22.127: INFO: 80 pods remaining
    Dec 30 03:39:22.127: INFO: 80 pods has nil DeletionTimestamp
    Dec 30 03:39:22.127: INFO: 
    Dec 30 03:39:23.125: INFO: 71 pods remaining
    Dec 30 03:39:23.125: INFO: 71 pods has nil DeletionTimestamp
    Dec 30 03:39:23.125: INFO: 
    Dec 30 03:39:24.125: INFO: 60 pods remaining
    Dec 30 03:39:24.125: INFO: 60 pods has nil DeletionTimestamp
    Dec 30 03:39:24.125: INFO: 
    Dec 30 03:39:25.118: INFO: 40 pods remaining
    Dec 30 03:39:25.118: INFO: 40 pods has nil DeletionTimestamp
    Dec 30 03:39:25.118: INFO: 
    Dec 30 03:39:26.121: INFO: 31 pods remaining
    Dec 30 03:39:26.121: INFO: 30 pods has nil DeletionTimestamp
    Dec 30 03:39:26.121: INFO: 
    Dec 30 03:39:27.118: INFO: 20 pods remaining
    Dec 30 03:39:27.118: INFO: 20 pods has nil DeletionTimestamp
    Dec 30 03:39:27.118: INFO: 
    STEP: Gathering metrics 12/30/22 03:39:28.115
    Dec 30 03:39:28.156: INFO: Waiting up to 5m0s for pod "kube-controller-manager-k8s-mgmt03" in namespace "kube-system" to be "running and ready"
    Dec 30 03:39:28.160: INFO: Pod "kube-controller-manager-k8s-mgmt03": Phase="Running", Reason="", readiness=true. Elapsed: 4.036142ms
    Dec 30 03:39:28.160: INFO: The phase of Pod kube-controller-manager-k8s-mgmt03 is Running (Ready = true)
    Dec 30 03:39:28.160: INFO: Pod "kube-controller-manager-k8s-mgmt03" satisfied condition "running and ready"
    Dec 30 03:39:28.256: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Dec 30 03:39:28.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-9961" for this suite. 12/30/22 03:39:28.261
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:39:28.27
Dec 30 03:39:28.270: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename deployment 12/30/22 03:39:28.271
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:39:28.284
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:39:28.287
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
Dec 30 03:39:28.290: INFO: Creating deployment "webserver-deployment"
Dec 30 03:39:28.296: INFO: Waiting for observed generation 1
Dec 30 03:39:30.305: INFO: Waiting for all required pods to come up
Dec 30 03:39:30.311: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 12/30/22 03:39:30.311
Dec 30 03:39:30.311: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-76xs2" in namespace "deployment-5386" to be "running"
Dec 30 03:39:30.311: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-v4skw" in namespace "deployment-5386" to be "running"
Dec 30 03:39:30.311: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-gsvv8" in namespace "deployment-5386" to be "running"
Dec 30 03:39:30.311: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-289dk" in namespace "deployment-5386" to be "running"
Dec 30 03:39:30.312: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-rfnkb" in namespace "deployment-5386" to be "running"
Dec 30 03:39:30.312: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-d4986" in namespace "deployment-5386" to be "running"
Dec 30 03:39:30.312: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-n6b52" in namespace "deployment-5386" to be "running"
Dec 30 03:39:30.312: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-rjztj" in namespace "deployment-5386" to be "running"
Dec 30 03:39:30.312: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-7cfpw" in namespace "deployment-5386" to be "running"
Dec 30 03:39:30.315: INFO: Pod "webserver-deployment-845c8977d9-76xs2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041188ms
Dec 30 03:39:30.316: INFO: Pod "webserver-deployment-845c8977d9-rfnkb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034897ms
Dec 30 03:39:30.316: INFO: Pod "webserver-deployment-845c8977d9-v4skw": Phase="Pending", Reason="", readiness=false. Elapsed: 4.261698ms
Dec 30 03:39:30.316: INFO: Pod "webserver-deployment-845c8977d9-gsvv8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.402541ms
Dec 30 03:39:30.316: INFO: Pod "webserver-deployment-845c8977d9-d4986": Phase="Pending", Reason="", readiness=false. Elapsed: 4.363307ms
Dec 30 03:39:30.317: INFO: Pod "webserver-deployment-845c8977d9-7cfpw": Phase="Pending", Reason="", readiness=false. Elapsed: 4.752076ms
Dec 30 03:39:30.317: INFO: Pod "webserver-deployment-845c8977d9-n6b52": Phase="Pending", Reason="", readiness=false. Elapsed: 4.975199ms
Dec 30 03:39:30.317: INFO: Pod "webserver-deployment-845c8977d9-rjztj": Phase="Pending", Reason="", readiness=false. Elapsed: 4.945697ms
Dec 30 03:39:30.317: INFO: Pod "webserver-deployment-845c8977d9-289dk": Phase="Pending", Reason="", readiness=false. Elapsed: 5.176271ms
Dec 30 03:39:32.321: INFO: Pod "webserver-deployment-845c8977d9-gsvv8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009396634s
Dec 30 03:39:32.321: INFO: Pod "webserver-deployment-845c8977d9-v4skw": Phase="Running", Reason="", readiness=true. Elapsed: 2.009882011s
Dec 30 03:39:32.321: INFO: Pod "webserver-deployment-845c8977d9-v4skw" satisfied condition "running"
Dec 30 03:39:32.321: INFO: Pod "webserver-deployment-845c8977d9-d4986": Phase="Running", Reason="", readiness=true. Elapsed: 2.009820978s
Dec 30 03:39:32.321: INFO: Pod "webserver-deployment-845c8977d9-d4986" satisfied condition "running"
Dec 30 03:39:32.321: INFO: Pod "webserver-deployment-845c8977d9-rfnkb": Phase="Running", Reason="", readiness=true. Elapsed: 2.009859769s
Dec 30 03:39:32.321: INFO: Pod "webserver-deployment-845c8977d9-rfnkb" satisfied condition "running"
Dec 30 03:39:32.321: INFO: Pod "webserver-deployment-845c8977d9-289dk": Phase="Running", Reason="", readiness=true. Elapsed: 2.009956289s
Dec 30 03:39:32.322: INFO: Pod "webserver-deployment-845c8977d9-289dk" satisfied condition "running"
Dec 30 03:39:32.322: INFO: Pod "webserver-deployment-845c8977d9-n6b52": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010761802s
Dec 30 03:39:32.322: INFO: Pod "webserver-deployment-845c8977d9-76xs2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01108958s
Dec 30 03:39:32.322: INFO: Pod "webserver-deployment-845c8977d9-rjztj": Phase="Running", Reason="", readiness=true. Elapsed: 2.01078226s
Dec 30 03:39:32.323: INFO: Pod "webserver-deployment-845c8977d9-rjztj" satisfied condition "running"
Dec 30 03:39:32.322: INFO: Pod "webserver-deployment-845c8977d9-7cfpw": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010737348s
Dec 30 03:39:34.320: INFO: Pod "webserver-deployment-845c8977d9-gsvv8": Phase="Running", Reason="", readiness=true. Elapsed: 4.008540389s
Dec 30 03:39:34.320: INFO: Pod "webserver-deployment-845c8977d9-gsvv8" satisfied condition "running"
Dec 30 03:39:34.320: INFO: Pod "webserver-deployment-845c8977d9-n6b52": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008555226s
Dec 30 03:39:34.321: INFO: Pod "webserver-deployment-845c8977d9-76xs2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009291553s
Dec 30 03:39:34.321: INFO: Pod "webserver-deployment-845c8977d9-7cfpw": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009025823s
Dec 30 03:39:36.320: INFO: Pod "webserver-deployment-845c8977d9-76xs2": Phase="Running", Reason="", readiness=true. Elapsed: 6.008522823s
Dec 30 03:39:36.320: INFO: Pod "webserver-deployment-845c8977d9-76xs2" satisfied condition "running"
Dec 30 03:39:36.321: INFO: Pod "webserver-deployment-845c8977d9-7cfpw": Phase="Running", Reason="", readiness=true. Elapsed: 6.009344701s
Dec 30 03:39:36.321: INFO: Pod "webserver-deployment-845c8977d9-n6b52": Phase="Running", Reason="", readiness=true. Elapsed: 6.009514563s
Dec 30 03:39:36.321: INFO: Pod "webserver-deployment-845c8977d9-n6b52" satisfied condition "running"
Dec 30 03:39:36.321: INFO: Pod "webserver-deployment-845c8977d9-7cfpw" satisfied condition "running"
Dec 30 03:39:36.321: INFO: Waiting for deployment "webserver-deployment" to complete
Dec 30 03:39:36.329: INFO: Updating deployment "webserver-deployment" with a non-existent image
Dec 30 03:39:36.339: INFO: Updating deployment webserver-deployment
Dec 30 03:39:36.339: INFO: Waiting for observed generation 2
Dec 30 03:39:38.348: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Dec 30 03:39:38.352: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Dec 30 03:39:38.356: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Dec 30 03:39:38.367: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Dec 30 03:39:38.367: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Dec 30 03:39:38.371: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Dec 30 03:39:38.377: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Dec 30 03:39:38.377: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Dec 30 03:39:38.389: INFO: Updating deployment webserver-deployment
Dec 30 03:39:38.389: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Dec 30 03:39:38.396: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Dec 30 03:39:38.400: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Dec 30 03:39:40.413: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-5386  b16626a2-dae6-4abe-b294-25710dafc4ad 429111 3 2022-12-30 03:39:28 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2022-12-30 03:39:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-30 03:39:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004524008 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:12,UnavailableReplicas:21,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-12-30 03:39:38 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-69b7448995" is progressing.,LastUpdateTime:2022-12-30 03:39:40 +0000 UTC,LastTransitionTime:2022-12-30 03:39:28 +0000 UTC,},},ReadyReplicas:12,CollisionCount:nil,},}

Dec 30 03:39:40.418: INFO: New ReplicaSet "webserver-deployment-69b7448995" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-69b7448995  deployment-5386  133cfa6c-6a52-4d81-bc64-6854c375125c 428976 3 2022-12-30 03:39:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment b16626a2-dae6-4abe-b294-25710dafc4ad 0xc004524447 0xc004524448}] [] [{kube-controller-manager Update apps/v1 2022-12-30 03:39:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b16626a2-dae6-4abe-b294-25710dafc4ad\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-30 03:39:38 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 69b7448995,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0045244e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 30 03:39:40.418: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Dec 30 03:39:40.418: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-845c8977d9  deployment-5386  3bfb6c67-ec49-43b7-8061-b53d2c4e367c 429109 3 2022-12-30 03:39:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment b16626a2-dae6-4abe-b294-25710dafc4ad 0xc004524547 0xc004524548}] [] [{kube-controller-manager Update apps/v1 2022-12-30 03:39:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b16626a2-dae6-4abe-b294-25710dafc4ad\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-30 03:39:40 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0045245d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:12,AvailableReplicas:12,Conditions:[]ReplicaSetCondition{},},}
Dec 30 03:39:40.430: INFO: Pod "webserver-deployment-69b7448995-46pb6" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-46pb6 webserver-deployment-69b7448995- deployment-5386  4a4ef635-7eb1-46c2-9ad2-ef4f57b987b9 429100 0 2022-12-30 03:39:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:ae541f17714e026d3257d3e8c0ad0ee91393931446981398bf55403a1b7701ef cni.projectcalico.org/podIP:10.233.125.245/32 cni.projectcalico.org/podIPs:10.233.125.245/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 133cfa6c-6a52-4d81-bc64-6854c375125c 0xc004524aa7 0xc004524aa8}] [] [{kube-controller-manager Update v1 2022-12-30 03:39:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"133cfa6c-6a52-4d81-bc64-6854c375125c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-12-30 03:39:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6fdh6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6fdh6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-mgmt02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 30 03:39:40.430: INFO: Pod "webserver-deployment-69b7448995-6624g" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-6624g webserver-deployment-69b7448995- deployment-5386  6e5b14d2-cafb-44fd-9f2f-0ea26b2d35e3 428881 0 2022-12-30 03:39:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:47858ef53c22222a47b2a91c77aae5cf200ee2727cf224034b34263f8ebae536 cni.projectcalico.org/podIP:10.233.125.240/32 cni.projectcalico.org/podIPs:10.233.125.240/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 133cfa6c-6a52-4d81-bc64-6854c375125c 0xc004524c40 0xc004524c41}] [] [{kube-controller-manager Update v1 2022-12-30 03:39:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"133cfa6c-6a52-4d81-bc64-6854c375125c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-30 03:39:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2022-12-30 03:39:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7cxpp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7cxpp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-mgmt02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.141,PodIP:,StartTime:2022-12-30 03:39:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 30 03:39:40.431: INFO: Pod "webserver-deployment-69b7448995-966sq" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-966sq webserver-deployment-69b7448995- deployment-5386  de85a217-1594-4a00-9fd7-03232251f2b8 428987 0 2022-12-30 03:39:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 133cfa6c-6a52-4d81-bc64-6854c375125c 0xc004524e57 0xc004524e58}] [] [{kube-controller-manager Update v1 2022-12-30 03:39:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"133cfa6c-6a52-4d81-bc64-6854c375125c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-30 03:39:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-65tcd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-65tcd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.195,PodIP:,StartTime:2022-12-30 03:39:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 30 03:39:40.432: INFO: Pod "webserver-deployment-69b7448995-bfdjl" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-bfdjl webserver-deployment-69b7448995- deployment-5386  82d13265-e228-4106-9f01-588636a9d201 429104 0 2022-12-30 03:39:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:83509c7a6ca18a435ae57948a4e197a510a6b6a58249f44159c65eb60eeb7c1c cni.projectcalico.org/podIP:10.233.78.176/32 cni.projectcalico.org/podIPs:10.233.78.176/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 133cfa6c-6a52-4d81-bc64-6854c375125c 0xc004525067 0xc004525068}] [] [{Go-http-client Update v1 2022-12-30 03:39:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-12-30 03:39:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"133cfa6c-6a52-4d81-bc64-6854c375125c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-30 03:39:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.78.176\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-24vx7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-24vx7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-mgmt03,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.142,PodIP:10.233.78.176,StartTime:2022-12-30 03:39:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.78.176,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 30 03:39:40.433: INFO: Pod "webserver-deployment-69b7448995-fbf8r" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-fbf8r webserver-deployment-69b7448995- deployment-5386  d00a4cd8-b78e-495a-9b80-90106c5e38ea 428895 0 2022-12-30 03:39:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:08a0e407abd49938aa8a1f2ab8251bcae485a50f2129d0b77476957b0ab4d56a cni.projectcalico.org/podIP:10.233.109.91/32 cni.projectcalico.org/podIPs:10.233.109.91/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 133cfa6c-6a52-4d81-bc64-6854c375125c 0xc0045252c7 0xc0045252c8}] [] [{kube-controller-manager Update v1 2022-12-30 03:39:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"133cfa6c-6a52-4d81-bc64-6854c375125c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-30 03:39:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2022-12-30 03:39:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wn62f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wn62f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.195,PodIP:,StartTime:2022-12-30 03:39:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 30 03:39:40.433: INFO: Pod "webserver-deployment-69b7448995-fcgfn" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-fcgfn webserver-deployment-69b7448995- deployment-5386  9454780f-4fb5-4b7c-82e2-45dbc218a23a 429024 0 2022-12-30 03:39:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:6d830c1962e81e60f7111797af3d4d2b3ea6c9bec19183eed39d6ed019e9e10f cni.projectcalico.org/podIP:10.233.112.154/32 cni.projectcalico.org/podIPs:10.233.112.154/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 133cfa6c-6a52-4d81-bc64-6854c375125c 0xc0045254d7 0xc0045254d8}] [] [{kube-controller-manager Update v1 2022-12-30 03:39:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"133cfa6c-6a52-4d81-bc64-6854c375125c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-30 03:39:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2022-12-30 03:39:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6m5ql,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6m5ql,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-mgmt01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.140,PodIP:,StartTime:2022-12-30 03:39:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 30 03:39:40.434: INFO: Pod "webserver-deployment-69b7448995-gb4gj" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-gb4gj webserver-deployment-69b7448995- deployment-5386  85ff0456-2e05-4222-98d4-ee3668df62e9 429053 0 2022-12-30 03:39:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:5351489992783bae9657bf9007b7d8271ba00680be995a6a02e503a18eebf356 cni.projectcalico.org/podIP:10.233.125.241/32 cni.projectcalico.org/podIPs:10.233.125.241/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 133cfa6c-6a52-4d81-bc64-6854c375125c 0xc0045256e7 0xc0045256e8}] [] [{kube-controller-manager Update v1 2022-12-30 03:39:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"133cfa6c-6a52-4d81-bc64-6854c375125c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-30 03:39:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2022-12-30 03:39:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2ngv2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2ngv2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-mgmt02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.141,PodIP:,StartTime:2022-12-30 03:39:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 30 03:39:40.435: INFO: Pod "webserver-deployment-69b7448995-hdl7c" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-hdl7c webserver-deployment-69b7448995- deployment-5386  c3608c45-ccc9-42c1-8635-ea383a43dd11 428986 0 2022-12-30 03:39:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 133cfa6c-6a52-4d81-bc64-6854c375125c 0xc0045258f7 0xc0045258f8}] [] [{kube-controller-manager Update v1 2022-12-30 03:39:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"133cfa6c-6a52-4d81-bc64-6854c375125c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-30 03:39:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ctfs6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ctfs6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.194,PodIP:,StartTime:2022-12-30 03:39:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 30 03:39:40.436: INFO: Pod "webserver-deployment-69b7448995-nbrzz" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-nbrzz webserver-deployment-69b7448995- deployment-5386  f5269cf5-c1fb-46cd-a57b-6598a5783383 429027 0 2022-12-30 03:39:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:4600302ee68bb064322a1fca809ca120b9b4ddc6cabee840e07ef2d08b3a84a0 cni.projectcalico.org/podIP:10.233.78.178/32 cni.projectcalico.org/podIPs:10.233.78.178/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 133cfa6c-6a52-4d81-bc64-6854c375125c 0xc004525b07 0xc004525b08}] [] [{kube-controller-manager Update v1 2022-12-30 03:39:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"133cfa6c-6a52-4d81-bc64-6854c375125c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-30 03:39:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2022-12-30 03:39:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wht6k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wht6k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-mgmt03,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.142,PodIP:,StartTime:2022-12-30 03:39:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 30 03:39:40.437: INFO: Pod "webserver-deployment-69b7448995-pf44w" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-pf44w webserver-deployment-69b7448995- deployment-5386  e002ab97-7c35-45c7-b0db-ce02b4eedcf3 429014 0 2022-12-30 03:39:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:79824f2c8d1946c1fe064b3e85260e3a85c5048ba2a602eb0b19b3a1e5d0251f cni.projectcalico.org/podIP:10.233.112.152/32 cni.projectcalico.org/podIPs:10.233.112.152/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 133cfa6c-6a52-4d81-bc64-6854c375125c 0xc004525d17 0xc004525d18}] [] [{kube-controller-manager Update v1 2022-12-30 03:39:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"133cfa6c-6a52-4d81-bc64-6854c375125c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-30 03:39:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2022-12-30 03:39:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-72fpg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-72fpg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-mgmt01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.140,PodIP:,StartTime:2022-12-30 03:39:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 30 03:39:40.437: INFO: Pod "webserver-deployment-69b7448995-rbzm2" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-rbzm2 webserver-deployment-69b7448995- deployment-5386  12a43f94-cb37-42ad-a606-99d50be0c576 429002 0 2022-12-30 03:39:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:a281c9fec8f8a253ade446964163047739c26fb2656f51fdcd0c50a1642098de cni.projectcalico.org/podIP:10.233.79.92/32 cni.projectcalico.org/podIPs:10.233.79.92/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 133cfa6c-6a52-4d81-bc64-6854c375125c 0xc004525f47 0xc004525f48}] [] [{kube-controller-manager Update v1 2022-12-30 03:39:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"133cfa6c-6a52-4d81-bc64-6854c375125c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-30 03:39:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2022-12-30 03:39:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-985r4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-985r4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.194,PodIP:,StartTime:2022-12-30 03:39:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 30 03:39:40.437: INFO: Pod "webserver-deployment-69b7448995-x8hcw" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-x8hcw webserver-deployment-69b7448995- deployment-5386  fa20d727-41c0-453a-bd5c-358403a93a40 428878 0 2022-12-30 03:39:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:a9a9ef1a7294415021a63cf463cf2e16f9448c02c3cc0dc11b751f75e09443b7 cni.projectcalico.org/podIP:10.233.112.151/32 cni.projectcalico.org/podIPs:10.233.112.151/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 133cfa6c-6a52-4d81-bc64-6854c375125c 0xc004678157 0xc004678158}] [] [{Go-http-client Update v1 2022-12-30 03:39:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-12-30 03:39:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"133cfa6c-6a52-4d81-bc64-6854c375125c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-30 03:39:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-htdsx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-htdsx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-mgmt01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.140,PodIP:,StartTime:2022-12-30 03:39:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 30 03:39:40.438: INFO: Pod "webserver-deployment-69b7448995-zgwsn" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-zgwsn webserver-deployment-69b7448995- deployment-5386  c530c195-c6af-4ffa-a4e4-404011050f0c 429051 0 2022-12-30 03:39:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:438c6cfdf8dd26452035992232819207efaac0e1ef51fe6af95aeb30254fa289 cni.projectcalico.org/podIP:10.233.78.180/32 cni.projectcalico.org/podIPs:10.233.78.180/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 133cfa6c-6a52-4d81-bc64-6854c375125c 0xc004678387 0xc004678388}] [] [{kube-controller-manager Update v1 2022-12-30 03:39:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"133cfa6c-6a52-4d81-bc64-6854c375125c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-30 03:39:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2022-12-30 03:39:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2cwxm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2cwxm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-mgmt03,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.142,PodIP:,StartTime:2022-12-30 03:39:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 30 03:39:40.438: INFO: Pod "webserver-deployment-845c8977d9-289dk" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-289dk webserver-deployment-845c8977d9- deployment-5386  5ed34e22-e152-48c6-902c-2f4971fa8dcc 428373 0 2022-12-30 03:39:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:4468a59ac7306978b09cc6c1e48bd10a8d74fd2f0f5f5dcde40ef05f30523210 cni.projectcalico.org/podIP:10.233.78.175/32 cni.projectcalico.org/podIPs:10.233.78.175/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 3bfb6c67-ec49-43b7-8061-b53d2c4e367c 0xc0046785b7 0xc0046785b8}] [] [{kube-controller-manager Update v1 2022-12-30 03:39:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3bfb6c67-ec49-43b7-8061-b53d2c4e367c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-12-30 03:39:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-30 03:39:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.78.175\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s6h6b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s6h6b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-mgmt03,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.142,PodIP:10.233.78.175,StartTime:2022-12-30 03:39:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-30 03:39:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://61d39ba37036bc54a77c9b3e8fed21522cc4918210d847c5f3fc0bb37660cc27,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.78.175,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 30 03:39:40.438: INFO: Pod "webserver-deployment-845c8977d9-4btbv" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-4btbv webserver-deployment-845c8977d9- deployment-5386  70824ca3-ffdb-44aa-8ea1-0c4fefbade2d 428973 0 2022-12-30 03:39:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 3bfb6c67-ec49-43b7-8061-b53d2c4e367c 0xc0046787c7 0xc0046787c8}] [] [{kube-controller-manager Update v1 2022-12-30 03:39:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3bfb6c67-ec49-43b7-8061-b53d2c4e367c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-30 03:39:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4s49c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4s49c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.195,PodIP:,StartTime:2022-12-30 03:39:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 30 03:39:40.438: INFO: Pod "webserver-deployment-845c8977d9-655qz" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-655qz webserver-deployment-845c8977d9- deployment-5386  f95dd904-a5b3-414f-ad13-99923fc41bb1 429108 0 2022-12-30 03:39:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:1c3e2006b12974be3ba0a39c5ae49f0586a7aa2518d226e6071feea4ded191c7 cni.projectcalico.org/podIP:10.233.78.177/32 cni.projectcalico.org/podIPs:10.233.78.177/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 3bfb6c67-ec49-43b7-8061-b53d2c4e367c 0xc0046789b7 0xc0046789b8}] [] [{kube-controller-manager Update v1 2022-12-30 03:39:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3bfb6c67-ec49-43b7-8061-b53d2c4e367c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-12-30 03:39:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-30 03:39:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.78.177\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-scwjc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-scwjc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-mgmt03,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.142,PodIP:10.233.78.177,StartTime:2022-12-30 03:39:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-30 03:39:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://b881cda86b69b421b0b617ee30cd9f6c07396347f435c45d0d1f4abfbf03f740,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.78.177,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 30 03:39:40.438: INFO: Pod "webserver-deployment-845c8977d9-76xs2" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-76xs2 webserver-deployment-845c8977d9- deployment-5386  7e8945a6-2ac0-437a-918c-a8799938871e 428665 0 2022-12-30 03:39:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:f5de4a3f10a7c24ad1027e87b69d462ddafb377ee69f65fb43bca81df607d609 cni.projectcalico.org/podIP:10.233.79.91/32 cni.projectcalico.org/podIPs:10.233.79.91/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 3bfb6c67-ec49-43b7-8061-b53d2c4e367c 0xc004678be7 0xc004678be8}] [] [{kube-controller-manager Update v1 2022-12-30 03:39:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3bfb6c67-ec49-43b7-8061-b53d2c4e367c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-12-30 03:39:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-30 03:39:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.79.91\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sscbb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sscbb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.194,PodIP:10.233.79.91,StartTime:2022-12-30 03:39:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-30 03:39:34 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://25bdceaceaf7960def9de03401b64ba51be0ca0fcfc7ab9eec3b00651e1ad838,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.79.91,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 30 03:39:40.439: INFO: Pod "webserver-deployment-845c8977d9-7cfpw" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-7cfpw webserver-deployment-845c8977d9- deployment-5386  f9b3ce95-0099-4b92-b5e2-1b33aebc9f0f 428667 0 2022-12-30 03:39:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:08d36a78879e0c5623953a73ab6aa4e8754cd7820f2a5b9d0c29684767f1ac81 cni.projectcalico.org/podIP:10.233.109.89/32 cni.projectcalico.org/podIPs:10.233.109.89/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 3bfb6c67-ec49-43b7-8061-b53d2c4e367c 0xc004678e17 0xc004678e18}] [] [{kube-controller-manager Update v1 2022-12-30 03:39:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3bfb6c67-ec49-43b7-8061-b53d2c4e367c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-12-30 03:39:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-30 03:39:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.109.89\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-58dj7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-58dj7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.195,PodIP:10.233.109.89,StartTime:2022-12-30 03:39:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-30 03:39:34 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://4a18d0ae0dc2ce193d8e5eb473f73301a062d2df9f67c9d5ad534472f7c45ae1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.109.89,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 30 03:39:40.439: INFO: Pod "webserver-deployment-845c8977d9-chf87" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-chf87 webserver-deployment-845c8977d9- deployment-5386  56d741b0-f214-45ca-ac03-0f654e64e3d9 428365 0 2022-12-30 03:39:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:dde7e9d356a060c2c11ad80340f92de7e53a4ba6707482f7096d31a3690e4507 cni.projectcalico.org/podIP:10.233.112.149/32 cni.projectcalico.org/podIPs:10.233.112.149/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 3bfb6c67-ec49-43b7-8061-b53d2c4e367c 0xc004679027 0xc004679028}] [] [{kube-controller-manager Update v1 2022-12-30 03:39:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3bfb6c67-ec49-43b7-8061-b53d2c4e367c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-12-30 03:39:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-30 03:39:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.112.149\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p4cd5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p4cd5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-mgmt01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.140,PodIP:10.233.112.149,StartTime:2022-12-30 03:39:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-30 03:39:29 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://f8b010f96ca9a97f6c5d45cc2c23a0cb1041b990ead44d7288581a679206e44d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.112.149,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 30 03:39:40.439: INFO: Pod "webserver-deployment-845c8977d9-d4986" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-d4986 webserver-deployment-845c8977d9- deployment-5386  a24fe659-26dd-4655-95bc-06f5bd240853 428370 0 2022-12-30 03:39:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:dbaae4939d9c6d311b8d7c07f2b7267885aa4afa62a18901ce883c85d2c3e47b cni.projectcalico.org/podIP:10.233.112.150/32 cni.projectcalico.org/podIPs:10.233.112.150/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 3bfb6c67-ec49-43b7-8061-b53d2c4e367c 0xc004679237 0xc004679238}] [] [{kube-controller-manager Update v1 2022-12-30 03:39:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3bfb6c67-ec49-43b7-8061-b53d2c4e367c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-12-30 03:39:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-30 03:39:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.112.150\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-856qh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-856qh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-mgmt01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.140,PodIP:10.233.112.150,StartTime:2022-12-30 03:39:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-30 03:39:29 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://af8513e28a089f7ffc976938a43efea3407eb69a84f321635bbcc107b3e1af31,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.112.150,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 30 03:39:40.439: INFO: Pod "webserver-deployment-845c8977d9-gd8qk" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-gd8qk webserver-deployment-845c8977d9- deployment-5386  cb0437ae-74b7-45c8-8443-96fcf6e7df5f 429069 0 2022-12-30 03:39:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:8e5de33a52d5a746447ae5692c314ba820f540a3c17af02ed1ce7b03fe4e9af9 cni.projectcalico.org/podIP:10.233.125.243/32 cni.projectcalico.org/podIPs:10.233.125.243/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 3bfb6c67-ec49-43b7-8061-b53d2c4e367c 0xc004679447 0xc004679448}] [] [{kube-controller-manager Update v1 2022-12-30 03:39:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3bfb6c67-ec49-43b7-8061-b53d2c4e367c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-30 03:39:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2022-12-30 03:39:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4jsxq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4jsxq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-mgmt02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.141,PodIP:,StartTime:2022-12-30 03:39:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 30 03:39:40.440: INFO: Pod "webserver-deployment-845c8977d9-gsvv8" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-gsvv8 webserver-deployment-845c8977d9- deployment-5386  78dc3fe5-2615-4b90-a2b4-36c543c67c15 428509 0 2022-12-30 03:39:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:4192283c64c2fca3827cb9b12f13fca9c1ff517910d7e4a73628ec14a7e1946a cni.projectcalico.org/podIP:10.233.79.90/32 cni.projectcalico.org/podIPs:10.233.79.90/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 3bfb6c67-ec49-43b7-8061-b53d2c4e367c 0xc004679657 0xc004679658}] [] [{kube-controller-manager Update v1 2022-12-30 03:39:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3bfb6c67-ec49-43b7-8061-b53d2c4e367c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-12-30 03:39:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-30 03:39:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.79.90\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vwqtn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vwqtn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.194,PodIP:10.233.79.90,StartTime:2022-12-30 03:39:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-30 03:39:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://67510d60fc7ca8b0b84a9c404dfb3b55f71b55649351d9244f253db52ec7f1a3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.79.90,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 30 03:39:40.440: INFO: Pod "webserver-deployment-845c8977d9-hfm7x" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-hfm7x webserver-deployment-845c8977d9- deployment-5386  54cabcf9-bdfc-4627-8ebc-b4ca842ac0e0 429081 0 2022-12-30 03:39:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:71466fb8a30766b1fcb3d1bd54f13be6df31c2c63ffdd8e46801816e23aa911a cni.projectcalico.org/podIP:10.233.112.155/32 cni.projectcalico.org/podIPs:10.233.112.155/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 3bfb6c67-ec49-43b7-8061-b53d2c4e367c 0xc004679867 0xc004679868}] [] [{kube-controller-manager Update v1 2022-12-30 03:39:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3bfb6c67-ec49-43b7-8061-b53d2c4e367c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-12-30 03:39:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-30 03:39:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.112.155\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hsnf4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hsnf4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-mgmt01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.140,PodIP:10.233.112.155,StartTime:2022-12-30 03:39:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-30 03:39:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://aa00996a0c878d81018574570f8969f5cc937ffab846e2f610df3e17117b5619,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.112.155,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 30 03:39:40.440: INFO: Pod "webserver-deployment-845c8977d9-hkxqv" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-hkxqv webserver-deployment-845c8977d9- deployment-5386  0bdc3eee-5362-40ac-aee6-c3fdf682f6d1 429084 0 2022-12-30 03:39:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:283ec4988d7be263f5a638a0cb199777f2f7b3965083e153c495c74ba8f3ebe6 cni.projectcalico.org/podIP:10.233.112.153/32 cni.projectcalico.org/podIPs:10.233.112.153/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 3bfb6c67-ec49-43b7-8061-b53d2c4e367c 0xc004679a77 0xc004679a78}] [] [{kube-controller-manager Update v1 2022-12-30 03:39:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3bfb6c67-ec49-43b7-8061-b53d2c4e367c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-12-30 03:39:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-30 03:39:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.112.153\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k6gxl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k6gxl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-mgmt01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.140,PodIP:10.233.112.153,StartTime:2022-12-30 03:39:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-30 03:39:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://524c2d1b31991efb7d8e44d747b2dac49dc7139410ac8aae8d310935beabaedc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.112.153,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 30 03:39:40.440: INFO: Pod "webserver-deployment-845c8977d9-mfprp" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-mfprp webserver-deployment-845c8977d9- deployment-5386  7c2f525b-ac34-43ec-b433-13ce79282208 429067 0 2022-12-30 03:39:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:bab44dba07c9575719f4c9100184eb2068e0d46c278a8dcf509262a872447c4a cni.projectcalico.org/podIP:10.233.125.242/32 cni.projectcalico.org/podIPs:10.233.125.242/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 3bfb6c67-ec49-43b7-8061-b53d2c4e367c 0xc004679c87 0xc004679c88}] [] [{kube-controller-manager Update v1 2022-12-30 03:39:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3bfb6c67-ec49-43b7-8061-b53d2c4e367c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-30 03:39:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2022-12-30 03:39:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vxhdg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vxhdg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-mgmt02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.141,PodIP:,StartTime:2022-12-30 03:39:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 30 03:39:40.440: INFO: Pod "webserver-deployment-845c8977d9-n6b52" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-n6b52 webserver-deployment-845c8977d9- deployment-5386  ab29ffe3-c063-4218-a7e1-9bdd2f745da4 428662 0 2022-12-30 03:39:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:26d08432ffcd8c756244c159e528158c70aad97dd45423e7fe25513938054aba cni.projectcalico.org/podIP:10.233.109.90/32 cni.projectcalico.org/podIPs:10.233.109.90/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 3bfb6c67-ec49-43b7-8061-b53d2c4e367c 0xc004679e97 0xc004679e98}] [] [{kube-controller-manager Update v1 2022-12-30 03:39:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3bfb6c67-ec49-43b7-8061-b53d2c4e367c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-12-30 03:39:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-30 03:39:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.109.90\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w899l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w899l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.195,PodIP:10.233.109.90,StartTime:2022-12-30 03:39:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-30 03:39:34 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://84c3b1c7eb06bde13edc93dcbbf03c313ea4466c27856e1c6d5edfeb5ceeb217,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.109.90,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 30 03:39:40.441: INFO: Pod "webserver-deployment-845c8977d9-n7h8j" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-n7h8j webserver-deployment-845c8977d9- deployment-5386  12b16197-bcf4-4e21-a9de-85265c2879a0 429102 0 2022-12-30 03:39:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:48813bb6cc63d9e70aba1d9fb8284988d1266c2f5936748c22b252b79b25ef43 cni.projectcalico.org/podIP:10.233.78.179/32 cni.projectcalico.org/podIPs:10.233.78.179/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 3bfb6c67-ec49-43b7-8061-b53d2c4e367c 0xc0046ce0c7 0xc0046ce0c8}] [] [{kube-controller-manager Update v1 2022-12-30 03:39:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3bfb6c67-ec49-43b7-8061-b53d2c4e367c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-12-30 03:39:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-30 03:39:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.78.179\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g6lc6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g6lc6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-mgmt03,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.142,PodIP:10.233.78.179,StartTime:2022-12-30 03:39:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-30 03:39:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://975e181613e9a0faf6306a93a99794cbec6a2800d64e82cbc75ce8aa987d9566,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.78.179,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 30 03:39:40.441: INFO: Pod "webserver-deployment-845c8977d9-pbt2t" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-pbt2t webserver-deployment-845c8977d9- deployment-5386  1b2cc933-56ee-441b-b78f-152954d006ea 428983 0 2022-12-30 03:39:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 3bfb6c67-ec49-43b7-8061-b53d2c4e367c 0xc0046ce2d7 0xc0046ce2d8}] [] [{kube-controller-manager Update v1 2022-12-30 03:39:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3bfb6c67-ec49-43b7-8061-b53d2c4e367c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-30 03:39:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6t6wl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6t6wl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.195,PodIP:,StartTime:2022-12-30 03:39:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 30 03:39:40.441: INFO: Pod "webserver-deployment-845c8977d9-rjztj" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-rjztj webserver-deployment-845c8977d9- deployment-5386  999b1223-f884-4efc-8489-ef185579109a 428379 0 2022-12-30 03:39:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:140901f6c6016fa436acd59aacf100295c308651a4fbda1f329d19ff6c466eba cni.projectcalico.org/podIP:10.233.78.174/32 cni.projectcalico.org/podIPs:10.233.78.174/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 3bfb6c67-ec49-43b7-8061-b53d2c4e367c 0xc0046ce4c7 0xc0046ce4c8}] [] [{kube-controller-manager Update v1 2022-12-30 03:39:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3bfb6c67-ec49-43b7-8061-b53d2c4e367c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-12-30 03:39:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-30 03:39:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.78.174\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tpmkd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tpmkd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-mgmt03,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.142,PodIP:10.233.78.174,StartTime:2022-12-30 03:39:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-30 03:39:29 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://23bed5c1b542fadaeb394822cfeb307a3bedab051d04636c4cbc267010ff81b4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.78.174,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 30 03:39:40.441: INFO: Pod "webserver-deployment-845c8977d9-rkxgx" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-rkxgx webserver-deployment-845c8977d9- deployment-5386  656500b1-6f43-47d3-ada8-731edc1c4a83 428982 0 2022-12-30 03:39:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 3bfb6c67-ec49-43b7-8061-b53d2c4e367c 0xc0046ce6d7 0xc0046ce6d8}] [] [{kube-controller-manager Update v1 2022-12-30 03:39:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3bfb6c67-ec49-43b7-8061-b53d2c4e367c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-30 03:39:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9d5rh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9d5rh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.194,PodIP:,StartTime:2022-12-30 03:39:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 30 03:39:40.442: INFO: Pod "webserver-deployment-845c8977d9-s5887" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-s5887 webserver-deployment-845c8977d9- deployment-5386  fa2011bf-1ea7-4aa8-ad8e-2ea7095b7441 429076 0 2022-12-30 03:39:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:1c12db93be5d4b33fd61ef64762094da53623da6e2c2ba81f8e7220723a6c8d9 cni.projectcalico.org/podIP:10.233.125.244/32 cni.projectcalico.org/podIPs:10.233.125.244/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 3bfb6c67-ec49-43b7-8061-b53d2c4e367c 0xc0046ce8a7 0xc0046ce8a8}] [] [{kube-controller-manager Update v1 2022-12-30 03:39:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3bfb6c67-ec49-43b7-8061-b53d2c4e367c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-30 03:39:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2022-12-30 03:39:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w8jc2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w8jc2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-mgmt02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.141,PodIP:,StartTime:2022-12-30 03:39:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 30 03:39:40.442: INFO: Pod "webserver-deployment-845c8977d9-sdgrr" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-sdgrr webserver-deployment-845c8977d9- deployment-5386  a1a26119-a547-4800-bdaf-2e35b2ff93fe 428962 0 2022-12-30 03:39:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 3bfb6c67-ec49-43b7-8061-b53d2c4e367c 0xc0046cea97 0xc0046cea98}] [] [{kube-controller-manager Update v1 2022-12-30 03:39:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3bfb6c67-ec49-43b7-8061-b53d2c4e367c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-30 03:39:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7lvrg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7lvrg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.194,PodIP:,StartTime:2022-12-30 03:39:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 30 03:39:40.442: INFO: Pod "webserver-deployment-845c8977d9-xw4cm" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-xw4cm webserver-deployment-845c8977d9- deployment-5386  067e9269-b22e-4ac3-b113-d3ed870e2fc3 429113 0 2022-12-30 03:39:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:e80c849954513bbc1d7f36df413f2bdf67bbb632ffe271ec6574bf657afa8199 cni.projectcalico.org/podIP:10.233.125.246/32 cni.projectcalico.org/podIPs:10.233.125.246/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 3bfb6c67-ec49-43b7-8061-b53d2c4e367c 0xc0046cec67 0xc0046cec68}] [] [{kube-controller-manager Update v1 2022-12-30 03:39:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3bfb6c67-ec49-43b7-8061-b53d2c4e367c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-12-30 03:39:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-30 03:39:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xg5nd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xg5nd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-mgmt02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.141,PodIP:,StartTime:2022-12-30 03:39:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Dec 30 03:39:40.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5386" for this suite. 12/30/22 03:39:40.447
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","completed":81,"skipped":1508,"failed":0}
------------------------------
• [SLOW TEST] [12.184 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:39:28.27
    Dec 30 03:39:28.270: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename deployment 12/30/22 03:39:28.271
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:39:28.284
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:39:28.287
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    Dec 30 03:39:28.290: INFO: Creating deployment "webserver-deployment"
    Dec 30 03:39:28.296: INFO: Waiting for observed generation 1
    Dec 30 03:39:30.305: INFO: Waiting for all required pods to come up
    Dec 30 03:39:30.311: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 12/30/22 03:39:30.311
    Dec 30 03:39:30.311: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-76xs2" in namespace "deployment-5386" to be "running"
    Dec 30 03:39:30.311: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-v4skw" in namespace "deployment-5386" to be "running"
    Dec 30 03:39:30.311: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-gsvv8" in namespace "deployment-5386" to be "running"
    Dec 30 03:39:30.311: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-289dk" in namespace "deployment-5386" to be "running"
    Dec 30 03:39:30.312: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-rfnkb" in namespace "deployment-5386" to be "running"
    Dec 30 03:39:30.312: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-d4986" in namespace "deployment-5386" to be "running"
    Dec 30 03:39:30.312: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-n6b52" in namespace "deployment-5386" to be "running"
    Dec 30 03:39:30.312: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-rjztj" in namespace "deployment-5386" to be "running"
    Dec 30 03:39:30.312: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-7cfpw" in namespace "deployment-5386" to be "running"
    Dec 30 03:39:30.315: INFO: Pod "webserver-deployment-845c8977d9-76xs2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041188ms
    Dec 30 03:39:30.316: INFO: Pod "webserver-deployment-845c8977d9-rfnkb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034897ms
    Dec 30 03:39:30.316: INFO: Pod "webserver-deployment-845c8977d9-v4skw": Phase="Pending", Reason="", readiness=false. Elapsed: 4.261698ms
    Dec 30 03:39:30.316: INFO: Pod "webserver-deployment-845c8977d9-gsvv8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.402541ms
    Dec 30 03:39:30.316: INFO: Pod "webserver-deployment-845c8977d9-d4986": Phase="Pending", Reason="", readiness=false. Elapsed: 4.363307ms
    Dec 30 03:39:30.317: INFO: Pod "webserver-deployment-845c8977d9-7cfpw": Phase="Pending", Reason="", readiness=false. Elapsed: 4.752076ms
    Dec 30 03:39:30.317: INFO: Pod "webserver-deployment-845c8977d9-n6b52": Phase="Pending", Reason="", readiness=false. Elapsed: 4.975199ms
    Dec 30 03:39:30.317: INFO: Pod "webserver-deployment-845c8977d9-rjztj": Phase="Pending", Reason="", readiness=false. Elapsed: 4.945697ms
    Dec 30 03:39:30.317: INFO: Pod "webserver-deployment-845c8977d9-289dk": Phase="Pending", Reason="", readiness=false. Elapsed: 5.176271ms
    Dec 30 03:39:32.321: INFO: Pod "webserver-deployment-845c8977d9-gsvv8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009396634s
    Dec 30 03:39:32.321: INFO: Pod "webserver-deployment-845c8977d9-v4skw": Phase="Running", Reason="", readiness=true. Elapsed: 2.009882011s
    Dec 30 03:39:32.321: INFO: Pod "webserver-deployment-845c8977d9-v4skw" satisfied condition "running"
    Dec 30 03:39:32.321: INFO: Pod "webserver-deployment-845c8977d9-d4986": Phase="Running", Reason="", readiness=true. Elapsed: 2.009820978s
    Dec 30 03:39:32.321: INFO: Pod "webserver-deployment-845c8977d9-d4986" satisfied condition "running"
    Dec 30 03:39:32.321: INFO: Pod "webserver-deployment-845c8977d9-rfnkb": Phase="Running", Reason="", readiness=true. Elapsed: 2.009859769s
    Dec 30 03:39:32.321: INFO: Pod "webserver-deployment-845c8977d9-rfnkb" satisfied condition "running"
    Dec 30 03:39:32.321: INFO: Pod "webserver-deployment-845c8977d9-289dk": Phase="Running", Reason="", readiness=true. Elapsed: 2.009956289s
    Dec 30 03:39:32.322: INFO: Pod "webserver-deployment-845c8977d9-289dk" satisfied condition "running"
    Dec 30 03:39:32.322: INFO: Pod "webserver-deployment-845c8977d9-n6b52": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010761802s
    Dec 30 03:39:32.322: INFO: Pod "webserver-deployment-845c8977d9-76xs2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01108958s
    Dec 30 03:39:32.322: INFO: Pod "webserver-deployment-845c8977d9-rjztj": Phase="Running", Reason="", readiness=true. Elapsed: 2.01078226s
    Dec 30 03:39:32.323: INFO: Pod "webserver-deployment-845c8977d9-rjztj" satisfied condition "running"
    Dec 30 03:39:32.322: INFO: Pod "webserver-deployment-845c8977d9-7cfpw": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010737348s
    Dec 30 03:39:34.320: INFO: Pod "webserver-deployment-845c8977d9-gsvv8": Phase="Running", Reason="", readiness=true. Elapsed: 4.008540389s
    Dec 30 03:39:34.320: INFO: Pod "webserver-deployment-845c8977d9-gsvv8" satisfied condition "running"
    Dec 30 03:39:34.320: INFO: Pod "webserver-deployment-845c8977d9-n6b52": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008555226s
    Dec 30 03:39:34.321: INFO: Pod "webserver-deployment-845c8977d9-76xs2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009291553s
    Dec 30 03:39:34.321: INFO: Pod "webserver-deployment-845c8977d9-7cfpw": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009025823s
    Dec 30 03:39:36.320: INFO: Pod "webserver-deployment-845c8977d9-76xs2": Phase="Running", Reason="", readiness=true. Elapsed: 6.008522823s
    Dec 30 03:39:36.320: INFO: Pod "webserver-deployment-845c8977d9-76xs2" satisfied condition "running"
    Dec 30 03:39:36.321: INFO: Pod "webserver-deployment-845c8977d9-7cfpw": Phase="Running", Reason="", readiness=true. Elapsed: 6.009344701s
    Dec 30 03:39:36.321: INFO: Pod "webserver-deployment-845c8977d9-n6b52": Phase="Running", Reason="", readiness=true. Elapsed: 6.009514563s
    Dec 30 03:39:36.321: INFO: Pod "webserver-deployment-845c8977d9-n6b52" satisfied condition "running"
    Dec 30 03:39:36.321: INFO: Pod "webserver-deployment-845c8977d9-7cfpw" satisfied condition "running"
    Dec 30 03:39:36.321: INFO: Waiting for deployment "webserver-deployment" to complete
    Dec 30 03:39:36.329: INFO: Updating deployment "webserver-deployment" with a non-existent image
    Dec 30 03:39:36.339: INFO: Updating deployment webserver-deployment
    Dec 30 03:39:36.339: INFO: Waiting for observed generation 2
    Dec 30 03:39:38.348: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    Dec 30 03:39:38.352: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    Dec 30 03:39:38.356: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Dec 30 03:39:38.367: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    Dec 30 03:39:38.367: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    Dec 30 03:39:38.371: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Dec 30 03:39:38.377: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    Dec 30 03:39:38.377: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    Dec 30 03:39:38.389: INFO: Updating deployment webserver-deployment
    Dec 30 03:39:38.389: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    Dec 30 03:39:38.396: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    Dec 30 03:39:38.400: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Dec 30 03:39:40.413: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-5386  b16626a2-dae6-4abe-b294-25710dafc4ad 429111 3 2022-12-30 03:39:28 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2022-12-30 03:39:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-30 03:39:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004524008 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:12,UnavailableReplicas:21,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-12-30 03:39:38 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-69b7448995" is progressing.,LastUpdateTime:2022-12-30 03:39:40 +0000 UTC,LastTransitionTime:2022-12-30 03:39:28 +0000 UTC,},},ReadyReplicas:12,CollisionCount:nil,},}

    Dec 30 03:39:40.418: INFO: New ReplicaSet "webserver-deployment-69b7448995" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-69b7448995  deployment-5386  133cfa6c-6a52-4d81-bc64-6854c375125c 428976 3 2022-12-30 03:39:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment b16626a2-dae6-4abe-b294-25710dafc4ad 0xc004524447 0xc004524448}] [] [{kube-controller-manager Update apps/v1 2022-12-30 03:39:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b16626a2-dae6-4abe-b294-25710dafc4ad\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-30 03:39:38 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 69b7448995,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0045244e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Dec 30 03:39:40.418: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    Dec 30 03:39:40.418: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-845c8977d9  deployment-5386  3bfb6c67-ec49-43b7-8061-b53d2c4e367c 429109 3 2022-12-30 03:39:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment b16626a2-dae6-4abe-b294-25710dafc4ad 0xc004524547 0xc004524548}] [] [{kube-controller-manager Update apps/v1 2022-12-30 03:39:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b16626a2-dae6-4abe-b294-25710dafc4ad\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-30 03:39:40 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0045245d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:12,AvailableReplicas:12,Conditions:[]ReplicaSetCondition{},},}
    Dec 30 03:39:40.430: INFO: Pod "webserver-deployment-69b7448995-46pb6" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-46pb6 webserver-deployment-69b7448995- deployment-5386  4a4ef635-7eb1-46c2-9ad2-ef4f57b987b9 429100 0 2022-12-30 03:39:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:ae541f17714e026d3257d3e8c0ad0ee91393931446981398bf55403a1b7701ef cni.projectcalico.org/podIP:10.233.125.245/32 cni.projectcalico.org/podIPs:10.233.125.245/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 133cfa6c-6a52-4d81-bc64-6854c375125c 0xc004524aa7 0xc004524aa8}] [] [{kube-controller-manager Update v1 2022-12-30 03:39:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"133cfa6c-6a52-4d81-bc64-6854c375125c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-12-30 03:39:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6fdh6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6fdh6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-mgmt02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 30 03:39:40.430: INFO: Pod "webserver-deployment-69b7448995-6624g" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-6624g webserver-deployment-69b7448995- deployment-5386  6e5b14d2-cafb-44fd-9f2f-0ea26b2d35e3 428881 0 2022-12-30 03:39:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:47858ef53c22222a47b2a91c77aae5cf200ee2727cf224034b34263f8ebae536 cni.projectcalico.org/podIP:10.233.125.240/32 cni.projectcalico.org/podIPs:10.233.125.240/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 133cfa6c-6a52-4d81-bc64-6854c375125c 0xc004524c40 0xc004524c41}] [] [{kube-controller-manager Update v1 2022-12-30 03:39:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"133cfa6c-6a52-4d81-bc64-6854c375125c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-30 03:39:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2022-12-30 03:39:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7cxpp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7cxpp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-mgmt02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.141,PodIP:,StartTime:2022-12-30 03:39:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 30 03:39:40.431: INFO: Pod "webserver-deployment-69b7448995-966sq" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-966sq webserver-deployment-69b7448995- deployment-5386  de85a217-1594-4a00-9fd7-03232251f2b8 428987 0 2022-12-30 03:39:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 133cfa6c-6a52-4d81-bc64-6854c375125c 0xc004524e57 0xc004524e58}] [] [{kube-controller-manager Update v1 2022-12-30 03:39:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"133cfa6c-6a52-4d81-bc64-6854c375125c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-30 03:39:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-65tcd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-65tcd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.195,PodIP:,StartTime:2022-12-30 03:39:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 30 03:39:40.432: INFO: Pod "webserver-deployment-69b7448995-bfdjl" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-bfdjl webserver-deployment-69b7448995- deployment-5386  82d13265-e228-4106-9f01-588636a9d201 429104 0 2022-12-30 03:39:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:83509c7a6ca18a435ae57948a4e197a510a6b6a58249f44159c65eb60eeb7c1c cni.projectcalico.org/podIP:10.233.78.176/32 cni.projectcalico.org/podIPs:10.233.78.176/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 133cfa6c-6a52-4d81-bc64-6854c375125c 0xc004525067 0xc004525068}] [] [{Go-http-client Update v1 2022-12-30 03:39:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-12-30 03:39:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"133cfa6c-6a52-4d81-bc64-6854c375125c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-30 03:39:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.78.176\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-24vx7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-24vx7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-mgmt03,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.142,PodIP:10.233.78.176,StartTime:2022-12-30 03:39:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.78.176,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 30 03:39:40.433: INFO: Pod "webserver-deployment-69b7448995-fbf8r" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-fbf8r webserver-deployment-69b7448995- deployment-5386  d00a4cd8-b78e-495a-9b80-90106c5e38ea 428895 0 2022-12-30 03:39:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:08a0e407abd49938aa8a1f2ab8251bcae485a50f2129d0b77476957b0ab4d56a cni.projectcalico.org/podIP:10.233.109.91/32 cni.projectcalico.org/podIPs:10.233.109.91/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 133cfa6c-6a52-4d81-bc64-6854c375125c 0xc0045252c7 0xc0045252c8}] [] [{kube-controller-manager Update v1 2022-12-30 03:39:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"133cfa6c-6a52-4d81-bc64-6854c375125c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-30 03:39:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2022-12-30 03:39:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wn62f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wn62f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.195,PodIP:,StartTime:2022-12-30 03:39:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 30 03:39:40.433: INFO: Pod "webserver-deployment-69b7448995-fcgfn" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-fcgfn webserver-deployment-69b7448995- deployment-5386  9454780f-4fb5-4b7c-82e2-45dbc218a23a 429024 0 2022-12-30 03:39:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:6d830c1962e81e60f7111797af3d4d2b3ea6c9bec19183eed39d6ed019e9e10f cni.projectcalico.org/podIP:10.233.112.154/32 cni.projectcalico.org/podIPs:10.233.112.154/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 133cfa6c-6a52-4d81-bc64-6854c375125c 0xc0045254d7 0xc0045254d8}] [] [{kube-controller-manager Update v1 2022-12-30 03:39:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"133cfa6c-6a52-4d81-bc64-6854c375125c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-30 03:39:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2022-12-30 03:39:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6m5ql,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6m5ql,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-mgmt01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.140,PodIP:,StartTime:2022-12-30 03:39:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 30 03:39:40.434: INFO: Pod "webserver-deployment-69b7448995-gb4gj" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-gb4gj webserver-deployment-69b7448995- deployment-5386  85ff0456-2e05-4222-98d4-ee3668df62e9 429053 0 2022-12-30 03:39:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:5351489992783bae9657bf9007b7d8271ba00680be995a6a02e503a18eebf356 cni.projectcalico.org/podIP:10.233.125.241/32 cni.projectcalico.org/podIPs:10.233.125.241/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 133cfa6c-6a52-4d81-bc64-6854c375125c 0xc0045256e7 0xc0045256e8}] [] [{kube-controller-manager Update v1 2022-12-30 03:39:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"133cfa6c-6a52-4d81-bc64-6854c375125c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-30 03:39:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2022-12-30 03:39:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2ngv2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2ngv2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-mgmt02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.141,PodIP:,StartTime:2022-12-30 03:39:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 30 03:39:40.435: INFO: Pod "webserver-deployment-69b7448995-hdl7c" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-hdl7c webserver-deployment-69b7448995- deployment-5386  c3608c45-ccc9-42c1-8635-ea383a43dd11 428986 0 2022-12-30 03:39:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 133cfa6c-6a52-4d81-bc64-6854c375125c 0xc0045258f7 0xc0045258f8}] [] [{kube-controller-manager Update v1 2022-12-30 03:39:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"133cfa6c-6a52-4d81-bc64-6854c375125c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-30 03:39:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ctfs6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ctfs6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.194,PodIP:,StartTime:2022-12-30 03:39:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 30 03:39:40.436: INFO: Pod "webserver-deployment-69b7448995-nbrzz" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-nbrzz webserver-deployment-69b7448995- deployment-5386  f5269cf5-c1fb-46cd-a57b-6598a5783383 429027 0 2022-12-30 03:39:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:4600302ee68bb064322a1fca809ca120b9b4ddc6cabee840e07ef2d08b3a84a0 cni.projectcalico.org/podIP:10.233.78.178/32 cni.projectcalico.org/podIPs:10.233.78.178/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 133cfa6c-6a52-4d81-bc64-6854c375125c 0xc004525b07 0xc004525b08}] [] [{kube-controller-manager Update v1 2022-12-30 03:39:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"133cfa6c-6a52-4d81-bc64-6854c375125c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-30 03:39:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2022-12-30 03:39:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wht6k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wht6k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-mgmt03,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.142,PodIP:,StartTime:2022-12-30 03:39:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 30 03:39:40.437: INFO: Pod "webserver-deployment-69b7448995-pf44w" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-pf44w webserver-deployment-69b7448995- deployment-5386  e002ab97-7c35-45c7-b0db-ce02b4eedcf3 429014 0 2022-12-30 03:39:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:79824f2c8d1946c1fe064b3e85260e3a85c5048ba2a602eb0b19b3a1e5d0251f cni.projectcalico.org/podIP:10.233.112.152/32 cni.projectcalico.org/podIPs:10.233.112.152/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 133cfa6c-6a52-4d81-bc64-6854c375125c 0xc004525d17 0xc004525d18}] [] [{kube-controller-manager Update v1 2022-12-30 03:39:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"133cfa6c-6a52-4d81-bc64-6854c375125c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-30 03:39:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2022-12-30 03:39:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-72fpg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-72fpg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-mgmt01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.140,PodIP:,StartTime:2022-12-30 03:39:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 30 03:39:40.437: INFO: Pod "webserver-deployment-69b7448995-rbzm2" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-rbzm2 webserver-deployment-69b7448995- deployment-5386  12a43f94-cb37-42ad-a606-99d50be0c576 429002 0 2022-12-30 03:39:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:a281c9fec8f8a253ade446964163047739c26fb2656f51fdcd0c50a1642098de cni.projectcalico.org/podIP:10.233.79.92/32 cni.projectcalico.org/podIPs:10.233.79.92/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 133cfa6c-6a52-4d81-bc64-6854c375125c 0xc004525f47 0xc004525f48}] [] [{kube-controller-manager Update v1 2022-12-30 03:39:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"133cfa6c-6a52-4d81-bc64-6854c375125c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-30 03:39:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2022-12-30 03:39:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-985r4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-985r4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.194,PodIP:,StartTime:2022-12-30 03:39:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 30 03:39:40.437: INFO: Pod "webserver-deployment-69b7448995-x8hcw" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-x8hcw webserver-deployment-69b7448995- deployment-5386  fa20d727-41c0-453a-bd5c-358403a93a40 428878 0 2022-12-30 03:39:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:a9a9ef1a7294415021a63cf463cf2e16f9448c02c3cc0dc11b751f75e09443b7 cni.projectcalico.org/podIP:10.233.112.151/32 cni.projectcalico.org/podIPs:10.233.112.151/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 133cfa6c-6a52-4d81-bc64-6854c375125c 0xc004678157 0xc004678158}] [] [{Go-http-client Update v1 2022-12-30 03:39:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-12-30 03:39:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"133cfa6c-6a52-4d81-bc64-6854c375125c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-30 03:39:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-htdsx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-htdsx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-mgmt01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.140,PodIP:,StartTime:2022-12-30 03:39:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 30 03:39:40.438: INFO: Pod "webserver-deployment-69b7448995-zgwsn" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-zgwsn webserver-deployment-69b7448995- deployment-5386  c530c195-c6af-4ffa-a4e4-404011050f0c 429051 0 2022-12-30 03:39:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:438c6cfdf8dd26452035992232819207efaac0e1ef51fe6af95aeb30254fa289 cni.projectcalico.org/podIP:10.233.78.180/32 cni.projectcalico.org/podIPs:10.233.78.180/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 133cfa6c-6a52-4d81-bc64-6854c375125c 0xc004678387 0xc004678388}] [] [{kube-controller-manager Update v1 2022-12-30 03:39:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"133cfa6c-6a52-4d81-bc64-6854c375125c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-30 03:39:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2022-12-30 03:39:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2cwxm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2cwxm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-mgmt03,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.142,PodIP:,StartTime:2022-12-30 03:39:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 30 03:39:40.438: INFO: Pod "webserver-deployment-845c8977d9-289dk" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-289dk webserver-deployment-845c8977d9- deployment-5386  5ed34e22-e152-48c6-902c-2f4971fa8dcc 428373 0 2022-12-30 03:39:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:4468a59ac7306978b09cc6c1e48bd10a8d74fd2f0f5f5dcde40ef05f30523210 cni.projectcalico.org/podIP:10.233.78.175/32 cni.projectcalico.org/podIPs:10.233.78.175/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 3bfb6c67-ec49-43b7-8061-b53d2c4e367c 0xc0046785b7 0xc0046785b8}] [] [{kube-controller-manager Update v1 2022-12-30 03:39:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3bfb6c67-ec49-43b7-8061-b53d2c4e367c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-12-30 03:39:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-30 03:39:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.78.175\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s6h6b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s6h6b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-mgmt03,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.142,PodIP:10.233.78.175,StartTime:2022-12-30 03:39:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-30 03:39:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://61d39ba37036bc54a77c9b3e8fed21522cc4918210d847c5f3fc0bb37660cc27,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.78.175,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 30 03:39:40.438: INFO: Pod "webserver-deployment-845c8977d9-4btbv" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-4btbv webserver-deployment-845c8977d9- deployment-5386  70824ca3-ffdb-44aa-8ea1-0c4fefbade2d 428973 0 2022-12-30 03:39:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 3bfb6c67-ec49-43b7-8061-b53d2c4e367c 0xc0046787c7 0xc0046787c8}] [] [{kube-controller-manager Update v1 2022-12-30 03:39:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3bfb6c67-ec49-43b7-8061-b53d2c4e367c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-30 03:39:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4s49c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4s49c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.195,PodIP:,StartTime:2022-12-30 03:39:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 30 03:39:40.438: INFO: Pod "webserver-deployment-845c8977d9-655qz" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-655qz webserver-deployment-845c8977d9- deployment-5386  f95dd904-a5b3-414f-ad13-99923fc41bb1 429108 0 2022-12-30 03:39:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:1c3e2006b12974be3ba0a39c5ae49f0586a7aa2518d226e6071feea4ded191c7 cni.projectcalico.org/podIP:10.233.78.177/32 cni.projectcalico.org/podIPs:10.233.78.177/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 3bfb6c67-ec49-43b7-8061-b53d2c4e367c 0xc0046789b7 0xc0046789b8}] [] [{kube-controller-manager Update v1 2022-12-30 03:39:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3bfb6c67-ec49-43b7-8061-b53d2c4e367c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-12-30 03:39:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-30 03:39:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.78.177\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-scwjc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-scwjc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-mgmt03,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.142,PodIP:10.233.78.177,StartTime:2022-12-30 03:39:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-30 03:39:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://b881cda86b69b421b0b617ee30cd9f6c07396347f435c45d0d1f4abfbf03f740,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.78.177,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 30 03:39:40.438: INFO: Pod "webserver-deployment-845c8977d9-76xs2" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-76xs2 webserver-deployment-845c8977d9- deployment-5386  7e8945a6-2ac0-437a-918c-a8799938871e 428665 0 2022-12-30 03:39:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:f5de4a3f10a7c24ad1027e87b69d462ddafb377ee69f65fb43bca81df607d609 cni.projectcalico.org/podIP:10.233.79.91/32 cni.projectcalico.org/podIPs:10.233.79.91/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 3bfb6c67-ec49-43b7-8061-b53d2c4e367c 0xc004678be7 0xc004678be8}] [] [{kube-controller-manager Update v1 2022-12-30 03:39:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3bfb6c67-ec49-43b7-8061-b53d2c4e367c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-12-30 03:39:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-30 03:39:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.79.91\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sscbb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sscbb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.194,PodIP:10.233.79.91,StartTime:2022-12-30 03:39:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-30 03:39:34 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://25bdceaceaf7960def9de03401b64ba51be0ca0fcfc7ab9eec3b00651e1ad838,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.79.91,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 30 03:39:40.439: INFO: Pod "webserver-deployment-845c8977d9-7cfpw" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-7cfpw webserver-deployment-845c8977d9- deployment-5386  f9b3ce95-0099-4b92-b5e2-1b33aebc9f0f 428667 0 2022-12-30 03:39:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:08d36a78879e0c5623953a73ab6aa4e8754cd7820f2a5b9d0c29684767f1ac81 cni.projectcalico.org/podIP:10.233.109.89/32 cni.projectcalico.org/podIPs:10.233.109.89/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 3bfb6c67-ec49-43b7-8061-b53d2c4e367c 0xc004678e17 0xc004678e18}] [] [{kube-controller-manager Update v1 2022-12-30 03:39:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3bfb6c67-ec49-43b7-8061-b53d2c4e367c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-12-30 03:39:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-30 03:39:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.109.89\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-58dj7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-58dj7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.195,PodIP:10.233.109.89,StartTime:2022-12-30 03:39:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-30 03:39:34 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://4a18d0ae0dc2ce193d8e5eb473f73301a062d2df9f67c9d5ad534472f7c45ae1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.109.89,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 30 03:39:40.439: INFO: Pod "webserver-deployment-845c8977d9-chf87" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-chf87 webserver-deployment-845c8977d9- deployment-5386  56d741b0-f214-45ca-ac03-0f654e64e3d9 428365 0 2022-12-30 03:39:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:dde7e9d356a060c2c11ad80340f92de7e53a4ba6707482f7096d31a3690e4507 cni.projectcalico.org/podIP:10.233.112.149/32 cni.projectcalico.org/podIPs:10.233.112.149/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 3bfb6c67-ec49-43b7-8061-b53d2c4e367c 0xc004679027 0xc004679028}] [] [{kube-controller-manager Update v1 2022-12-30 03:39:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3bfb6c67-ec49-43b7-8061-b53d2c4e367c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-12-30 03:39:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-30 03:39:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.112.149\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p4cd5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p4cd5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-mgmt01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.140,PodIP:10.233.112.149,StartTime:2022-12-30 03:39:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-30 03:39:29 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://f8b010f96ca9a97f6c5d45cc2c23a0cb1041b990ead44d7288581a679206e44d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.112.149,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 30 03:39:40.439: INFO: Pod "webserver-deployment-845c8977d9-d4986" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-d4986 webserver-deployment-845c8977d9- deployment-5386  a24fe659-26dd-4655-95bc-06f5bd240853 428370 0 2022-12-30 03:39:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:dbaae4939d9c6d311b8d7c07f2b7267885aa4afa62a18901ce883c85d2c3e47b cni.projectcalico.org/podIP:10.233.112.150/32 cni.projectcalico.org/podIPs:10.233.112.150/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 3bfb6c67-ec49-43b7-8061-b53d2c4e367c 0xc004679237 0xc004679238}] [] [{kube-controller-manager Update v1 2022-12-30 03:39:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3bfb6c67-ec49-43b7-8061-b53d2c4e367c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-12-30 03:39:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-30 03:39:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.112.150\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-856qh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-856qh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-mgmt01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.140,PodIP:10.233.112.150,StartTime:2022-12-30 03:39:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-30 03:39:29 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://af8513e28a089f7ffc976938a43efea3407eb69a84f321635bbcc107b3e1af31,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.112.150,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 30 03:39:40.439: INFO: Pod "webserver-deployment-845c8977d9-gd8qk" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-gd8qk webserver-deployment-845c8977d9- deployment-5386  cb0437ae-74b7-45c8-8443-96fcf6e7df5f 429069 0 2022-12-30 03:39:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:8e5de33a52d5a746447ae5692c314ba820f540a3c17af02ed1ce7b03fe4e9af9 cni.projectcalico.org/podIP:10.233.125.243/32 cni.projectcalico.org/podIPs:10.233.125.243/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 3bfb6c67-ec49-43b7-8061-b53d2c4e367c 0xc004679447 0xc004679448}] [] [{kube-controller-manager Update v1 2022-12-30 03:39:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3bfb6c67-ec49-43b7-8061-b53d2c4e367c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-30 03:39:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2022-12-30 03:39:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4jsxq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4jsxq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-mgmt02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.141,PodIP:,StartTime:2022-12-30 03:39:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 30 03:39:40.440: INFO: Pod "webserver-deployment-845c8977d9-gsvv8" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-gsvv8 webserver-deployment-845c8977d9- deployment-5386  78dc3fe5-2615-4b90-a2b4-36c543c67c15 428509 0 2022-12-30 03:39:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:4192283c64c2fca3827cb9b12f13fca9c1ff517910d7e4a73628ec14a7e1946a cni.projectcalico.org/podIP:10.233.79.90/32 cni.projectcalico.org/podIPs:10.233.79.90/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 3bfb6c67-ec49-43b7-8061-b53d2c4e367c 0xc004679657 0xc004679658}] [] [{kube-controller-manager Update v1 2022-12-30 03:39:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3bfb6c67-ec49-43b7-8061-b53d2c4e367c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-12-30 03:39:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-30 03:39:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.79.90\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vwqtn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vwqtn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.194,PodIP:10.233.79.90,StartTime:2022-12-30 03:39:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-30 03:39:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://67510d60fc7ca8b0b84a9c404dfb3b55f71b55649351d9244f253db52ec7f1a3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.79.90,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 30 03:39:40.440: INFO: Pod "webserver-deployment-845c8977d9-hfm7x" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-hfm7x webserver-deployment-845c8977d9- deployment-5386  54cabcf9-bdfc-4627-8ebc-b4ca842ac0e0 429081 0 2022-12-30 03:39:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:71466fb8a30766b1fcb3d1bd54f13be6df31c2c63ffdd8e46801816e23aa911a cni.projectcalico.org/podIP:10.233.112.155/32 cni.projectcalico.org/podIPs:10.233.112.155/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 3bfb6c67-ec49-43b7-8061-b53d2c4e367c 0xc004679867 0xc004679868}] [] [{kube-controller-manager Update v1 2022-12-30 03:39:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3bfb6c67-ec49-43b7-8061-b53d2c4e367c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-12-30 03:39:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-30 03:39:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.112.155\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hsnf4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hsnf4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-mgmt01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.140,PodIP:10.233.112.155,StartTime:2022-12-30 03:39:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-30 03:39:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://aa00996a0c878d81018574570f8969f5cc937ffab846e2f610df3e17117b5619,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.112.155,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 30 03:39:40.440: INFO: Pod "webserver-deployment-845c8977d9-hkxqv" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-hkxqv webserver-deployment-845c8977d9- deployment-5386  0bdc3eee-5362-40ac-aee6-c3fdf682f6d1 429084 0 2022-12-30 03:39:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:283ec4988d7be263f5a638a0cb199777f2f7b3965083e153c495c74ba8f3ebe6 cni.projectcalico.org/podIP:10.233.112.153/32 cni.projectcalico.org/podIPs:10.233.112.153/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 3bfb6c67-ec49-43b7-8061-b53d2c4e367c 0xc004679a77 0xc004679a78}] [] [{kube-controller-manager Update v1 2022-12-30 03:39:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3bfb6c67-ec49-43b7-8061-b53d2c4e367c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-12-30 03:39:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-30 03:39:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.112.153\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k6gxl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k6gxl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-mgmt01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.140,PodIP:10.233.112.153,StartTime:2022-12-30 03:39:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-30 03:39:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://524c2d1b31991efb7d8e44d747b2dac49dc7139410ac8aae8d310935beabaedc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.112.153,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 30 03:39:40.440: INFO: Pod "webserver-deployment-845c8977d9-mfprp" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-mfprp webserver-deployment-845c8977d9- deployment-5386  7c2f525b-ac34-43ec-b433-13ce79282208 429067 0 2022-12-30 03:39:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:bab44dba07c9575719f4c9100184eb2068e0d46c278a8dcf509262a872447c4a cni.projectcalico.org/podIP:10.233.125.242/32 cni.projectcalico.org/podIPs:10.233.125.242/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 3bfb6c67-ec49-43b7-8061-b53d2c4e367c 0xc004679c87 0xc004679c88}] [] [{kube-controller-manager Update v1 2022-12-30 03:39:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3bfb6c67-ec49-43b7-8061-b53d2c4e367c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-30 03:39:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2022-12-30 03:39:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vxhdg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vxhdg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-mgmt02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.141,PodIP:,StartTime:2022-12-30 03:39:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 30 03:39:40.440: INFO: Pod "webserver-deployment-845c8977d9-n6b52" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-n6b52 webserver-deployment-845c8977d9- deployment-5386  ab29ffe3-c063-4218-a7e1-9bdd2f745da4 428662 0 2022-12-30 03:39:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:26d08432ffcd8c756244c159e528158c70aad97dd45423e7fe25513938054aba cni.projectcalico.org/podIP:10.233.109.90/32 cni.projectcalico.org/podIPs:10.233.109.90/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 3bfb6c67-ec49-43b7-8061-b53d2c4e367c 0xc004679e97 0xc004679e98}] [] [{kube-controller-manager Update v1 2022-12-30 03:39:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3bfb6c67-ec49-43b7-8061-b53d2c4e367c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-12-30 03:39:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-30 03:39:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.109.90\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w899l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w899l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.195,PodIP:10.233.109.90,StartTime:2022-12-30 03:39:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-30 03:39:34 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://84c3b1c7eb06bde13edc93dcbbf03c313ea4466c27856e1c6d5edfeb5ceeb217,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.109.90,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 30 03:39:40.441: INFO: Pod "webserver-deployment-845c8977d9-n7h8j" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-n7h8j webserver-deployment-845c8977d9- deployment-5386  12b16197-bcf4-4e21-a9de-85265c2879a0 429102 0 2022-12-30 03:39:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:48813bb6cc63d9e70aba1d9fb8284988d1266c2f5936748c22b252b79b25ef43 cni.projectcalico.org/podIP:10.233.78.179/32 cni.projectcalico.org/podIPs:10.233.78.179/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 3bfb6c67-ec49-43b7-8061-b53d2c4e367c 0xc0046ce0c7 0xc0046ce0c8}] [] [{kube-controller-manager Update v1 2022-12-30 03:39:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3bfb6c67-ec49-43b7-8061-b53d2c4e367c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-12-30 03:39:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-30 03:39:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.78.179\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g6lc6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g6lc6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-mgmt03,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.142,PodIP:10.233.78.179,StartTime:2022-12-30 03:39:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-30 03:39:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://975e181613e9a0faf6306a93a99794cbec6a2800d64e82cbc75ce8aa987d9566,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.78.179,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 30 03:39:40.441: INFO: Pod "webserver-deployment-845c8977d9-pbt2t" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-pbt2t webserver-deployment-845c8977d9- deployment-5386  1b2cc933-56ee-441b-b78f-152954d006ea 428983 0 2022-12-30 03:39:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 3bfb6c67-ec49-43b7-8061-b53d2c4e367c 0xc0046ce2d7 0xc0046ce2d8}] [] [{kube-controller-manager Update v1 2022-12-30 03:39:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3bfb6c67-ec49-43b7-8061-b53d2c4e367c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-30 03:39:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6t6wl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6t6wl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.195,PodIP:,StartTime:2022-12-30 03:39:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 30 03:39:40.441: INFO: Pod "webserver-deployment-845c8977d9-rjztj" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-rjztj webserver-deployment-845c8977d9- deployment-5386  999b1223-f884-4efc-8489-ef185579109a 428379 0 2022-12-30 03:39:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:140901f6c6016fa436acd59aacf100295c308651a4fbda1f329d19ff6c466eba cni.projectcalico.org/podIP:10.233.78.174/32 cni.projectcalico.org/podIPs:10.233.78.174/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 3bfb6c67-ec49-43b7-8061-b53d2c4e367c 0xc0046ce4c7 0xc0046ce4c8}] [] [{kube-controller-manager Update v1 2022-12-30 03:39:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3bfb6c67-ec49-43b7-8061-b53d2c4e367c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-12-30 03:39:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-30 03:39:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.78.174\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tpmkd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tpmkd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-mgmt03,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.142,PodIP:10.233.78.174,StartTime:2022-12-30 03:39:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-30 03:39:29 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://23bed5c1b542fadaeb394822cfeb307a3bedab051d04636c4cbc267010ff81b4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.78.174,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 30 03:39:40.441: INFO: Pod "webserver-deployment-845c8977d9-rkxgx" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-rkxgx webserver-deployment-845c8977d9- deployment-5386  656500b1-6f43-47d3-ada8-731edc1c4a83 428982 0 2022-12-30 03:39:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 3bfb6c67-ec49-43b7-8061-b53d2c4e367c 0xc0046ce6d7 0xc0046ce6d8}] [] [{kube-controller-manager Update v1 2022-12-30 03:39:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3bfb6c67-ec49-43b7-8061-b53d2c4e367c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-30 03:39:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9d5rh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9d5rh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.194,PodIP:,StartTime:2022-12-30 03:39:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 30 03:39:40.442: INFO: Pod "webserver-deployment-845c8977d9-s5887" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-s5887 webserver-deployment-845c8977d9- deployment-5386  fa2011bf-1ea7-4aa8-ad8e-2ea7095b7441 429076 0 2022-12-30 03:39:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:1c12db93be5d4b33fd61ef64762094da53623da6e2c2ba81f8e7220723a6c8d9 cni.projectcalico.org/podIP:10.233.125.244/32 cni.projectcalico.org/podIPs:10.233.125.244/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 3bfb6c67-ec49-43b7-8061-b53d2c4e367c 0xc0046ce8a7 0xc0046ce8a8}] [] [{kube-controller-manager Update v1 2022-12-30 03:39:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3bfb6c67-ec49-43b7-8061-b53d2c4e367c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-30 03:39:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2022-12-30 03:39:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w8jc2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w8jc2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-mgmt02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.141,PodIP:,StartTime:2022-12-30 03:39:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 30 03:39:40.442: INFO: Pod "webserver-deployment-845c8977d9-sdgrr" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-sdgrr webserver-deployment-845c8977d9- deployment-5386  a1a26119-a547-4800-bdaf-2e35b2ff93fe 428962 0 2022-12-30 03:39:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 3bfb6c67-ec49-43b7-8061-b53d2c4e367c 0xc0046cea97 0xc0046cea98}] [] [{kube-controller-manager Update v1 2022-12-30 03:39:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3bfb6c67-ec49-43b7-8061-b53d2c4e367c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-30 03:39:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7lvrg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7lvrg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.194,PodIP:,StartTime:2022-12-30 03:39:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 30 03:39:40.442: INFO: Pod "webserver-deployment-845c8977d9-xw4cm" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-xw4cm webserver-deployment-845c8977d9- deployment-5386  067e9269-b22e-4ac3-b113-d3ed870e2fc3 429113 0 2022-12-30 03:39:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:e80c849954513bbc1d7f36df413f2bdf67bbb632ffe271ec6574bf657afa8199 cni.projectcalico.org/podIP:10.233.125.246/32 cni.projectcalico.org/podIPs:10.233.125.246/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 3bfb6c67-ec49-43b7-8061-b53d2c4e367c 0xc0046cec67 0xc0046cec68}] [] [{kube-controller-manager Update v1 2022-12-30 03:39:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3bfb6c67-ec49-43b7-8061-b53d2c4e367c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-12-30 03:39:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-30 03:39:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xg5nd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xg5nd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-mgmt02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:39:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.141,PodIP:,StartTime:2022-12-30 03:39:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Dec 30 03:39:40.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-5386" for this suite. 12/30/22 03:39:40.447
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:39:40.456
Dec 30 03:39:40.456: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename projected 12/30/22 03:39:40.457
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:39:40.471
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:39:40.474
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43
STEP: Creating configMap with name configmap-projected-all-test-volume-93abb7bd-dfef-45db-805a-18a5591cfa48 12/30/22 03:39:40.477
STEP: Creating secret with name secret-projected-all-test-volume-9032d709-c313-4b9b-8948-64b2d618c9d1 12/30/22 03:39:40.481
STEP: Creating a pod to test Check all projections for projected volume plugin 12/30/22 03:39:40.486
Dec 30 03:39:40.495: INFO: Waiting up to 5m0s for pod "projected-volume-e541053f-0b78-42d0-b791-d667003f8d6f" in namespace "projected-2110" to be "Succeeded or Failed"
Dec 30 03:39:40.498: INFO: Pod "projected-volume-e541053f-0b78-42d0-b791-d667003f8d6f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.133864ms
Dec 30 03:39:42.504: INFO: Pod "projected-volume-e541053f-0b78-42d0-b791-d667003f8d6f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008702606s
Dec 30 03:39:44.504: INFO: Pod "projected-volume-e541053f-0b78-42d0-b791-d667003f8d6f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008771162s
STEP: Saw pod success 12/30/22 03:39:44.504
Dec 30 03:39:44.504: INFO: Pod "projected-volume-e541053f-0b78-42d0-b791-d667003f8d6f" satisfied condition "Succeeded or Failed"
Dec 30 03:39:44.508: INFO: Trying to get logs from node k8s-mgmt01 pod projected-volume-e541053f-0b78-42d0-b791-d667003f8d6f container projected-all-volume-test: <nil>
STEP: delete the pod 12/30/22 03:39:44.516
Dec 30 03:39:44.529: INFO: Waiting for pod projected-volume-e541053f-0b78-42d0-b791-d667003f8d6f to disappear
Dec 30 03:39:44.533: INFO: Pod projected-volume-e541053f-0b78-42d0-b791-d667003f8d6f no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:187
Dec 30 03:39:44.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2110" for this suite. 12/30/22 03:39:44.538
{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","completed":82,"skipped":1513,"failed":0}
------------------------------
• [4.088 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:39:40.456
    Dec 30 03:39:40.456: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename projected 12/30/22 03:39:40.457
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:39:40.471
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:39:40.474
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:43
    STEP: Creating configMap with name configmap-projected-all-test-volume-93abb7bd-dfef-45db-805a-18a5591cfa48 12/30/22 03:39:40.477
    STEP: Creating secret with name secret-projected-all-test-volume-9032d709-c313-4b9b-8948-64b2d618c9d1 12/30/22 03:39:40.481
    STEP: Creating a pod to test Check all projections for projected volume plugin 12/30/22 03:39:40.486
    Dec 30 03:39:40.495: INFO: Waiting up to 5m0s for pod "projected-volume-e541053f-0b78-42d0-b791-d667003f8d6f" in namespace "projected-2110" to be "Succeeded or Failed"
    Dec 30 03:39:40.498: INFO: Pod "projected-volume-e541053f-0b78-42d0-b791-d667003f8d6f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.133864ms
    Dec 30 03:39:42.504: INFO: Pod "projected-volume-e541053f-0b78-42d0-b791-d667003f8d6f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008702606s
    Dec 30 03:39:44.504: INFO: Pod "projected-volume-e541053f-0b78-42d0-b791-d667003f8d6f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008771162s
    STEP: Saw pod success 12/30/22 03:39:44.504
    Dec 30 03:39:44.504: INFO: Pod "projected-volume-e541053f-0b78-42d0-b791-d667003f8d6f" satisfied condition "Succeeded or Failed"
    Dec 30 03:39:44.508: INFO: Trying to get logs from node k8s-mgmt01 pod projected-volume-e541053f-0b78-42d0-b791-d667003f8d6f container projected-all-volume-test: <nil>
    STEP: delete the pod 12/30/22 03:39:44.516
    Dec 30 03:39:44.529: INFO: Waiting for pod projected-volume-e541053f-0b78-42d0-b791-d667003f8d6f to disappear
    Dec 30 03:39:44.533: INFO: Pod projected-volume-e541053f-0b78-42d0-b791-d667003f8d6f no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/framework.go:187
    Dec 30 03:39:44.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2110" for this suite. 12/30/22 03:39:44.538
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:39:44.55
Dec 30 03:39:44.551: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename runtimeclass 12/30/22 03:39:44.552
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:39:44.565
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:39:44.568
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-1338-delete-me 12/30/22 03:39:44.576
STEP: Waiting for the RuntimeClass to disappear 12/30/22 03:39:44.583
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Dec 30 03:39:44.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-1338" for this suite. 12/30/22 03:39:44.6
{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]","completed":83,"skipped":1600,"failed":0}
------------------------------
• [0.056 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:39:44.55
    Dec 30 03:39:44.551: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename runtimeclass 12/30/22 03:39:44.552
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:39:44.565
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:39:44.568
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-1338-delete-me 12/30/22 03:39:44.576
    STEP: Waiting for the RuntimeClass to disappear 12/30/22 03:39:44.583
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Dec 30 03:39:44.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-1338" for this suite. 12/30/22 03:39:44.6
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:39:44.609
Dec 30 03:39:44.609: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename configmap 12/30/22 03:39:44.61
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:39:44.623
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:39:44.626
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44
STEP: Creating configMap configmap-9805/configmap-test-7cc0de94-df63-44ad-b08d-2b6e1534c683 12/30/22 03:39:44.629
STEP: Creating a pod to test consume configMaps 12/30/22 03:39:44.633
Dec 30 03:39:44.642: INFO: Waiting up to 5m0s for pod "pod-configmaps-fca5f89e-dfeb-457e-8e8d-e5948c743c7d" in namespace "configmap-9805" to be "Succeeded or Failed"
Dec 30 03:39:44.646: INFO: Pod "pod-configmaps-fca5f89e-dfeb-457e-8e8d-e5948c743c7d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.94646ms
Dec 30 03:39:46.651: INFO: Pod "pod-configmaps-fca5f89e-dfeb-457e-8e8d-e5948c743c7d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008757719s
Dec 30 03:39:48.651: INFO: Pod "pod-configmaps-fca5f89e-dfeb-457e-8e8d-e5948c743c7d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009172615s
STEP: Saw pod success 12/30/22 03:39:48.651
Dec 30 03:39:48.651: INFO: Pod "pod-configmaps-fca5f89e-dfeb-457e-8e8d-e5948c743c7d" satisfied condition "Succeeded or Failed"
Dec 30 03:39:48.655: INFO: Trying to get logs from node k8s-mgmt01 pod pod-configmaps-fca5f89e-dfeb-457e-8e8d-e5948c743c7d container env-test: <nil>
STEP: delete the pod 12/30/22 03:39:48.664
Dec 30 03:39:48.678: INFO: Waiting for pod pod-configmaps-fca5f89e-dfeb-457e-8e8d-e5948c743c7d to disappear
Dec 30 03:39:48.682: INFO: Pod pod-configmaps-fca5f89e-dfeb-457e-8e8d-e5948c743c7d no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Dec 30 03:39:48.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9805" for this suite. 12/30/22 03:39:48.688
{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","completed":84,"skipped":1624,"failed":0}
------------------------------
• [4.086 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:39:44.609
    Dec 30 03:39:44.609: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename configmap 12/30/22 03:39:44.61
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:39:44.623
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:39:44.626
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:44
    STEP: Creating configMap configmap-9805/configmap-test-7cc0de94-df63-44ad-b08d-2b6e1534c683 12/30/22 03:39:44.629
    STEP: Creating a pod to test consume configMaps 12/30/22 03:39:44.633
    Dec 30 03:39:44.642: INFO: Waiting up to 5m0s for pod "pod-configmaps-fca5f89e-dfeb-457e-8e8d-e5948c743c7d" in namespace "configmap-9805" to be "Succeeded or Failed"
    Dec 30 03:39:44.646: INFO: Pod "pod-configmaps-fca5f89e-dfeb-457e-8e8d-e5948c743c7d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.94646ms
    Dec 30 03:39:46.651: INFO: Pod "pod-configmaps-fca5f89e-dfeb-457e-8e8d-e5948c743c7d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008757719s
    Dec 30 03:39:48.651: INFO: Pod "pod-configmaps-fca5f89e-dfeb-457e-8e8d-e5948c743c7d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009172615s
    STEP: Saw pod success 12/30/22 03:39:48.651
    Dec 30 03:39:48.651: INFO: Pod "pod-configmaps-fca5f89e-dfeb-457e-8e8d-e5948c743c7d" satisfied condition "Succeeded or Failed"
    Dec 30 03:39:48.655: INFO: Trying to get logs from node k8s-mgmt01 pod pod-configmaps-fca5f89e-dfeb-457e-8e8d-e5948c743c7d container env-test: <nil>
    STEP: delete the pod 12/30/22 03:39:48.664
    Dec 30 03:39:48.678: INFO: Waiting for pod pod-configmaps-fca5f89e-dfeb-457e-8e8d-e5948c743c7d to disappear
    Dec 30 03:39:48.682: INFO: Pod pod-configmaps-fca5f89e-dfeb-457e-8e8d-e5948c743c7d no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Dec 30 03:39:48.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-9805" for this suite. 12/30/22 03:39:48.688
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:39:48.695
Dec 30 03:39:48.696: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename containers 12/30/22 03:39:48.697
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:39:48.71
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:39:48.713
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58
STEP: Creating a pod to test override arguments 12/30/22 03:39:48.716
Dec 30 03:39:48.724: INFO: Waiting up to 5m0s for pod "client-containers-d1f377c7-21ff-4f40-b5c3-1726e4453e2d" in namespace "containers-1765" to be "Succeeded or Failed"
Dec 30 03:39:48.728: INFO: Pod "client-containers-d1f377c7-21ff-4f40-b5c3-1726e4453e2d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.70485ms
Dec 30 03:39:50.733: INFO: Pod "client-containers-d1f377c7-21ff-4f40-b5c3-1726e4453e2d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008834169s
Dec 30 03:39:52.733: INFO: Pod "client-containers-d1f377c7-21ff-4f40-b5c3-1726e4453e2d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008424381s
STEP: Saw pod success 12/30/22 03:39:52.733
Dec 30 03:39:52.733: INFO: Pod "client-containers-d1f377c7-21ff-4f40-b5c3-1726e4453e2d" satisfied condition "Succeeded or Failed"
Dec 30 03:39:52.737: INFO: Trying to get logs from node k8s-mgmt01 pod client-containers-d1f377c7-21ff-4f40-b5c3-1726e4453e2d container agnhost-container: <nil>
STEP: delete the pod 12/30/22 03:39:52.745
Dec 30 03:39:52.756: INFO: Waiting for pod client-containers-d1f377c7-21ff-4f40-b5c3-1726e4453e2d to disappear
Dec 30 03:39:52.760: INFO: Pod client-containers-d1f377c7-21ff-4f40-b5c3-1726e4453e2d no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Dec 30 03:39:52.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1765" for this suite. 12/30/22 03:39:52.766
{"msg":"PASSED [sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]","completed":85,"skipped":1629,"failed":0}
------------------------------
• [4.077 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:39:48.695
    Dec 30 03:39:48.696: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename containers 12/30/22 03:39:48.697
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:39:48.71
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:39:48.713
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:58
    STEP: Creating a pod to test override arguments 12/30/22 03:39:48.716
    Dec 30 03:39:48.724: INFO: Waiting up to 5m0s for pod "client-containers-d1f377c7-21ff-4f40-b5c3-1726e4453e2d" in namespace "containers-1765" to be "Succeeded or Failed"
    Dec 30 03:39:48.728: INFO: Pod "client-containers-d1f377c7-21ff-4f40-b5c3-1726e4453e2d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.70485ms
    Dec 30 03:39:50.733: INFO: Pod "client-containers-d1f377c7-21ff-4f40-b5c3-1726e4453e2d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008834169s
    Dec 30 03:39:52.733: INFO: Pod "client-containers-d1f377c7-21ff-4f40-b5c3-1726e4453e2d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008424381s
    STEP: Saw pod success 12/30/22 03:39:52.733
    Dec 30 03:39:52.733: INFO: Pod "client-containers-d1f377c7-21ff-4f40-b5c3-1726e4453e2d" satisfied condition "Succeeded or Failed"
    Dec 30 03:39:52.737: INFO: Trying to get logs from node k8s-mgmt01 pod client-containers-d1f377c7-21ff-4f40-b5c3-1726e4453e2d container agnhost-container: <nil>
    STEP: delete the pod 12/30/22 03:39:52.745
    Dec 30 03:39:52.756: INFO: Waiting for pod client-containers-d1f377c7-21ff-4f40-b5c3-1726e4453e2d to disappear
    Dec 30 03:39:52.760: INFO: Pod client-containers-d1f377c7-21ff-4f40-b5c3-1726e4453e2d no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Dec 30 03:39:52.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-1765" for this suite. 12/30/22 03:39:52.766
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:114
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:39:52.773
Dec 30 03:39:52.773: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename container-lifecycle-hook 12/30/22 03:39:52.775
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:39:52.788
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:39:52.791
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 12/30/22 03:39:52.8
Dec 30 03:39:52.813: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4083" to be "running and ready"
Dec 30 03:39:52.816: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.192544ms
Dec 30 03:39:52.816: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Dec 30 03:39:54.821: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007964675s
Dec 30 03:39:54.821: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Dec 30 03:39:56.822: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008642364s
Dec 30 03:39:56.822: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Dec 30 03:39:58.821: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 6.007654183s
Dec 30 03:39:58.821: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Dec 30 03:39:58.821: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:114
STEP: create the pod with lifecycle hook 12/30/22 03:39:58.825
Dec 30 03:39:58.832: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-4083" to be "running and ready"
Dec 30 03:39:58.836: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.185882ms
Dec 30 03:39:58.836: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Dec 30 03:40:00.840: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.007828706s
Dec 30 03:40:00.840: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
Dec 30 03:40:00.840: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 12/30/22 03:40:00.844
Dec 30 03:40:00.852: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 30 03:40:00.856: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 30 03:40:02.857: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 30 03:40:02.862: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 30 03:40:04.858: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 30 03:40:04.862: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 12/30/22 03:40:04.862
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Dec 30 03:40:04.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4083" for this suite. 12/30/22 03:40:04.888
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","completed":86,"skipped":1632,"failed":0}
------------------------------
• [SLOW TEST] [12.122 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:114

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:39:52.773
    Dec 30 03:39:52.773: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename container-lifecycle-hook 12/30/22 03:39:52.775
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:39:52.788
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:39:52.791
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 12/30/22 03:39:52.8
    Dec 30 03:39:52.813: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4083" to be "running and ready"
    Dec 30 03:39:52.816: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.192544ms
    Dec 30 03:39:52.816: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Dec 30 03:39:54.821: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007964675s
    Dec 30 03:39:54.821: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Dec 30 03:39:56.822: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008642364s
    Dec 30 03:39:56.822: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Dec 30 03:39:58.821: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 6.007654183s
    Dec 30 03:39:58.821: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Dec 30 03:39:58.821: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:114
    STEP: create the pod with lifecycle hook 12/30/22 03:39:58.825
    Dec 30 03:39:58.832: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-4083" to be "running and ready"
    Dec 30 03:39:58.836: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.185882ms
    Dec 30 03:39:58.836: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Dec 30 03:40:00.840: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.007828706s
    Dec 30 03:40:00.840: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    Dec 30 03:40:00.840: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 12/30/22 03:40:00.844
    Dec 30 03:40:00.852: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Dec 30 03:40:00.856: INFO: Pod pod-with-prestop-exec-hook still exists
    Dec 30 03:40:02.857: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Dec 30 03:40:02.862: INFO: Pod pod-with-prestop-exec-hook still exists
    Dec 30 03:40:04.858: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Dec 30 03:40:04.862: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 12/30/22 03:40:04.862
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Dec 30 03:40:04.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-4083" for this suite. 12/30/22 03:40:04.888
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1745
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:40:04.897
Dec 30 03:40:04.897: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename kubectl 12/30/22 03:40:04.898
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:40:04.913
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:40:04.916
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1732
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1745
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 12/30/22 03:40:04.92
Dec 30 03:40:04.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-1996 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Dec 30 03:40:05.031: INFO: stderr: ""
Dec 30 03:40:05.031: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 12/30/22 03:40:05.031
STEP: verifying the pod e2e-test-httpd-pod was created 12/30/22 03:40:10.084
Dec 30 03:40:10.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-1996 get pod e2e-test-httpd-pod -o json'
Dec 30 03:40:10.186: INFO: stderr: ""
Dec 30 03:40:10.186: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"257b9ce720b9f81776de25f5ca777acd3003ea58d066d8a145e84c788d0cfce5\",\n            \"cni.projectcalico.org/podIP\": \"10.233.112.161/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.233.112.161/32\"\n        },\n        \"creationTimestamp\": \"2022-12-30T03:40:05Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-1996\",\n        \"resourceVersion\": \"429772\",\n        \"uid\": \"38bc6ef6-7aef-48b5-8eb3-1627d0adcdb7\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-dhz5w\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"k8s-mgmt01\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-dhz5w\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-12-30T03:40:05Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-12-30T03:40:06Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-12-30T03:40:06Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-12-30T03:40:05Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://d02ea91acc17b3159edcba9b80cb12bec183f036ff18b447da28704867edc7e9\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2022-12-30T03:40:05Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.78.26.140\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.233.112.161\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.233.112.161\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2022-12-30T03:40:05Z\"\n    }\n}\n"
STEP: replace the image in the pod 12/30/22 03:40:10.186
Dec 30 03:40:10.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-1996 replace -f -'
Dec 30 03:40:11.296: INFO: stderr: ""
Dec 30 03:40:11.296: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-2 12/30/22 03:40:11.296
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1736
Dec 30 03:40:11.301: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-1996 delete pods e2e-test-httpd-pod'
Dec 30 03:40:13.130: INFO: stderr: ""
Dec 30 03:40:13.130: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Dec 30 03:40:13.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1996" for this suite. 12/30/22 03:40:13.136
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","completed":87,"skipped":1648,"failed":0}
------------------------------
• [SLOW TEST] [8.246 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1729
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1745

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:40:04.897
    Dec 30 03:40:04.897: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename kubectl 12/30/22 03:40:04.898
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:40:04.913
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:40:04.916
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1732
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1745
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 12/30/22 03:40:04.92
    Dec 30 03:40:04.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-1996 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Dec 30 03:40:05.031: INFO: stderr: ""
    Dec 30 03:40:05.031: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 12/30/22 03:40:05.031
    STEP: verifying the pod e2e-test-httpd-pod was created 12/30/22 03:40:10.084
    Dec 30 03:40:10.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-1996 get pod e2e-test-httpd-pod -o json'
    Dec 30 03:40:10.186: INFO: stderr: ""
    Dec 30 03:40:10.186: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"257b9ce720b9f81776de25f5ca777acd3003ea58d066d8a145e84c788d0cfce5\",\n            \"cni.projectcalico.org/podIP\": \"10.233.112.161/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.233.112.161/32\"\n        },\n        \"creationTimestamp\": \"2022-12-30T03:40:05Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-1996\",\n        \"resourceVersion\": \"429772\",\n        \"uid\": \"38bc6ef6-7aef-48b5-8eb3-1627d0adcdb7\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-dhz5w\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"k8s-mgmt01\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-dhz5w\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-12-30T03:40:05Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-12-30T03:40:06Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-12-30T03:40:06Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-12-30T03:40:05Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://d02ea91acc17b3159edcba9b80cb12bec183f036ff18b447da28704867edc7e9\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2022-12-30T03:40:05Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.78.26.140\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.233.112.161\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.233.112.161\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2022-12-30T03:40:05Z\"\n    }\n}\n"
    STEP: replace the image in the pod 12/30/22 03:40:10.186
    Dec 30 03:40:10.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-1996 replace -f -'
    Dec 30 03:40:11.296: INFO: stderr: ""
    Dec 30 03:40:11.296: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-2 12/30/22 03:40:11.296
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1736
    Dec 30 03:40:11.301: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-1996 delete pods e2e-test-httpd-pod'
    Dec 30 03:40:13.130: INFO: stderr: ""
    Dec 30 03:40:13.130: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Dec 30 03:40:13.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-1996" for this suite. 12/30/22 03:40:13.136
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:40:13.143
Dec 30 03:40:13.143: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename projected 12/30/22 03:40:13.145
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:40:13.158
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:40:13.161
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214
STEP: Creating secret with name s-test-opt-del-0c4313e6-d177-4ee0-8b98-a225c9702fd2 12/30/22 03:40:13.169
STEP: Creating secret with name s-test-opt-upd-2f79ad19-f1c0-49b4-80b3-eb94145fd89a 12/30/22 03:40:13.174
STEP: Creating the pod 12/30/22 03:40:13.178
Dec 30 03:40:13.189: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-19cbbd42-c686-4155-8c60-fd1bc8957b30" in namespace "projected-2421" to be "running and ready"
Dec 30 03:40:13.192: INFO: Pod "pod-projected-secrets-19cbbd42-c686-4155-8c60-fd1bc8957b30": Phase="Pending", Reason="", readiness=false. Elapsed: 3.285061ms
Dec 30 03:40:13.193: INFO: The phase of Pod pod-projected-secrets-19cbbd42-c686-4155-8c60-fd1bc8957b30 is Pending, waiting for it to be Running (with Ready = true)
Dec 30 03:40:15.198: INFO: Pod "pod-projected-secrets-19cbbd42-c686-4155-8c60-fd1bc8957b30": Phase="Running", Reason="", readiness=true. Elapsed: 2.008464561s
Dec 30 03:40:15.198: INFO: The phase of Pod pod-projected-secrets-19cbbd42-c686-4155-8c60-fd1bc8957b30 is Running (Ready = true)
Dec 30 03:40:15.198: INFO: Pod "pod-projected-secrets-19cbbd42-c686-4155-8c60-fd1bc8957b30" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-0c4313e6-d177-4ee0-8b98-a225c9702fd2 12/30/22 03:40:15.225
STEP: Updating secret s-test-opt-upd-2f79ad19-f1c0-49b4-80b3-eb94145fd89a 12/30/22 03:40:15.232
STEP: Creating secret with name s-test-opt-create-37c48de4-f8f1-4cad-a7ca-e725e68bf128 12/30/22 03:40:15.237
STEP: waiting to observe update in volume 12/30/22 03:40:15.241
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Dec 30 03:40:19.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2421" for this suite. 12/30/22 03:40:19.287
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":88,"skipped":1648,"failed":0}
------------------------------
• [SLOW TEST] [6.151 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:40:13.143
    Dec 30 03:40:13.143: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename projected 12/30/22 03:40:13.145
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:40:13.158
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:40:13.161
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:214
    STEP: Creating secret with name s-test-opt-del-0c4313e6-d177-4ee0-8b98-a225c9702fd2 12/30/22 03:40:13.169
    STEP: Creating secret with name s-test-opt-upd-2f79ad19-f1c0-49b4-80b3-eb94145fd89a 12/30/22 03:40:13.174
    STEP: Creating the pod 12/30/22 03:40:13.178
    Dec 30 03:40:13.189: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-19cbbd42-c686-4155-8c60-fd1bc8957b30" in namespace "projected-2421" to be "running and ready"
    Dec 30 03:40:13.192: INFO: Pod "pod-projected-secrets-19cbbd42-c686-4155-8c60-fd1bc8957b30": Phase="Pending", Reason="", readiness=false. Elapsed: 3.285061ms
    Dec 30 03:40:13.193: INFO: The phase of Pod pod-projected-secrets-19cbbd42-c686-4155-8c60-fd1bc8957b30 is Pending, waiting for it to be Running (with Ready = true)
    Dec 30 03:40:15.198: INFO: Pod "pod-projected-secrets-19cbbd42-c686-4155-8c60-fd1bc8957b30": Phase="Running", Reason="", readiness=true. Elapsed: 2.008464561s
    Dec 30 03:40:15.198: INFO: The phase of Pod pod-projected-secrets-19cbbd42-c686-4155-8c60-fd1bc8957b30 is Running (Ready = true)
    Dec 30 03:40:15.198: INFO: Pod "pod-projected-secrets-19cbbd42-c686-4155-8c60-fd1bc8957b30" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-0c4313e6-d177-4ee0-8b98-a225c9702fd2 12/30/22 03:40:15.225
    STEP: Updating secret s-test-opt-upd-2f79ad19-f1c0-49b4-80b3-eb94145fd89a 12/30/22 03:40:15.232
    STEP: Creating secret with name s-test-opt-create-37c48de4-f8f1-4cad-a7ca-e725e68bf128 12/30/22 03:40:15.237
    STEP: waiting to observe update in volume 12/30/22 03:40:15.241
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Dec 30 03:40:19.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2421" for this suite. 12/30/22 03:40:19.287
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:392
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:40:19.299
Dec 30 03:40:19.299: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename kubectl 12/30/22 03:40:19.3
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:40:19.315
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:40:19.318
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:392
STEP: creating all guestbook components 12/30/22 03:40:19.321
Dec 30 03:40:19.321: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Dec 30 03:40:19.321: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-257 create -f -'
Dec 30 03:40:20.427: INFO: stderr: ""
Dec 30 03:40:20.427: INFO: stdout: "service/agnhost-replica created\n"
Dec 30 03:40:20.427: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Dec 30 03:40:20.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-257 create -f -'
Dec 30 03:40:20.719: INFO: stderr: ""
Dec 30 03:40:20.719: INFO: stdout: "service/agnhost-primary created\n"
Dec 30 03:40:20.719: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Dec 30 03:40:20.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-257 create -f -'
Dec 30 03:40:21.802: INFO: stderr: ""
Dec 30 03:40:21.802: INFO: stdout: "service/frontend created\n"
Dec 30 03:40:21.802: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Dec 30 03:40:21.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-257 create -f -'
Dec 30 03:40:22.061: INFO: stderr: ""
Dec 30 03:40:22.061: INFO: stdout: "deployment.apps/frontend created\n"
Dec 30 03:40:22.061: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Dec 30 03:40:22.061: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-257 create -f -'
Dec 30 03:40:22.347: INFO: stderr: ""
Dec 30 03:40:22.347: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Dec 30 03:40:22.347: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Dec 30 03:40:22.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-257 create -f -'
Dec 30 03:40:22.625: INFO: stderr: ""
Dec 30 03:40:22.625: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 12/30/22 03:40:22.625
Dec 30 03:40:22.625: INFO: Waiting for all frontend pods to be Running.
Dec 30 03:40:27.678: INFO: Waiting for frontend to serve content.
Dec 30 03:40:27.689: INFO: Trying to add a new entry to the guestbook.
Dec 30 03:40:27.702: INFO: Verifying that added entry can be retrieved.
Dec 30 03:40:27.714: INFO: Failed to get response from guestbook. err: <nil>, response: {"data":""}
STEP: using delete to clean up resources 12/30/22 03:40:32.73
Dec 30 03:40:32.730: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-257 delete --grace-period=0 --force -f -'
Dec 30 03:40:32.831: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 30 03:40:32.831: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 12/30/22 03:40:32.831
Dec 30 03:40:32.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-257 delete --grace-period=0 --force -f -'
Dec 30 03:40:32.956: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 30 03:40:32.956: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 12/30/22 03:40:32.956
Dec 30 03:40:32.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-257 delete --grace-period=0 --force -f -'
Dec 30 03:40:33.064: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 30 03:40:33.064: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 12/30/22 03:40:33.064
Dec 30 03:40:33.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-257 delete --grace-period=0 --force -f -'
Dec 30 03:40:33.199: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 30 03:40:33.199: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 12/30/22 03:40:33.199
Dec 30 03:40:33.199: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-257 delete --grace-period=0 --force -f -'
Dec 30 03:40:33.299: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 30 03:40:33.299: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 12/30/22 03:40:33.299
Dec 30 03:40:33.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-257 delete --grace-period=0 --force -f -'
Dec 30 03:40:33.404: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 30 03:40:33.405: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Dec 30 03:40:33.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-257" for this suite. 12/30/22 03:40:33.411
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","completed":89,"skipped":1709,"failed":0}
------------------------------
• [SLOW TEST] [14.119 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:367
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:40:19.299
    Dec 30 03:40:19.299: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename kubectl 12/30/22 03:40:19.3
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:40:19.315
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:40:19.318
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:392
    STEP: creating all guestbook components 12/30/22 03:40:19.321
    Dec 30 03:40:19.321: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    Dec 30 03:40:19.321: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-257 create -f -'
    Dec 30 03:40:20.427: INFO: stderr: ""
    Dec 30 03:40:20.427: INFO: stdout: "service/agnhost-replica created\n"
    Dec 30 03:40:20.427: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    Dec 30 03:40:20.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-257 create -f -'
    Dec 30 03:40:20.719: INFO: stderr: ""
    Dec 30 03:40:20.719: INFO: stdout: "service/agnhost-primary created\n"
    Dec 30 03:40:20.719: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    Dec 30 03:40:20.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-257 create -f -'
    Dec 30 03:40:21.802: INFO: stderr: ""
    Dec 30 03:40:21.802: INFO: stdout: "service/frontend created\n"
    Dec 30 03:40:21.802: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    Dec 30 03:40:21.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-257 create -f -'
    Dec 30 03:40:22.061: INFO: stderr: ""
    Dec 30 03:40:22.061: INFO: stdout: "deployment.apps/frontend created\n"
    Dec 30 03:40:22.061: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Dec 30 03:40:22.061: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-257 create -f -'
    Dec 30 03:40:22.347: INFO: stderr: ""
    Dec 30 03:40:22.347: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    Dec 30 03:40:22.347: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Dec 30 03:40:22.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-257 create -f -'
    Dec 30 03:40:22.625: INFO: stderr: ""
    Dec 30 03:40:22.625: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 12/30/22 03:40:22.625
    Dec 30 03:40:22.625: INFO: Waiting for all frontend pods to be Running.
    Dec 30 03:40:27.678: INFO: Waiting for frontend to serve content.
    Dec 30 03:40:27.689: INFO: Trying to add a new entry to the guestbook.
    Dec 30 03:40:27.702: INFO: Verifying that added entry can be retrieved.
    Dec 30 03:40:27.714: INFO: Failed to get response from guestbook. err: <nil>, response: {"data":""}
    STEP: using delete to clean up resources 12/30/22 03:40:32.73
    Dec 30 03:40:32.730: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-257 delete --grace-period=0 --force -f -'
    Dec 30 03:40:32.831: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Dec 30 03:40:32.831: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 12/30/22 03:40:32.831
    Dec 30 03:40:32.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-257 delete --grace-period=0 --force -f -'
    Dec 30 03:40:32.956: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Dec 30 03:40:32.956: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 12/30/22 03:40:32.956
    Dec 30 03:40:32.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-257 delete --grace-period=0 --force -f -'
    Dec 30 03:40:33.064: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Dec 30 03:40:33.064: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 12/30/22 03:40:33.064
    Dec 30 03:40:33.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-257 delete --grace-period=0 --force -f -'
    Dec 30 03:40:33.199: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Dec 30 03:40:33.199: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 12/30/22 03:40:33.199
    Dec 30 03:40:33.199: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-257 delete --grace-period=0 --force -f -'
    Dec 30 03:40:33.299: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Dec 30 03:40:33.299: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 12/30/22 03:40:33.299
    Dec 30 03:40:33.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-257 delete --grace-period=0 --force -f -'
    Dec 30 03:40:33.404: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Dec 30 03:40:33.405: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Dec 30 03:40:33.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-257" for this suite. 12/30/22 03:40:33.411
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:40:33.419
Dec 30 03:40:33.419: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename container-probe 12/30/22 03:40:33.421
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:40:33.436
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:40:33.438
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148
STEP: Creating pod busybox-507aa48b-b546-41ba-b8b3-3e45d6f1154a in namespace container-probe-6103 12/30/22 03:40:33.441
Dec 30 03:40:33.449: INFO: Waiting up to 5m0s for pod "busybox-507aa48b-b546-41ba-b8b3-3e45d6f1154a" in namespace "container-probe-6103" to be "not pending"
Dec 30 03:40:33.452: INFO: Pod "busybox-507aa48b-b546-41ba-b8b3-3e45d6f1154a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.281181ms
Dec 30 03:40:35.457: INFO: Pod "busybox-507aa48b-b546-41ba-b8b3-3e45d6f1154a": Phase="Running", Reason="", readiness=true. Elapsed: 2.007517071s
Dec 30 03:40:35.457: INFO: Pod "busybox-507aa48b-b546-41ba-b8b3-3e45d6f1154a" satisfied condition "not pending"
Dec 30 03:40:35.457: INFO: Started pod busybox-507aa48b-b546-41ba-b8b3-3e45d6f1154a in namespace container-probe-6103
STEP: checking the pod's current state and verifying that restartCount is present 12/30/22 03:40:35.457
Dec 30 03:40:35.461: INFO: Initial restart count of pod busybox-507aa48b-b546-41ba-b8b3-3e45d6f1154a is 0
STEP: deleting the pod 12/30/22 03:44:36.104
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Dec 30 03:44:36.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6103" for this suite. 12/30/22 03:44:36.125
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","completed":90,"skipped":1717,"failed":0}
------------------------------
• [SLOW TEST] [242.713 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:40:33.419
    Dec 30 03:40:33.419: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename container-probe 12/30/22 03:40:33.421
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:40:33.436
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:40:33.438
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:148
    STEP: Creating pod busybox-507aa48b-b546-41ba-b8b3-3e45d6f1154a in namespace container-probe-6103 12/30/22 03:40:33.441
    Dec 30 03:40:33.449: INFO: Waiting up to 5m0s for pod "busybox-507aa48b-b546-41ba-b8b3-3e45d6f1154a" in namespace "container-probe-6103" to be "not pending"
    Dec 30 03:40:33.452: INFO: Pod "busybox-507aa48b-b546-41ba-b8b3-3e45d6f1154a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.281181ms
    Dec 30 03:40:35.457: INFO: Pod "busybox-507aa48b-b546-41ba-b8b3-3e45d6f1154a": Phase="Running", Reason="", readiness=true. Elapsed: 2.007517071s
    Dec 30 03:40:35.457: INFO: Pod "busybox-507aa48b-b546-41ba-b8b3-3e45d6f1154a" satisfied condition "not pending"
    Dec 30 03:40:35.457: INFO: Started pod busybox-507aa48b-b546-41ba-b8b3-3e45d6f1154a in namespace container-probe-6103
    STEP: checking the pod's current state and verifying that restartCount is present 12/30/22 03:40:35.457
    Dec 30 03:40:35.461: INFO: Initial restart count of pod busybox-507aa48b-b546-41ba-b8b3-3e45d6f1154a is 0
    STEP: deleting the pod 12/30/22 03:44:36.104
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Dec 30 03:44:36.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-6103" for this suite. 12/30/22 03:44:36.125
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:44:36.132
Dec 30 03:44:36.132: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename projected 12/30/22 03:44:36.134
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:44:36.147
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:44:36.15
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374
STEP: Creating configMap with name projected-configmap-test-volume-ba8f7f39-a602-4af0-84e0-20da857c3955 12/30/22 03:44:36.154
STEP: Creating a pod to test consume configMaps 12/30/22 03:44:36.159
Dec 30 03:44:36.168: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b2b559c8-b432-4ee4-9aaa-45b7743f1a9d" in namespace "projected-1940" to be "Succeeded or Failed"
Dec 30 03:44:36.172: INFO: Pod "pod-projected-configmaps-b2b559c8-b432-4ee4-9aaa-45b7743f1a9d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.825079ms
Dec 30 03:44:38.176: INFO: Pod "pod-projected-configmaps-b2b559c8-b432-4ee4-9aaa-45b7743f1a9d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008175315s
Dec 30 03:44:40.178: INFO: Pod "pod-projected-configmaps-b2b559c8-b432-4ee4-9aaa-45b7743f1a9d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009528291s
STEP: Saw pod success 12/30/22 03:44:40.178
Dec 30 03:44:40.178: INFO: Pod "pod-projected-configmaps-b2b559c8-b432-4ee4-9aaa-45b7743f1a9d" satisfied condition "Succeeded or Failed"
Dec 30 03:44:40.182: INFO: Trying to get logs from node k8s-mgmt01 pod pod-projected-configmaps-b2b559c8-b432-4ee4-9aaa-45b7743f1a9d container projected-configmap-volume-test: <nil>
STEP: delete the pod 12/30/22 03:44:40.202
Dec 30 03:44:40.214: INFO: Waiting for pod pod-projected-configmaps-b2b559c8-b432-4ee4-9aaa-45b7743f1a9d to disappear
Dec 30 03:44:40.218: INFO: Pod pod-projected-configmaps-b2b559c8-b432-4ee4-9aaa-45b7743f1a9d no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Dec 30 03:44:40.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1940" for this suite. 12/30/22 03:44:40.224
{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","completed":91,"skipped":1721,"failed":0}
------------------------------
• [4.099 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:44:36.132
    Dec 30 03:44:36.132: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename projected 12/30/22 03:44:36.134
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:44:36.147
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:44:36.15
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:374
    STEP: Creating configMap with name projected-configmap-test-volume-ba8f7f39-a602-4af0-84e0-20da857c3955 12/30/22 03:44:36.154
    STEP: Creating a pod to test consume configMaps 12/30/22 03:44:36.159
    Dec 30 03:44:36.168: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b2b559c8-b432-4ee4-9aaa-45b7743f1a9d" in namespace "projected-1940" to be "Succeeded or Failed"
    Dec 30 03:44:36.172: INFO: Pod "pod-projected-configmaps-b2b559c8-b432-4ee4-9aaa-45b7743f1a9d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.825079ms
    Dec 30 03:44:38.176: INFO: Pod "pod-projected-configmaps-b2b559c8-b432-4ee4-9aaa-45b7743f1a9d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008175315s
    Dec 30 03:44:40.178: INFO: Pod "pod-projected-configmaps-b2b559c8-b432-4ee4-9aaa-45b7743f1a9d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009528291s
    STEP: Saw pod success 12/30/22 03:44:40.178
    Dec 30 03:44:40.178: INFO: Pod "pod-projected-configmaps-b2b559c8-b432-4ee4-9aaa-45b7743f1a9d" satisfied condition "Succeeded or Failed"
    Dec 30 03:44:40.182: INFO: Trying to get logs from node k8s-mgmt01 pod pod-projected-configmaps-b2b559c8-b432-4ee4-9aaa-45b7743f1a9d container projected-configmap-volume-test: <nil>
    STEP: delete the pod 12/30/22 03:44:40.202
    Dec 30 03:44:40.214: INFO: Waiting for pod pod-projected-configmaps-b2b559c8-b432-4ee4-9aaa-45b7743f1a9d to disappear
    Dec 30 03:44:40.218: INFO: Pod pod-projected-configmaps-b2b559c8-b432-4ee4-9aaa-45b7743f1a9d no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Dec 30 03:44:40.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1940" for this suite. 12/30/22 03:44:40.224
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:44:40.234
Dec 30 03:44:40.234: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename services 12/30/22 03:44:40.236
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:44:40.249
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:44:40.253
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852
STEP: creating service multi-endpoint-test in namespace services-6642 12/30/22 03:44:40.256
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6642 to expose endpoints map[] 12/30/22 03:44:40.266
Dec 30 03:44:40.270: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Dec 30 03:44:41.279: INFO: successfully validated that service multi-endpoint-test in namespace services-6642 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-6642 12/30/22 03:44:41.279
Dec 30 03:44:41.290: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-6642" to be "running and ready"
Dec 30 03:44:41.293: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.86944ms
Dec 30 03:44:41.294: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Dec 30 03:44:43.298: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.008599599s
Dec 30 03:44:43.298: INFO: The phase of Pod pod1 is Running (Ready = true)
Dec 30 03:44:43.298: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6642 to expose endpoints map[pod1:[100]] 12/30/22 03:44:43.302
Dec 30 03:44:43.315: INFO: successfully validated that service multi-endpoint-test in namespace services-6642 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-6642 12/30/22 03:44:43.315
Dec 30 03:44:43.321: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-6642" to be "running and ready"
Dec 30 03:44:43.325: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.743349ms
Dec 30 03:44:43.325: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Dec 30 03:44:45.331: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.009750966s
Dec 30 03:44:45.331: INFO: The phase of Pod pod2 is Running (Ready = true)
Dec 30 03:44:45.331: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6642 to expose endpoints map[pod1:[100] pod2:[101]] 12/30/22 03:44:45.335
Dec 30 03:44:45.349: INFO: successfully validated that service multi-endpoint-test in namespace services-6642 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 12/30/22 03:44:45.349
Dec 30 03:44:45.349: INFO: Creating new exec pod
Dec 30 03:44:45.355: INFO: Waiting up to 5m0s for pod "execpodqvg2s" in namespace "services-6642" to be "running"
Dec 30 03:44:45.358: INFO: Pod "execpodqvg2s": Phase="Pending", Reason="", readiness=false. Elapsed: 3.174005ms
Dec 30 03:44:47.363: INFO: Pod "execpodqvg2s": Phase="Running", Reason="", readiness=true. Elapsed: 2.008076888s
Dec 30 03:44:47.363: INFO: Pod "execpodqvg2s" satisfied condition "running"
Dec 30 03:44:48.364: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-6642 exec execpodqvg2s -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
Dec 30 03:44:48.594: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Dec 30 03:44:48.594: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 30 03:44:48.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-6642 exec execpodqvg2s -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.60.199 80'
Dec 30 03:44:48.783: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.60.199 80\nConnection to 10.233.60.199 80 port [tcp/http] succeeded!\n"
Dec 30 03:44:48.783: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 30 03:44:48.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-6642 exec execpodqvg2s -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
Dec 30 03:44:48.979: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Dec 30 03:44:48.979: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 30 03:44:48.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-6642 exec execpodqvg2s -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.60.199 81'
Dec 30 03:44:49.175: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.60.199 81\nConnection to 10.233.60.199 81 port [tcp/*] succeeded!\n"
Dec 30 03:44:49.175: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-6642 12/30/22 03:44:49.175
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6642 to expose endpoints map[pod2:[101]] 12/30/22 03:44:49.189
Dec 30 03:44:49.202: INFO: successfully validated that service multi-endpoint-test in namespace services-6642 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-6642 12/30/22 03:44:49.202
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6642 to expose endpoints map[] 12/30/22 03:44:49.215
Dec 30 03:44:49.223: INFO: successfully validated that service multi-endpoint-test in namespace services-6642 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Dec 30 03:44:49.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6642" for this suite. 12/30/22 03:44:49.243
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","completed":92,"skipped":1724,"failed":0}
------------------------------
• [SLOW TEST] [9.015 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:44:40.234
    Dec 30 03:44:40.234: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename services 12/30/22 03:44:40.236
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:44:40.249
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:44:40.253
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:852
    STEP: creating service multi-endpoint-test in namespace services-6642 12/30/22 03:44:40.256
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6642 to expose endpoints map[] 12/30/22 03:44:40.266
    Dec 30 03:44:40.270: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
    Dec 30 03:44:41.279: INFO: successfully validated that service multi-endpoint-test in namespace services-6642 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-6642 12/30/22 03:44:41.279
    Dec 30 03:44:41.290: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-6642" to be "running and ready"
    Dec 30 03:44:41.293: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.86944ms
    Dec 30 03:44:41.294: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Dec 30 03:44:43.298: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.008599599s
    Dec 30 03:44:43.298: INFO: The phase of Pod pod1 is Running (Ready = true)
    Dec 30 03:44:43.298: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6642 to expose endpoints map[pod1:[100]] 12/30/22 03:44:43.302
    Dec 30 03:44:43.315: INFO: successfully validated that service multi-endpoint-test in namespace services-6642 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-6642 12/30/22 03:44:43.315
    Dec 30 03:44:43.321: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-6642" to be "running and ready"
    Dec 30 03:44:43.325: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.743349ms
    Dec 30 03:44:43.325: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Dec 30 03:44:45.331: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.009750966s
    Dec 30 03:44:45.331: INFO: The phase of Pod pod2 is Running (Ready = true)
    Dec 30 03:44:45.331: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6642 to expose endpoints map[pod1:[100] pod2:[101]] 12/30/22 03:44:45.335
    Dec 30 03:44:45.349: INFO: successfully validated that service multi-endpoint-test in namespace services-6642 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 12/30/22 03:44:45.349
    Dec 30 03:44:45.349: INFO: Creating new exec pod
    Dec 30 03:44:45.355: INFO: Waiting up to 5m0s for pod "execpodqvg2s" in namespace "services-6642" to be "running"
    Dec 30 03:44:45.358: INFO: Pod "execpodqvg2s": Phase="Pending", Reason="", readiness=false. Elapsed: 3.174005ms
    Dec 30 03:44:47.363: INFO: Pod "execpodqvg2s": Phase="Running", Reason="", readiness=true. Elapsed: 2.008076888s
    Dec 30 03:44:47.363: INFO: Pod "execpodqvg2s" satisfied condition "running"
    Dec 30 03:44:48.364: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-6642 exec execpodqvg2s -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
    Dec 30 03:44:48.594: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    Dec 30 03:44:48.594: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Dec 30 03:44:48.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-6642 exec execpodqvg2s -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.60.199 80'
    Dec 30 03:44:48.783: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.60.199 80\nConnection to 10.233.60.199 80 port [tcp/http] succeeded!\n"
    Dec 30 03:44:48.783: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Dec 30 03:44:48.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-6642 exec execpodqvg2s -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
    Dec 30 03:44:48.979: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    Dec 30 03:44:48.979: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Dec 30 03:44:48.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-6642 exec execpodqvg2s -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.60.199 81'
    Dec 30 03:44:49.175: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.60.199 81\nConnection to 10.233.60.199 81 port [tcp/*] succeeded!\n"
    Dec 30 03:44:49.175: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod1 in namespace services-6642 12/30/22 03:44:49.175
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6642 to expose endpoints map[pod2:[101]] 12/30/22 03:44:49.189
    Dec 30 03:44:49.202: INFO: successfully validated that service multi-endpoint-test in namespace services-6642 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-6642 12/30/22 03:44:49.202
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6642 to expose endpoints map[] 12/30/22 03:44:49.215
    Dec 30 03:44:49.223: INFO: successfully validated that service multi-endpoint-test in namespace services-6642 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Dec 30 03:44:49.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-6642" for this suite. 12/30/22 03:44:49.243
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:44:49.25
Dec 30 03:44:49.251: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename projected 12/30/22 03:44:49.252
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:44:49.267
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:44:49.27
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56
STEP: Creating configMap with name projected-configmap-test-volume-1e58e231-c491-4005-a060-bbe45df39b92 12/30/22 03:44:49.272
STEP: Creating a pod to test consume configMaps 12/30/22 03:44:49.276
Dec 30 03:44:49.286: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-cb9abad9-7b04-4607-81af-e8deeb1fd5c4" in namespace "projected-1709" to be "Succeeded or Failed"
Dec 30 03:44:49.290: INFO: Pod "pod-projected-configmaps-cb9abad9-7b04-4607-81af-e8deeb1fd5c4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032034ms
Dec 30 03:44:51.296: INFO: Pod "pod-projected-configmaps-cb9abad9-7b04-4607-81af-e8deeb1fd5c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009600132s
Dec 30 03:44:53.297: INFO: Pod "pod-projected-configmaps-cb9abad9-7b04-4607-81af-e8deeb1fd5c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010745633s
STEP: Saw pod success 12/30/22 03:44:53.297
Dec 30 03:44:53.297: INFO: Pod "pod-projected-configmaps-cb9abad9-7b04-4607-81af-e8deeb1fd5c4" satisfied condition "Succeeded or Failed"
Dec 30 03:44:53.301: INFO: Trying to get logs from node k8s-mgmt01 pod pod-projected-configmaps-cb9abad9-7b04-4607-81af-e8deeb1fd5c4 container agnhost-container: <nil>
STEP: delete the pod 12/30/22 03:44:53.309
Dec 30 03:44:53.323: INFO: Waiting for pod pod-projected-configmaps-cb9abad9-7b04-4607-81af-e8deeb1fd5c4 to disappear
Dec 30 03:44:53.327: INFO: Pod pod-projected-configmaps-cb9abad9-7b04-4607-81af-e8deeb1fd5c4 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Dec 30 03:44:53.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1709" for this suite. 12/30/22 03:44:53.331
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":93,"skipped":1743,"failed":0}
------------------------------
• [4.088 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:44:49.25
    Dec 30 03:44:49.251: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename projected 12/30/22 03:44:49.252
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:44:49.267
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:44:49.27
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:56
    STEP: Creating configMap with name projected-configmap-test-volume-1e58e231-c491-4005-a060-bbe45df39b92 12/30/22 03:44:49.272
    STEP: Creating a pod to test consume configMaps 12/30/22 03:44:49.276
    Dec 30 03:44:49.286: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-cb9abad9-7b04-4607-81af-e8deeb1fd5c4" in namespace "projected-1709" to be "Succeeded or Failed"
    Dec 30 03:44:49.290: INFO: Pod "pod-projected-configmaps-cb9abad9-7b04-4607-81af-e8deeb1fd5c4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032034ms
    Dec 30 03:44:51.296: INFO: Pod "pod-projected-configmaps-cb9abad9-7b04-4607-81af-e8deeb1fd5c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009600132s
    Dec 30 03:44:53.297: INFO: Pod "pod-projected-configmaps-cb9abad9-7b04-4607-81af-e8deeb1fd5c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010745633s
    STEP: Saw pod success 12/30/22 03:44:53.297
    Dec 30 03:44:53.297: INFO: Pod "pod-projected-configmaps-cb9abad9-7b04-4607-81af-e8deeb1fd5c4" satisfied condition "Succeeded or Failed"
    Dec 30 03:44:53.301: INFO: Trying to get logs from node k8s-mgmt01 pod pod-projected-configmaps-cb9abad9-7b04-4607-81af-e8deeb1fd5c4 container agnhost-container: <nil>
    STEP: delete the pod 12/30/22 03:44:53.309
    Dec 30 03:44:53.323: INFO: Waiting for pod pod-projected-configmaps-cb9abad9-7b04-4607-81af-e8deeb1fd5c4 to disappear
    Dec 30 03:44:53.327: INFO: Pod pod-projected-configmaps-cb9abad9-7b04-4607-81af-e8deeb1fd5c4 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Dec 30 03:44:53.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1709" for this suite. 12/30/22 03:44:53.331
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:44:53.343
Dec 30 03:44:53.343: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename container-probe 12/30/22 03:44:53.344
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:44:53.358
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:44:53.361
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Dec 30 03:45:53.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-942" for this suite. 12/30/22 03:45:53.384
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","completed":94,"skipped":1795,"failed":0}
------------------------------
• [SLOW TEST] [60.048 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:44:53.343
    Dec 30 03:44:53.343: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename container-probe 12/30/22 03:44:53.344
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:44:53.358
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:44:53.361
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:104
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Dec 30 03:45:53.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-942" for this suite. 12/30/22 03:45:53.384
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:45:53.392
Dec 30 03:45:53.392: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename custom-resource-definition 12/30/22 03:45:53.393
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:45:53.407
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:45:53.411
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 12/30/22 03:45:53.414
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 12/30/22 03:45:53.415
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 12/30/22 03:45:53.415
STEP: fetching the /apis/apiextensions.k8s.io discovery document 12/30/22 03:45:53.415
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 12/30/22 03:45:53.417
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 12/30/22 03:45:53.417
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 12/30/22 03:45:53.418
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Dec 30 03:45:53.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1195" for this suite. 12/30/22 03:45:53.424
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","completed":95,"skipped":1804,"failed":0}
------------------------------
• [0.039 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:45:53.392
    Dec 30 03:45:53.392: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename custom-resource-definition 12/30/22 03:45:53.393
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:45:53.407
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:45:53.411
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 12/30/22 03:45:53.414
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 12/30/22 03:45:53.415
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 12/30/22 03:45:53.415
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 12/30/22 03:45:53.415
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 12/30/22 03:45:53.417
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 12/30/22 03:45:53.417
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 12/30/22 03:45:53.418
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Dec 30 03:45:53.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-1195" for this suite. 12/30/22 03:45:53.424
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:45:53.432
Dec 30 03:45:53.432: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename crd-publish-openapi 12/30/22 03:45:53.433
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:45:53.445
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:45:53.448
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235
Dec 30 03:45:53.452: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 12/30/22 03:46:03.262
Dec 30 03:46:03.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=crd-publish-openapi-9818 --namespace=crd-publish-openapi-9818 create -f -'
Dec 30 03:46:04.084: INFO: stderr: ""
Dec 30 03:46:04.084: INFO: stdout: "e2e-test-crd-publish-openapi-4644-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Dec 30 03:46:04.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=crd-publish-openapi-9818 --namespace=crd-publish-openapi-9818 delete e2e-test-crd-publish-openapi-4644-crds test-cr'
Dec 30 03:46:04.234: INFO: stderr: ""
Dec 30 03:46:04.234: INFO: stdout: "e2e-test-crd-publish-openapi-4644-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Dec 30 03:46:04.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=crd-publish-openapi-9818 --namespace=crd-publish-openapi-9818 apply -f -'
Dec 30 03:46:05.012: INFO: stderr: ""
Dec 30 03:46:05.012: INFO: stdout: "e2e-test-crd-publish-openapi-4644-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Dec 30 03:46:05.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=crd-publish-openapi-9818 --namespace=crd-publish-openapi-9818 delete e2e-test-crd-publish-openapi-4644-crds test-cr'
Dec 30 03:46:05.113: INFO: stderr: ""
Dec 30 03:46:05.113: INFO: stdout: "e2e-test-crd-publish-openapi-4644-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 12/30/22 03:46:05.113
Dec 30 03:46:05.114: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=crd-publish-openapi-9818 explain e2e-test-crd-publish-openapi-4644-crds'
Dec 30 03:46:05.903: INFO: stderr: ""
Dec 30 03:46:05.903: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4644-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Dec 30 03:46:09.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9818" for this suite. 12/30/22 03:46:09.119
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","completed":96,"skipped":1810,"failed":0}
------------------------------
• [SLOW TEST] [15.694 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:45:53.432
    Dec 30 03:45:53.432: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename crd-publish-openapi 12/30/22 03:45:53.433
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:45:53.445
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:45:53.448
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:235
    Dec 30 03:45:53.452: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 12/30/22 03:46:03.262
    Dec 30 03:46:03.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=crd-publish-openapi-9818 --namespace=crd-publish-openapi-9818 create -f -'
    Dec 30 03:46:04.084: INFO: stderr: ""
    Dec 30 03:46:04.084: INFO: stdout: "e2e-test-crd-publish-openapi-4644-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Dec 30 03:46:04.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=crd-publish-openapi-9818 --namespace=crd-publish-openapi-9818 delete e2e-test-crd-publish-openapi-4644-crds test-cr'
    Dec 30 03:46:04.234: INFO: stderr: ""
    Dec 30 03:46:04.234: INFO: stdout: "e2e-test-crd-publish-openapi-4644-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    Dec 30 03:46:04.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=crd-publish-openapi-9818 --namespace=crd-publish-openapi-9818 apply -f -'
    Dec 30 03:46:05.012: INFO: stderr: ""
    Dec 30 03:46:05.012: INFO: stdout: "e2e-test-crd-publish-openapi-4644-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Dec 30 03:46:05.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=crd-publish-openapi-9818 --namespace=crd-publish-openapi-9818 delete e2e-test-crd-publish-openapi-4644-crds test-cr'
    Dec 30 03:46:05.113: INFO: stderr: ""
    Dec 30 03:46:05.113: INFO: stdout: "e2e-test-crd-publish-openapi-4644-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 12/30/22 03:46:05.113
    Dec 30 03:46:05.114: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=crd-publish-openapi-9818 explain e2e-test-crd-publish-openapi-4644-crds'
    Dec 30 03:46:05.903: INFO: stderr: ""
    Dec 30 03:46:05.903: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4644-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Dec 30 03:46:09.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-9818" for this suite. 12/30/22 03:46:09.119
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:46:09.131
Dec 30 03:46:09.131: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename endpointslice 12/30/22 03:46:09.132
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:46:09.147
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:46:09.15
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352
STEP: getting /apis 12/30/22 03:46:09.154
STEP: getting /apis/discovery.k8s.io 12/30/22 03:46:09.157
STEP: getting /apis/discovery.k8s.iov1 12/30/22 03:46:09.158
STEP: creating 12/30/22 03:46:09.16
STEP: getting 12/30/22 03:46:09.183
STEP: listing 12/30/22 03:46:09.187
STEP: watching 12/30/22 03:46:09.191
Dec 30 03:46:09.191: INFO: starting watch
STEP: cluster-wide listing 12/30/22 03:46:09.192
STEP: cluster-wide watching 12/30/22 03:46:09.197
Dec 30 03:46:09.197: INFO: starting watch
STEP: patching 12/30/22 03:46:09.198
STEP: updating 12/30/22 03:46:09.205
Dec 30 03:46:09.214: INFO: waiting for watch events with expected annotations
Dec 30 03:46:09.214: INFO: saw patched and updated annotations
STEP: deleting 12/30/22 03:46:09.214
STEP: deleting a collection 12/30/22 03:46:09.228
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Dec 30 03:46:09.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-9870" for this suite. 12/30/22 03:46:09.251
{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","completed":97,"skipped":1843,"failed":0}
------------------------------
• [0.127 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:46:09.131
    Dec 30 03:46:09.131: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename endpointslice 12/30/22 03:46:09.132
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:46:09.147
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:46:09.15
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:352
    STEP: getting /apis 12/30/22 03:46:09.154
    STEP: getting /apis/discovery.k8s.io 12/30/22 03:46:09.157
    STEP: getting /apis/discovery.k8s.iov1 12/30/22 03:46:09.158
    STEP: creating 12/30/22 03:46:09.16
    STEP: getting 12/30/22 03:46:09.183
    STEP: listing 12/30/22 03:46:09.187
    STEP: watching 12/30/22 03:46:09.191
    Dec 30 03:46:09.191: INFO: starting watch
    STEP: cluster-wide listing 12/30/22 03:46:09.192
    STEP: cluster-wide watching 12/30/22 03:46:09.197
    Dec 30 03:46:09.197: INFO: starting watch
    STEP: patching 12/30/22 03:46:09.198
    STEP: updating 12/30/22 03:46:09.205
    Dec 30 03:46:09.214: INFO: waiting for watch events with expected annotations
    Dec 30 03:46:09.214: INFO: saw patched and updated annotations
    STEP: deleting 12/30/22 03:46:09.214
    STEP: deleting a collection 12/30/22 03:46:09.228
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Dec 30 03:46:09.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-9870" for this suite. 12/30/22 03:46:09.251
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:46:09.258
Dec 30 03:46:09.258: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename resourcequota 12/30/22 03:46:09.26
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:46:09.273
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:46:09.276
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438
STEP: Counting existing ResourceQuota 12/30/22 03:46:09.279
STEP: Creating a ResourceQuota 12/30/22 03:46:14.283
STEP: Ensuring resource quota status is calculated 12/30/22 03:46:14.289
STEP: Creating a ReplicaSet 12/30/22 03:46:16.294
STEP: Ensuring resource quota status captures replicaset creation 12/30/22 03:46:16.309
STEP: Deleting a ReplicaSet 12/30/22 03:46:18.314
STEP: Ensuring resource quota status released usage 12/30/22 03:46:18.321
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Dec 30 03:46:20.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4007" for this suite. 12/30/22 03:46:20.333
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","completed":98,"skipped":1846,"failed":0}
------------------------------
• [SLOW TEST] [11.083 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:46:09.258
    Dec 30 03:46:09.258: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename resourcequota 12/30/22 03:46:09.26
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:46:09.273
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:46:09.276
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:438
    STEP: Counting existing ResourceQuota 12/30/22 03:46:09.279
    STEP: Creating a ResourceQuota 12/30/22 03:46:14.283
    STEP: Ensuring resource quota status is calculated 12/30/22 03:46:14.289
    STEP: Creating a ReplicaSet 12/30/22 03:46:16.294
    STEP: Ensuring resource quota status captures replicaset creation 12/30/22 03:46:16.309
    STEP: Deleting a ReplicaSet 12/30/22 03:46:18.314
    STEP: Ensuring resource quota status released usage 12/30/22 03:46:18.321
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Dec 30 03:46:20.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-4007" for this suite. 12/30/22 03:46:20.333
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:46:20.342
Dec 30 03:46:20.342: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename svcaccounts 12/30/22 03:46:20.344
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:46:20.358
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:46:20.361
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158
Dec 30 03:46:20.383: INFO: created pod pod-service-account-defaultsa
Dec 30 03:46:20.383: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Dec 30 03:46:20.389: INFO: created pod pod-service-account-mountsa
Dec 30 03:46:20.389: INFO: pod pod-service-account-mountsa service account token volume mount: true
Dec 30 03:46:20.395: INFO: created pod pod-service-account-nomountsa
Dec 30 03:46:20.395: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Dec 30 03:46:20.400: INFO: created pod pod-service-account-defaultsa-mountspec
Dec 30 03:46:20.400: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Dec 30 03:46:20.406: INFO: created pod pod-service-account-mountsa-mountspec
Dec 30 03:46:20.406: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Dec 30 03:46:20.412: INFO: created pod pod-service-account-nomountsa-mountspec
Dec 30 03:46:20.412: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Dec 30 03:46:20.417: INFO: created pod pod-service-account-defaultsa-nomountspec
Dec 30 03:46:20.417: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Dec 30 03:46:20.422: INFO: created pod pod-service-account-mountsa-nomountspec
Dec 30 03:46:20.422: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Dec 30 03:46:20.427: INFO: created pod pod-service-account-nomountsa-nomountspec
Dec 30 03:46:20.427: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Dec 30 03:46:20.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7764" for this suite. 12/30/22 03:46:20.432
{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","completed":99,"skipped":1856,"failed":0}
------------------------------
• [0.098 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:46:20.342
    Dec 30 03:46:20.342: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename svcaccounts 12/30/22 03:46:20.344
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:46:20.358
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:46:20.361
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:158
    Dec 30 03:46:20.383: INFO: created pod pod-service-account-defaultsa
    Dec 30 03:46:20.383: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    Dec 30 03:46:20.389: INFO: created pod pod-service-account-mountsa
    Dec 30 03:46:20.389: INFO: pod pod-service-account-mountsa service account token volume mount: true
    Dec 30 03:46:20.395: INFO: created pod pod-service-account-nomountsa
    Dec 30 03:46:20.395: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    Dec 30 03:46:20.400: INFO: created pod pod-service-account-defaultsa-mountspec
    Dec 30 03:46:20.400: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    Dec 30 03:46:20.406: INFO: created pod pod-service-account-mountsa-mountspec
    Dec 30 03:46:20.406: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    Dec 30 03:46:20.412: INFO: created pod pod-service-account-nomountsa-mountspec
    Dec 30 03:46:20.412: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    Dec 30 03:46:20.417: INFO: created pod pod-service-account-defaultsa-nomountspec
    Dec 30 03:46:20.417: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    Dec 30 03:46:20.422: INFO: created pod pod-service-account-mountsa-nomountspec
    Dec 30 03:46:20.422: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    Dec 30 03:46:20.427: INFO: created pod pod-service-account-nomountsa-nomountspec
    Dec 30 03:46:20.427: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Dec 30 03:46:20.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-7764" for this suite. 12/30/22 03:46:20.432
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:46:20.441
Dec 30 03:46:20.441: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename emptydir 12/30/22 03:46:20.443
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:46:20.455
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:46:20.459
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166
STEP: Creating a pod to test emptydir 0644 on node default medium 12/30/22 03:46:20.462
Dec 30 03:46:20.471: INFO: Waiting up to 5m0s for pod "pod-6bf23aa3-67b7-4d96-9754-0bd9102079cc" in namespace "emptydir-295" to be "Succeeded or Failed"
Dec 30 03:46:20.476: INFO: Pod "pod-6bf23aa3-67b7-4d96-9754-0bd9102079cc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.344979ms
Dec 30 03:46:22.482: INFO: Pod "pod-6bf23aa3-67b7-4d96-9754-0bd9102079cc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010122603s
Dec 30 03:46:24.482: INFO: Pod "pod-6bf23aa3-67b7-4d96-9754-0bd9102079cc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010697797s
Dec 30 03:46:26.481: INFO: Pod "pod-6bf23aa3-67b7-4d96-9754-0bd9102079cc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009445801s
STEP: Saw pod success 12/30/22 03:46:26.481
Dec 30 03:46:26.481: INFO: Pod "pod-6bf23aa3-67b7-4d96-9754-0bd9102079cc" satisfied condition "Succeeded or Failed"
Dec 30 03:46:26.485: INFO: Trying to get logs from node k8s-worker01 pod pod-6bf23aa3-67b7-4d96-9754-0bd9102079cc container test-container: <nil>
STEP: delete the pod 12/30/22 03:46:26.505
Dec 30 03:46:26.515: INFO: Waiting for pod pod-6bf23aa3-67b7-4d96-9754-0bd9102079cc to disappear
Dec 30 03:46:26.519: INFO: Pod pod-6bf23aa3-67b7-4d96-9754-0bd9102079cc no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Dec 30 03:46:26.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-295" for this suite. 12/30/22 03:46:26.525
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":100,"skipped":1870,"failed":0}
------------------------------
• [SLOW TEST] [6.091 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:46:20.441
    Dec 30 03:46:20.441: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename emptydir 12/30/22 03:46:20.443
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:46:20.455
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:46:20.459
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:166
    STEP: Creating a pod to test emptydir 0644 on node default medium 12/30/22 03:46:20.462
    Dec 30 03:46:20.471: INFO: Waiting up to 5m0s for pod "pod-6bf23aa3-67b7-4d96-9754-0bd9102079cc" in namespace "emptydir-295" to be "Succeeded or Failed"
    Dec 30 03:46:20.476: INFO: Pod "pod-6bf23aa3-67b7-4d96-9754-0bd9102079cc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.344979ms
    Dec 30 03:46:22.482: INFO: Pod "pod-6bf23aa3-67b7-4d96-9754-0bd9102079cc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010122603s
    Dec 30 03:46:24.482: INFO: Pod "pod-6bf23aa3-67b7-4d96-9754-0bd9102079cc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010697797s
    Dec 30 03:46:26.481: INFO: Pod "pod-6bf23aa3-67b7-4d96-9754-0bd9102079cc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009445801s
    STEP: Saw pod success 12/30/22 03:46:26.481
    Dec 30 03:46:26.481: INFO: Pod "pod-6bf23aa3-67b7-4d96-9754-0bd9102079cc" satisfied condition "Succeeded or Failed"
    Dec 30 03:46:26.485: INFO: Trying to get logs from node k8s-worker01 pod pod-6bf23aa3-67b7-4d96-9754-0bd9102079cc container test-container: <nil>
    STEP: delete the pod 12/30/22 03:46:26.505
    Dec 30 03:46:26.515: INFO: Waiting for pod pod-6bf23aa3-67b7-4d96-9754-0bd9102079cc to disappear
    Dec 30 03:46:26.519: INFO: Pod pod-6bf23aa3-67b7-4d96-9754-0bd9102079cc no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Dec 30 03:46:26.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-295" for this suite. 12/30/22 03:46:26.525
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:46:26.533
Dec 30 03:46:26.534: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename emptydir 12/30/22 03:46:26.535
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:46:26.548
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:46:26.551
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116
STEP: Creating a pod to test emptydir 0777 on tmpfs 12/30/22 03:46:26.555
Dec 30 03:46:26.565: INFO: Waiting up to 5m0s for pod "pod-e7c59bd1-7e5e-44eb-a6e4-0cd40a3f2d68" in namespace "emptydir-230" to be "Succeeded or Failed"
Dec 30 03:46:26.568: INFO: Pod "pod-e7c59bd1-7e5e-44eb-a6e4-0cd40a3f2d68": Phase="Pending", Reason="", readiness=false. Elapsed: 3.025214ms
Dec 30 03:46:28.574: INFO: Pod "pod-e7c59bd1-7e5e-44eb-a6e4-0cd40a3f2d68": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0090958s
Dec 30 03:46:30.573: INFO: Pod "pod-e7c59bd1-7e5e-44eb-a6e4-0cd40a3f2d68": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008386461s
STEP: Saw pod success 12/30/22 03:46:30.573
Dec 30 03:46:30.574: INFO: Pod "pod-e7c59bd1-7e5e-44eb-a6e4-0cd40a3f2d68" satisfied condition "Succeeded or Failed"
Dec 30 03:46:30.578: INFO: Trying to get logs from node k8s-mgmt03 pod pod-e7c59bd1-7e5e-44eb-a6e4-0cd40a3f2d68 container test-container: <nil>
STEP: delete the pod 12/30/22 03:46:30.598
Dec 30 03:46:30.611: INFO: Waiting for pod pod-e7c59bd1-7e5e-44eb-a6e4-0cd40a3f2d68 to disappear
Dec 30 03:46:30.614: INFO: Pod pod-e7c59bd1-7e5e-44eb-a6e4-0cd40a3f2d68 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Dec 30 03:46:30.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-230" for this suite. 12/30/22 03:46:30.62
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":101,"skipped":1880,"failed":0}
------------------------------
• [4.093 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:46:26.533
    Dec 30 03:46:26.534: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename emptydir 12/30/22 03:46:26.535
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:46:26.548
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:46:26.551
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:116
    STEP: Creating a pod to test emptydir 0777 on tmpfs 12/30/22 03:46:26.555
    Dec 30 03:46:26.565: INFO: Waiting up to 5m0s for pod "pod-e7c59bd1-7e5e-44eb-a6e4-0cd40a3f2d68" in namespace "emptydir-230" to be "Succeeded or Failed"
    Dec 30 03:46:26.568: INFO: Pod "pod-e7c59bd1-7e5e-44eb-a6e4-0cd40a3f2d68": Phase="Pending", Reason="", readiness=false. Elapsed: 3.025214ms
    Dec 30 03:46:28.574: INFO: Pod "pod-e7c59bd1-7e5e-44eb-a6e4-0cd40a3f2d68": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0090958s
    Dec 30 03:46:30.573: INFO: Pod "pod-e7c59bd1-7e5e-44eb-a6e4-0cd40a3f2d68": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008386461s
    STEP: Saw pod success 12/30/22 03:46:30.573
    Dec 30 03:46:30.574: INFO: Pod "pod-e7c59bd1-7e5e-44eb-a6e4-0cd40a3f2d68" satisfied condition "Succeeded or Failed"
    Dec 30 03:46:30.578: INFO: Trying to get logs from node k8s-mgmt03 pod pod-e7c59bd1-7e5e-44eb-a6e4-0cd40a3f2d68 container test-container: <nil>
    STEP: delete the pod 12/30/22 03:46:30.598
    Dec 30 03:46:30.611: INFO: Waiting for pod pod-e7c59bd1-7e5e-44eb-a6e4-0cd40a3f2d68 to disappear
    Dec 30 03:46:30.614: INFO: Pod pod-e7c59bd1-7e5e-44eb-a6e4-0cd40a3f2d68 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Dec 30 03:46:30.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-230" for this suite. 12/30/22 03:46:30.62
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:130
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:46:30.627
Dec 30 03:46:30.627: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename container-lifecycle-hook 12/30/22 03:46:30.628
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:46:30.641
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:46:30.644
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 12/30/22 03:46:30.653
Dec 30 03:46:30.662: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-5407" to be "running and ready"
Dec 30 03:46:30.666: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.234104ms
Dec 30 03:46:30.666: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Dec 30 03:46:32.671: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.008308294s
Dec 30 03:46:32.671: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Dec 30 03:46:32.671: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:130
STEP: create the pod with lifecycle hook 12/30/22 03:46:32.675
Dec 30 03:46:32.682: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-5407" to be "running and ready"
Dec 30 03:46:32.685: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.393529ms
Dec 30 03:46:32.685: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Dec 30 03:46:34.690: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.008371175s
Dec 30 03:46:34.690: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
Dec 30 03:46:34.690: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 12/30/22 03:46:34.694
STEP: delete the pod with lifecycle hook 12/30/22 03:46:34.716
Dec 30 03:46:34.723: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 30 03:46:34.728: INFO: Pod pod-with-poststart-http-hook still exists
Dec 30 03:46:36.728: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 30 03:46:36.733: INFO: Pod pod-with-poststart-http-hook still exists
Dec 30 03:46:38.729: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 30 03:46:38.733: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Dec 30 03:46:38.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5407" for this suite. 12/30/22 03:46:38.739
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","completed":102,"skipped":1881,"failed":0}
------------------------------
• [SLOW TEST] [8.120 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:46:30.627
    Dec 30 03:46:30.627: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename container-lifecycle-hook 12/30/22 03:46:30.628
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:46:30.641
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:46:30.644
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 12/30/22 03:46:30.653
    Dec 30 03:46:30.662: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-5407" to be "running and ready"
    Dec 30 03:46:30.666: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.234104ms
    Dec 30 03:46:30.666: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Dec 30 03:46:32.671: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.008308294s
    Dec 30 03:46:32.671: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Dec 30 03:46:32.671: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:130
    STEP: create the pod with lifecycle hook 12/30/22 03:46:32.675
    Dec 30 03:46:32.682: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-5407" to be "running and ready"
    Dec 30 03:46:32.685: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.393529ms
    Dec 30 03:46:32.685: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Dec 30 03:46:34.690: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.008371175s
    Dec 30 03:46:34.690: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    Dec 30 03:46:34.690: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 12/30/22 03:46:34.694
    STEP: delete the pod with lifecycle hook 12/30/22 03:46:34.716
    Dec 30 03:46:34.723: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Dec 30 03:46:34.728: INFO: Pod pod-with-poststart-http-hook still exists
    Dec 30 03:46:36.728: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Dec 30 03:46:36.733: INFO: Pod pod-with-poststart-http-hook still exists
    Dec 30 03:46:38.729: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Dec 30 03:46:38.733: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Dec 30 03:46:38.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-5407" for this suite. 12/30/22 03:46:38.739
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:309
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:46:38.75
Dec 30 03:46:38.750: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename job 12/30/22 03:46:38.751
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:46:38.764
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:46:38.767
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:309
STEP: Creating a job 12/30/22 03:46:38.77
STEP: Ensuring active pods == parallelism 12/30/22 03:46:38.776
STEP: delete a job 12/30/22 03:46:40.783
STEP: deleting Job.batch foo in namespace job-6712, will wait for the garbage collector to delete the pods 12/30/22 03:46:40.783
Dec 30 03:46:40.845: INFO: Deleting Job.batch foo took: 6.645458ms
Dec 30 03:46:40.946: INFO: Terminating Job.batch foo pods took: 100.662276ms
STEP: Ensuring job was deleted 12/30/22 03:47:13.246
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Dec 30 03:47:13.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6712" for this suite. 12/30/22 03:47:13.257
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","completed":103,"skipped":1905,"failed":0}
------------------------------
• [SLOW TEST] [34.514 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:46:38.75
    Dec 30 03:46:38.750: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename job 12/30/22 03:46:38.751
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:46:38.764
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:46:38.767
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:309
    STEP: Creating a job 12/30/22 03:46:38.77
    STEP: Ensuring active pods == parallelism 12/30/22 03:46:38.776
    STEP: delete a job 12/30/22 03:46:40.783
    STEP: deleting Job.batch foo in namespace job-6712, will wait for the garbage collector to delete the pods 12/30/22 03:46:40.783
    Dec 30 03:46:40.845: INFO: Deleting Job.batch foo took: 6.645458ms
    Dec 30 03:46:40.946: INFO: Terminating Job.batch foo pods took: 100.662276ms
    STEP: Ensuring job was deleted 12/30/22 03:47:13.246
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Dec 30 03:47:13.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-6712" for this suite. 12/30/22 03:47:13.257
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:47:13.264
Dec 30 03:47:13.264: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename var-expansion 12/30/22 03:47:13.266
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:47:13.279
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:47:13.283
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43
STEP: Creating a pod to test env composition 12/30/22 03:47:13.286
Dec 30 03:47:13.296: INFO: Waiting up to 5m0s for pod "var-expansion-e82effbd-b656-4f41-b520-9bd5e8af5dd8" in namespace "var-expansion-5926" to be "Succeeded or Failed"
Dec 30 03:47:13.299: INFO: Pod "var-expansion-e82effbd-b656-4f41-b520-9bd5e8af5dd8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.125797ms
Dec 30 03:47:15.304: INFO: Pod "var-expansion-e82effbd-b656-4f41-b520-9bd5e8af5dd8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008810172s
Dec 30 03:47:17.304: INFO: Pod "var-expansion-e82effbd-b656-4f41-b520-9bd5e8af5dd8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008310599s
STEP: Saw pod success 12/30/22 03:47:17.304
Dec 30 03:47:17.304: INFO: Pod "var-expansion-e82effbd-b656-4f41-b520-9bd5e8af5dd8" satisfied condition "Succeeded or Failed"
Dec 30 03:47:17.308: INFO: Trying to get logs from node k8s-mgmt02 pod var-expansion-e82effbd-b656-4f41-b520-9bd5e8af5dd8 container dapi-container: <nil>
STEP: delete the pod 12/30/22 03:47:17.329
Dec 30 03:47:17.342: INFO: Waiting for pod var-expansion-e82effbd-b656-4f41-b520-9bd5e8af5dd8 to disappear
Dec 30 03:47:17.345: INFO: Pod var-expansion-e82effbd-b656-4f41-b520-9bd5e8af5dd8 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Dec 30 03:47:17.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5926" for this suite. 12/30/22 03:47:17.351
{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","completed":104,"skipped":1905,"failed":0}
------------------------------
• [4.093 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:47:13.264
    Dec 30 03:47:13.264: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename var-expansion 12/30/22 03:47:13.266
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:47:13.279
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:47:13.283
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:43
    STEP: Creating a pod to test env composition 12/30/22 03:47:13.286
    Dec 30 03:47:13.296: INFO: Waiting up to 5m0s for pod "var-expansion-e82effbd-b656-4f41-b520-9bd5e8af5dd8" in namespace "var-expansion-5926" to be "Succeeded or Failed"
    Dec 30 03:47:13.299: INFO: Pod "var-expansion-e82effbd-b656-4f41-b520-9bd5e8af5dd8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.125797ms
    Dec 30 03:47:15.304: INFO: Pod "var-expansion-e82effbd-b656-4f41-b520-9bd5e8af5dd8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008810172s
    Dec 30 03:47:17.304: INFO: Pod "var-expansion-e82effbd-b656-4f41-b520-9bd5e8af5dd8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008310599s
    STEP: Saw pod success 12/30/22 03:47:17.304
    Dec 30 03:47:17.304: INFO: Pod "var-expansion-e82effbd-b656-4f41-b520-9bd5e8af5dd8" satisfied condition "Succeeded or Failed"
    Dec 30 03:47:17.308: INFO: Trying to get logs from node k8s-mgmt02 pod var-expansion-e82effbd-b656-4f41-b520-9bd5e8af5dd8 container dapi-container: <nil>
    STEP: delete the pod 12/30/22 03:47:17.329
    Dec 30 03:47:17.342: INFO: Waiting for pod var-expansion-e82effbd-b656-4f41-b520-9bd5e8af5dd8 to disappear
    Dec 30 03:47:17.345: INFO: Pod var-expansion-e82effbd-b656-4f41-b520-9bd5e8af5dd8 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Dec 30 03:47:17.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-5926" for this suite. 12/30/22 03:47:17.351
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:47:17.36
Dec 30 03:47:17.360: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename daemonsets 12/30/22 03:47:17.362
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:47:17.375
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:47:17.378
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193
Dec 30 03:47:17.414: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 12/30/22 03:47:17.42
Dec 30 03:47:17.424: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 30 03:47:17.424: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 12/30/22 03:47:17.424
Dec 30 03:47:17.447: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 30 03:47:17.447: INFO: Node k8s-mgmt01 is running 0 daemon pod, expected 1
Dec 30 03:47:18.451: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 30 03:47:18.451: INFO: Node k8s-mgmt01 is running 0 daemon pod, expected 1
Dec 30 03:47:19.451: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Dec 30 03:47:19.451: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 12/30/22 03:47:19.455
Dec 30 03:47:19.474: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Dec 30 03:47:19.474: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Dec 30 03:47:20.479: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 30 03:47:20.479: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 12/30/22 03:47:20.479
Dec 30 03:47:20.500: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 30 03:47:20.500: INFO: Node k8s-mgmt01 is running 0 daemon pod, expected 1
Dec 30 03:47:21.505: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 30 03:47:21.505: INFO: Node k8s-mgmt01 is running 0 daemon pod, expected 1
Dec 30 03:47:22.505: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 30 03:47:22.505: INFO: Node k8s-mgmt01 is running 0 daemon pod, expected 1
Dec 30 03:47:23.506: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Dec 30 03:47:23.506: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 12/30/22 03:47:23.513
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7296, will wait for the garbage collector to delete the pods 12/30/22 03:47:23.514
Dec 30 03:47:23.575: INFO: Deleting DaemonSet.extensions daemon-set took: 6.601412ms
Dec 30 03:47:23.676: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.167374ms
Dec 30 03:47:26.280: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 30 03:47:26.280: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Dec 30 03:47:26.284: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"431889"},"items":null}

Dec 30 03:47:26.288: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"431889"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Dec 30 03:47:26.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7296" for this suite. 12/30/22 03:47:26.332
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","completed":105,"skipped":1939,"failed":0}
------------------------------
• [SLOW TEST] [8.979 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:47:17.36
    Dec 30 03:47:17.360: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename daemonsets 12/30/22 03:47:17.362
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:47:17.375
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:47:17.378
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:193
    Dec 30 03:47:17.414: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 12/30/22 03:47:17.42
    Dec 30 03:47:17.424: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 30 03:47:17.424: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 12/30/22 03:47:17.424
    Dec 30 03:47:17.447: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 30 03:47:17.447: INFO: Node k8s-mgmt01 is running 0 daemon pod, expected 1
    Dec 30 03:47:18.451: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 30 03:47:18.451: INFO: Node k8s-mgmt01 is running 0 daemon pod, expected 1
    Dec 30 03:47:19.451: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Dec 30 03:47:19.451: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 12/30/22 03:47:19.455
    Dec 30 03:47:19.474: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Dec 30 03:47:19.474: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
    Dec 30 03:47:20.479: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 30 03:47:20.479: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 12/30/22 03:47:20.479
    Dec 30 03:47:20.500: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 30 03:47:20.500: INFO: Node k8s-mgmt01 is running 0 daemon pod, expected 1
    Dec 30 03:47:21.505: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 30 03:47:21.505: INFO: Node k8s-mgmt01 is running 0 daemon pod, expected 1
    Dec 30 03:47:22.505: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 30 03:47:22.505: INFO: Node k8s-mgmt01 is running 0 daemon pod, expected 1
    Dec 30 03:47:23.506: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Dec 30 03:47:23.506: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 12/30/22 03:47:23.513
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7296, will wait for the garbage collector to delete the pods 12/30/22 03:47:23.514
    Dec 30 03:47:23.575: INFO: Deleting DaemonSet.extensions daemon-set took: 6.601412ms
    Dec 30 03:47:23.676: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.167374ms
    Dec 30 03:47:26.280: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 30 03:47:26.280: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Dec 30 03:47:26.284: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"431889"},"items":null}

    Dec 30 03:47:26.288: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"431889"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Dec 30 03:47:26.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-7296" for this suite. 12/30/22 03:47:26.332
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:47:26.342
Dec 30 03:47:26.343: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename downward-api 12/30/22 03:47:26.344
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:47:26.358
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:47:26.362
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89
STEP: Creating a pod to test downward api env vars 12/30/22 03:47:26.365
Dec 30 03:47:26.375: INFO: Waiting up to 5m0s for pod "downward-api-c57bee19-d402-4d6e-9817-9e257e6ff9af" in namespace "downward-api-3129" to be "Succeeded or Failed"
Dec 30 03:47:26.378: INFO: Pod "downward-api-c57bee19-d402-4d6e-9817-9e257e6ff9af": Phase="Pending", Reason="", readiness=false. Elapsed: 3.258234ms
Dec 30 03:47:28.383: INFO: Pod "downward-api-c57bee19-d402-4d6e-9817-9e257e6ff9af": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008058331s
Dec 30 03:47:30.382: INFO: Pod "downward-api-c57bee19-d402-4d6e-9817-9e257e6ff9af": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007289504s
STEP: Saw pod success 12/30/22 03:47:30.382
Dec 30 03:47:30.382: INFO: Pod "downward-api-c57bee19-d402-4d6e-9817-9e257e6ff9af" satisfied condition "Succeeded or Failed"
Dec 30 03:47:30.387: INFO: Trying to get logs from node k8s-mgmt02 pod downward-api-c57bee19-d402-4d6e-9817-9e257e6ff9af container dapi-container: <nil>
STEP: delete the pod 12/30/22 03:47:30.397
Dec 30 03:47:30.408: INFO: Waiting for pod downward-api-c57bee19-d402-4d6e-9817-9e257e6ff9af to disappear
Dec 30 03:47:30.411: INFO: Pod downward-api-c57bee19-d402-4d6e-9817-9e257e6ff9af no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Dec 30 03:47:30.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3129" for this suite. 12/30/22 03:47:30.418
{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","completed":106,"skipped":1973,"failed":0}
------------------------------
• [4.082 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:47:26.342
    Dec 30 03:47:26.343: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename downward-api 12/30/22 03:47:26.344
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:47:26.358
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:47:26.362
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:89
    STEP: Creating a pod to test downward api env vars 12/30/22 03:47:26.365
    Dec 30 03:47:26.375: INFO: Waiting up to 5m0s for pod "downward-api-c57bee19-d402-4d6e-9817-9e257e6ff9af" in namespace "downward-api-3129" to be "Succeeded or Failed"
    Dec 30 03:47:26.378: INFO: Pod "downward-api-c57bee19-d402-4d6e-9817-9e257e6ff9af": Phase="Pending", Reason="", readiness=false. Elapsed: 3.258234ms
    Dec 30 03:47:28.383: INFO: Pod "downward-api-c57bee19-d402-4d6e-9817-9e257e6ff9af": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008058331s
    Dec 30 03:47:30.382: INFO: Pod "downward-api-c57bee19-d402-4d6e-9817-9e257e6ff9af": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007289504s
    STEP: Saw pod success 12/30/22 03:47:30.382
    Dec 30 03:47:30.382: INFO: Pod "downward-api-c57bee19-d402-4d6e-9817-9e257e6ff9af" satisfied condition "Succeeded or Failed"
    Dec 30 03:47:30.387: INFO: Trying to get logs from node k8s-mgmt02 pod downward-api-c57bee19-d402-4d6e-9817-9e257e6ff9af container dapi-container: <nil>
    STEP: delete the pod 12/30/22 03:47:30.397
    Dec 30 03:47:30.408: INFO: Waiting for pod downward-api-c57bee19-d402-4d6e-9817-9e257e6ff9af to disappear
    Dec 30 03:47:30.411: INFO: Pod downward-api-c57bee19-d402-4d6e-9817-9e257e6ff9af no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Dec 30 03:47:30.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-3129" for this suite. 12/30/22 03:47:30.418
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:47:30.425
Dec 30 03:47:30.425: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename downward-api 12/30/22 03:47:30.427
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:47:30.44
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:47:30.443
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52
STEP: Creating a pod to test downward API volume plugin 12/30/22 03:47:30.447
Dec 30 03:47:30.455: INFO: Waiting up to 5m0s for pod "downwardapi-volume-313f3a1c-9035-41dd-9ef1-09a0a360d471" in namespace "downward-api-9097" to be "Succeeded or Failed"
Dec 30 03:47:30.459: INFO: Pod "downwardapi-volume-313f3a1c-9035-41dd-9ef1-09a0a360d471": Phase="Pending", Reason="", readiness=false. Elapsed: 3.842365ms
Dec 30 03:47:32.465: INFO: Pod "downwardapi-volume-313f3a1c-9035-41dd-9ef1-09a0a360d471": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009602751s
Dec 30 03:47:34.465: INFO: Pod "downwardapi-volume-313f3a1c-9035-41dd-9ef1-09a0a360d471": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009977443s
STEP: Saw pod success 12/30/22 03:47:34.465
Dec 30 03:47:34.465: INFO: Pod "downwardapi-volume-313f3a1c-9035-41dd-9ef1-09a0a360d471" satisfied condition "Succeeded or Failed"
Dec 30 03:47:34.469: INFO: Trying to get logs from node k8s-mgmt01 pod downwardapi-volume-313f3a1c-9035-41dd-9ef1-09a0a360d471 container client-container: <nil>
STEP: delete the pod 12/30/22 03:47:34.478
Dec 30 03:47:34.491: INFO: Waiting for pod downwardapi-volume-313f3a1c-9035-41dd-9ef1-09a0a360d471 to disappear
Dec 30 03:47:34.495: INFO: Pod downwardapi-volume-313f3a1c-9035-41dd-9ef1-09a0a360d471 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Dec 30 03:47:34.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9097" for this suite. 12/30/22 03:47:34.5
{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","completed":107,"skipped":1979,"failed":0}
------------------------------
• [4.081 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:47:30.425
    Dec 30 03:47:30.425: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename downward-api 12/30/22 03:47:30.427
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:47:30.44
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:47:30.443
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:52
    STEP: Creating a pod to test downward API volume plugin 12/30/22 03:47:30.447
    Dec 30 03:47:30.455: INFO: Waiting up to 5m0s for pod "downwardapi-volume-313f3a1c-9035-41dd-9ef1-09a0a360d471" in namespace "downward-api-9097" to be "Succeeded or Failed"
    Dec 30 03:47:30.459: INFO: Pod "downwardapi-volume-313f3a1c-9035-41dd-9ef1-09a0a360d471": Phase="Pending", Reason="", readiness=false. Elapsed: 3.842365ms
    Dec 30 03:47:32.465: INFO: Pod "downwardapi-volume-313f3a1c-9035-41dd-9ef1-09a0a360d471": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009602751s
    Dec 30 03:47:34.465: INFO: Pod "downwardapi-volume-313f3a1c-9035-41dd-9ef1-09a0a360d471": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009977443s
    STEP: Saw pod success 12/30/22 03:47:34.465
    Dec 30 03:47:34.465: INFO: Pod "downwardapi-volume-313f3a1c-9035-41dd-9ef1-09a0a360d471" satisfied condition "Succeeded or Failed"
    Dec 30 03:47:34.469: INFO: Trying to get logs from node k8s-mgmt01 pod downwardapi-volume-313f3a1c-9035-41dd-9ef1-09a0a360d471 container client-container: <nil>
    STEP: delete the pod 12/30/22 03:47:34.478
    Dec 30 03:47:34.491: INFO: Waiting for pod downwardapi-volume-313f3a1c-9035-41dd-9ef1-09a0a360d471 to disappear
    Dec 30 03:47:34.495: INFO: Pod downwardapi-volume-313f3a1c-9035-41dd-9ef1-09a0a360d471 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Dec 30 03:47:34.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-9097" for this suite. 12/30/22 03:47:34.5
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:47:34.508
Dec 30 03:47:34.509: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename services 12/30/22 03:47:34.51
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:47:34.522
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:47:34.526
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231
STEP: creating an Endpoint 12/30/22 03:47:34.533
STEP: waiting for available Endpoint 12/30/22 03:47:34.538
STEP: listing all Endpoints 12/30/22 03:47:34.54
STEP: updating the Endpoint 12/30/22 03:47:34.543
STEP: fetching the Endpoint 12/30/22 03:47:34.55
STEP: patching the Endpoint 12/30/22 03:47:34.554
STEP: fetching the Endpoint 12/30/22 03:47:34.565
STEP: deleting the Endpoint by Collection 12/30/22 03:47:34.569
STEP: waiting for Endpoint deletion 12/30/22 03:47:34.578
STEP: fetching the Endpoint 12/30/22 03:47:34.579
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Dec 30 03:47:34.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5222" for this suite. 12/30/22 03:47:34.588
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","completed":108,"skipped":1997,"failed":0}
------------------------------
• [0.086 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:47:34.508
    Dec 30 03:47:34.509: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename services 12/30/22 03:47:34.51
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:47:34.522
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:47:34.526
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3231
    STEP: creating an Endpoint 12/30/22 03:47:34.533
    STEP: waiting for available Endpoint 12/30/22 03:47:34.538
    STEP: listing all Endpoints 12/30/22 03:47:34.54
    STEP: updating the Endpoint 12/30/22 03:47:34.543
    STEP: fetching the Endpoint 12/30/22 03:47:34.55
    STEP: patching the Endpoint 12/30/22 03:47:34.554
    STEP: fetching the Endpoint 12/30/22 03:47:34.565
    STEP: deleting the Endpoint by Collection 12/30/22 03:47:34.569
    STEP: waiting for Endpoint deletion 12/30/22 03:47:34.578
    STEP: fetching the Endpoint 12/30/22 03:47:34.579
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Dec 30 03:47:34.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-5222" for this suite. 12/30/22 03:47:34.588
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:47:34.596
Dec 30 03:47:34.596: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename emptydir 12/30/22 03:47:34.597
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:47:34.61
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:47:34.613
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136
STEP: Creating a pod to test emptydir 0666 on tmpfs 12/30/22 03:47:34.616
Dec 30 03:47:34.625: INFO: Waiting up to 5m0s for pod "pod-fb8182e6-fcbf-4524-a6e3-1b14f9b14668" in namespace "emptydir-1400" to be "Succeeded or Failed"
Dec 30 03:47:34.628: INFO: Pod "pod-fb8182e6-fcbf-4524-a6e3-1b14f9b14668": Phase="Pending", Reason="", readiness=false. Elapsed: 3.121568ms
Dec 30 03:47:36.633: INFO: Pod "pod-fb8182e6-fcbf-4524-a6e3-1b14f9b14668": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007939549s
Dec 30 03:47:38.635: INFO: Pod "pod-fb8182e6-fcbf-4524-a6e3-1b14f9b14668": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009320183s
STEP: Saw pod success 12/30/22 03:47:38.635
Dec 30 03:47:38.635: INFO: Pod "pod-fb8182e6-fcbf-4524-a6e3-1b14f9b14668" satisfied condition "Succeeded or Failed"
Dec 30 03:47:38.638: INFO: Trying to get logs from node k8s-mgmt02 pod pod-fb8182e6-fcbf-4524-a6e3-1b14f9b14668 container test-container: <nil>
STEP: delete the pod 12/30/22 03:47:38.647
Dec 30 03:47:38.660: INFO: Waiting for pod pod-fb8182e6-fcbf-4524-a6e3-1b14f9b14668 to disappear
Dec 30 03:47:38.663: INFO: Pod pod-fb8182e6-fcbf-4524-a6e3-1b14f9b14668 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Dec 30 03:47:38.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1400" for this suite. 12/30/22 03:47:38.668
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":109,"skipped":2002,"failed":0}
------------------------------
• [4.078 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:47:34.596
    Dec 30 03:47:34.596: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename emptydir 12/30/22 03:47:34.597
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:47:34.61
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:47:34.613
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:136
    STEP: Creating a pod to test emptydir 0666 on tmpfs 12/30/22 03:47:34.616
    Dec 30 03:47:34.625: INFO: Waiting up to 5m0s for pod "pod-fb8182e6-fcbf-4524-a6e3-1b14f9b14668" in namespace "emptydir-1400" to be "Succeeded or Failed"
    Dec 30 03:47:34.628: INFO: Pod "pod-fb8182e6-fcbf-4524-a6e3-1b14f9b14668": Phase="Pending", Reason="", readiness=false. Elapsed: 3.121568ms
    Dec 30 03:47:36.633: INFO: Pod "pod-fb8182e6-fcbf-4524-a6e3-1b14f9b14668": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007939549s
    Dec 30 03:47:38.635: INFO: Pod "pod-fb8182e6-fcbf-4524-a6e3-1b14f9b14668": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009320183s
    STEP: Saw pod success 12/30/22 03:47:38.635
    Dec 30 03:47:38.635: INFO: Pod "pod-fb8182e6-fcbf-4524-a6e3-1b14f9b14668" satisfied condition "Succeeded or Failed"
    Dec 30 03:47:38.638: INFO: Trying to get logs from node k8s-mgmt02 pod pod-fb8182e6-fcbf-4524-a6e3-1b14f9b14668 container test-container: <nil>
    STEP: delete the pod 12/30/22 03:47:38.647
    Dec 30 03:47:38.660: INFO: Waiting for pod pod-fb8182e6-fcbf-4524-a6e3-1b14f9b14668 to disappear
    Dec 30 03:47:38.663: INFO: Pod pod-fb8182e6-fcbf-4524-a6e3-1b14f9b14668 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Dec 30 03:47:38.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-1400" for this suite. 12/30/22 03:47:38.668
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:47:38.675
Dec 30 03:47:38.675: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename webhook 12/30/22 03:47:38.677
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:47:38.69
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:47:38.693
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 12/30/22 03:47:38.709
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/30/22 03:47:39.268
STEP: Deploying the webhook pod 12/30/22 03:47:39.276
STEP: Wait for the deployment to be ready 12/30/22 03:47:39.288
Dec 30 03:47:39.295: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 12/30/22 03:47:41.309
STEP: Verifying the service has paired with the endpoint 12/30/22 03:47:41.32
Dec 30 03:47:42.321: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 12/30/22 03:47:42.325
STEP: create a pod that should be updated by the webhook 12/30/22 03:47:42.346
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Dec 30 03:47:42.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7836" for this suite. 12/30/22 03:47:42.381
STEP: Destroying namespace "webhook-7836-markers" for this suite. 12/30/22 03:47:42.387
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","completed":110,"skipped":2012,"failed":0}
------------------------------
• [3.753 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:47:38.675
    Dec 30 03:47:38.675: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename webhook 12/30/22 03:47:38.677
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:47:38.69
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:47:38.693
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 12/30/22 03:47:38.709
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/30/22 03:47:39.268
    STEP: Deploying the webhook pod 12/30/22 03:47:39.276
    STEP: Wait for the deployment to be ready 12/30/22 03:47:39.288
    Dec 30 03:47:39.295: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 12/30/22 03:47:41.309
    STEP: Verifying the service has paired with the endpoint 12/30/22 03:47:41.32
    Dec 30 03:47:42.321: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:263
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 12/30/22 03:47:42.325
    STEP: create a pod that should be updated by the webhook 12/30/22 03:47:42.346
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Dec 30 03:47:42.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-7836" for this suite. 12/30/22 03:47:42.381
    STEP: Destroying namespace "webhook-7836-markers" for this suite. 12/30/22 03:47:42.387
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:47:42.43
Dec 30 03:47:42.430: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename proxy 12/30/22 03:47:42.432
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:47:42.444
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:47:42.447
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
Dec 30 03:47:42.450: INFO: Creating pod...
Dec 30 03:47:42.460: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-3441" to be "running"
Dec 30 03:47:42.464: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 3.732005ms
Dec 30 03:47:44.469: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.009347586s
Dec 30 03:47:44.469: INFO: Pod "agnhost" satisfied condition "running"
Dec 30 03:47:44.469: INFO: Creating service...
Dec 30 03:47:44.480: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3441/pods/agnhost/proxy/some/path/with/DELETE
Dec 30 03:47:44.485: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Dec 30 03:47:44.485: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3441/pods/agnhost/proxy/some/path/with/GET
Dec 30 03:47:44.489: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Dec 30 03:47:44.489: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3441/pods/agnhost/proxy/some/path/with/HEAD
Dec 30 03:47:44.492: INFO: http.Client request:HEAD | StatusCode:200
Dec 30 03:47:44.492: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3441/pods/agnhost/proxy/some/path/with/OPTIONS
Dec 30 03:47:44.496: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Dec 30 03:47:44.496: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3441/pods/agnhost/proxy/some/path/with/PATCH
Dec 30 03:47:44.500: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Dec 30 03:47:44.500: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3441/pods/agnhost/proxy/some/path/with/POST
Dec 30 03:47:44.503: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Dec 30 03:47:44.503: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3441/pods/agnhost/proxy/some/path/with/PUT
Dec 30 03:47:44.507: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Dec 30 03:47:44.507: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3441/services/test-service/proxy/some/path/with/DELETE
Dec 30 03:47:44.513: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Dec 30 03:47:44.513: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3441/services/test-service/proxy/some/path/with/GET
Dec 30 03:47:44.519: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Dec 30 03:47:44.519: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3441/services/test-service/proxy/some/path/with/HEAD
Dec 30 03:47:44.524: INFO: http.Client request:HEAD | StatusCode:200
Dec 30 03:47:44.524: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3441/services/test-service/proxy/some/path/with/OPTIONS
Dec 30 03:47:44.531: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Dec 30 03:47:44.531: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3441/services/test-service/proxy/some/path/with/PATCH
Dec 30 03:47:44.536: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Dec 30 03:47:44.536: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3441/services/test-service/proxy/some/path/with/POST
Dec 30 03:47:44.542: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Dec 30 03:47:44.542: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3441/services/test-service/proxy/some/path/with/PUT
Dec 30 03:47:44.548: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Dec 30 03:47:44.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-3441" for this suite. 12/30/22 03:47:44.554
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","completed":111,"skipped":2028,"failed":0}
------------------------------
• [2.132 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:47:42.43
    Dec 30 03:47:42.430: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename proxy 12/30/22 03:47:42.432
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:47:42.444
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:47:42.447
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    Dec 30 03:47:42.450: INFO: Creating pod...
    Dec 30 03:47:42.460: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-3441" to be "running"
    Dec 30 03:47:42.464: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 3.732005ms
    Dec 30 03:47:44.469: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.009347586s
    Dec 30 03:47:44.469: INFO: Pod "agnhost" satisfied condition "running"
    Dec 30 03:47:44.469: INFO: Creating service...
    Dec 30 03:47:44.480: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3441/pods/agnhost/proxy/some/path/with/DELETE
    Dec 30 03:47:44.485: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Dec 30 03:47:44.485: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3441/pods/agnhost/proxy/some/path/with/GET
    Dec 30 03:47:44.489: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Dec 30 03:47:44.489: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3441/pods/agnhost/proxy/some/path/with/HEAD
    Dec 30 03:47:44.492: INFO: http.Client request:HEAD | StatusCode:200
    Dec 30 03:47:44.492: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3441/pods/agnhost/proxy/some/path/with/OPTIONS
    Dec 30 03:47:44.496: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Dec 30 03:47:44.496: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3441/pods/agnhost/proxy/some/path/with/PATCH
    Dec 30 03:47:44.500: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Dec 30 03:47:44.500: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3441/pods/agnhost/proxy/some/path/with/POST
    Dec 30 03:47:44.503: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Dec 30 03:47:44.503: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3441/pods/agnhost/proxy/some/path/with/PUT
    Dec 30 03:47:44.507: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Dec 30 03:47:44.507: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3441/services/test-service/proxy/some/path/with/DELETE
    Dec 30 03:47:44.513: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Dec 30 03:47:44.513: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3441/services/test-service/proxy/some/path/with/GET
    Dec 30 03:47:44.519: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Dec 30 03:47:44.519: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3441/services/test-service/proxy/some/path/with/HEAD
    Dec 30 03:47:44.524: INFO: http.Client request:HEAD | StatusCode:200
    Dec 30 03:47:44.524: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3441/services/test-service/proxy/some/path/with/OPTIONS
    Dec 30 03:47:44.531: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Dec 30 03:47:44.531: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3441/services/test-service/proxy/some/path/with/PATCH
    Dec 30 03:47:44.536: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Dec 30 03:47:44.536: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3441/services/test-service/proxy/some/path/with/POST
    Dec 30 03:47:44.542: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Dec 30 03:47:44.542: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3441/services/test-service/proxy/some/path/with/PUT
    Dec 30 03:47:44.548: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Dec 30 03:47:44.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-3441" for this suite. 12/30/22 03:47:44.554
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:47:44.563
Dec 30 03:47:44.563: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename emptydir 12/30/22 03:47:44.564
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:47:44.578
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:47:44.581
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126
STEP: Creating a pod to test emptydir 0644 on tmpfs 12/30/22 03:47:44.584
Dec 30 03:47:44.593: INFO: Waiting up to 5m0s for pod "pod-df5db231-826e-41b1-9136-47daf4c4731b" in namespace "emptydir-294" to be "Succeeded or Failed"
Dec 30 03:47:44.597: INFO: Pod "pod-df5db231-826e-41b1-9136-47daf4c4731b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.874012ms
Dec 30 03:47:46.602: INFO: Pod "pod-df5db231-826e-41b1-9136-47daf4c4731b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009197447s
Dec 30 03:47:48.602: INFO: Pod "pod-df5db231-826e-41b1-9136-47daf4c4731b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008852813s
STEP: Saw pod success 12/30/22 03:47:48.602
Dec 30 03:47:48.602: INFO: Pod "pod-df5db231-826e-41b1-9136-47daf4c4731b" satisfied condition "Succeeded or Failed"
Dec 30 03:47:48.605: INFO: Trying to get logs from node k8s-mgmt01 pod pod-df5db231-826e-41b1-9136-47daf4c4731b container test-container: <nil>
STEP: delete the pod 12/30/22 03:47:48.613
Dec 30 03:47:48.626: INFO: Waiting for pod pod-df5db231-826e-41b1-9136-47daf4c4731b to disappear
Dec 30 03:47:48.629: INFO: Pod pod-df5db231-826e-41b1-9136-47daf4c4731b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Dec 30 03:47:48.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-294" for this suite. 12/30/22 03:47:48.634
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":112,"skipped":2032,"failed":0}
------------------------------
• [4.078 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:47:44.563
    Dec 30 03:47:44.563: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename emptydir 12/30/22 03:47:44.564
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:47:44.578
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:47:44.581
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:126
    STEP: Creating a pod to test emptydir 0644 on tmpfs 12/30/22 03:47:44.584
    Dec 30 03:47:44.593: INFO: Waiting up to 5m0s for pod "pod-df5db231-826e-41b1-9136-47daf4c4731b" in namespace "emptydir-294" to be "Succeeded or Failed"
    Dec 30 03:47:44.597: INFO: Pod "pod-df5db231-826e-41b1-9136-47daf4c4731b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.874012ms
    Dec 30 03:47:46.602: INFO: Pod "pod-df5db231-826e-41b1-9136-47daf4c4731b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009197447s
    Dec 30 03:47:48.602: INFO: Pod "pod-df5db231-826e-41b1-9136-47daf4c4731b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008852813s
    STEP: Saw pod success 12/30/22 03:47:48.602
    Dec 30 03:47:48.602: INFO: Pod "pod-df5db231-826e-41b1-9136-47daf4c4731b" satisfied condition "Succeeded or Failed"
    Dec 30 03:47:48.605: INFO: Trying to get logs from node k8s-mgmt01 pod pod-df5db231-826e-41b1-9136-47daf4c4731b container test-container: <nil>
    STEP: delete the pod 12/30/22 03:47:48.613
    Dec 30 03:47:48.626: INFO: Waiting for pod pod-df5db231-826e-41b1-9136-47daf4c4731b to disappear
    Dec 30 03:47:48.629: INFO: Pod pod-df5db231-826e-41b1-9136-47daf4c4731b no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Dec 30 03:47:48.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-294" for this suite. 12/30/22 03:47:48.634
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:47:48.641
Dec 30 03:47:48.641: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename pods 12/30/22 03:47:48.643
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:47:48.659
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:47:48.662
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443
Dec 30 03:47:48.674: INFO: Waiting up to 5m0s for pod "server-envvars-d83f6ddc-30c8-4fd0-ad2b-e193dd90550f" in namespace "pods-6626" to be "running and ready"
Dec 30 03:47:48.678: INFO: Pod "server-envvars-d83f6ddc-30c8-4fd0-ad2b-e193dd90550f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.762978ms
Dec 30 03:47:48.678: INFO: The phase of Pod server-envvars-d83f6ddc-30c8-4fd0-ad2b-e193dd90550f is Pending, waiting for it to be Running (with Ready = true)
Dec 30 03:47:50.683: INFO: Pod "server-envvars-d83f6ddc-30c8-4fd0-ad2b-e193dd90550f": Phase="Running", Reason="", readiness=true. Elapsed: 2.00931263s
Dec 30 03:47:50.683: INFO: The phase of Pod server-envvars-d83f6ddc-30c8-4fd0-ad2b-e193dd90550f is Running (Ready = true)
Dec 30 03:47:50.683: INFO: Pod "server-envvars-d83f6ddc-30c8-4fd0-ad2b-e193dd90550f" satisfied condition "running and ready"
Dec 30 03:47:50.704: INFO: Waiting up to 5m0s for pod "client-envvars-1f24ed5e-e3a0-4ae8-9868-6afcfe905584" in namespace "pods-6626" to be "Succeeded or Failed"
Dec 30 03:47:50.707: INFO: Pod "client-envvars-1f24ed5e-e3a0-4ae8-9868-6afcfe905584": Phase="Pending", Reason="", readiness=false. Elapsed: 3.561733ms
Dec 30 03:47:52.713: INFO: Pod "client-envvars-1f24ed5e-e3a0-4ae8-9868-6afcfe905584": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008836226s
Dec 30 03:47:54.713: INFO: Pod "client-envvars-1f24ed5e-e3a0-4ae8-9868-6afcfe905584": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009049159s
STEP: Saw pod success 12/30/22 03:47:54.713
Dec 30 03:47:54.713: INFO: Pod "client-envvars-1f24ed5e-e3a0-4ae8-9868-6afcfe905584" satisfied condition "Succeeded or Failed"
Dec 30 03:47:54.717: INFO: Trying to get logs from node k8s-mgmt01 pod client-envvars-1f24ed5e-e3a0-4ae8-9868-6afcfe905584 container env3cont: <nil>
STEP: delete the pod 12/30/22 03:47:54.726
Dec 30 03:47:54.737: INFO: Waiting for pod client-envvars-1f24ed5e-e3a0-4ae8-9868-6afcfe905584 to disappear
Dec 30 03:47:54.741: INFO: Pod client-envvars-1f24ed5e-e3a0-4ae8-9868-6afcfe905584 no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Dec 30 03:47:54.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6626" for this suite. 12/30/22 03:47:54.746
{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","completed":113,"skipped":2032,"failed":0}
------------------------------
• [SLOW TEST] [6.111 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:47:48.641
    Dec 30 03:47:48.641: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename pods 12/30/22 03:47:48.643
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:47:48.659
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:47:48.662
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:443
    Dec 30 03:47:48.674: INFO: Waiting up to 5m0s for pod "server-envvars-d83f6ddc-30c8-4fd0-ad2b-e193dd90550f" in namespace "pods-6626" to be "running and ready"
    Dec 30 03:47:48.678: INFO: Pod "server-envvars-d83f6ddc-30c8-4fd0-ad2b-e193dd90550f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.762978ms
    Dec 30 03:47:48.678: INFO: The phase of Pod server-envvars-d83f6ddc-30c8-4fd0-ad2b-e193dd90550f is Pending, waiting for it to be Running (with Ready = true)
    Dec 30 03:47:50.683: INFO: Pod "server-envvars-d83f6ddc-30c8-4fd0-ad2b-e193dd90550f": Phase="Running", Reason="", readiness=true. Elapsed: 2.00931263s
    Dec 30 03:47:50.683: INFO: The phase of Pod server-envvars-d83f6ddc-30c8-4fd0-ad2b-e193dd90550f is Running (Ready = true)
    Dec 30 03:47:50.683: INFO: Pod "server-envvars-d83f6ddc-30c8-4fd0-ad2b-e193dd90550f" satisfied condition "running and ready"
    Dec 30 03:47:50.704: INFO: Waiting up to 5m0s for pod "client-envvars-1f24ed5e-e3a0-4ae8-9868-6afcfe905584" in namespace "pods-6626" to be "Succeeded or Failed"
    Dec 30 03:47:50.707: INFO: Pod "client-envvars-1f24ed5e-e3a0-4ae8-9868-6afcfe905584": Phase="Pending", Reason="", readiness=false. Elapsed: 3.561733ms
    Dec 30 03:47:52.713: INFO: Pod "client-envvars-1f24ed5e-e3a0-4ae8-9868-6afcfe905584": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008836226s
    Dec 30 03:47:54.713: INFO: Pod "client-envvars-1f24ed5e-e3a0-4ae8-9868-6afcfe905584": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009049159s
    STEP: Saw pod success 12/30/22 03:47:54.713
    Dec 30 03:47:54.713: INFO: Pod "client-envvars-1f24ed5e-e3a0-4ae8-9868-6afcfe905584" satisfied condition "Succeeded or Failed"
    Dec 30 03:47:54.717: INFO: Trying to get logs from node k8s-mgmt01 pod client-envvars-1f24ed5e-e3a0-4ae8-9868-6afcfe905584 container env3cont: <nil>
    STEP: delete the pod 12/30/22 03:47:54.726
    Dec 30 03:47:54.737: INFO: Waiting for pod client-envvars-1f24ed5e-e3a0-4ae8-9868-6afcfe905584 to disappear
    Dec 30 03:47:54.741: INFO: Pod client-envvars-1f24ed5e-e3a0-4ae8-9868-6afcfe905584 no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Dec 30 03:47:54.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-6626" for this suite. 12/30/22 03:47:54.746
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:47:54.753
Dec 30 03:47:54.753: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename webhook 12/30/22 03:47:54.755
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:47:54.768
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:47:54.772
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 12/30/22 03:47:54.788
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/30/22 03:47:55.046
STEP: Deploying the webhook pod 12/30/22 03:47:55.05
STEP: Wait for the deployment to be ready 12/30/22 03:47:55.061
Dec 30 03:47:55.068: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 12/30/22 03:47:57.081
STEP: Verifying the service has paired with the endpoint 12/30/22 03:47:57.09
Dec 30 03:47:58.091: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380
STEP: Setting timeout (1s) shorter than webhook latency (5s) 12/30/22 03:47:58.095
STEP: Registering slow webhook via the AdmissionRegistration API 12/30/22 03:47:58.095
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 12/30/22 03:47:58.114
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 12/30/22 03:47:59.124
STEP: Registering slow webhook via the AdmissionRegistration API 12/30/22 03:47:59.125
STEP: Having no error when timeout is longer than webhook latency 12/30/22 03:48:00.154
STEP: Registering slow webhook via the AdmissionRegistration API 12/30/22 03:48:00.154
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 12/30/22 03:48:05.19
STEP: Registering slow webhook via the AdmissionRegistration API 12/30/22 03:48:05.191
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Dec 30 03:48:10.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9662" for this suite. 12/30/22 03:48:10.226
STEP: Destroying namespace "webhook-9662-markers" for this suite. 12/30/22 03:48:10.232
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","completed":114,"skipped":2036,"failed":0}
------------------------------
• [SLOW TEST] [15.521 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:47:54.753
    Dec 30 03:47:54.753: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename webhook 12/30/22 03:47:54.755
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:47:54.768
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:47:54.772
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 12/30/22 03:47:54.788
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/30/22 03:47:55.046
    STEP: Deploying the webhook pod 12/30/22 03:47:55.05
    STEP: Wait for the deployment to be ready 12/30/22 03:47:55.061
    Dec 30 03:47:55.068: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 12/30/22 03:47:57.081
    STEP: Verifying the service has paired with the endpoint 12/30/22 03:47:57.09
    Dec 30 03:47:58.091: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:380
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 12/30/22 03:47:58.095
    STEP: Registering slow webhook via the AdmissionRegistration API 12/30/22 03:47:58.095
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 12/30/22 03:47:58.114
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 12/30/22 03:47:59.124
    STEP: Registering slow webhook via the AdmissionRegistration API 12/30/22 03:47:59.125
    STEP: Having no error when timeout is longer than webhook latency 12/30/22 03:48:00.154
    STEP: Registering slow webhook via the AdmissionRegistration API 12/30/22 03:48:00.154
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 12/30/22 03:48:05.19
    STEP: Registering slow webhook via the AdmissionRegistration API 12/30/22 03:48:05.191
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Dec 30 03:48:10.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-9662" for this suite. 12/30/22 03:48:10.226
    STEP: Destroying namespace "webhook-9662-markers" for this suite. 12/30/22 03:48:10.232
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:48:10.278
Dec 30 03:48:10.278: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename projected 12/30/22 03:48:10.279
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:48:10.302
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:48:10.304
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83
STEP: Creating a pod to test downward API volume plugin 12/30/22 03:48:10.307
Dec 30 03:48:10.315: INFO: Waiting up to 5m0s for pod "downwardapi-volume-99cba7bb-8359-41c8-aa0a-c2d569c94adb" in namespace "projected-6950" to be "Succeeded or Failed"
Dec 30 03:48:10.319: INFO: Pod "downwardapi-volume-99cba7bb-8359-41c8-aa0a-c2d569c94adb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.419721ms
Dec 30 03:48:12.324: INFO: Pod "downwardapi-volume-99cba7bb-8359-41c8-aa0a-c2d569c94adb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008937523s
Dec 30 03:48:14.324: INFO: Pod "downwardapi-volume-99cba7bb-8359-41c8-aa0a-c2d569c94adb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008227804s
STEP: Saw pod success 12/30/22 03:48:14.324
Dec 30 03:48:14.324: INFO: Pod "downwardapi-volume-99cba7bb-8359-41c8-aa0a-c2d569c94adb" satisfied condition "Succeeded or Failed"
Dec 30 03:48:14.328: INFO: Trying to get logs from node k8s-mgmt03 pod downwardapi-volume-99cba7bb-8359-41c8-aa0a-c2d569c94adb container client-container: <nil>
STEP: delete the pod 12/30/22 03:48:14.348
Dec 30 03:48:14.362: INFO: Waiting for pod downwardapi-volume-99cba7bb-8359-41c8-aa0a-c2d569c94adb to disappear
Dec 30 03:48:14.366: INFO: Pod downwardapi-volume-99cba7bb-8359-41c8-aa0a-c2d569c94adb no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Dec 30 03:48:14.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6950" for this suite. 12/30/22 03:48:14.372
{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","completed":115,"skipped":2082,"failed":0}
------------------------------
• [4.102 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:48:10.278
    Dec 30 03:48:10.278: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename projected 12/30/22 03:48:10.279
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:48:10.302
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:48:10.304
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:83
    STEP: Creating a pod to test downward API volume plugin 12/30/22 03:48:10.307
    Dec 30 03:48:10.315: INFO: Waiting up to 5m0s for pod "downwardapi-volume-99cba7bb-8359-41c8-aa0a-c2d569c94adb" in namespace "projected-6950" to be "Succeeded or Failed"
    Dec 30 03:48:10.319: INFO: Pod "downwardapi-volume-99cba7bb-8359-41c8-aa0a-c2d569c94adb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.419721ms
    Dec 30 03:48:12.324: INFO: Pod "downwardapi-volume-99cba7bb-8359-41c8-aa0a-c2d569c94adb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008937523s
    Dec 30 03:48:14.324: INFO: Pod "downwardapi-volume-99cba7bb-8359-41c8-aa0a-c2d569c94adb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008227804s
    STEP: Saw pod success 12/30/22 03:48:14.324
    Dec 30 03:48:14.324: INFO: Pod "downwardapi-volume-99cba7bb-8359-41c8-aa0a-c2d569c94adb" satisfied condition "Succeeded or Failed"
    Dec 30 03:48:14.328: INFO: Trying to get logs from node k8s-mgmt03 pod downwardapi-volume-99cba7bb-8359-41c8-aa0a-c2d569c94adb container client-container: <nil>
    STEP: delete the pod 12/30/22 03:48:14.348
    Dec 30 03:48:14.362: INFO: Waiting for pod downwardapi-volume-99cba7bb-8359-41c8-aa0a-c2d569c94adb to disappear
    Dec 30 03:48:14.366: INFO: Pod downwardapi-volume-99cba7bb-8359-41c8-aa0a-c2d569c94adb no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Dec 30 03:48:14.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6950" for this suite. 12/30/22 03:48:14.372
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:48:14.382
Dec 30 03:48:14.382: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename replicaset 12/30/22 03:48:14.383
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:48:14.396
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:48:14.399
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
Dec 30 03:48:14.402: INFO: Creating ReplicaSet my-hostname-basic-85e0715d-db3a-4360-9c1f-8c8869a5e5bd
Dec 30 03:48:14.411: INFO: Pod name my-hostname-basic-85e0715d-db3a-4360-9c1f-8c8869a5e5bd: Found 0 pods out of 1
Dec 30 03:48:19.418: INFO: Pod name my-hostname-basic-85e0715d-db3a-4360-9c1f-8c8869a5e5bd: Found 1 pods out of 1
Dec 30 03:48:19.418: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-85e0715d-db3a-4360-9c1f-8c8869a5e5bd" is running
Dec 30 03:48:19.418: INFO: Waiting up to 5m0s for pod "my-hostname-basic-85e0715d-db3a-4360-9c1f-8c8869a5e5bd-kbm7x" in namespace "replicaset-170" to be "running"
Dec 30 03:48:19.421: INFO: Pod "my-hostname-basic-85e0715d-db3a-4360-9c1f-8c8869a5e5bd-kbm7x": Phase="Running", Reason="", readiness=true. Elapsed: 3.250895ms
Dec 30 03:48:19.421: INFO: Pod "my-hostname-basic-85e0715d-db3a-4360-9c1f-8c8869a5e5bd-kbm7x" satisfied condition "running"
Dec 30 03:48:19.421: INFO: Pod "my-hostname-basic-85e0715d-db3a-4360-9c1f-8c8869a5e5bd-kbm7x" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-30 03:48:14 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-30 03:48:15 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-30 03:48:15 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-30 03:48:14 +0000 UTC Reason: Message:}])
Dec 30 03:48:19.421: INFO: Trying to dial the pod
Dec 30 03:48:24.434: INFO: Controller my-hostname-basic-85e0715d-db3a-4360-9c1f-8c8869a5e5bd: Got expected result from replica 1 [my-hostname-basic-85e0715d-db3a-4360-9c1f-8c8869a5e5bd-kbm7x]: "my-hostname-basic-85e0715d-db3a-4360-9c1f-8c8869a5e5bd-kbm7x", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Dec 30 03:48:24.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-170" for this suite. 12/30/22 03:48:24.44
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","completed":116,"skipped":2109,"failed":0}
------------------------------
• [SLOW TEST] [10.066 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:48:14.382
    Dec 30 03:48:14.382: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename replicaset 12/30/22 03:48:14.383
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:48:14.396
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:48:14.399
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    Dec 30 03:48:14.402: INFO: Creating ReplicaSet my-hostname-basic-85e0715d-db3a-4360-9c1f-8c8869a5e5bd
    Dec 30 03:48:14.411: INFO: Pod name my-hostname-basic-85e0715d-db3a-4360-9c1f-8c8869a5e5bd: Found 0 pods out of 1
    Dec 30 03:48:19.418: INFO: Pod name my-hostname-basic-85e0715d-db3a-4360-9c1f-8c8869a5e5bd: Found 1 pods out of 1
    Dec 30 03:48:19.418: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-85e0715d-db3a-4360-9c1f-8c8869a5e5bd" is running
    Dec 30 03:48:19.418: INFO: Waiting up to 5m0s for pod "my-hostname-basic-85e0715d-db3a-4360-9c1f-8c8869a5e5bd-kbm7x" in namespace "replicaset-170" to be "running"
    Dec 30 03:48:19.421: INFO: Pod "my-hostname-basic-85e0715d-db3a-4360-9c1f-8c8869a5e5bd-kbm7x": Phase="Running", Reason="", readiness=true. Elapsed: 3.250895ms
    Dec 30 03:48:19.421: INFO: Pod "my-hostname-basic-85e0715d-db3a-4360-9c1f-8c8869a5e5bd-kbm7x" satisfied condition "running"
    Dec 30 03:48:19.421: INFO: Pod "my-hostname-basic-85e0715d-db3a-4360-9c1f-8c8869a5e5bd-kbm7x" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-30 03:48:14 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-30 03:48:15 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-30 03:48:15 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-30 03:48:14 +0000 UTC Reason: Message:}])
    Dec 30 03:48:19.421: INFO: Trying to dial the pod
    Dec 30 03:48:24.434: INFO: Controller my-hostname-basic-85e0715d-db3a-4360-9c1f-8c8869a5e5bd: Got expected result from replica 1 [my-hostname-basic-85e0715d-db3a-4360-9c1f-8c8869a5e5bd-kbm7x]: "my-hostname-basic-85e0715d-db3a-4360-9c1f-8c8869a5e5bd-kbm7x", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Dec 30 03:48:24.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-170" for this suite. 12/30/22 03:48:24.44
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:48:24.449
Dec 30 03:48:24.449: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename var-expansion 12/30/22 03:48:24.451
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:48:24.464
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:48:24.467
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151
Dec 30 03:48:24.478: INFO: Waiting up to 2m0s for pod "var-expansion-cc26a9a8-fd58-471b-9693-494d4a6d1d56" in namespace "var-expansion-7822" to be "container 0 failed with reason CreateContainerConfigError"
Dec 30 03:48:24.481: INFO: Pod "var-expansion-cc26a9a8-fd58-471b-9693-494d4a6d1d56": Phase="Pending", Reason="", readiness=false. Elapsed: 2.987192ms
Dec 30 03:48:26.485: INFO: Pod "var-expansion-cc26a9a8-fd58-471b-9693-494d4a6d1d56": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006971429s
Dec 30 03:48:26.485: INFO: Pod "var-expansion-cc26a9a8-fd58-471b-9693-494d4a6d1d56" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Dec 30 03:48:26.485: INFO: Deleting pod "var-expansion-cc26a9a8-fd58-471b-9693-494d4a6d1d56" in namespace "var-expansion-7822"
Dec 30 03:48:26.493: INFO: Wait up to 5m0s for pod "var-expansion-cc26a9a8-fd58-471b-9693-494d4a6d1d56" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Dec 30 03:48:28.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7822" for this suite. 12/30/22 03:48:28.51
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","completed":117,"skipped":2120,"failed":0}
------------------------------
• [4.067 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:48:24.449
    Dec 30 03:48:24.449: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename var-expansion 12/30/22 03:48:24.451
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:48:24.464
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:48:24.467
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:151
    Dec 30 03:48:24.478: INFO: Waiting up to 2m0s for pod "var-expansion-cc26a9a8-fd58-471b-9693-494d4a6d1d56" in namespace "var-expansion-7822" to be "container 0 failed with reason CreateContainerConfigError"
    Dec 30 03:48:24.481: INFO: Pod "var-expansion-cc26a9a8-fd58-471b-9693-494d4a6d1d56": Phase="Pending", Reason="", readiness=false. Elapsed: 2.987192ms
    Dec 30 03:48:26.485: INFO: Pod "var-expansion-cc26a9a8-fd58-471b-9693-494d4a6d1d56": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006971429s
    Dec 30 03:48:26.485: INFO: Pod "var-expansion-cc26a9a8-fd58-471b-9693-494d4a6d1d56" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Dec 30 03:48:26.485: INFO: Deleting pod "var-expansion-cc26a9a8-fd58-471b-9693-494d4a6d1d56" in namespace "var-expansion-7822"
    Dec 30 03:48:26.493: INFO: Wait up to 5m0s for pod "var-expansion-cc26a9a8-fd58-471b-9693-494d4a6d1d56" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Dec 30 03:48:28.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-7822" for this suite. 12/30/22 03:48:28.51
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:543
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:48:28.518
Dec 30 03:48:28.518: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename sched-preemption 12/30/22 03:48:28.52
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:48:28.532
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:48:28.535
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Dec 30 03:48:28.551: INFO: Waiting up to 1m0s for all nodes to be ready
Dec 30 03:49:28.606: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:49:28.611
Dec 30 03:49:28.611: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename sched-preemption-path 12/30/22 03:49:28.613
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:49:28.626
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:49:28.63
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:496
STEP: Finding an available node 12/30/22 03:49:28.635
STEP: Trying to launch a pod without a label to get a node which can launch it. 12/30/22 03:49:28.635
Dec 30 03:49:28.646: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-7141" to be "running"
Dec 30 03:49:28.650: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 3.867775ms
Dec 30 03:49:30.656: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.0093492s
Dec 30 03:49:30.656: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 12/30/22 03:49:30.659
Dec 30 03:49:30.670: INFO: found a healthy node: k8s-mgmt01
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:543
Dec 30 03:49:44.747: INFO: pods created so far: [1 1 1]
Dec 30 03:49:44.747: INFO: length of pods created so far: 3
Dec 30 03:49:46.758: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:187
Dec 30 03:49:53.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-7141" for this suite. 12/30/22 03:49:53.767
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:470
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Dec 30 03:49:53.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-1406" for this suite. 12/30/22 03:49:53.818
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","completed":118,"skipped":2145,"failed":0}
------------------------------
• [SLOW TEST] [85.386 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:458
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:543

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:48:28.518
    Dec 30 03:48:28.518: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename sched-preemption 12/30/22 03:48:28.52
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:48:28.532
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:48:28.535
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Dec 30 03:48:28.551: INFO: Waiting up to 1m0s for all nodes to be ready
    Dec 30 03:49:28.606: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:49:28.611
    Dec 30 03:49:28.611: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename sched-preemption-path 12/30/22 03:49:28.613
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:49:28.626
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:49:28.63
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:496
    STEP: Finding an available node 12/30/22 03:49:28.635
    STEP: Trying to launch a pod without a label to get a node which can launch it. 12/30/22 03:49:28.635
    Dec 30 03:49:28.646: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-7141" to be "running"
    Dec 30 03:49:28.650: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 3.867775ms
    Dec 30 03:49:30.656: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.0093492s
    Dec 30 03:49:30.656: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 12/30/22 03:49:30.659
    Dec 30 03:49:30.670: INFO: found a healthy node: k8s-mgmt01
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:543
    Dec 30 03:49:44.747: INFO: pods created so far: [1 1 1]
    Dec 30 03:49:44.747: INFO: length of pods created so far: 3
    Dec 30 03:49:46.758: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/framework.go:187
    Dec 30 03:49:53.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-path-7141" for this suite. 12/30/22 03:49:53.767
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:470
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Dec 30 03:49:53.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-1406" for this suite. 12/30/22 03:49:53.818
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:49:53.908
Dec 30 03:49:53.908: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename webhook 12/30/22 03:49:53.91
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:49:53.924
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:49:53.927
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 12/30/22 03:49:53.943
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/30/22 03:49:54.396
STEP: Deploying the webhook pod 12/30/22 03:49:54.403
STEP: Wait for the deployment to be ready 12/30/22 03:49:54.414
Dec 30 03:49:54.420: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 12/30/22 03:49:56.434
STEP: Verifying the service has paired with the endpoint 12/30/22 03:49:56.445
Dec 30 03:49:57.446: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412
STEP: Creating a validating webhook configuration 12/30/22 03:49:57.449
STEP: Creating a configMap that does not comply to the validation webhook rules 12/30/22 03:49:57.469
STEP: Updating a validating webhook configuration's rules to not include the create operation 12/30/22 03:49:57.481
STEP: Creating a configMap that does not comply to the validation webhook rules 12/30/22 03:49:57.492
STEP: Patching a validating webhook configuration's rules to include the create operation 12/30/22 03:49:57.503
STEP: Creating a configMap that does not comply to the validation webhook rules 12/30/22 03:49:57.512
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Dec 30 03:49:57.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9632" for this suite. 12/30/22 03:49:57.527
STEP: Destroying namespace "webhook-9632-markers" for this suite. 12/30/22 03:49:57.533
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","completed":119,"skipped":2178,"failed":0}
------------------------------
• [3.667 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:49:53.908
    Dec 30 03:49:53.908: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename webhook 12/30/22 03:49:53.91
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:49:53.924
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:49:53.927
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 12/30/22 03:49:53.943
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/30/22 03:49:54.396
    STEP: Deploying the webhook pod 12/30/22 03:49:54.403
    STEP: Wait for the deployment to be ready 12/30/22 03:49:54.414
    Dec 30 03:49:54.420: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 12/30/22 03:49:56.434
    STEP: Verifying the service has paired with the endpoint 12/30/22 03:49:56.445
    Dec 30 03:49:57.446: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:412
    STEP: Creating a validating webhook configuration 12/30/22 03:49:57.449
    STEP: Creating a configMap that does not comply to the validation webhook rules 12/30/22 03:49:57.469
    STEP: Updating a validating webhook configuration's rules to not include the create operation 12/30/22 03:49:57.481
    STEP: Creating a configMap that does not comply to the validation webhook rules 12/30/22 03:49:57.492
    STEP: Patching a validating webhook configuration's rules to include the create operation 12/30/22 03:49:57.503
    STEP: Creating a configMap that does not comply to the validation webhook rules 12/30/22 03:49:57.512
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Dec 30 03:49:57.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-9632" for this suite. 12/30/22 03:49:57.527
    STEP: Destroying namespace "webhook-9632-markers" for this suite. 12/30/22 03:49:57.533
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:49:57.575
Dec 30 03:49:57.575: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename daemonsets 12/30/22 03:49:57.576
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:49:57.587
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:49:57.589
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165
STEP: Creating simple DaemonSet "daemon-set" 12/30/22 03:49:57.622
STEP: Check that daemon pods launch on every node of the cluster. 12/30/22 03:49:57.627
Dec 30 03:49:57.636: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 30 03:49:57.636: INFO: Node k8s-mgmt01 is running 0 daemon pod, expected 1
Dec 30 03:49:58.652: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 30 03:49:58.652: INFO: Node k8s-mgmt01 is running 0 daemon pod, expected 1
Dec 30 03:49:59.649: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
Dec 30 03:49:59.649: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 12/30/22 03:49:59.653
Dec 30 03:49:59.673: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Dec 30 03:49:59.673: INFO: Node k8s-worker01 is running 0 daemon pod, expected 1
Dec 30 03:50:00.685: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Dec 30 03:50:00.685: INFO: Node k8s-worker01 is running 0 daemon pod, expected 1
Dec 30 03:50:01.685: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Dec 30 03:50:01.685: INFO: Node k8s-worker01 is running 0 daemon pod, expected 1
Dec 30 03:50:02.685: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Dec 30 03:50:02.685: INFO: Node k8s-worker01 is running 0 daemon pod, expected 1
Dec 30 03:50:03.686: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Dec 30 03:50:03.686: INFO: Node k8s-worker01 is running 0 daemon pod, expected 1
Dec 30 03:50:04.684: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Dec 30 03:50:04.685: INFO: Node k8s-worker01 is running 0 daemon pod, expected 1
Dec 30 03:50:05.689: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
Dec 30 03:50:05.689: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 12/30/22 03:50:05.694
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9182, will wait for the garbage collector to delete the pods 12/30/22 03:50:05.694
Dec 30 03:50:05.756: INFO: Deleting DaemonSet.extensions daemon-set took: 6.831915ms
Dec 30 03:50:05.857: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.638893ms
Dec 30 03:50:09.362: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 30 03:50:09.362: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Dec 30 03:50:09.366: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"433227"},"items":null}

Dec 30 03:50:09.370: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"433227"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Dec 30 03:50:09.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9182" for this suite. 12/30/22 03:50:09.401
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","completed":120,"skipped":2179,"failed":0}
------------------------------
• [SLOW TEST] [11.833 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:49:57.575
    Dec 30 03:49:57.575: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename daemonsets 12/30/22 03:49:57.576
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:49:57.587
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:49:57.589
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:165
    STEP: Creating simple DaemonSet "daemon-set" 12/30/22 03:49:57.622
    STEP: Check that daemon pods launch on every node of the cluster. 12/30/22 03:49:57.627
    Dec 30 03:49:57.636: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 30 03:49:57.636: INFO: Node k8s-mgmt01 is running 0 daemon pod, expected 1
    Dec 30 03:49:58.652: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 30 03:49:58.652: INFO: Node k8s-mgmt01 is running 0 daemon pod, expected 1
    Dec 30 03:49:59.649: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
    Dec 30 03:49:59.649: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 12/30/22 03:49:59.653
    Dec 30 03:49:59.673: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Dec 30 03:49:59.673: INFO: Node k8s-worker01 is running 0 daemon pod, expected 1
    Dec 30 03:50:00.685: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Dec 30 03:50:00.685: INFO: Node k8s-worker01 is running 0 daemon pod, expected 1
    Dec 30 03:50:01.685: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Dec 30 03:50:01.685: INFO: Node k8s-worker01 is running 0 daemon pod, expected 1
    Dec 30 03:50:02.685: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Dec 30 03:50:02.685: INFO: Node k8s-worker01 is running 0 daemon pod, expected 1
    Dec 30 03:50:03.686: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Dec 30 03:50:03.686: INFO: Node k8s-worker01 is running 0 daemon pod, expected 1
    Dec 30 03:50:04.684: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Dec 30 03:50:04.685: INFO: Node k8s-worker01 is running 0 daemon pod, expected 1
    Dec 30 03:50:05.689: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
    Dec 30 03:50:05.689: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 12/30/22 03:50:05.694
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9182, will wait for the garbage collector to delete the pods 12/30/22 03:50:05.694
    Dec 30 03:50:05.756: INFO: Deleting DaemonSet.extensions daemon-set took: 6.831915ms
    Dec 30 03:50:05.857: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.638893ms
    Dec 30 03:50:09.362: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 30 03:50:09.362: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Dec 30 03:50:09.366: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"433227"},"items":null}

    Dec 30 03:50:09.370: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"433227"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Dec 30 03:50:09.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-9182" for this suite. 12/30/22 03:50:09.401
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:50:09.409
Dec 30 03:50:09.409: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename svcaccounts 12/30/22 03:50:09.41
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:50:09.424
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:50:09.427
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739
Dec 30 03:50:09.433: INFO: Got root ca configmap in namespace "svcaccounts-843"
Dec 30 03:50:09.439: INFO: Deleted root ca configmap in namespace "svcaccounts-843"
STEP: waiting for a new root ca configmap created 12/30/22 03:50:09.94
Dec 30 03:50:09.945: INFO: Recreated root ca configmap in namespace "svcaccounts-843"
Dec 30 03:50:09.950: INFO: Updated root ca configmap in namespace "svcaccounts-843"
STEP: waiting for the root ca configmap reconciled 12/30/22 03:50:10.451
Dec 30 03:50:10.456: INFO: Reconciled root ca configmap in namespace "svcaccounts-843"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Dec 30 03:50:10.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-843" for this suite. 12/30/22 03:50:10.462
{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","completed":121,"skipped":2186,"failed":0}
------------------------------
• [1.060 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:50:09.409
    Dec 30 03:50:09.409: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename svcaccounts 12/30/22 03:50:09.41
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:50:09.424
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:50:09.427
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:739
    Dec 30 03:50:09.433: INFO: Got root ca configmap in namespace "svcaccounts-843"
    Dec 30 03:50:09.439: INFO: Deleted root ca configmap in namespace "svcaccounts-843"
    STEP: waiting for a new root ca configmap created 12/30/22 03:50:09.94
    Dec 30 03:50:09.945: INFO: Recreated root ca configmap in namespace "svcaccounts-843"
    Dec 30 03:50:09.950: INFO: Updated root ca configmap in namespace "svcaccounts-843"
    STEP: waiting for the root ca configmap reconciled 12/30/22 03:50:10.451
    Dec 30 03:50:10.456: INFO: Reconciled root ca configmap in namespace "svcaccounts-843"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Dec 30 03:50:10.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-843" for this suite. 12/30/22 03:50:10.462
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:50:10.47
Dec 30 03:50:10.470: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename watch 12/30/22 03:50:10.471
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:50:10.485
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:50:10.488
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 12/30/22 03:50:10.492
STEP: creating a watch on configmaps with label B 12/30/22 03:50:10.493
STEP: creating a watch on configmaps with label A or B 12/30/22 03:50:10.494
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 12/30/22 03:50:10.496
Dec 30 03:50:10.500: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9772  ab6c4754-bab0-4adc-9777-80132d4c1741 433245 0 2022-12-30 03:50:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-12-30 03:50:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 30 03:50:10.500: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9772  ab6c4754-bab0-4adc-9777-80132d4c1741 433245 0 2022-12-30 03:50:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-12-30 03:50:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 12/30/22 03:50:10.5
Dec 30 03:50:10.509: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9772  ab6c4754-bab0-4adc-9777-80132d4c1741 433246 0 2022-12-30 03:50:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-12-30 03:50:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 30 03:50:10.509: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9772  ab6c4754-bab0-4adc-9777-80132d4c1741 433246 0 2022-12-30 03:50:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-12-30 03:50:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 12/30/22 03:50:10.509
Dec 30 03:50:10.517: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9772  ab6c4754-bab0-4adc-9777-80132d4c1741 433247 0 2022-12-30 03:50:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-12-30 03:50:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 30 03:50:10.518: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9772  ab6c4754-bab0-4adc-9777-80132d4c1741 433247 0 2022-12-30 03:50:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-12-30 03:50:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 12/30/22 03:50:10.518
Dec 30 03:50:10.524: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9772  ab6c4754-bab0-4adc-9777-80132d4c1741 433248 0 2022-12-30 03:50:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-12-30 03:50:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 30 03:50:10.524: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9772  ab6c4754-bab0-4adc-9777-80132d4c1741 433248 0 2022-12-30 03:50:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-12-30 03:50:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 12/30/22 03:50:10.524
Dec 30 03:50:10.529: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9772  9eb08a8c-ca3c-45d3-9cd6-028d3b3d2e16 433249 0 2022-12-30 03:50:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2022-12-30 03:50:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 30 03:50:10.529: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9772  9eb08a8c-ca3c-45d3-9cd6-028d3b3d2e16 433249 0 2022-12-30 03:50:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2022-12-30 03:50:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 12/30/22 03:50:20.53
Dec 30 03:50:20.538: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9772  9eb08a8c-ca3c-45d3-9cd6-028d3b3d2e16 433320 0 2022-12-30 03:50:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2022-12-30 03:50:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 30 03:50:20.538: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9772  9eb08a8c-ca3c-45d3-9cd6-028d3b3d2e16 433320 0 2022-12-30 03:50:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2022-12-30 03:50:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Dec 30 03:50:30.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9772" for this suite. 12/30/22 03:50:30.545
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","completed":122,"skipped":2198,"failed":0}
------------------------------
• [SLOW TEST] [20.083 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:50:10.47
    Dec 30 03:50:10.470: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename watch 12/30/22 03:50:10.471
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:50:10.485
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:50:10.488
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 12/30/22 03:50:10.492
    STEP: creating a watch on configmaps with label B 12/30/22 03:50:10.493
    STEP: creating a watch on configmaps with label A or B 12/30/22 03:50:10.494
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 12/30/22 03:50:10.496
    Dec 30 03:50:10.500: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9772  ab6c4754-bab0-4adc-9777-80132d4c1741 433245 0 2022-12-30 03:50:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-12-30 03:50:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Dec 30 03:50:10.500: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9772  ab6c4754-bab0-4adc-9777-80132d4c1741 433245 0 2022-12-30 03:50:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-12-30 03:50:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 12/30/22 03:50:10.5
    Dec 30 03:50:10.509: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9772  ab6c4754-bab0-4adc-9777-80132d4c1741 433246 0 2022-12-30 03:50:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-12-30 03:50:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Dec 30 03:50:10.509: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9772  ab6c4754-bab0-4adc-9777-80132d4c1741 433246 0 2022-12-30 03:50:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-12-30 03:50:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 12/30/22 03:50:10.509
    Dec 30 03:50:10.517: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9772  ab6c4754-bab0-4adc-9777-80132d4c1741 433247 0 2022-12-30 03:50:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-12-30 03:50:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Dec 30 03:50:10.518: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9772  ab6c4754-bab0-4adc-9777-80132d4c1741 433247 0 2022-12-30 03:50:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-12-30 03:50:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 12/30/22 03:50:10.518
    Dec 30 03:50:10.524: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9772  ab6c4754-bab0-4adc-9777-80132d4c1741 433248 0 2022-12-30 03:50:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-12-30 03:50:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Dec 30 03:50:10.524: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9772  ab6c4754-bab0-4adc-9777-80132d4c1741 433248 0 2022-12-30 03:50:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2022-12-30 03:50:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 12/30/22 03:50:10.524
    Dec 30 03:50:10.529: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9772  9eb08a8c-ca3c-45d3-9cd6-028d3b3d2e16 433249 0 2022-12-30 03:50:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2022-12-30 03:50:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Dec 30 03:50:10.529: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9772  9eb08a8c-ca3c-45d3-9cd6-028d3b3d2e16 433249 0 2022-12-30 03:50:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2022-12-30 03:50:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 12/30/22 03:50:20.53
    Dec 30 03:50:20.538: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9772  9eb08a8c-ca3c-45d3-9cd6-028d3b3d2e16 433320 0 2022-12-30 03:50:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2022-12-30 03:50:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Dec 30 03:50:20.538: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9772  9eb08a8c-ca3c-45d3-9cd6-028d3b3d2e16 433320 0 2022-12-30 03:50:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2022-12-30 03:50:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Dec 30 03:50:30.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-9772" for this suite. 12/30/22 03:50:30.545
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:50:30.554
Dec 30 03:50:30.554: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename podtemplate 12/30/22 03:50:30.555
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:50:30.57
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:50:30.573
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Dec 30 03:50:30.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-4814" for this suite. 12/30/22 03:50:30.614
{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","completed":123,"skipped":2203,"failed":0}
------------------------------
• [0.068 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:50:30.554
    Dec 30 03:50:30.554: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename podtemplate 12/30/22 03:50:30.555
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:50:30.57
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:50:30.573
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Dec 30 03:50:30.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-4814" for this suite. 12/30/22 03:50:30.614
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:50:30.631
Dec 30 03:50:30.631: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename namespaces 12/30/22 03:50:30.632
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:50:30.645
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:50:30.648
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267
STEP: creating a Namespace 12/30/22 03:50:30.651
STEP: patching the Namespace 12/30/22 03:50:30.663
STEP: get the Namespace and ensuring it has the label 12/30/22 03:50:30.669
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Dec 30 03:50:30.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-2012" for this suite. 12/30/22 03:50:30.678
STEP: Destroying namespace "nspatchtest-cc2fb3b8-a9f9-49d6-b200-6cef3ee8dbd5-7816" for this suite. 12/30/22 03:50:30.686
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","completed":124,"skipped":2307,"failed":0}
------------------------------
• [0.062 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:50:30.631
    Dec 30 03:50:30.631: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename namespaces 12/30/22 03:50:30.632
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:50:30.645
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:50:30.648
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:267
    STEP: creating a Namespace 12/30/22 03:50:30.651
    STEP: patching the Namespace 12/30/22 03:50:30.663
    STEP: get the Namespace and ensuring it has the label 12/30/22 03:50:30.669
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Dec 30 03:50:30.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-2012" for this suite. 12/30/22 03:50:30.678
    STEP: Destroying namespace "nspatchtest-cc2fb3b8-a9f9-49d6-b200-6cef3ee8dbd5-7816" for this suite. 12/30/22 03:50:30.686
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:50:30.694
Dec 30 03:50:30.694: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename webhook 12/30/22 03:50:30.696
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:50:30.709
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:50:30.712
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 12/30/22 03:50:30.728
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/30/22 03:50:31.192
STEP: Deploying the webhook pod 12/30/22 03:50:31.2
STEP: Wait for the deployment to be ready 12/30/22 03:50:31.21
Dec 30 03:50:31.217: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 12/30/22 03:50:33.231
STEP: Verifying the service has paired with the endpoint 12/30/22 03:50:33.242
Dec 30 03:50:34.242: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307
STEP: Registering the crd webhook via the AdmissionRegistration API 12/30/22 03:50:34.246
STEP: Creating a custom resource definition that should be denied by the webhook 12/30/22 03:50:34.266
Dec 30 03:50:34.267: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Dec 30 03:50:34.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4564" for this suite. 12/30/22 03:50:34.295
STEP: Destroying namespace "webhook-4564-markers" for this suite. 12/30/22 03:50:34.302
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","completed":125,"skipped":2320,"failed":0}
------------------------------
• [3.649 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:50:30.694
    Dec 30 03:50:30.694: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename webhook 12/30/22 03:50:30.696
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:50:30.709
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:50:30.712
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 12/30/22 03:50:30.728
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/30/22 03:50:31.192
    STEP: Deploying the webhook pod 12/30/22 03:50:31.2
    STEP: Wait for the deployment to be ready 12/30/22 03:50:31.21
    Dec 30 03:50:31.217: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 12/30/22 03:50:33.231
    STEP: Verifying the service has paired with the endpoint 12/30/22 03:50:33.242
    Dec 30 03:50:34.242: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:307
    STEP: Registering the crd webhook via the AdmissionRegistration API 12/30/22 03:50:34.246
    STEP: Creating a custom resource definition that should be denied by the webhook 12/30/22 03:50:34.266
    Dec 30 03:50:34.267: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Dec 30 03:50:34.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-4564" for this suite. 12/30/22 03:50:34.295
    STEP: Destroying namespace "webhook-4564-markers" for this suite. 12/30/22 03:50:34.302
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:50:34.344
Dec 30 03:50:34.344: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename configmap 12/30/22 03:50:34.346
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:50:34.36
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:50:34.363
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56
STEP: Creating configMap with name configmap-test-volume-8b39f6cc-56fd-4b13-a7b1-dff72450165f 12/30/22 03:50:34.366
STEP: Creating a pod to test consume configMaps 12/30/22 03:50:34.371
Dec 30 03:50:34.380: INFO: Waiting up to 5m0s for pod "pod-configmaps-4ca1d118-a966-4bd4-936f-dcdfe584d1ba" in namespace "configmap-9799" to be "Succeeded or Failed"
Dec 30 03:50:34.384: INFO: Pod "pod-configmaps-4ca1d118-a966-4bd4-936f-dcdfe584d1ba": Phase="Pending", Reason="", readiness=false. Elapsed: 3.29873ms
Dec 30 03:50:36.389: INFO: Pod "pod-configmaps-4ca1d118-a966-4bd4-936f-dcdfe584d1ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008397935s
Dec 30 03:50:38.389: INFO: Pod "pod-configmaps-4ca1d118-a966-4bd4-936f-dcdfe584d1ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008103818s
STEP: Saw pod success 12/30/22 03:50:38.389
Dec 30 03:50:38.389: INFO: Pod "pod-configmaps-4ca1d118-a966-4bd4-936f-dcdfe584d1ba" satisfied condition "Succeeded or Failed"
Dec 30 03:50:38.393: INFO: Trying to get logs from node k8s-mgmt01 pod pod-configmaps-4ca1d118-a966-4bd4-936f-dcdfe584d1ba container agnhost-container: <nil>
STEP: delete the pod 12/30/22 03:50:38.414
Dec 30 03:50:38.427: INFO: Waiting for pod pod-configmaps-4ca1d118-a966-4bd4-936f-dcdfe584d1ba to disappear
Dec 30 03:50:38.430: INFO: Pod pod-configmaps-4ca1d118-a966-4bd4-936f-dcdfe584d1ba no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Dec 30 03:50:38.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9799" for this suite. 12/30/22 03:50:38.436
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":126,"skipped":2331,"failed":0}
------------------------------
• [4.098 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:50:34.344
    Dec 30 03:50:34.344: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename configmap 12/30/22 03:50:34.346
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:50:34.36
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:50:34.363
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:56
    STEP: Creating configMap with name configmap-test-volume-8b39f6cc-56fd-4b13-a7b1-dff72450165f 12/30/22 03:50:34.366
    STEP: Creating a pod to test consume configMaps 12/30/22 03:50:34.371
    Dec 30 03:50:34.380: INFO: Waiting up to 5m0s for pod "pod-configmaps-4ca1d118-a966-4bd4-936f-dcdfe584d1ba" in namespace "configmap-9799" to be "Succeeded or Failed"
    Dec 30 03:50:34.384: INFO: Pod "pod-configmaps-4ca1d118-a966-4bd4-936f-dcdfe584d1ba": Phase="Pending", Reason="", readiness=false. Elapsed: 3.29873ms
    Dec 30 03:50:36.389: INFO: Pod "pod-configmaps-4ca1d118-a966-4bd4-936f-dcdfe584d1ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008397935s
    Dec 30 03:50:38.389: INFO: Pod "pod-configmaps-4ca1d118-a966-4bd4-936f-dcdfe584d1ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008103818s
    STEP: Saw pod success 12/30/22 03:50:38.389
    Dec 30 03:50:38.389: INFO: Pod "pod-configmaps-4ca1d118-a966-4bd4-936f-dcdfe584d1ba" satisfied condition "Succeeded or Failed"
    Dec 30 03:50:38.393: INFO: Trying to get logs from node k8s-mgmt01 pod pod-configmaps-4ca1d118-a966-4bd4-936f-dcdfe584d1ba container agnhost-container: <nil>
    STEP: delete the pod 12/30/22 03:50:38.414
    Dec 30 03:50:38.427: INFO: Waiting for pod pod-configmaps-4ca1d118-a966-4bd4-936f-dcdfe584d1ba to disappear
    Dec 30 03:50:38.430: INFO: Pod pod-configmaps-4ca1d118-a966-4bd4-936f-dcdfe584d1ba no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Dec 30 03:50:38.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-9799" for this suite. 12/30/22 03:50:38.436
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:50:38.443
Dec 30 03:50:38.443: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename downward-api 12/30/22 03:50:38.444
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:50:38.457
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:50:38.46
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67
STEP: Creating a pod to test downward API volume plugin 12/30/22 03:50:38.463
Dec 30 03:50:38.473: INFO: Waiting up to 5m0s for pod "downwardapi-volume-445ff7b7-e03f-4f01-bcdc-b21e022f4280" in namespace "downward-api-2065" to be "Succeeded or Failed"
Dec 30 03:50:38.476: INFO: Pod "downwardapi-volume-445ff7b7-e03f-4f01-bcdc-b21e022f4280": Phase="Pending", Reason="", readiness=false. Elapsed: 3.258975ms
Dec 30 03:50:40.481: INFO: Pod "downwardapi-volume-445ff7b7-e03f-4f01-bcdc-b21e022f4280": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00861783s
Dec 30 03:50:42.482: INFO: Pod "downwardapi-volume-445ff7b7-e03f-4f01-bcdc-b21e022f4280": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009855807s
STEP: Saw pod success 12/30/22 03:50:42.482
Dec 30 03:50:42.483: INFO: Pod "downwardapi-volume-445ff7b7-e03f-4f01-bcdc-b21e022f4280" satisfied condition "Succeeded or Failed"
Dec 30 03:50:42.486: INFO: Trying to get logs from node k8s-mgmt01 pod downwardapi-volume-445ff7b7-e03f-4f01-bcdc-b21e022f4280 container client-container: <nil>
STEP: delete the pod 12/30/22 03:50:42.494
Dec 30 03:50:42.508: INFO: Waiting for pod downwardapi-volume-445ff7b7-e03f-4f01-bcdc-b21e022f4280 to disappear
Dec 30 03:50:42.511: INFO: Pod downwardapi-volume-445ff7b7-e03f-4f01-bcdc-b21e022f4280 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Dec 30 03:50:42.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2065" for this suite. 12/30/22 03:50:42.517
{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","completed":127,"skipped":2331,"failed":0}
------------------------------
• [4.081 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:50:38.443
    Dec 30 03:50:38.443: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename downward-api 12/30/22 03:50:38.444
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:50:38.457
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:50:38.46
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:67
    STEP: Creating a pod to test downward API volume plugin 12/30/22 03:50:38.463
    Dec 30 03:50:38.473: INFO: Waiting up to 5m0s for pod "downwardapi-volume-445ff7b7-e03f-4f01-bcdc-b21e022f4280" in namespace "downward-api-2065" to be "Succeeded or Failed"
    Dec 30 03:50:38.476: INFO: Pod "downwardapi-volume-445ff7b7-e03f-4f01-bcdc-b21e022f4280": Phase="Pending", Reason="", readiness=false. Elapsed: 3.258975ms
    Dec 30 03:50:40.481: INFO: Pod "downwardapi-volume-445ff7b7-e03f-4f01-bcdc-b21e022f4280": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00861783s
    Dec 30 03:50:42.482: INFO: Pod "downwardapi-volume-445ff7b7-e03f-4f01-bcdc-b21e022f4280": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009855807s
    STEP: Saw pod success 12/30/22 03:50:42.482
    Dec 30 03:50:42.483: INFO: Pod "downwardapi-volume-445ff7b7-e03f-4f01-bcdc-b21e022f4280" satisfied condition "Succeeded or Failed"
    Dec 30 03:50:42.486: INFO: Trying to get logs from node k8s-mgmt01 pod downwardapi-volume-445ff7b7-e03f-4f01-bcdc-b21e022f4280 container client-container: <nil>
    STEP: delete the pod 12/30/22 03:50:42.494
    Dec 30 03:50:42.508: INFO: Waiting for pod downwardapi-volume-445ff7b7-e03f-4f01-bcdc-b21e022f4280 to disappear
    Dec 30 03:50:42.511: INFO: Pod downwardapi-volume-445ff7b7-e03f-4f01-bcdc-b21e022f4280 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Dec 30 03:50:42.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-2065" for this suite. 12/30/22 03:50:42.517
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:50:42.524
Dec 30 03:50:42.524: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename svc-latency 12/30/22 03:50:42.526
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:50:42.539
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:50:42.542
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
Dec 30 03:50:42.546: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: creating replication controller svc-latency-rc in namespace svc-latency-4175 12/30/22 03:50:42.547
I1230 03:50:42.554425      25 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-4175, replica count: 1
I1230 03:50:43.606474      25 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1230 03:50:44.607068      25 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 30 03:50:44.718: INFO: Created: latency-svc-szbb7
Dec 30 03:50:44.723: INFO: Got endpoints: latency-svc-szbb7 [16.298859ms]
Dec 30 03:50:44.735: INFO: Created: latency-svc-mvttq
Dec 30 03:50:44.738: INFO: Got endpoints: latency-svc-mvttq [14.950894ms]
Dec 30 03:50:44.739: INFO: Created: latency-svc-4bj7n
Dec 30 03:50:44.742: INFO: Got endpoints: latency-svc-4bj7n [18.999534ms]
Dec 30 03:50:44.743: INFO: Created: latency-svc-x6vt8
Dec 30 03:50:44.746: INFO: Got endpoints: latency-svc-x6vt8 [22.985301ms]
Dec 30 03:50:44.748: INFO: Created: latency-svc-m4wxg
Dec 30 03:50:44.751: INFO: Got endpoints: latency-svc-m4wxg [27.548646ms]
Dec 30 03:50:44.753: INFO: Created: latency-svc-t62sf
Dec 30 03:50:44.756: INFO: Got endpoints: latency-svc-t62sf [32.748194ms]
Dec 30 03:50:44.757: INFO: Created: latency-svc-wmfj9
Dec 30 03:50:44.761: INFO: Got endpoints: latency-svc-wmfj9 [37.166482ms]
Dec 30 03:50:44.763: INFO: Created: latency-svc-nqswz
Dec 30 03:50:44.766: INFO: Got endpoints: latency-svc-nqswz [42.916667ms]
Dec 30 03:50:44.767: INFO: Created: latency-svc-zmjhh
Dec 30 03:50:44.770: INFO: Got endpoints: latency-svc-zmjhh [46.762007ms]
Dec 30 03:50:44.771: INFO: Created: latency-svc-rnhtv
Dec 30 03:50:44.774: INFO: Got endpoints: latency-svc-rnhtv [50.426888ms]
Dec 30 03:50:44.776: INFO: Created: latency-svc-swhxq
Dec 30 03:50:44.779: INFO: Got endpoints: latency-svc-swhxq [55.351882ms]
Dec 30 03:50:44.781: INFO: Created: latency-svc-l9mpx
Dec 30 03:50:44.784: INFO: Got endpoints: latency-svc-l9mpx [59.714248ms]
Dec 30 03:50:44.785: INFO: Created: latency-svc-wvxnk
Dec 30 03:50:44.789: INFO: Got endpoints: latency-svc-wvxnk [64.939265ms]
Dec 30 03:50:44.790: INFO: Created: latency-svc-xbllj
Dec 30 03:50:44.793: INFO: Got endpoints: latency-svc-xbllj [69.174825ms]
Dec 30 03:50:44.795: INFO: Created: latency-svc-hhgl7
Dec 30 03:50:44.798: INFO: Created: latency-svc-m7j9p
Dec 30 03:50:44.799: INFO: Got endpoints: latency-svc-hhgl7 [75.023933ms]
Dec 30 03:50:44.802: INFO: Got endpoints: latency-svc-m7j9p [77.274292ms]
Dec 30 03:50:44.803: INFO: Created: latency-svc-lbnx6
Dec 30 03:50:44.806: INFO: Got endpoints: latency-svc-lbnx6 [67.457199ms]
Dec 30 03:50:44.808: INFO: Created: latency-svc-lqbkj
Dec 30 03:50:44.812: INFO: Got endpoints: latency-svc-lqbkj [69.806042ms]
Dec 30 03:50:44.812: INFO: Created: latency-svc-dpcdh
Dec 30 03:50:44.814: INFO: Got endpoints: latency-svc-dpcdh [68.069121ms]
Dec 30 03:50:44.817: INFO: Created: latency-svc-fw8fr
Dec 30 03:50:44.820: INFO: Got endpoints: latency-svc-fw8fr [68.672974ms]
Dec 30 03:50:44.822: INFO: Created: latency-svc-cttvk
Dec 30 03:50:44.825: INFO: Created: latency-svc-lkdff
Dec 30 03:50:44.825: INFO: Got endpoints: latency-svc-cttvk [69.125969ms]
Dec 30 03:50:44.829: INFO: Got endpoints: latency-svc-lkdff [67.871511ms]
Dec 30 03:50:44.830: INFO: Created: latency-svc-jllgt
Dec 30 03:50:44.834: INFO: Got endpoints: latency-svc-jllgt [67.372518ms]
Dec 30 03:50:44.835: INFO: Created: latency-svc-28f6w
Dec 30 03:50:44.839: INFO: Got endpoints: latency-svc-28f6w [69.219898ms]
Dec 30 03:50:44.840: INFO: Created: latency-svc-cvlr2
Dec 30 03:50:44.843: INFO: Got endpoints: latency-svc-cvlr2 [69.525712ms]
Dec 30 03:50:44.845: INFO: Created: latency-svc-cnw2x
Dec 30 03:50:44.850: INFO: Got endpoints: latency-svc-cnw2x [70.815123ms]
Dec 30 03:50:44.850: INFO: Created: latency-svc-rg8w4
Dec 30 03:50:44.856: INFO: Got endpoints: latency-svc-rg8w4 [71.035771ms]
Dec 30 03:50:44.859: INFO: Created: latency-svc-cvqtv
Dec 30 03:50:44.863: INFO: Created: latency-svc-6gpcb
Dec 30 03:50:44.864: INFO: Got endpoints: latency-svc-cvqtv [75.777375ms]
Dec 30 03:50:44.866: INFO: Got endpoints: latency-svc-6gpcb [72.944779ms]
Dec 30 03:50:44.868: INFO: Created: latency-svc-mlmww
Dec 30 03:50:44.871: INFO: Got endpoints: latency-svc-mlmww [72.599105ms]
Dec 30 03:50:44.872: INFO: Created: latency-svc-hdqhp
Dec 30 03:50:44.876: INFO: Got endpoints: latency-svc-hdqhp [73.877761ms]
Dec 30 03:50:44.877: INFO: Created: latency-svc-bksc2
Dec 30 03:50:44.880: INFO: Got endpoints: latency-svc-bksc2 [74.288608ms]
Dec 30 03:50:44.882: INFO: Created: latency-svc-xf74v
Dec 30 03:50:44.885: INFO: Created: latency-svc-z2h9m
Dec 30 03:50:44.885: INFO: Got endpoints: latency-svc-xf74v [73.313901ms]
Dec 30 03:50:44.890: INFO: Created: latency-svc-jnkmg
Dec 30 03:50:44.895: INFO: Created: latency-svc-zcjg7
Dec 30 03:50:44.898: INFO: Created: latency-svc-qnzlg
Dec 30 03:50:44.904: INFO: Created: latency-svc-7gv58
Dec 30 03:50:44.909: INFO: Created: latency-svc-zdw64
Dec 30 03:50:44.914: INFO: Created: latency-svc-jzr7x
Dec 30 03:50:44.918: INFO: Created: latency-svc-blqf8
Dec 30 03:50:44.922: INFO: Got endpoints: latency-svc-z2h9m [107.936468ms]
Dec 30 03:50:44.924: INFO: Created: latency-svc-9nhqd
Dec 30 03:50:44.929: INFO: Created: latency-svc-5xq7g
Dec 30 03:50:44.933: INFO: Created: latency-svc-bxdwm
Dec 30 03:50:44.938: INFO: Created: latency-svc-9h58j
Dec 30 03:50:44.942: INFO: Created: latency-svc-xrg2k
Dec 30 03:50:44.947: INFO: Created: latency-svc-sj547
Dec 30 03:50:44.952: INFO: Created: latency-svc-bxlvb
Dec 30 03:50:44.956: INFO: Created: latency-svc-r4j47
Dec 30 03:50:44.973: INFO: Got endpoints: latency-svc-jnkmg [153.252608ms]
Dec 30 03:50:44.983: INFO: Created: latency-svc-gw5hw
Dec 30 03:50:45.022: INFO: Got endpoints: latency-svc-zcjg7 [196.905155ms]
Dec 30 03:50:45.033: INFO: Created: latency-svc-tjkcf
Dec 30 03:50:45.073: INFO: Got endpoints: latency-svc-qnzlg [244.358336ms]
Dec 30 03:50:45.083: INFO: Created: latency-svc-f24nv
Dec 30 03:50:45.124: INFO: Got endpoints: latency-svc-7gv58 [289.861541ms]
Dec 30 03:50:45.135: INFO: Created: latency-svc-qglct
Dec 30 03:50:45.176: INFO: Got endpoints: latency-svc-zdw64 [336.46844ms]
Dec 30 03:50:45.186: INFO: Created: latency-svc-vs2rf
Dec 30 03:50:45.224: INFO: Got endpoints: latency-svc-jzr7x [379.989977ms]
Dec 30 03:50:45.234: INFO: Created: latency-svc-hbk74
Dec 30 03:50:45.273: INFO: Got endpoints: latency-svc-blqf8 [423.193909ms]
Dec 30 03:50:45.283: INFO: Created: latency-svc-n67r9
Dec 30 03:50:45.323: INFO: Got endpoints: latency-svc-9nhqd [467.376889ms]
Dec 30 03:50:45.334: INFO: Created: latency-svc-lr2lh
Dec 30 03:50:45.373: INFO: Got endpoints: latency-svc-5xq7g [508.140996ms]
Dec 30 03:50:45.382: INFO: Created: latency-svc-5nlv4
Dec 30 03:50:45.423: INFO: Got endpoints: latency-svc-bxdwm [557.045251ms]
Dec 30 03:50:45.433: INFO: Created: latency-svc-klwhk
Dec 30 03:50:45.472: INFO: Got endpoints: latency-svc-9h58j [601.005197ms]
Dec 30 03:50:45.483: INFO: Created: latency-svc-tsx5w
Dec 30 03:50:45.524: INFO: Got endpoints: latency-svc-xrg2k [647.954047ms]
Dec 30 03:50:45.534: INFO: Created: latency-svc-shtbr
Dec 30 03:50:45.573: INFO: Got endpoints: latency-svc-sj547 [692.775295ms]
Dec 30 03:50:45.583: INFO: Created: latency-svc-848pm
Dec 30 03:50:45.624: INFO: Got endpoints: latency-svc-bxlvb [738.089247ms]
Dec 30 03:50:45.635: INFO: Created: latency-svc-zk2gf
Dec 30 03:50:45.673: INFO: Got endpoints: latency-svc-r4j47 [750.556725ms]
Dec 30 03:50:45.682: INFO: Created: latency-svc-cn8p5
Dec 30 03:50:45.723: INFO: Got endpoints: latency-svc-gw5hw [749.635863ms]
Dec 30 03:50:45.733: INFO: Created: latency-svc-t8876
Dec 30 03:50:45.774: INFO: Got endpoints: latency-svc-tjkcf [751.060626ms]
Dec 30 03:50:45.783: INFO: Created: latency-svc-ftl5g
Dec 30 03:50:45.823: INFO: Got endpoints: latency-svc-f24nv [750.259737ms]
Dec 30 03:50:45.833: INFO: Created: latency-svc-7lw7q
Dec 30 03:50:45.873: INFO: Got endpoints: latency-svc-qglct [749.287174ms]
Dec 30 03:50:45.883: INFO: Created: latency-svc-28bwj
Dec 30 03:50:45.924: INFO: Got endpoints: latency-svc-vs2rf [747.657483ms]
Dec 30 03:50:45.934: INFO: Created: latency-svc-jh2hd
Dec 30 03:50:45.974: INFO: Got endpoints: latency-svc-hbk74 [750.297653ms]
Dec 30 03:50:45.984: INFO: Created: latency-svc-rp7r2
Dec 30 03:50:46.023: INFO: Got endpoints: latency-svc-n67r9 [749.586462ms]
Dec 30 03:50:46.034: INFO: Created: latency-svc-rkhpn
Dec 30 03:50:46.073: INFO: Got endpoints: latency-svc-lr2lh [750.267638ms]
Dec 30 03:50:46.084: INFO: Created: latency-svc-8cgrh
Dec 30 03:50:46.123: INFO: Got endpoints: latency-svc-5nlv4 [750.46263ms]
Dec 30 03:50:46.133: INFO: Created: latency-svc-bxm4d
Dec 30 03:50:46.173: INFO: Got endpoints: latency-svc-klwhk [750.286565ms]
Dec 30 03:50:46.183: INFO: Created: latency-svc-jqhck
Dec 30 03:50:46.223: INFO: Got endpoints: latency-svc-tsx5w [750.834119ms]
Dec 30 03:50:46.235: INFO: Created: latency-svc-thdxs
Dec 30 03:50:46.273: INFO: Got endpoints: latency-svc-shtbr [749.089211ms]
Dec 30 03:50:46.283: INFO: Created: latency-svc-txvs6
Dec 30 03:50:46.322: INFO: Got endpoints: latency-svc-848pm [749.091077ms]
Dec 30 03:50:46.332: INFO: Created: latency-svc-7nlgj
Dec 30 03:50:46.374: INFO: Got endpoints: latency-svc-zk2gf [750.068192ms]
Dec 30 03:50:46.384: INFO: Created: latency-svc-c6shp
Dec 30 03:50:46.423: INFO: Got endpoints: latency-svc-cn8p5 [749.735596ms]
Dec 30 03:50:46.432: INFO: Created: latency-svc-9wtbw
Dec 30 03:50:46.473: INFO: Got endpoints: latency-svc-t8876 [750.470746ms]
Dec 30 03:50:46.484: INFO: Created: latency-svc-2fz8l
Dec 30 03:50:46.523: INFO: Got endpoints: latency-svc-ftl5g [749.249175ms]
Dec 30 03:50:46.533: INFO: Created: latency-svc-nqkjr
Dec 30 03:50:46.573: INFO: Got endpoints: latency-svc-7lw7q [749.266077ms]
Dec 30 03:50:46.583: INFO: Created: latency-svc-v2xlp
Dec 30 03:50:46.623: INFO: Got endpoints: latency-svc-28bwj [750.167497ms]
Dec 30 03:50:46.633: INFO: Created: latency-svc-dz4sd
Dec 30 03:50:46.673: INFO: Got endpoints: latency-svc-jh2hd [749.701001ms]
Dec 30 03:50:46.684: INFO: Created: latency-svc-m5nkk
Dec 30 03:50:46.723: INFO: Got endpoints: latency-svc-rp7r2 [749.487136ms]
Dec 30 03:50:46.734: INFO: Created: latency-svc-svgk4
Dec 30 03:50:46.772: INFO: Got endpoints: latency-svc-rkhpn [749.512983ms]
Dec 30 03:50:46.787: INFO: Created: latency-svc-s54rv
Dec 30 03:50:46.826: INFO: Got endpoints: latency-svc-8cgrh [752.955459ms]
Dec 30 03:50:46.837: INFO: Created: latency-svc-nlcsb
Dec 30 03:50:46.873: INFO: Got endpoints: latency-svc-bxm4d [749.571481ms]
Dec 30 03:50:46.882: INFO: Created: latency-svc-6p4zp
Dec 30 03:50:46.923: INFO: Got endpoints: latency-svc-jqhck [749.914658ms]
Dec 30 03:50:46.933: INFO: Created: latency-svc-lt66k
Dec 30 03:50:46.974: INFO: Got endpoints: latency-svc-thdxs [750.176647ms]
Dec 30 03:50:46.986: INFO: Created: latency-svc-h74db
Dec 30 03:50:47.023: INFO: Got endpoints: latency-svc-txvs6 [750.002415ms]
Dec 30 03:50:47.033: INFO: Created: latency-svc-d59kj
Dec 30 03:50:47.074: INFO: Got endpoints: latency-svc-7nlgj [751.500239ms]
Dec 30 03:50:47.084: INFO: Created: latency-svc-v4vmb
Dec 30 03:50:47.123: INFO: Got endpoints: latency-svc-c6shp [749.016ms]
Dec 30 03:50:47.134: INFO: Created: latency-svc-rhvdk
Dec 30 03:50:47.174: INFO: Got endpoints: latency-svc-9wtbw [750.74754ms]
Dec 30 03:50:47.184: INFO: Created: latency-svc-rmlc9
Dec 30 03:50:47.223: INFO: Got endpoints: latency-svc-2fz8l [749.24816ms]
Dec 30 03:50:47.233: INFO: Created: latency-svc-jsz2g
Dec 30 03:50:47.274: INFO: Got endpoints: latency-svc-nqkjr [751.129495ms]
Dec 30 03:50:47.285: INFO: Created: latency-svc-977nk
Dec 30 03:50:47.323: INFO: Got endpoints: latency-svc-v2xlp [750.016309ms]
Dec 30 03:50:47.332: INFO: Created: latency-svc-dqr92
Dec 30 03:50:47.372: INFO: Got endpoints: latency-svc-dz4sd [749.029077ms]
Dec 30 03:50:47.383: INFO: Created: latency-svc-lrmsz
Dec 30 03:50:47.423: INFO: Got endpoints: latency-svc-m5nkk [749.838288ms]
Dec 30 03:50:47.434: INFO: Created: latency-svc-8gpqg
Dec 30 03:50:47.473: INFO: Got endpoints: latency-svc-svgk4 [749.745895ms]
Dec 30 03:50:47.482: INFO: Created: latency-svc-n9b2c
Dec 30 03:50:47.522: INFO: Got endpoints: latency-svc-s54rv [749.935605ms]
Dec 30 03:50:47.533: INFO: Created: latency-svc-gbm9g
Dec 30 03:50:47.572: INFO: Got endpoints: latency-svc-nlcsb [745.853632ms]
Dec 30 03:50:47.583: INFO: Created: latency-svc-gkcwf
Dec 30 03:50:47.623: INFO: Got endpoints: latency-svc-6p4zp [750.187261ms]
Dec 30 03:50:47.632: INFO: Created: latency-svc-v7vtb
Dec 30 03:50:47.672: INFO: Got endpoints: latency-svc-lt66k [748.831855ms]
Dec 30 03:50:47.687: INFO: Created: latency-svc-95djr
Dec 30 03:50:47.723: INFO: Got endpoints: latency-svc-h74db [749.115699ms]
Dec 30 03:50:47.734: INFO: Created: latency-svc-696rh
Dec 30 03:50:47.773: INFO: Got endpoints: latency-svc-d59kj [749.610645ms]
Dec 30 03:50:47.783: INFO: Created: latency-svc-rth8z
Dec 30 03:50:47.822: INFO: Got endpoints: latency-svc-v4vmb [748.678886ms]
Dec 30 03:50:47.831: INFO: Created: latency-svc-9mxbr
Dec 30 03:50:47.873: INFO: Got endpoints: latency-svc-rhvdk [749.806229ms]
Dec 30 03:50:47.884: INFO: Created: latency-svc-wvwjd
Dec 30 03:50:47.923: INFO: Got endpoints: latency-svc-rmlc9 [749.569064ms]
Dec 30 03:50:47.933: INFO: Created: latency-svc-d2x94
Dec 30 03:50:47.973: INFO: Got endpoints: latency-svc-jsz2g [750.845232ms]
Dec 30 03:50:47.983: INFO: Created: latency-svc-vpgn5
Dec 30 03:50:48.023: INFO: Got endpoints: latency-svc-977nk [748.605139ms]
Dec 30 03:50:48.034: INFO: Created: latency-svc-4hqqg
Dec 30 03:50:48.074: INFO: Got endpoints: latency-svc-dqr92 [750.895629ms]
Dec 30 03:50:48.084: INFO: Created: latency-svc-xj499
Dec 30 03:50:48.124: INFO: Got endpoints: latency-svc-lrmsz [751.673506ms]
Dec 30 03:50:48.133: INFO: Created: latency-svc-ht4d9
Dec 30 03:50:48.173: INFO: Got endpoints: latency-svc-8gpqg [749.843038ms]
Dec 30 03:50:48.185: INFO: Created: latency-svc-gl6xs
Dec 30 03:50:48.223: INFO: Got endpoints: latency-svc-n9b2c [750.197812ms]
Dec 30 03:50:48.233: INFO: Created: latency-svc-bfthx
Dec 30 03:50:48.273: INFO: Got endpoints: latency-svc-gbm9g [750.891744ms]
Dec 30 03:50:48.283: INFO: Created: latency-svc-pggmg
Dec 30 03:50:48.323: INFO: Got endpoints: latency-svc-gkcwf [750.274986ms]
Dec 30 03:50:48.333: INFO: Created: latency-svc-9bfbf
Dec 30 03:50:48.373: INFO: Got endpoints: latency-svc-v7vtb [750.441512ms]
Dec 30 03:50:48.384: INFO: Created: latency-svc-5fttk
Dec 30 03:50:48.423: INFO: Got endpoints: latency-svc-95djr [750.613741ms]
Dec 30 03:50:48.432: INFO: Created: latency-svc-jwpcv
Dec 30 03:50:48.474: INFO: Got endpoints: latency-svc-696rh [750.87574ms]
Dec 30 03:50:48.484: INFO: Created: latency-svc-cs7r4
Dec 30 03:50:48.522: INFO: Got endpoints: latency-svc-rth8z [749.286501ms]
Dec 30 03:50:48.533: INFO: Created: latency-svc-nwctp
Dec 30 03:50:48.574: INFO: Got endpoints: latency-svc-9mxbr [751.232198ms]
Dec 30 03:50:48.583: INFO: Created: latency-svc-psvsf
Dec 30 03:50:48.624: INFO: Got endpoints: latency-svc-wvwjd [751.062918ms]
Dec 30 03:50:48.635: INFO: Created: latency-svc-4c9dl
Dec 30 03:50:48.674: INFO: Got endpoints: latency-svc-d2x94 [750.504406ms]
Dec 30 03:50:48.685: INFO: Created: latency-svc-zmpvg
Dec 30 03:50:48.723: INFO: Got endpoints: latency-svc-vpgn5 [749.061545ms]
Dec 30 03:50:48.732: INFO: Created: latency-svc-skg9p
Dec 30 03:50:48.773: INFO: Got endpoints: latency-svc-4hqqg [750.378351ms]
Dec 30 03:50:48.784: INFO: Created: latency-svc-77b5d
Dec 30 03:50:48.823: INFO: Got endpoints: latency-svc-xj499 [749.536052ms]
Dec 30 03:50:48.834: INFO: Created: latency-svc-t8tbq
Dec 30 03:50:48.874: INFO: Got endpoints: latency-svc-ht4d9 [749.735607ms]
Dec 30 03:50:48.883: INFO: Created: latency-svc-gl6sp
Dec 30 03:50:48.923: INFO: Got endpoints: latency-svc-gl6xs [749.84304ms]
Dec 30 03:50:48.939: INFO: Created: latency-svc-sqntq
Dec 30 03:50:48.973: INFO: Got endpoints: latency-svc-bfthx [749.783553ms]
Dec 30 03:50:48.983: INFO: Created: latency-svc-kjd6p
Dec 30 03:50:49.022: INFO: Got endpoints: latency-svc-pggmg [748.977431ms]
Dec 30 03:50:49.032: INFO: Created: latency-svc-8qf7d
Dec 30 03:50:49.073: INFO: Got endpoints: latency-svc-9bfbf [750.052855ms]
Dec 30 03:50:49.085: INFO: Created: latency-svc-v4jh7
Dec 30 03:50:49.124: INFO: Got endpoints: latency-svc-5fttk [750.755457ms]
Dec 30 03:50:49.134: INFO: Created: latency-svc-l9nzz
Dec 30 03:50:49.180: INFO: Got endpoints: latency-svc-jwpcv [757.053373ms]
Dec 30 03:50:49.191: INFO: Created: latency-svc-tq9q7
Dec 30 03:50:49.222: INFO: Got endpoints: latency-svc-cs7r4 [748.427126ms]
Dec 30 03:50:49.233: INFO: Created: latency-svc-jj96t
Dec 30 03:50:49.274: INFO: Got endpoints: latency-svc-nwctp [751.649963ms]
Dec 30 03:50:49.284: INFO: Created: latency-svc-kf8mb
Dec 30 03:50:49.323: INFO: Got endpoints: latency-svc-psvsf [749.063746ms]
Dec 30 03:50:49.332: INFO: Created: latency-svc-msl72
Dec 30 03:50:49.373: INFO: Got endpoints: latency-svc-4c9dl [749.299803ms]
Dec 30 03:50:49.384: INFO: Created: latency-svc-5zzz9
Dec 30 03:50:49.423: INFO: Got endpoints: latency-svc-zmpvg [749.273666ms]
Dec 30 03:50:49.433: INFO: Created: latency-svc-qx7v6
Dec 30 03:50:49.474: INFO: Got endpoints: latency-svc-skg9p [750.966406ms]
Dec 30 03:50:49.484: INFO: Created: latency-svc-27ctp
Dec 30 03:50:49.524: INFO: Got endpoints: latency-svc-77b5d [750.631498ms]
Dec 30 03:50:49.535: INFO: Created: latency-svc-jk6sb
Dec 30 03:50:49.573: INFO: Got endpoints: latency-svc-t8tbq [749.802812ms]
Dec 30 03:50:49.582: INFO: Created: latency-svc-2fvp4
Dec 30 03:50:49.624: INFO: Got endpoints: latency-svc-gl6sp [749.774692ms]
Dec 30 03:50:49.634: INFO: Created: latency-svc-wf5gf
Dec 30 03:50:49.673: INFO: Got endpoints: latency-svc-sqntq [749.993408ms]
Dec 30 03:50:49.684: INFO: Created: latency-svc-7jvph
Dec 30 03:50:49.723: INFO: Got endpoints: latency-svc-kjd6p [749.489091ms]
Dec 30 03:50:49.732: INFO: Created: latency-svc-fwt7v
Dec 30 03:50:49.773: INFO: Got endpoints: latency-svc-8qf7d [750.350173ms]
Dec 30 03:50:49.783: INFO: Created: latency-svc-6zfgw
Dec 30 03:50:49.824: INFO: Got endpoints: latency-svc-v4jh7 [750.812584ms]
Dec 30 03:50:49.834: INFO: Created: latency-svc-kfct9
Dec 30 03:50:49.873: INFO: Got endpoints: latency-svc-l9nzz [748.522422ms]
Dec 30 03:50:49.882: INFO: Created: latency-svc-hzvrb
Dec 30 03:50:49.922: INFO: Got endpoints: latency-svc-tq9q7 [742.327028ms]
Dec 30 03:50:49.932: INFO: Created: latency-svc-r8w4n
Dec 30 03:50:49.973: INFO: Got endpoints: latency-svc-jj96t [751.273893ms]
Dec 30 03:50:49.984: INFO: Created: latency-svc-9xrgv
Dec 30 03:50:50.024: INFO: Got endpoints: latency-svc-kf8mb [750.192445ms]
Dec 30 03:50:50.034: INFO: Created: latency-svc-7wlqf
Dec 30 03:50:50.073: INFO: Got endpoints: latency-svc-msl72 [750.311703ms]
Dec 30 03:50:50.083: INFO: Created: latency-svc-hfpcv
Dec 30 03:50:50.124: INFO: Got endpoints: latency-svc-5zzz9 [750.735351ms]
Dec 30 03:50:50.134: INFO: Created: latency-svc-5vj84
Dec 30 03:50:50.174: INFO: Got endpoints: latency-svc-qx7v6 [750.729008ms]
Dec 30 03:50:50.184: INFO: Created: latency-svc-lvb8r
Dec 30 03:50:50.223: INFO: Got endpoints: latency-svc-27ctp [749.090123ms]
Dec 30 03:50:50.234: INFO: Created: latency-svc-d8lpk
Dec 30 03:50:50.273: INFO: Got endpoints: latency-svc-jk6sb [749.122779ms]
Dec 30 03:50:50.284: INFO: Created: latency-svc-qkwjb
Dec 30 03:50:50.323: INFO: Got endpoints: latency-svc-2fvp4 [750.169098ms]
Dec 30 03:50:50.333: INFO: Created: latency-svc-4gdxj
Dec 30 03:50:50.372: INFO: Got endpoints: latency-svc-wf5gf [748.644817ms]
Dec 30 03:50:50.382: INFO: Created: latency-svc-7492z
Dec 30 03:50:50.424: INFO: Got endpoints: latency-svc-7jvph [750.591886ms]
Dec 30 03:50:50.434: INFO: Created: latency-svc-bc2w6
Dec 30 03:50:50.473: INFO: Got endpoints: latency-svc-fwt7v [750.159381ms]
Dec 30 03:50:50.483: INFO: Created: latency-svc-xj4bj
Dec 30 03:50:50.522: INFO: Got endpoints: latency-svc-6zfgw [749.667838ms]
Dec 30 03:50:50.533: INFO: Created: latency-svc-x4xn2
Dec 30 03:50:50.574: INFO: Got endpoints: latency-svc-kfct9 [750.242244ms]
Dec 30 03:50:50.584: INFO: Created: latency-svc-htqwn
Dec 30 03:50:50.623: INFO: Got endpoints: latency-svc-hzvrb [749.698395ms]
Dec 30 03:50:50.632: INFO: Created: latency-svc-5jwf2
Dec 30 03:50:50.673: INFO: Got endpoints: latency-svc-r8w4n [750.858814ms]
Dec 30 03:50:50.683: INFO: Created: latency-svc-wflkr
Dec 30 03:50:50.722: INFO: Got endpoints: latency-svc-9xrgv [748.816846ms]
Dec 30 03:50:50.732: INFO: Created: latency-svc-fzj6j
Dec 30 03:50:50.773: INFO: Got endpoints: latency-svc-7wlqf [749.191902ms]
Dec 30 03:50:50.783: INFO: Created: latency-svc-7nt6w
Dec 30 03:50:50.823: INFO: Got endpoints: latency-svc-hfpcv [749.768965ms]
Dec 30 03:50:50.833: INFO: Created: latency-svc-szs24
Dec 30 03:50:50.873: INFO: Got endpoints: latency-svc-5vj84 [749.494579ms]
Dec 30 03:50:50.884: INFO: Created: latency-svc-hgqwf
Dec 30 03:50:50.923: INFO: Got endpoints: latency-svc-lvb8r [748.60788ms]
Dec 30 03:50:50.933: INFO: Created: latency-svc-lr926
Dec 30 03:50:50.974: INFO: Got endpoints: latency-svc-d8lpk [750.827168ms]
Dec 30 03:50:50.984: INFO: Created: latency-svc-5v5nq
Dec 30 03:50:51.024: INFO: Got endpoints: latency-svc-qkwjb [750.860598ms]
Dec 30 03:50:51.034: INFO: Created: latency-svc-bdkrs
Dec 30 03:50:51.072: INFO: Got endpoints: latency-svc-4gdxj [749.055742ms]
Dec 30 03:50:51.082: INFO: Created: latency-svc-pscz2
Dec 30 03:50:51.123: INFO: Got endpoints: latency-svc-7492z [750.301512ms]
Dec 30 03:50:51.132: INFO: Created: latency-svc-h7t5k
Dec 30 03:50:51.173: INFO: Got endpoints: latency-svc-bc2w6 [748.739704ms]
Dec 30 03:50:51.183: INFO: Created: latency-svc-tv68j
Dec 30 03:50:51.223: INFO: Got endpoints: latency-svc-xj4bj [749.580934ms]
Dec 30 03:50:51.232: INFO: Created: latency-svc-xd9t2
Dec 30 03:50:51.272: INFO: Got endpoints: latency-svc-x4xn2 [749.725762ms]
Dec 30 03:50:51.282: INFO: Created: latency-svc-vv8jk
Dec 30 03:50:51.323: INFO: Got endpoints: latency-svc-htqwn [749.095438ms]
Dec 30 03:50:51.334: INFO: Created: latency-svc-5q2h9
Dec 30 03:50:51.374: INFO: Got endpoints: latency-svc-5jwf2 [751.18799ms]
Dec 30 03:50:51.384: INFO: Created: latency-svc-c24nd
Dec 30 03:50:51.422: INFO: Got endpoints: latency-svc-wflkr [748.987125ms]
Dec 30 03:50:51.433: INFO: Created: latency-svc-mbj58
Dec 30 03:50:51.473: INFO: Got endpoints: latency-svc-fzj6j [750.990479ms]
Dec 30 03:50:51.484: INFO: Created: latency-svc-6fj5t
Dec 30 03:50:51.524: INFO: Got endpoints: latency-svc-7nt6w [750.244313ms]
Dec 30 03:50:51.534: INFO: Created: latency-svc-b56q4
Dec 30 03:50:51.573: INFO: Got endpoints: latency-svc-szs24 [750.085274ms]
Dec 30 03:50:51.584: INFO: Created: latency-svc-jcvkc
Dec 30 03:50:51.622: INFO: Got endpoints: latency-svc-hgqwf [749.055234ms]
Dec 30 03:50:51.633: INFO: Created: latency-svc-h9sd6
Dec 30 03:50:51.674: INFO: Got endpoints: latency-svc-lr926 [751.275685ms]
Dec 30 03:50:51.684: INFO: Created: latency-svc-klbv5
Dec 30 03:50:51.723: INFO: Got endpoints: latency-svc-5v5nq [749.066084ms]
Dec 30 03:50:51.737: INFO: Created: latency-svc-w6vhm
Dec 30 03:50:51.774: INFO: Got endpoints: latency-svc-bdkrs [750.024869ms]
Dec 30 03:50:51.785: INFO: Created: latency-svc-dpccp
Dec 30 03:50:51.823: INFO: Got endpoints: latency-svc-pscz2 [750.65814ms]
Dec 30 03:50:51.833: INFO: Created: latency-svc-b8qzm
Dec 30 03:50:51.873: INFO: Got endpoints: latency-svc-h7t5k [750.281214ms]
Dec 30 03:50:51.883: INFO: Created: latency-svc-l8qth
Dec 30 03:50:51.923: INFO: Got endpoints: latency-svc-tv68j [750.461004ms]
Dec 30 03:50:51.934: INFO: Created: latency-svc-ljp89
Dec 30 03:50:51.973: INFO: Got endpoints: latency-svc-xd9t2 [750.200044ms]
Dec 30 03:50:51.982: INFO: Created: latency-svc-qh8sc
Dec 30 03:50:52.023: INFO: Got endpoints: latency-svc-vv8jk [750.809673ms]
Dec 30 03:50:52.034: INFO: Created: latency-svc-cbpbh
Dec 30 03:50:52.074: INFO: Got endpoints: latency-svc-5q2h9 [750.449816ms]
Dec 30 03:50:52.085: INFO: Created: latency-svc-97ntv
Dec 30 03:50:52.123: INFO: Got endpoints: latency-svc-c24nd [749.299301ms]
Dec 30 03:50:52.133: INFO: Created: latency-svc-hz6s2
Dec 30 03:50:52.173: INFO: Got endpoints: latency-svc-mbj58 [751.213825ms]
Dec 30 03:50:52.184: INFO: Created: latency-svc-9mtw4
Dec 30 03:50:52.224: INFO: Got endpoints: latency-svc-6fj5t [750.174737ms]
Dec 30 03:50:52.235: INFO: Created: latency-svc-8mxm5
Dec 30 03:50:52.273: INFO: Got endpoints: latency-svc-b56q4 [749.338481ms]
Dec 30 03:50:52.283: INFO: Created: latency-svc-lcjq4
Dec 30 03:50:52.323: INFO: Got endpoints: latency-svc-jcvkc [750.27186ms]
Dec 30 03:50:52.333: INFO: Created: latency-svc-jkbbg
Dec 30 03:50:52.373: INFO: Got endpoints: latency-svc-h9sd6 [750.770807ms]
Dec 30 03:50:52.384: INFO: Created: latency-svc-8vs4m
Dec 30 03:50:52.423: INFO: Got endpoints: latency-svc-klbv5 [749.340877ms]
Dec 30 03:50:52.433: INFO: Created: latency-svc-dnvmh
Dec 30 03:50:52.473: INFO: Got endpoints: latency-svc-w6vhm [750.127769ms]
Dec 30 03:50:52.483: INFO: Created: latency-svc-bgxrb
Dec 30 03:50:52.524: INFO: Got endpoints: latency-svc-dpccp [750.140395ms]
Dec 30 03:50:52.535: INFO: Created: latency-svc-2wmk5
Dec 30 03:50:52.573: INFO: Got endpoints: latency-svc-b8qzm [750.146868ms]
Dec 30 03:50:52.623: INFO: Got endpoints: latency-svc-l8qth [749.811432ms]
Dec 30 03:50:52.673: INFO: Got endpoints: latency-svc-ljp89 [749.741468ms]
Dec 30 03:50:52.724: INFO: Got endpoints: latency-svc-qh8sc [751.285208ms]
Dec 30 03:50:52.773: INFO: Got endpoints: latency-svc-cbpbh [749.745831ms]
Dec 30 03:50:52.823: INFO: Got endpoints: latency-svc-97ntv [749.538305ms]
Dec 30 03:50:52.873: INFO: Got endpoints: latency-svc-hz6s2 [750.114993ms]
Dec 30 03:50:52.922: INFO: Got endpoints: latency-svc-9mtw4 [748.273445ms]
Dec 30 03:50:52.973: INFO: Got endpoints: latency-svc-8mxm5 [749.695097ms]
Dec 30 03:50:53.023: INFO: Got endpoints: latency-svc-lcjq4 [749.823458ms]
Dec 30 03:50:53.072: INFO: Got endpoints: latency-svc-jkbbg [748.655828ms]
Dec 30 03:50:53.123: INFO: Got endpoints: latency-svc-8vs4m [750.07262ms]
Dec 30 03:50:53.180: INFO: Got endpoints: latency-svc-dnvmh [756.886684ms]
Dec 30 03:50:53.223: INFO: Got endpoints: latency-svc-bgxrb [749.986573ms]
Dec 30 03:50:53.274: INFO: Got endpoints: latency-svc-2wmk5 [749.848048ms]
Dec 30 03:50:53.274: INFO: Latencies: [14.950894ms 18.999534ms 22.985301ms 27.548646ms 32.748194ms 37.166482ms 42.916667ms 46.762007ms 50.426888ms 55.351882ms 59.714248ms 64.939265ms 67.372518ms 67.457199ms 67.871511ms 68.069121ms 68.672974ms 69.125969ms 69.174825ms 69.219898ms 69.525712ms 69.806042ms 70.815123ms 71.035771ms 72.599105ms 72.944779ms 73.313901ms 73.877761ms 74.288608ms 75.023933ms 75.777375ms 77.274292ms 107.936468ms 153.252608ms 196.905155ms 244.358336ms 289.861541ms 336.46844ms 379.989977ms 423.193909ms 467.376889ms 508.140996ms 557.045251ms 601.005197ms 647.954047ms 692.775295ms 738.089247ms 742.327028ms 745.853632ms 747.657483ms 748.273445ms 748.427126ms 748.522422ms 748.605139ms 748.60788ms 748.644817ms 748.655828ms 748.678886ms 748.739704ms 748.816846ms 748.831855ms 748.977431ms 748.987125ms 749.016ms 749.029077ms 749.055234ms 749.055742ms 749.061545ms 749.063746ms 749.066084ms 749.089211ms 749.090123ms 749.091077ms 749.095438ms 749.115699ms 749.122779ms 749.191902ms 749.24816ms 749.249175ms 749.266077ms 749.273666ms 749.286501ms 749.287174ms 749.299301ms 749.299803ms 749.338481ms 749.340877ms 749.487136ms 749.489091ms 749.494579ms 749.512983ms 749.536052ms 749.538305ms 749.569064ms 749.571481ms 749.580934ms 749.586462ms 749.610645ms 749.635863ms 749.667838ms 749.695097ms 749.698395ms 749.701001ms 749.725762ms 749.735596ms 749.735607ms 749.741468ms 749.745831ms 749.745895ms 749.768965ms 749.774692ms 749.783553ms 749.802812ms 749.806229ms 749.811432ms 749.823458ms 749.838288ms 749.843038ms 749.84304ms 749.848048ms 749.914658ms 749.935605ms 749.986573ms 749.993408ms 750.002415ms 750.016309ms 750.024869ms 750.052855ms 750.068192ms 750.07262ms 750.085274ms 750.114993ms 750.127769ms 750.140395ms 750.146868ms 750.159381ms 750.167497ms 750.169098ms 750.174737ms 750.176647ms 750.187261ms 750.192445ms 750.197812ms 750.200044ms 750.242244ms 750.244313ms 750.259737ms 750.267638ms 750.27186ms 750.274986ms 750.281214ms 750.286565ms 750.297653ms 750.301512ms 750.311703ms 750.350173ms 750.378351ms 750.441512ms 750.449816ms 750.461004ms 750.46263ms 750.470746ms 750.504406ms 750.556725ms 750.591886ms 750.613741ms 750.631498ms 750.65814ms 750.729008ms 750.735351ms 750.74754ms 750.755457ms 750.770807ms 750.809673ms 750.812584ms 750.827168ms 750.834119ms 750.845232ms 750.858814ms 750.860598ms 750.87574ms 750.891744ms 750.895629ms 750.966406ms 750.990479ms 751.060626ms 751.062918ms 751.129495ms 751.18799ms 751.213825ms 751.232198ms 751.273893ms 751.275685ms 751.285208ms 751.500239ms 751.649963ms 751.673506ms 752.955459ms 756.886684ms 757.053373ms]
Dec 30 03:50:53.274: INFO: 50 %ile: 749.695097ms
Dec 30 03:50:53.274: INFO: 90 %ile: 750.87574ms
Dec 30 03:50:53.274: INFO: 99 %ile: 756.886684ms
Dec 30 03:50:53.274: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:187
Dec 30 03:50:53.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-4175" for this suite. 12/30/22 03:50:53.284
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","completed":128,"skipped":2332,"failed":0}
------------------------------
• [SLOW TEST] [10.768 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:50:42.524
    Dec 30 03:50:42.524: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename svc-latency 12/30/22 03:50:42.526
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:50:42.539
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:50:42.542
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    Dec 30 03:50:42.546: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-4175 12/30/22 03:50:42.547
    I1230 03:50:42.554425      25 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-4175, replica count: 1
    I1230 03:50:43.606474      25 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I1230 03:50:44.607068      25 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Dec 30 03:50:44.718: INFO: Created: latency-svc-szbb7
    Dec 30 03:50:44.723: INFO: Got endpoints: latency-svc-szbb7 [16.298859ms]
    Dec 30 03:50:44.735: INFO: Created: latency-svc-mvttq
    Dec 30 03:50:44.738: INFO: Got endpoints: latency-svc-mvttq [14.950894ms]
    Dec 30 03:50:44.739: INFO: Created: latency-svc-4bj7n
    Dec 30 03:50:44.742: INFO: Got endpoints: latency-svc-4bj7n [18.999534ms]
    Dec 30 03:50:44.743: INFO: Created: latency-svc-x6vt8
    Dec 30 03:50:44.746: INFO: Got endpoints: latency-svc-x6vt8 [22.985301ms]
    Dec 30 03:50:44.748: INFO: Created: latency-svc-m4wxg
    Dec 30 03:50:44.751: INFO: Got endpoints: latency-svc-m4wxg [27.548646ms]
    Dec 30 03:50:44.753: INFO: Created: latency-svc-t62sf
    Dec 30 03:50:44.756: INFO: Got endpoints: latency-svc-t62sf [32.748194ms]
    Dec 30 03:50:44.757: INFO: Created: latency-svc-wmfj9
    Dec 30 03:50:44.761: INFO: Got endpoints: latency-svc-wmfj9 [37.166482ms]
    Dec 30 03:50:44.763: INFO: Created: latency-svc-nqswz
    Dec 30 03:50:44.766: INFO: Got endpoints: latency-svc-nqswz [42.916667ms]
    Dec 30 03:50:44.767: INFO: Created: latency-svc-zmjhh
    Dec 30 03:50:44.770: INFO: Got endpoints: latency-svc-zmjhh [46.762007ms]
    Dec 30 03:50:44.771: INFO: Created: latency-svc-rnhtv
    Dec 30 03:50:44.774: INFO: Got endpoints: latency-svc-rnhtv [50.426888ms]
    Dec 30 03:50:44.776: INFO: Created: latency-svc-swhxq
    Dec 30 03:50:44.779: INFO: Got endpoints: latency-svc-swhxq [55.351882ms]
    Dec 30 03:50:44.781: INFO: Created: latency-svc-l9mpx
    Dec 30 03:50:44.784: INFO: Got endpoints: latency-svc-l9mpx [59.714248ms]
    Dec 30 03:50:44.785: INFO: Created: latency-svc-wvxnk
    Dec 30 03:50:44.789: INFO: Got endpoints: latency-svc-wvxnk [64.939265ms]
    Dec 30 03:50:44.790: INFO: Created: latency-svc-xbllj
    Dec 30 03:50:44.793: INFO: Got endpoints: latency-svc-xbllj [69.174825ms]
    Dec 30 03:50:44.795: INFO: Created: latency-svc-hhgl7
    Dec 30 03:50:44.798: INFO: Created: latency-svc-m7j9p
    Dec 30 03:50:44.799: INFO: Got endpoints: latency-svc-hhgl7 [75.023933ms]
    Dec 30 03:50:44.802: INFO: Got endpoints: latency-svc-m7j9p [77.274292ms]
    Dec 30 03:50:44.803: INFO: Created: latency-svc-lbnx6
    Dec 30 03:50:44.806: INFO: Got endpoints: latency-svc-lbnx6 [67.457199ms]
    Dec 30 03:50:44.808: INFO: Created: latency-svc-lqbkj
    Dec 30 03:50:44.812: INFO: Got endpoints: latency-svc-lqbkj [69.806042ms]
    Dec 30 03:50:44.812: INFO: Created: latency-svc-dpcdh
    Dec 30 03:50:44.814: INFO: Got endpoints: latency-svc-dpcdh [68.069121ms]
    Dec 30 03:50:44.817: INFO: Created: latency-svc-fw8fr
    Dec 30 03:50:44.820: INFO: Got endpoints: latency-svc-fw8fr [68.672974ms]
    Dec 30 03:50:44.822: INFO: Created: latency-svc-cttvk
    Dec 30 03:50:44.825: INFO: Created: latency-svc-lkdff
    Dec 30 03:50:44.825: INFO: Got endpoints: latency-svc-cttvk [69.125969ms]
    Dec 30 03:50:44.829: INFO: Got endpoints: latency-svc-lkdff [67.871511ms]
    Dec 30 03:50:44.830: INFO: Created: latency-svc-jllgt
    Dec 30 03:50:44.834: INFO: Got endpoints: latency-svc-jllgt [67.372518ms]
    Dec 30 03:50:44.835: INFO: Created: latency-svc-28f6w
    Dec 30 03:50:44.839: INFO: Got endpoints: latency-svc-28f6w [69.219898ms]
    Dec 30 03:50:44.840: INFO: Created: latency-svc-cvlr2
    Dec 30 03:50:44.843: INFO: Got endpoints: latency-svc-cvlr2 [69.525712ms]
    Dec 30 03:50:44.845: INFO: Created: latency-svc-cnw2x
    Dec 30 03:50:44.850: INFO: Got endpoints: latency-svc-cnw2x [70.815123ms]
    Dec 30 03:50:44.850: INFO: Created: latency-svc-rg8w4
    Dec 30 03:50:44.856: INFO: Got endpoints: latency-svc-rg8w4 [71.035771ms]
    Dec 30 03:50:44.859: INFO: Created: latency-svc-cvqtv
    Dec 30 03:50:44.863: INFO: Created: latency-svc-6gpcb
    Dec 30 03:50:44.864: INFO: Got endpoints: latency-svc-cvqtv [75.777375ms]
    Dec 30 03:50:44.866: INFO: Got endpoints: latency-svc-6gpcb [72.944779ms]
    Dec 30 03:50:44.868: INFO: Created: latency-svc-mlmww
    Dec 30 03:50:44.871: INFO: Got endpoints: latency-svc-mlmww [72.599105ms]
    Dec 30 03:50:44.872: INFO: Created: latency-svc-hdqhp
    Dec 30 03:50:44.876: INFO: Got endpoints: latency-svc-hdqhp [73.877761ms]
    Dec 30 03:50:44.877: INFO: Created: latency-svc-bksc2
    Dec 30 03:50:44.880: INFO: Got endpoints: latency-svc-bksc2 [74.288608ms]
    Dec 30 03:50:44.882: INFO: Created: latency-svc-xf74v
    Dec 30 03:50:44.885: INFO: Created: latency-svc-z2h9m
    Dec 30 03:50:44.885: INFO: Got endpoints: latency-svc-xf74v [73.313901ms]
    Dec 30 03:50:44.890: INFO: Created: latency-svc-jnkmg
    Dec 30 03:50:44.895: INFO: Created: latency-svc-zcjg7
    Dec 30 03:50:44.898: INFO: Created: latency-svc-qnzlg
    Dec 30 03:50:44.904: INFO: Created: latency-svc-7gv58
    Dec 30 03:50:44.909: INFO: Created: latency-svc-zdw64
    Dec 30 03:50:44.914: INFO: Created: latency-svc-jzr7x
    Dec 30 03:50:44.918: INFO: Created: latency-svc-blqf8
    Dec 30 03:50:44.922: INFO: Got endpoints: latency-svc-z2h9m [107.936468ms]
    Dec 30 03:50:44.924: INFO: Created: latency-svc-9nhqd
    Dec 30 03:50:44.929: INFO: Created: latency-svc-5xq7g
    Dec 30 03:50:44.933: INFO: Created: latency-svc-bxdwm
    Dec 30 03:50:44.938: INFO: Created: latency-svc-9h58j
    Dec 30 03:50:44.942: INFO: Created: latency-svc-xrg2k
    Dec 30 03:50:44.947: INFO: Created: latency-svc-sj547
    Dec 30 03:50:44.952: INFO: Created: latency-svc-bxlvb
    Dec 30 03:50:44.956: INFO: Created: latency-svc-r4j47
    Dec 30 03:50:44.973: INFO: Got endpoints: latency-svc-jnkmg [153.252608ms]
    Dec 30 03:50:44.983: INFO: Created: latency-svc-gw5hw
    Dec 30 03:50:45.022: INFO: Got endpoints: latency-svc-zcjg7 [196.905155ms]
    Dec 30 03:50:45.033: INFO: Created: latency-svc-tjkcf
    Dec 30 03:50:45.073: INFO: Got endpoints: latency-svc-qnzlg [244.358336ms]
    Dec 30 03:50:45.083: INFO: Created: latency-svc-f24nv
    Dec 30 03:50:45.124: INFO: Got endpoints: latency-svc-7gv58 [289.861541ms]
    Dec 30 03:50:45.135: INFO: Created: latency-svc-qglct
    Dec 30 03:50:45.176: INFO: Got endpoints: latency-svc-zdw64 [336.46844ms]
    Dec 30 03:50:45.186: INFO: Created: latency-svc-vs2rf
    Dec 30 03:50:45.224: INFO: Got endpoints: latency-svc-jzr7x [379.989977ms]
    Dec 30 03:50:45.234: INFO: Created: latency-svc-hbk74
    Dec 30 03:50:45.273: INFO: Got endpoints: latency-svc-blqf8 [423.193909ms]
    Dec 30 03:50:45.283: INFO: Created: latency-svc-n67r9
    Dec 30 03:50:45.323: INFO: Got endpoints: latency-svc-9nhqd [467.376889ms]
    Dec 30 03:50:45.334: INFO: Created: latency-svc-lr2lh
    Dec 30 03:50:45.373: INFO: Got endpoints: latency-svc-5xq7g [508.140996ms]
    Dec 30 03:50:45.382: INFO: Created: latency-svc-5nlv4
    Dec 30 03:50:45.423: INFO: Got endpoints: latency-svc-bxdwm [557.045251ms]
    Dec 30 03:50:45.433: INFO: Created: latency-svc-klwhk
    Dec 30 03:50:45.472: INFO: Got endpoints: latency-svc-9h58j [601.005197ms]
    Dec 30 03:50:45.483: INFO: Created: latency-svc-tsx5w
    Dec 30 03:50:45.524: INFO: Got endpoints: latency-svc-xrg2k [647.954047ms]
    Dec 30 03:50:45.534: INFO: Created: latency-svc-shtbr
    Dec 30 03:50:45.573: INFO: Got endpoints: latency-svc-sj547 [692.775295ms]
    Dec 30 03:50:45.583: INFO: Created: latency-svc-848pm
    Dec 30 03:50:45.624: INFO: Got endpoints: latency-svc-bxlvb [738.089247ms]
    Dec 30 03:50:45.635: INFO: Created: latency-svc-zk2gf
    Dec 30 03:50:45.673: INFO: Got endpoints: latency-svc-r4j47 [750.556725ms]
    Dec 30 03:50:45.682: INFO: Created: latency-svc-cn8p5
    Dec 30 03:50:45.723: INFO: Got endpoints: latency-svc-gw5hw [749.635863ms]
    Dec 30 03:50:45.733: INFO: Created: latency-svc-t8876
    Dec 30 03:50:45.774: INFO: Got endpoints: latency-svc-tjkcf [751.060626ms]
    Dec 30 03:50:45.783: INFO: Created: latency-svc-ftl5g
    Dec 30 03:50:45.823: INFO: Got endpoints: latency-svc-f24nv [750.259737ms]
    Dec 30 03:50:45.833: INFO: Created: latency-svc-7lw7q
    Dec 30 03:50:45.873: INFO: Got endpoints: latency-svc-qglct [749.287174ms]
    Dec 30 03:50:45.883: INFO: Created: latency-svc-28bwj
    Dec 30 03:50:45.924: INFO: Got endpoints: latency-svc-vs2rf [747.657483ms]
    Dec 30 03:50:45.934: INFO: Created: latency-svc-jh2hd
    Dec 30 03:50:45.974: INFO: Got endpoints: latency-svc-hbk74 [750.297653ms]
    Dec 30 03:50:45.984: INFO: Created: latency-svc-rp7r2
    Dec 30 03:50:46.023: INFO: Got endpoints: latency-svc-n67r9 [749.586462ms]
    Dec 30 03:50:46.034: INFO: Created: latency-svc-rkhpn
    Dec 30 03:50:46.073: INFO: Got endpoints: latency-svc-lr2lh [750.267638ms]
    Dec 30 03:50:46.084: INFO: Created: latency-svc-8cgrh
    Dec 30 03:50:46.123: INFO: Got endpoints: latency-svc-5nlv4 [750.46263ms]
    Dec 30 03:50:46.133: INFO: Created: latency-svc-bxm4d
    Dec 30 03:50:46.173: INFO: Got endpoints: latency-svc-klwhk [750.286565ms]
    Dec 30 03:50:46.183: INFO: Created: latency-svc-jqhck
    Dec 30 03:50:46.223: INFO: Got endpoints: latency-svc-tsx5w [750.834119ms]
    Dec 30 03:50:46.235: INFO: Created: latency-svc-thdxs
    Dec 30 03:50:46.273: INFO: Got endpoints: latency-svc-shtbr [749.089211ms]
    Dec 30 03:50:46.283: INFO: Created: latency-svc-txvs6
    Dec 30 03:50:46.322: INFO: Got endpoints: latency-svc-848pm [749.091077ms]
    Dec 30 03:50:46.332: INFO: Created: latency-svc-7nlgj
    Dec 30 03:50:46.374: INFO: Got endpoints: latency-svc-zk2gf [750.068192ms]
    Dec 30 03:50:46.384: INFO: Created: latency-svc-c6shp
    Dec 30 03:50:46.423: INFO: Got endpoints: latency-svc-cn8p5 [749.735596ms]
    Dec 30 03:50:46.432: INFO: Created: latency-svc-9wtbw
    Dec 30 03:50:46.473: INFO: Got endpoints: latency-svc-t8876 [750.470746ms]
    Dec 30 03:50:46.484: INFO: Created: latency-svc-2fz8l
    Dec 30 03:50:46.523: INFO: Got endpoints: latency-svc-ftl5g [749.249175ms]
    Dec 30 03:50:46.533: INFO: Created: latency-svc-nqkjr
    Dec 30 03:50:46.573: INFO: Got endpoints: latency-svc-7lw7q [749.266077ms]
    Dec 30 03:50:46.583: INFO: Created: latency-svc-v2xlp
    Dec 30 03:50:46.623: INFO: Got endpoints: latency-svc-28bwj [750.167497ms]
    Dec 30 03:50:46.633: INFO: Created: latency-svc-dz4sd
    Dec 30 03:50:46.673: INFO: Got endpoints: latency-svc-jh2hd [749.701001ms]
    Dec 30 03:50:46.684: INFO: Created: latency-svc-m5nkk
    Dec 30 03:50:46.723: INFO: Got endpoints: latency-svc-rp7r2 [749.487136ms]
    Dec 30 03:50:46.734: INFO: Created: latency-svc-svgk4
    Dec 30 03:50:46.772: INFO: Got endpoints: latency-svc-rkhpn [749.512983ms]
    Dec 30 03:50:46.787: INFO: Created: latency-svc-s54rv
    Dec 30 03:50:46.826: INFO: Got endpoints: latency-svc-8cgrh [752.955459ms]
    Dec 30 03:50:46.837: INFO: Created: latency-svc-nlcsb
    Dec 30 03:50:46.873: INFO: Got endpoints: latency-svc-bxm4d [749.571481ms]
    Dec 30 03:50:46.882: INFO: Created: latency-svc-6p4zp
    Dec 30 03:50:46.923: INFO: Got endpoints: latency-svc-jqhck [749.914658ms]
    Dec 30 03:50:46.933: INFO: Created: latency-svc-lt66k
    Dec 30 03:50:46.974: INFO: Got endpoints: latency-svc-thdxs [750.176647ms]
    Dec 30 03:50:46.986: INFO: Created: latency-svc-h74db
    Dec 30 03:50:47.023: INFO: Got endpoints: latency-svc-txvs6 [750.002415ms]
    Dec 30 03:50:47.033: INFO: Created: latency-svc-d59kj
    Dec 30 03:50:47.074: INFO: Got endpoints: latency-svc-7nlgj [751.500239ms]
    Dec 30 03:50:47.084: INFO: Created: latency-svc-v4vmb
    Dec 30 03:50:47.123: INFO: Got endpoints: latency-svc-c6shp [749.016ms]
    Dec 30 03:50:47.134: INFO: Created: latency-svc-rhvdk
    Dec 30 03:50:47.174: INFO: Got endpoints: latency-svc-9wtbw [750.74754ms]
    Dec 30 03:50:47.184: INFO: Created: latency-svc-rmlc9
    Dec 30 03:50:47.223: INFO: Got endpoints: latency-svc-2fz8l [749.24816ms]
    Dec 30 03:50:47.233: INFO: Created: latency-svc-jsz2g
    Dec 30 03:50:47.274: INFO: Got endpoints: latency-svc-nqkjr [751.129495ms]
    Dec 30 03:50:47.285: INFO: Created: latency-svc-977nk
    Dec 30 03:50:47.323: INFO: Got endpoints: latency-svc-v2xlp [750.016309ms]
    Dec 30 03:50:47.332: INFO: Created: latency-svc-dqr92
    Dec 30 03:50:47.372: INFO: Got endpoints: latency-svc-dz4sd [749.029077ms]
    Dec 30 03:50:47.383: INFO: Created: latency-svc-lrmsz
    Dec 30 03:50:47.423: INFO: Got endpoints: latency-svc-m5nkk [749.838288ms]
    Dec 30 03:50:47.434: INFO: Created: latency-svc-8gpqg
    Dec 30 03:50:47.473: INFO: Got endpoints: latency-svc-svgk4 [749.745895ms]
    Dec 30 03:50:47.482: INFO: Created: latency-svc-n9b2c
    Dec 30 03:50:47.522: INFO: Got endpoints: latency-svc-s54rv [749.935605ms]
    Dec 30 03:50:47.533: INFO: Created: latency-svc-gbm9g
    Dec 30 03:50:47.572: INFO: Got endpoints: latency-svc-nlcsb [745.853632ms]
    Dec 30 03:50:47.583: INFO: Created: latency-svc-gkcwf
    Dec 30 03:50:47.623: INFO: Got endpoints: latency-svc-6p4zp [750.187261ms]
    Dec 30 03:50:47.632: INFO: Created: latency-svc-v7vtb
    Dec 30 03:50:47.672: INFO: Got endpoints: latency-svc-lt66k [748.831855ms]
    Dec 30 03:50:47.687: INFO: Created: latency-svc-95djr
    Dec 30 03:50:47.723: INFO: Got endpoints: latency-svc-h74db [749.115699ms]
    Dec 30 03:50:47.734: INFO: Created: latency-svc-696rh
    Dec 30 03:50:47.773: INFO: Got endpoints: latency-svc-d59kj [749.610645ms]
    Dec 30 03:50:47.783: INFO: Created: latency-svc-rth8z
    Dec 30 03:50:47.822: INFO: Got endpoints: latency-svc-v4vmb [748.678886ms]
    Dec 30 03:50:47.831: INFO: Created: latency-svc-9mxbr
    Dec 30 03:50:47.873: INFO: Got endpoints: latency-svc-rhvdk [749.806229ms]
    Dec 30 03:50:47.884: INFO: Created: latency-svc-wvwjd
    Dec 30 03:50:47.923: INFO: Got endpoints: latency-svc-rmlc9 [749.569064ms]
    Dec 30 03:50:47.933: INFO: Created: latency-svc-d2x94
    Dec 30 03:50:47.973: INFO: Got endpoints: latency-svc-jsz2g [750.845232ms]
    Dec 30 03:50:47.983: INFO: Created: latency-svc-vpgn5
    Dec 30 03:50:48.023: INFO: Got endpoints: latency-svc-977nk [748.605139ms]
    Dec 30 03:50:48.034: INFO: Created: latency-svc-4hqqg
    Dec 30 03:50:48.074: INFO: Got endpoints: latency-svc-dqr92 [750.895629ms]
    Dec 30 03:50:48.084: INFO: Created: latency-svc-xj499
    Dec 30 03:50:48.124: INFO: Got endpoints: latency-svc-lrmsz [751.673506ms]
    Dec 30 03:50:48.133: INFO: Created: latency-svc-ht4d9
    Dec 30 03:50:48.173: INFO: Got endpoints: latency-svc-8gpqg [749.843038ms]
    Dec 30 03:50:48.185: INFO: Created: latency-svc-gl6xs
    Dec 30 03:50:48.223: INFO: Got endpoints: latency-svc-n9b2c [750.197812ms]
    Dec 30 03:50:48.233: INFO: Created: latency-svc-bfthx
    Dec 30 03:50:48.273: INFO: Got endpoints: latency-svc-gbm9g [750.891744ms]
    Dec 30 03:50:48.283: INFO: Created: latency-svc-pggmg
    Dec 30 03:50:48.323: INFO: Got endpoints: latency-svc-gkcwf [750.274986ms]
    Dec 30 03:50:48.333: INFO: Created: latency-svc-9bfbf
    Dec 30 03:50:48.373: INFO: Got endpoints: latency-svc-v7vtb [750.441512ms]
    Dec 30 03:50:48.384: INFO: Created: latency-svc-5fttk
    Dec 30 03:50:48.423: INFO: Got endpoints: latency-svc-95djr [750.613741ms]
    Dec 30 03:50:48.432: INFO: Created: latency-svc-jwpcv
    Dec 30 03:50:48.474: INFO: Got endpoints: latency-svc-696rh [750.87574ms]
    Dec 30 03:50:48.484: INFO: Created: latency-svc-cs7r4
    Dec 30 03:50:48.522: INFO: Got endpoints: latency-svc-rth8z [749.286501ms]
    Dec 30 03:50:48.533: INFO: Created: latency-svc-nwctp
    Dec 30 03:50:48.574: INFO: Got endpoints: latency-svc-9mxbr [751.232198ms]
    Dec 30 03:50:48.583: INFO: Created: latency-svc-psvsf
    Dec 30 03:50:48.624: INFO: Got endpoints: latency-svc-wvwjd [751.062918ms]
    Dec 30 03:50:48.635: INFO: Created: latency-svc-4c9dl
    Dec 30 03:50:48.674: INFO: Got endpoints: latency-svc-d2x94 [750.504406ms]
    Dec 30 03:50:48.685: INFO: Created: latency-svc-zmpvg
    Dec 30 03:50:48.723: INFO: Got endpoints: latency-svc-vpgn5 [749.061545ms]
    Dec 30 03:50:48.732: INFO: Created: latency-svc-skg9p
    Dec 30 03:50:48.773: INFO: Got endpoints: latency-svc-4hqqg [750.378351ms]
    Dec 30 03:50:48.784: INFO: Created: latency-svc-77b5d
    Dec 30 03:50:48.823: INFO: Got endpoints: latency-svc-xj499 [749.536052ms]
    Dec 30 03:50:48.834: INFO: Created: latency-svc-t8tbq
    Dec 30 03:50:48.874: INFO: Got endpoints: latency-svc-ht4d9 [749.735607ms]
    Dec 30 03:50:48.883: INFO: Created: latency-svc-gl6sp
    Dec 30 03:50:48.923: INFO: Got endpoints: latency-svc-gl6xs [749.84304ms]
    Dec 30 03:50:48.939: INFO: Created: latency-svc-sqntq
    Dec 30 03:50:48.973: INFO: Got endpoints: latency-svc-bfthx [749.783553ms]
    Dec 30 03:50:48.983: INFO: Created: latency-svc-kjd6p
    Dec 30 03:50:49.022: INFO: Got endpoints: latency-svc-pggmg [748.977431ms]
    Dec 30 03:50:49.032: INFO: Created: latency-svc-8qf7d
    Dec 30 03:50:49.073: INFO: Got endpoints: latency-svc-9bfbf [750.052855ms]
    Dec 30 03:50:49.085: INFO: Created: latency-svc-v4jh7
    Dec 30 03:50:49.124: INFO: Got endpoints: latency-svc-5fttk [750.755457ms]
    Dec 30 03:50:49.134: INFO: Created: latency-svc-l9nzz
    Dec 30 03:50:49.180: INFO: Got endpoints: latency-svc-jwpcv [757.053373ms]
    Dec 30 03:50:49.191: INFO: Created: latency-svc-tq9q7
    Dec 30 03:50:49.222: INFO: Got endpoints: latency-svc-cs7r4 [748.427126ms]
    Dec 30 03:50:49.233: INFO: Created: latency-svc-jj96t
    Dec 30 03:50:49.274: INFO: Got endpoints: latency-svc-nwctp [751.649963ms]
    Dec 30 03:50:49.284: INFO: Created: latency-svc-kf8mb
    Dec 30 03:50:49.323: INFO: Got endpoints: latency-svc-psvsf [749.063746ms]
    Dec 30 03:50:49.332: INFO: Created: latency-svc-msl72
    Dec 30 03:50:49.373: INFO: Got endpoints: latency-svc-4c9dl [749.299803ms]
    Dec 30 03:50:49.384: INFO: Created: latency-svc-5zzz9
    Dec 30 03:50:49.423: INFO: Got endpoints: latency-svc-zmpvg [749.273666ms]
    Dec 30 03:50:49.433: INFO: Created: latency-svc-qx7v6
    Dec 30 03:50:49.474: INFO: Got endpoints: latency-svc-skg9p [750.966406ms]
    Dec 30 03:50:49.484: INFO: Created: latency-svc-27ctp
    Dec 30 03:50:49.524: INFO: Got endpoints: latency-svc-77b5d [750.631498ms]
    Dec 30 03:50:49.535: INFO: Created: latency-svc-jk6sb
    Dec 30 03:50:49.573: INFO: Got endpoints: latency-svc-t8tbq [749.802812ms]
    Dec 30 03:50:49.582: INFO: Created: latency-svc-2fvp4
    Dec 30 03:50:49.624: INFO: Got endpoints: latency-svc-gl6sp [749.774692ms]
    Dec 30 03:50:49.634: INFO: Created: latency-svc-wf5gf
    Dec 30 03:50:49.673: INFO: Got endpoints: latency-svc-sqntq [749.993408ms]
    Dec 30 03:50:49.684: INFO: Created: latency-svc-7jvph
    Dec 30 03:50:49.723: INFO: Got endpoints: latency-svc-kjd6p [749.489091ms]
    Dec 30 03:50:49.732: INFO: Created: latency-svc-fwt7v
    Dec 30 03:50:49.773: INFO: Got endpoints: latency-svc-8qf7d [750.350173ms]
    Dec 30 03:50:49.783: INFO: Created: latency-svc-6zfgw
    Dec 30 03:50:49.824: INFO: Got endpoints: latency-svc-v4jh7 [750.812584ms]
    Dec 30 03:50:49.834: INFO: Created: latency-svc-kfct9
    Dec 30 03:50:49.873: INFO: Got endpoints: latency-svc-l9nzz [748.522422ms]
    Dec 30 03:50:49.882: INFO: Created: latency-svc-hzvrb
    Dec 30 03:50:49.922: INFO: Got endpoints: latency-svc-tq9q7 [742.327028ms]
    Dec 30 03:50:49.932: INFO: Created: latency-svc-r8w4n
    Dec 30 03:50:49.973: INFO: Got endpoints: latency-svc-jj96t [751.273893ms]
    Dec 30 03:50:49.984: INFO: Created: latency-svc-9xrgv
    Dec 30 03:50:50.024: INFO: Got endpoints: latency-svc-kf8mb [750.192445ms]
    Dec 30 03:50:50.034: INFO: Created: latency-svc-7wlqf
    Dec 30 03:50:50.073: INFO: Got endpoints: latency-svc-msl72 [750.311703ms]
    Dec 30 03:50:50.083: INFO: Created: latency-svc-hfpcv
    Dec 30 03:50:50.124: INFO: Got endpoints: latency-svc-5zzz9 [750.735351ms]
    Dec 30 03:50:50.134: INFO: Created: latency-svc-5vj84
    Dec 30 03:50:50.174: INFO: Got endpoints: latency-svc-qx7v6 [750.729008ms]
    Dec 30 03:50:50.184: INFO: Created: latency-svc-lvb8r
    Dec 30 03:50:50.223: INFO: Got endpoints: latency-svc-27ctp [749.090123ms]
    Dec 30 03:50:50.234: INFO: Created: latency-svc-d8lpk
    Dec 30 03:50:50.273: INFO: Got endpoints: latency-svc-jk6sb [749.122779ms]
    Dec 30 03:50:50.284: INFO: Created: latency-svc-qkwjb
    Dec 30 03:50:50.323: INFO: Got endpoints: latency-svc-2fvp4 [750.169098ms]
    Dec 30 03:50:50.333: INFO: Created: latency-svc-4gdxj
    Dec 30 03:50:50.372: INFO: Got endpoints: latency-svc-wf5gf [748.644817ms]
    Dec 30 03:50:50.382: INFO: Created: latency-svc-7492z
    Dec 30 03:50:50.424: INFO: Got endpoints: latency-svc-7jvph [750.591886ms]
    Dec 30 03:50:50.434: INFO: Created: latency-svc-bc2w6
    Dec 30 03:50:50.473: INFO: Got endpoints: latency-svc-fwt7v [750.159381ms]
    Dec 30 03:50:50.483: INFO: Created: latency-svc-xj4bj
    Dec 30 03:50:50.522: INFO: Got endpoints: latency-svc-6zfgw [749.667838ms]
    Dec 30 03:50:50.533: INFO: Created: latency-svc-x4xn2
    Dec 30 03:50:50.574: INFO: Got endpoints: latency-svc-kfct9 [750.242244ms]
    Dec 30 03:50:50.584: INFO: Created: latency-svc-htqwn
    Dec 30 03:50:50.623: INFO: Got endpoints: latency-svc-hzvrb [749.698395ms]
    Dec 30 03:50:50.632: INFO: Created: latency-svc-5jwf2
    Dec 30 03:50:50.673: INFO: Got endpoints: latency-svc-r8w4n [750.858814ms]
    Dec 30 03:50:50.683: INFO: Created: latency-svc-wflkr
    Dec 30 03:50:50.722: INFO: Got endpoints: latency-svc-9xrgv [748.816846ms]
    Dec 30 03:50:50.732: INFO: Created: latency-svc-fzj6j
    Dec 30 03:50:50.773: INFO: Got endpoints: latency-svc-7wlqf [749.191902ms]
    Dec 30 03:50:50.783: INFO: Created: latency-svc-7nt6w
    Dec 30 03:50:50.823: INFO: Got endpoints: latency-svc-hfpcv [749.768965ms]
    Dec 30 03:50:50.833: INFO: Created: latency-svc-szs24
    Dec 30 03:50:50.873: INFO: Got endpoints: latency-svc-5vj84 [749.494579ms]
    Dec 30 03:50:50.884: INFO: Created: latency-svc-hgqwf
    Dec 30 03:50:50.923: INFO: Got endpoints: latency-svc-lvb8r [748.60788ms]
    Dec 30 03:50:50.933: INFO: Created: latency-svc-lr926
    Dec 30 03:50:50.974: INFO: Got endpoints: latency-svc-d8lpk [750.827168ms]
    Dec 30 03:50:50.984: INFO: Created: latency-svc-5v5nq
    Dec 30 03:50:51.024: INFO: Got endpoints: latency-svc-qkwjb [750.860598ms]
    Dec 30 03:50:51.034: INFO: Created: latency-svc-bdkrs
    Dec 30 03:50:51.072: INFO: Got endpoints: latency-svc-4gdxj [749.055742ms]
    Dec 30 03:50:51.082: INFO: Created: latency-svc-pscz2
    Dec 30 03:50:51.123: INFO: Got endpoints: latency-svc-7492z [750.301512ms]
    Dec 30 03:50:51.132: INFO: Created: latency-svc-h7t5k
    Dec 30 03:50:51.173: INFO: Got endpoints: latency-svc-bc2w6 [748.739704ms]
    Dec 30 03:50:51.183: INFO: Created: latency-svc-tv68j
    Dec 30 03:50:51.223: INFO: Got endpoints: latency-svc-xj4bj [749.580934ms]
    Dec 30 03:50:51.232: INFO: Created: latency-svc-xd9t2
    Dec 30 03:50:51.272: INFO: Got endpoints: latency-svc-x4xn2 [749.725762ms]
    Dec 30 03:50:51.282: INFO: Created: latency-svc-vv8jk
    Dec 30 03:50:51.323: INFO: Got endpoints: latency-svc-htqwn [749.095438ms]
    Dec 30 03:50:51.334: INFO: Created: latency-svc-5q2h9
    Dec 30 03:50:51.374: INFO: Got endpoints: latency-svc-5jwf2 [751.18799ms]
    Dec 30 03:50:51.384: INFO: Created: latency-svc-c24nd
    Dec 30 03:50:51.422: INFO: Got endpoints: latency-svc-wflkr [748.987125ms]
    Dec 30 03:50:51.433: INFO: Created: latency-svc-mbj58
    Dec 30 03:50:51.473: INFO: Got endpoints: latency-svc-fzj6j [750.990479ms]
    Dec 30 03:50:51.484: INFO: Created: latency-svc-6fj5t
    Dec 30 03:50:51.524: INFO: Got endpoints: latency-svc-7nt6w [750.244313ms]
    Dec 30 03:50:51.534: INFO: Created: latency-svc-b56q4
    Dec 30 03:50:51.573: INFO: Got endpoints: latency-svc-szs24 [750.085274ms]
    Dec 30 03:50:51.584: INFO: Created: latency-svc-jcvkc
    Dec 30 03:50:51.622: INFO: Got endpoints: latency-svc-hgqwf [749.055234ms]
    Dec 30 03:50:51.633: INFO: Created: latency-svc-h9sd6
    Dec 30 03:50:51.674: INFO: Got endpoints: latency-svc-lr926 [751.275685ms]
    Dec 30 03:50:51.684: INFO: Created: latency-svc-klbv5
    Dec 30 03:50:51.723: INFO: Got endpoints: latency-svc-5v5nq [749.066084ms]
    Dec 30 03:50:51.737: INFO: Created: latency-svc-w6vhm
    Dec 30 03:50:51.774: INFO: Got endpoints: latency-svc-bdkrs [750.024869ms]
    Dec 30 03:50:51.785: INFO: Created: latency-svc-dpccp
    Dec 30 03:50:51.823: INFO: Got endpoints: latency-svc-pscz2 [750.65814ms]
    Dec 30 03:50:51.833: INFO: Created: latency-svc-b8qzm
    Dec 30 03:50:51.873: INFO: Got endpoints: latency-svc-h7t5k [750.281214ms]
    Dec 30 03:50:51.883: INFO: Created: latency-svc-l8qth
    Dec 30 03:50:51.923: INFO: Got endpoints: latency-svc-tv68j [750.461004ms]
    Dec 30 03:50:51.934: INFO: Created: latency-svc-ljp89
    Dec 30 03:50:51.973: INFO: Got endpoints: latency-svc-xd9t2 [750.200044ms]
    Dec 30 03:50:51.982: INFO: Created: latency-svc-qh8sc
    Dec 30 03:50:52.023: INFO: Got endpoints: latency-svc-vv8jk [750.809673ms]
    Dec 30 03:50:52.034: INFO: Created: latency-svc-cbpbh
    Dec 30 03:50:52.074: INFO: Got endpoints: latency-svc-5q2h9 [750.449816ms]
    Dec 30 03:50:52.085: INFO: Created: latency-svc-97ntv
    Dec 30 03:50:52.123: INFO: Got endpoints: latency-svc-c24nd [749.299301ms]
    Dec 30 03:50:52.133: INFO: Created: latency-svc-hz6s2
    Dec 30 03:50:52.173: INFO: Got endpoints: latency-svc-mbj58 [751.213825ms]
    Dec 30 03:50:52.184: INFO: Created: latency-svc-9mtw4
    Dec 30 03:50:52.224: INFO: Got endpoints: latency-svc-6fj5t [750.174737ms]
    Dec 30 03:50:52.235: INFO: Created: latency-svc-8mxm5
    Dec 30 03:50:52.273: INFO: Got endpoints: latency-svc-b56q4 [749.338481ms]
    Dec 30 03:50:52.283: INFO: Created: latency-svc-lcjq4
    Dec 30 03:50:52.323: INFO: Got endpoints: latency-svc-jcvkc [750.27186ms]
    Dec 30 03:50:52.333: INFO: Created: latency-svc-jkbbg
    Dec 30 03:50:52.373: INFO: Got endpoints: latency-svc-h9sd6 [750.770807ms]
    Dec 30 03:50:52.384: INFO: Created: latency-svc-8vs4m
    Dec 30 03:50:52.423: INFO: Got endpoints: latency-svc-klbv5 [749.340877ms]
    Dec 30 03:50:52.433: INFO: Created: latency-svc-dnvmh
    Dec 30 03:50:52.473: INFO: Got endpoints: latency-svc-w6vhm [750.127769ms]
    Dec 30 03:50:52.483: INFO: Created: latency-svc-bgxrb
    Dec 30 03:50:52.524: INFO: Got endpoints: latency-svc-dpccp [750.140395ms]
    Dec 30 03:50:52.535: INFO: Created: latency-svc-2wmk5
    Dec 30 03:50:52.573: INFO: Got endpoints: latency-svc-b8qzm [750.146868ms]
    Dec 30 03:50:52.623: INFO: Got endpoints: latency-svc-l8qth [749.811432ms]
    Dec 30 03:50:52.673: INFO: Got endpoints: latency-svc-ljp89 [749.741468ms]
    Dec 30 03:50:52.724: INFO: Got endpoints: latency-svc-qh8sc [751.285208ms]
    Dec 30 03:50:52.773: INFO: Got endpoints: latency-svc-cbpbh [749.745831ms]
    Dec 30 03:50:52.823: INFO: Got endpoints: latency-svc-97ntv [749.538305ms]
    Dec 30 03:50:52.873: INFO: Got endpoints: latency-svc-hz6s2 [750.114993ms]
    Dec 30 03:50:52.922: INFO: Got endpoints: latency-svc-9mtw4 [748.273445ms]
    Dec 30 03:50:52.973: INFO: Got endpoints: latency-svc-8mxm5 [749.695097ms]
    Dec 30 03:50:53.023: INFO: Got endpoints: latency-svc-lcjq4 [749.823458ms]
    Dec 30 03:50:53.072: INFO: Got endpoints: latency-svc-jkbbg [748.655828ms]
    Dec 30 03:50:53.123: INFO: Got endpoints: latency-svc-8vs4m [750.07262ms]
    Dec 30 03:50:53.180: INFO: Got endpoints: latency-svc-dnvmh [756.886684ms]
    Dec 30 03:50:53.223: INFO: Got endpoints: latency-svc-bgxrb [749.986573ms]
    Dec 30 03:50:53.274: INFO: Got endpoints: latency-svc-2wmk5 [749.848048ms]
    Dec 30 03:50:53.274: INFO: Latencies: [14.950894ms 18.999534ms 22.985301ms 27.548646ms 32.748194ms 37.166482ms 42.916667ms 46.762007ms 50.426888ms 55.351882ms 59.714248ms 64.939265ms 67.372518ms 67.457199ms 67.871511ms 68.069121ms 68.672974ms 69.125969ms 69.174825ms 69.219898ms 69.525712ms 69.806042ms 70.815123ms 71.035771ms 72.599105ms 72.944779ms 73.313901ms 73.877761ms 74.288608ms 75.023933ms 75.777375ms 77.274292ms 107.936468ms 153.252608ms 196.905155ms 244.358336ms 289.861541ms 336.46844ms 379.989977ms 423.193909ms 467.376889ms 508.140996ms 557.045251ms 601.005197ms 647.954047ms 692.775295ms 738.089247ms 742.327028ms 745.853632ms 747.657483ms 748.273445ms 748.427126ms 748.522422ms 748.605139ms 748.60788ms 748.644817ms 748.655828ms 748.678886ms 748.739704ms 748.816846ms 748.831855ms 748.977431ms 748.987125ms 749.016ms 749.029077ms 749.055234ms 749.055742ms 749.061545ms 749.063746ms 749.066084ms 749.089211ms 749.090123ms 749.091077ms 749.095438ms 749.115699ms 749.122779ms 749.191902ms 749.24816ms 749.249175ms 749.266077ms 749.273666ms 749.286501ms 749.287174ms 749.299301ms 749.299803ms 749.338481ms 749.340877ms 749.487136ms 749.489091ms 749.494579ms 749.512983ms 749.536052ms 749.538305ms 749.569064ms 749.571481ms 749.580934ms 749.586462ms 749.610645ms 749.635863ms 749.667838ms 749.695097ms 749.698395ms 749.701001ms 749.725762ms 749.735596ms 749.735607ms 749.741468ms 749.745831ms 749.745895ms 749.768965ms 749.774692ms 749.783553ms 749.802812ms 749.806229ms 749.811432ms 749.823458ms 749.838288ms 749.843038ms 749.84304ms 749.848048ms 749.914658ms 749.935605ms 749.986573ms 749.993408ms 750.002415ms 750.016309ms 750.024869ms 750.052855ms 750.068192ms 750.07262ms 750.085274ms 750.114993ms 750.127769ms 750.140395ms 750.146868ms 750.159381ms 750.167497ms 750.169098ms 750.174737ms 750.176647ms 750.187261ms 750.192445ms 750.197812ms 750.200044ms 750.242244ms 750.244313ms 750.259737ms 750.267638ms 750.27186ms 750.274986ms 750.281214ms 750.286565ms 750.297653ms 750.301512ms 750.311703ms 750.350173ms 750.378351ms 750.441512ms 750.449816ms 750.461004ms 750.46263ms 750.470746ms 750.504406ms 750.556725ms 750.591886ms 750.613741ms 750.631498ms 750.65814ms 750.729008ms 750.735351ms 750.74754ms 750.755457ms 750.770807ms 750.809673ms 750.812584ms 750.827168ms 750.834119ms 750.845232ms 750.858814ms 750.860598ms 750.87574ms 750.891744ms 750.895629ms 750.966406ms 750.990479ms 751.060626ms 751.062918ms 751.129495ms 751.18799ms 751.213825ms 751.232198ms 751.273893ms 751.275685ms 751.285208ms 751.500239ms 751.649963ms 751.673506ms 752.955459ms 756.886684ms 757.053373ms]
    Dec 30 03:50:53.274: INFO: 50 %ile: 749.695097ms
    Dec 30 03:50:53.274: INFO: 90 %ile: 750.87574ms
    Dec 30 03:50:53.274: INFO: 99 %ile: 756.886684ms
    Dec 30 03:50:53.274: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/framework.go:187
    Dec 30 03:50:53.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svc-latency-4175" for this suite. 12/30/22 03:50:53.284
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:50:53.296
Dec 30 03:50:53.296: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename downward-api 12/30/22 03:50:53.297
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:50:53.309
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:50:53.312
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165
STEP: Creating a pod to test downward api env vars 12/30/22 03:50:53.316
Dec 30 03:50:53.325: INFO: Waiting up to 5m0s for pod "downward-api-7257b274-92ee-430c-94be-b57156030118" in namespace "downward-api-9796" to be "Succeeded or Failed"
Dec 30 03:50:53.328: INFO: Pod "downward-api-7257b274-92ee-430c-94be-b57156030118": Phase="Pending", Reason="", readiness=false. Elapsed: 3.021502ms
Dec 30 03:50:55.333: INFO: Pod "downward-api-7257b274-92ee-430c-94be-b57156030118": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008150159s
Dec 30 03:50:57.334: INFO: Pod "downward-api-7257b274-92ee-430c-94be-b57156030118": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008780731s
STEP: Saw pod success 12/30/22 03:50:57.334
Dec 30 03:50:57.334: INFO: Pod "downward-api-7257b274-92ee-430c-94be-b57156030118" satisfied condition "Succeeded or Failed"
Dec 30 03:50:57.338: INFO: Trying to get logs from node k8s-mgmt02 pod downward-api-7257b274-92ee-430c-94be-b57156030118 container dapi-container: <nil>
STEP: delete the pod 12/30/22 03:50:57.36
Dec 30 03:50:57.371: INFO: Waiting for pod downward-api-7257b274-92ee-430c-94be-b57156030118 to disappear
Dec 30 03:50:57.375: INFO: Pod downward-api-7257b274-92ee-430c-94be-b57156030118 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Dec 30 03:50:57.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9796" for this suite. 12/30/22 03:50:57.38
{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","completed":129,"skipped":2357,"failed":0}
------------------------------
• [4.091 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:50:53.296
    Dec 30 03:50:53.296: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename downward-api 12/30/22 03:50:53.297
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:50:53.309
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:50:53.312
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:165
    STEP: Creating a pod to test downward api env vars 12/30/22 03:50:53.316
    Dec 30 03:50:53.325: INFO: Waiting up to 5m0s for pod "downward-api-7257b274-92ee-430c-94be-b57156030118" in namespace "downward-api-9796" to be "Succeeded or Failed"
    Dec 30 03:50:53.328: INFO: Pod "downward-api-7257b274-92ee-430c-94be-b57156030118": Phase="Pending", Reason="", readiness=false. Elapsed: 3.021502ms
    Dec 30 03:50:55.333: INFO: Pod "downward-api-7257b274-92ee-430c-94be-b57156030118": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008150159s
    Dec 30 03:50:57.334: INFO: Pod "downward-api-7257b274-92ee-430c-94be-b57156030118": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008780731s
    STEP: Saw pod success 12/30/22 03:50:57.334
    Dec 30 03:50:57.334: INFO: Pod "downward-api-7257b274-92ee-430c-94be-b57156030118" satisfied condition "Succeeded or Failed"
    Dec 30 03:50:57.338: INFO: Trying to get logs from node k8s-mgmt02 pod downward-api-7257b274-92ee-430c-94be-b57156030118 container dapi-container: <nil>
    STEP: delete the pod 12/30/22 03:50:57.36
    Dec 30 03:50:57.371: INFO: Waiting for pod downward-api-7257b274-92ee-430c-94be-b57156030118 to disappear
    Dec 30 03:50:57.375: INFO: Pod downward-api-7257b274-92ee-430c-94be-b57156030118 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Dec 30 03:50:57.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-9796" for this suite. 12/30/22 03:50:57.38
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:97
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:50:57.388
Dec 30 03:50:57.389: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename container-lifecycle-hook 12/30/22 03:50:57.39
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:50:57.403
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:50:57.406
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 12/30/22 03:50:57.414
Dec 30 03:50:57.424: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6661" to be "running and ready"
Dec 30 03:50:57.427: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.119878ms
Dec 30 03:50:57.427: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Dec 30 03:50:59.433: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.008637362s
Dec 30 03:50:59.433: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Dec 30 03:50:59.433: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:97
STEP: create the pod with lifecycle hook 12/30/22 03:50:59.436
Dec 30 03:50:59.441: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-6661" to be "running and ready"
Dec 30 03:50:59.445: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.382619ms
Dec 30 03:50:59.445: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Dec 30 03:51:01.450: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008529251s
Dec 30 03:51:01.450: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Dec 30 03:51:03.450: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.008375621s
Dec 30 03:51:03.450: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
Dec 30 03:51:03.450: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 12/30/22 03:51:03.453
STEP: delete the pod with lifecycle hook 12/30/22 03:51:03.461
Dec 30 03:51:03.469: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 30 03:51:03.473: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 30 03:51:05.474: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 30 03:51:05.479: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Dec 30 03:51:05.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6661" for this suite. 12/30/22 03:51:05.485
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","completed":130,"skipped":2375,"failed":0}
------------------------------
• [SLOW TEST] [8.105 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:50:57.388
    Dec 30 03:50:57.389: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename container-lifecycle-hook 12/30/22 03:50:57.39
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:50:57.403
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:50:57.406
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 12/30/22 03:50:57.414
    Dec 30 03:50:57.424: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6661" to be "running and ready"
    Dec 30 03:50:57.427: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.119878ms
    Dec 30 03:50:57.427: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Dec 30 03:50:59.433: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.008637362s
    Dec 30 03:50:59.433: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Dec 30 03:50:59.433: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:97
    STEP: create the pod with lifecycle hook 12/30/22 03:50:59.436
    Dec 30 03:50:59.441: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-6661" to be "running and ready"
    Dec 30 03:50:59.445: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.382619ms
    Dec 30 03:50:59.445: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Dec 30 03:51:01.450: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008529251s
    Dec 30 03:51:01.450: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Dec 30 03:51:03.450: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.008375621s
    Dec 30 03:51:03.450: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    Dec 30 03:51:03.450: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 12/30/22 03:51:03.453
    STEP: delete the pod with lifecycle hook 12/30/22 03:51:03.461
    Dec 30 03:51:03.469: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Dec 30 03:51:03.473: INFO: Pod pod-with-poststart-exec-hook still exists
    Dec 30 03:51:05.474: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Dec 30 03:51:05.479: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Dec 30 03:51:05.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-6661" for this suite. 12/30/22 03:51:05.485
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:51:05.498
Dec 30 03:51:05.498: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename webhook 12/30/22 03:51:05.5
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:51:05.513
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:51:05.516
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 12/30/22 03:51:05.533
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/30/22 03:51:06.32
STEP: Deploying the webhook pod 12/30/22 03:51:06.328
STEP: Wait for the deployment to be ready 12/30/22 03:51:06.34
Dec 30 03:51:06.348: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 12/30/22 03:51:08.362
STEP: Verifying the service has paired with the endpoint 12/30/22 03:51:08.373
Dec 30 03:51:09.374: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 12/30/22 03:51:09.378
STEP: create a namespace for the webhook 12/30/22 03:51:09.397
STEP: create a configmap should be unconditionally rejected by the webhook 12/30/22 03:51:09.405
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Dec 30 03:51:09.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7375" for this suite. 12/30/22 03:51:09.451
STEP: Destroying namespace "webhook-7375-markers" for this suite. 12/30/22 03:51:09.458
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","completed":131,"skipped":2447,"failed":0}
------------------------------
• [4.003 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:51:05.498
    Dec 30 03:51:05.498: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename webhook 12/30/22 03:51:05.5
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:51:05.513
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:51:05.516
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 12/30/22 03:51:05.533
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/30/22 03:51:06.32
    STEP: Deploying the webhook pod 12/30/22 03:51:06.328
    STEP: Wait for the deployment to be ready 12/30/22 03:51:06.34
    Dec 30 03:51:06.348: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 12/30/22 03:51:08.362
    STEP: Verifying the service has paired with the endpoint 12/30/22 03:51:08.373
    Dec 30 03:51:09.374: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:238
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 12/30/22 03:51:09.378
    STEP: create a namespace for the webhook 12/30/22 03:51:09.397
    STEP: create a configmap should be unconditionally rejected by the webhook 12/30/22 03:51:09.405
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Dec 30 03:51:09.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-7375" for this suite. 12/30/22 03:51:09.451
    STEP: Destroying namespace "webhook-7375-markers" for this suite. 12/30/22 03:51:09.458
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:51:09.502
Dec 30 03:51:09.502: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename services 12/30/22 03:51:09.504
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:51:09.517
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:51:09.521
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415
STEP: creating a Service 12/30/22 03:51:09.528
STEP: watching for the Service to be added 12/30/22 03:51:09.539
Dec 30 03:51:09.540: INFO: Found Service test-service-956vr in namespace services-3780 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Dec 30 03:51:09.541: INFO: Service test-service-956vr created
STEP: Getting /status 12/30/22 03:51:09.541
Dec 30 03:51:09.546: INFO: Service test-service-956vr has LoadBalancer: {[]}
STEP: patching the ServiceStatus 12/30/22 03:51:09.546
STEP: watching for the Service to be patched 12/30/22 03:51:09.552
Dec 30 03:51:09.554: INFO: observed Service test-service-956vr in namespace services-3780 with annotations: map[] & LoadBalancer: {[]}
Dec 30 03:51:09.554: INFO: Found Service test-service-956vr in namespace services-3780 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Dec 30 03:51:09.554: INFO: Service test-service-956vr has service status patched
STEP: updating the ServiceStatus 12/30/22 03:51:09.554
Dec 30 03:51:09.565: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 12/30/22 03:51:09.565
Dec 30 03:51:09.567: INFO: Observed Service test-service-956vr in namespace services-3780 with annotations: map[] & Conditions: {[]}
Dec 30 03:51:09.567: INFO: Observed event: &Service{ObjectMeta:{test-service-956vr  services-3780  7ca552f0-0caf-44c4-9d48-bf697c13d6ce 435382 0 2022-12-30 03:51:09 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2022-12-30 03:51:09 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2022-12-30 03:51:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.233.53.225,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.233.53.225],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Dec 30 03:51:09.567: INFO: Found Service test-service-956vr in namespace services-3780 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Dec 30 03:51:09.567: INFO: Service test-service-956vr has service status updated
STEP: patching the service 12/30/22 03:51:09.567
STEP: watching for the Service to be patched 12/30/22 03:51:09.584
Dec 30 03:51:09.585: INFO: observed Service test-service-956vr in namespace services-3780 with labels: map[test-service-static:true]
Dec 30 03:51:09.585: INFO: observed Service test-service-956vr in namespace services-3780 with labels: map[test-service-static:true]
Dec 30 03:51:09.585: INFO: observed Service test-service-956vr in namespace services-3780 with labels: map[test-service-static:true]
Dec 30 03:51:09.586: INFO: Found Service test-service-956vr in namespace services-3780 with labels: map[test-service:patched test-service-static:true]
Dec 30 03:51:09.586: INFO: Service test-service-956vr patched
STEP: deleting the service 12/30/22 03:51:09.586
STEP: watching for the Service to be deleted 12/30/22 03:51:09.599
Dec 30 03:51:09.601: INFO: Observed event: ADDED
Dec 30 03:51:09.601: INFO: Observed event: MODIFIED
Dec 30 03:51:09.601: INFO: Observed event: MODIFIED
Dec 30 03:51:09.601: INFO: Observed event: MODIFIED
Dec 30 03:51:09.601: INFO: Found Service test-service-956vr in namespace services-3780 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Dec 30 03:51:09.601: INFO: Service test-service-956vr deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Dec 30 03:51:09.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3780" for this suite. 12/30/22 03:51:09.607
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","completed":132,"skipped":2453,"failed":0}
------------------------------
• [0.111 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:51:09.502
    Dec 30 03:51:09.502: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename services 12/30/22 03:51:09.504
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:51:09.517
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:51:09.521
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3415
    STEP: creating a Service 12/30/22 03:51:09.528
    STEP: watching for the Service to be added 12/30/22 03:51:09.539
    Dec 30 03:51:09.540: INFO: Found Service test-service-956vr in namespace services-3780 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    Dec 30 03:51:09.541: INFO: Service test-service-956vr created
    STEP: Getting /status 12/30/22 03:51:09.541
    Dec 30 03:51:09.546: INFO: Service test-service-956vr has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 12/30/22 03:51:09.546
    STEP: watching for the Service to be patched 12/30/22 03:51:09.552
    Dec 30 03:51:09.554: INFO: observed Service test-service-956vr in namespace services-3780 with annotations: map[] & LoadBalancer: {[]}
    Dec 30 03:51:09.554: INFO: Found Service test-service-956vr in namespace services-3780 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    Dec 30 03:51:09.554: INFO: Service test-service-956vr has service status patched
    STEP: updating the ServiceStatus 12/30/22 03:51:09.554
    Dec 30 03:51:09.565: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 12/30/22 03:51:09.565
    Dec 30 03:51:09.567: INFO: Observed Service test-service-956vr in namespace services-3780 with annotations: map[] & Conditions: {[]}
    Dec 30 03:51:09.567: INFO: Observed event: &Service{ObjectMeta:{test-service-956vr  services-3780  7ca552f0-0caf-44c4-9d48-bf697c13d6ce 435382 0 2022-12-30 03:51:09 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2022-12-30 03:51:09 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2022-12-30 03:51:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.233.53.225,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.233.53.225],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    Dec 30 03:51:09.567: INFO: Found Service test-service-956vr in namespace services-3780 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Dec 30 03:51:09.567: INFO: Service test-service-956vr has service status updated
    STEP: patching the service 12/30/22 03:51:09.567
    STEP: watching for the Service to be patched 12/30/22 03:51:09.584
    Dec 30 03:51:09.585: INFO: observed Service test-service-956vr in namespace services-3780 with labels: map[test-service-static:true]
    Dec 30 03:51:09.585: INFO: observed Service test-service-956vr in namespace services-3780 with labels: map[test-service-static:true]
    Dec 30 03:51:09.585: INFO: observed Service test-service-956vr in namespace services-3780 with labels: map[test-service-static:true]
    Dec 30 03:51:09.586: INFO: Found Service test-service-956vr in namespace services-3780 with labels: map[test-service:patched test-service-static:true]
    Dec 30 03:51:09.586: INFO: Service test-service-956vr patched
    STEP: deleting the service 12/30/22 03:51:09.586
    STEP: watching for the Service to be deleted 12/30/22 03:51:09.599
    Dec 30 03:51:09.601: INFO: Observed event: ADDED
    Dec 30 03:51:09.601: INFO: Observed event: MODIFIED
    Dec 30 03:51:09.601: INFO: Observed event: MODIFIED
    Dec 30 03:51:09.601: INFO: Observed event: MODIFIED
    Dec 30 03:51:09.601: INFO: Found Service test-service-956vr in namespace services-3780 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    Dec 30 03:51:09.601: INFO: Service test-service-956vr deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Dec 30 03:51:09.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-3780" for this suite. 12/30/22 03:51:09.607
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:51:09.617
Dec 30 03:51:09.617: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename daemonsets 12/30/22 03:51:09.619
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:51:09.633
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:51:09.636
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822
STEP: Creating simple DaemonSet "daemon-set" 12/30/22 03:51:09.672
STEP: Check that daemon pods launch on every node of the cluster. 12/30/22 03:51:09.678
Dec 30 03:51:09.687: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 30 03:51:09.687: INFO: Node k8s-mgmt01 is running 0 daemon pod, expected 1
Dec 30 03:51:10.699: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 30 03:51:10.699: INFO: Node k8s-mgmt01 is running 0 daemon pod, expected 1
Dec 30 03:51:11.699: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Dec 30 03:51:11.700: INFO: Node k8s-worker02 is running 0 daemon pod, expected 1
Dec 30 03:51:12.701: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
Dec 30 03:51:12.701: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
STEP: listing all DeamonSets 12/30/22 03:51:12.705
STEP: DeleteCollection of the DaemonSets 12/30/22 03:51:12.71
STEP: Verify that ReplicaSets have been deleted 12/30/22 03:51:12.719
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
Dec 30 03:51:12.732: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"435493"},"items":null}

Dec 30 03:51:12.739: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"435494"},"items":[{"metadata":{"name":"daemon-set-7sfjl","generateName":"daemon-set-","namespace":"daemonsets-7173","uid":"2d5d8b5d-5750-4708-8bf0-771727ef0d6c","resourceVersion":"435489","creationTimestamp":"2022-12-30T03:51:09Z","deletionTimestamp":"2022-12-30T03:51:42Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"4c0de6ae0e5475e95c5de8c3558aefecf39fc5f7500d4fd289bca84829b7a2d7","cni.projectcalico.org/podIP":"10.233.79.100/32","cni.projectcalico.org/podIPs":"10.233.79.100/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"1589ffc2-3593-4160-8b40-b8484b3a832f","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-12-30T03:51:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1589ffc2-3593-4160-8b40-b8484b3a832f\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2022-12-30T03:51:10Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-12-30T03:51:11Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.79.100\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-s64fp","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-s64fp","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"k8s-worker01","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["k8s-worker01"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-30T03:51:09Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-30T03:51:11Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-30T03:51:11Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-30T03:51:09Z"}],"hostIP":"10.78.26.194","podIP":"10.233.79.100","podIPs":[{"ip":"10.233.79.100"}],"startTime":"2022-12-30T03:51:09Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-12-30T03:51:11Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://5ef0c9dbfa5dae44ca57e83d2cd563ecd9867f5835a40cef65378e7f136a7c7e","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-86fhg","generateName":"daemon-set-","namespace":"daemonsets-7173","uid":"906f7b7f-0e70-4144-a3a9-9debc93a16a2","resourceVersion":"435492","creationTimestamp":"2022-12-30T03:51:09Z","deletionTimestamp":"2022-12-30T03:51:42Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"558e8a8aca7ffff79c4c73278e15ff4603eac7d601d4282257b852c54e164de6","cni.projectcalico.org/podIP":"10.233.125.197/32","cni.projectcalico.org/podIPs":"10.233.125.197/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"1589ffc2-3593-4160-8b40-b8484b3a832f","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-12-30T03:51:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1589ffc2-3593-4160-8b40-b8484b3a832f\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2022-12-30T03:51:10Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-12-30T03:51:11Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.125.197\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-kt49q","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-kt49q","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"k8s-mgmt02","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["k8s-mgmt02"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-30T03:51:09Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-30T03:51:11Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-30T03:51:11Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-30T03:51:09Z"}],"hostIP":"10.78.26.141","podIP":"10.233.125.197","podIPs":[{"ip":"10.233.125.197"}],"startTime":"2022-12-30T03:51:09Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-12-30T03:51:10Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://306f3b2cbbbfff0679c994cc081e7b02f0b8939581aa69a1b606b67f67d34cda","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-f4r7p","generateName":"daemon-set-","namespace":"daemonsets-7173","uid":"6a6ccacd-fae2-42c5-8134-84d7e8276820","resourceVersion":"435490","creationTimestamp":"2022-12-30T03:51:09Z","deletionTimestamp":"2022-12-30T03:51:42Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"fb8410032a276ddbac81d65de057e39bb8e591deea87e7b572f0cb33bd6e996a","cni.projectcalico.org/podIP":"10.233.78.132/32","cni.projectcalico.org/podIPs":"10.233.78.132/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"1589ffc2-3593-4160-8b40-b8484b3a832f","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-12-30T03:51:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1589ffc2-3593-4160-8b40-b8484b3a832f\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2022-12-30T03:51:10Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-12-30T03:51:11Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.78.132\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-6dsdm","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-6dsdm","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"k8s-mgmt03","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["k8s-mgmt03"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-30T03:51:09Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-30T03:51:11Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-30T03:51:11Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-30T03:51:09Z"}],"hostIP":"10.78.26.142","podIP":"10.233.78.132","podIPs":[{"ip":"10.233.78.132"}],"startTime":"2022-12-30T03:51:09Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-12-30T03:51:10Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://6b9d5cc64ff413ec1df64e962dd9c245ef47c585af1097f6d8cf6b5a9f2c351b","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-mcxr4","generateName":"daemon-set-","namespace":"daemonsets-7173","uid":"be39594e-6701-4949-9016-f06c36a0e34a","resourceVersion":"435493","creationTimestamp":"2022-12-30T03:51:09Z","deletionTimestamp":"2022-12-30T03:51:42Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"864071a0b576d16a42df06d66d99a2c3ec09e1ce9ed6b4d69cc4acec131c58a4","cni.projectcalico.org/podIP":"10.233.109.96/32","cni.projectcalico.org/podIPs":"10.233.109.96/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"1589ffc2-3593-4160-8b40-b8484b3a832f","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-12-30T03:51:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1589ffc2-3593-4160-8b40-b8484b3a832f\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2022-12-30T03:51:10Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-12-30T03:51:11Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.109.96\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-mxtwc","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-mxtwc","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"k8s-worker02","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["k8s-worker02"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-30T03:51:09Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-30T03:51:11Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-30T03:51:11Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-30T03:51:09Z"}],"hostIP":"10.78.26.195","podIP":"10.233.109.96","podIPs":[{"ip":"10.233.109.96"}],"startTime":"2022-12-30T03:51:09Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-12-30T03:51:11Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://db4ed38c7e2087eac88d2bce3545270b38b30f7dc3e284041471fa461255008e","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-mkk7b","generateName":"daemon-set-","namespace":"daemonsets-7173","uid":"519591d4-837c-443d-bc7e-b98940f79ed0","resourceVersion":"435494","creationTimestamp":"2022-12-30T03:51:09Z","deletionTimestamp":"2022-12-30T03:51:42Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"6fafdc3148b9d55c995b2c256469d5f9a08a0c80219b4eeb8b29e8f903969dcd","cni.projectcalico.org/podIP":"10.233.112.137/32","cni.projectcalico.org/podIPs":"10.233.112.137/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"1589ffc2-3593-4160-8b40-b8484b3a832f","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-12-30T03:51:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1589ffc2-3593-4160-8b40-b8484b3a832f\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2022-12-30T03:51:10Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-12-30T03:51:10Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.112.137\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-b87m7","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-b87m7","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"k8s-mgmt01","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["k8s-mgmt01"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-30T03:51:09Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-30T03:51:10Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-30T03:51:10Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-30T03:51:09Z"}],"hostIP":"10.78.26.140","podIP":"10.233.112.137","podIPs":[{"ip":"10.233.112.137"}],"startTime":"2022-12-30T03:51:09Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-12-30T03:51:10Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://afd7286b9e0576f3af5374162bd33c9b59388690673d825fff806be6f4575eab","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Dec 30 03:51:12.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7173" for this suite. 12/30/22 03:51:12.773
{"msg":"PASSED [sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]","completed":133,"skipped":2474,"failed":0}
------------------------------
• [3.166 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:51:09.617
    Dec 30 03:51:09.617: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename daemonsets 12/30/22 03:51:09.619
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:51:09.633
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:51:09.636
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:822
    STEP: Creating simple DaemonSet "daemon-set" 12/30/22 03:51:09.672
    STEP: Check that daemon pods launch on every node of the cluster. 12/30/22 03:51:09.678
    Dec 30 03:51:09.687: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 30 03:51:09.687: INFO: Node k8s-mgmt01 is running 0 daemon pod, expected 1
    Dec 30 03:51:10.699: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 30 03:51:10.699: INFO: Node k8s-mgmt01 is running 0 daemon pod, expected 1
    Dec 30 03:51:11.699: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Dec 30 03:51:11.700: INFO: Node k8s-worker02 is running 0 daemon pod, expected 1
    Dec 30 03:51:12.701: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
    Dec 30 03:51:12.701: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
    STEP: listing all DeamonSets 12/30/22 03:51:12.705
    STEP: DeleteCollection of the DaemonSets 12/30/22 03:51:12.71
    STEP: Verify that ReplicaSets have been deleted 12/30/22 03:51:12.719
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    Dec 30 03:51:12.732: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"435493"},"items":null}

    Dec 30 03:51:12.739: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"435494"},"items":[{"metadata":{"name":"daemon-set-7sfjl","generateName":"daemon-set-","namespace":"daemonsets-7173","uid":"2d5d8b5d-5750-4708-8bf0-771727ef0d6c","resourceVersion":"435489","creationTimestamp":"2022-12-30T03:51:09Z","deletionTimestamp":"2022-12-30T03:51:42Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"4c0de6ae0e5475e95c5de8c3558aefecf39fc5f7500d4fd289bca84829b7a2d7","cni.projectcalico.org/podIP":"10.233.79.100/32","cni.projectcalico.org/podIPs":"10.233.79.100/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"1589ffc2-3593-4160-8b40-b8484b3a832f","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-12-30T03:51:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1589ffc2-3593-4160-8b40-b8484b3a832f\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2022-12-30T03:51:10Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-12-30T03:51:11Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.79.100\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-s64fp","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-s64fp","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"k8s-worker01","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["k8s-worker01"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-30T03:51:09Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-30T03:51:11Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-30T03:51:11Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-30T03:51:09Z"}],"hostIP":"10.78.26.194","podIP":"10.233.79.100","podIPs":[{"ip":"10.233.79.100"}],"startTime":"2022-12-30T03:51:09Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-12-30T03:51:11Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://5ef0c9dbfa5dae44ca57e83d2cd563ecd9867f5835a40cef65378e7f136a7c7e","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-86fhg","generateName":"daemon-set-","namespace":"daemonsets-7173","uid":"906f7b7f-0e70-4144-a3a9-9debc93a16a2","resourceVersion":"435492","creationTimestamp":"2022-12-30T03:51:09Z","deletionTimestamp":"2022-12-30T03:51:42Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"558e8a8aca7ffff79c4c73278e15ff4603eac7d601d4282257b852c54e164de6","cni.projectcalico.org/podIP":"10.233.125.197/32","cni.projectcalico.org/podIPs":"10.233.125.197/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"1589ffc2-3593-4160-8b40-b8484b3a832f","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-12-30T03:51:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1589ffc2-3593-4160-8b40-b8484b3a832f\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2022-12-30T03:51:10Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-12-30T03:51:11Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.125.197\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-kt49q","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-kt49q","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"k8s-mgmt02","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["k8s-mgmt02"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-30T03:51:09Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-30T03:51:11Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-30T03:51:11Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-30T03:51:09Z"}],"hostIP":"10.78.26.141","podIP":"10.233.125.197","podIPs":[{"ip":"10.233.125.197"}],"startTime":"2022-12-30T03:51:09Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-12-30T03:51:10Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://306f3b2cbbbfff0679c994cc081e7b02f0b8939581aa69a1b606b67f67d34cda","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-f4r7p","generateName":"daemon-set-","namespace":"daemonsets-7173","uid":"6a6ccacd-fae2-42c5-8134-84d7e8276820","resourceVersion":"435490","creationTimestamp":"2022-12-30T03:51:09Z","deletionTimestamp":"2022-12-30T03:51:42Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"fb8410032a276ddbac81d65de057e39bb8e591deea87e7b572f0cb33bd6e996a","cni.projectcalico.org/podIP":"10.233.78.132/32","cni.projectcalico.org/podIPs":"10.233.78.132/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"1589ffc2-3593-4160-8b40-b8484b3a832f","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-12-30T03:51:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1589ffc2-3593-4160-8b40-b8484b3a832f\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2022-12-30T03:51:10Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-12-30T03:51:11Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.78.132\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-6dsdm","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-6dsdm","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"k8s-mgmt03","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["k8s-mgmt03"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-30T03:51:09Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-30T03:51:11Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-30T03:51:11Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-30T03:51:09Z"}],"hostIP":"10.78.26.142","podIP":"10.233.78.132","podIPs":[{"ip":"10.233.78.132"}],"startTime":"2022-12-30T03:51:09Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-12-30T03:51:10Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://6b9d5cc64ff413ec1df64e962dd9c245ef47c585af1097f6d8cf6b5a9f2c351b","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-mcxr4","generateName":"daemon-set-","namespace":"daemonsets-7173","uid":"be39594e-6701-4949-9016-f06c36a0e34a","resourceVersion":"435493","creationTimestamp":"2022-12-30T03:51:09Z","deletionTimestamp":"2022-12-30T03:51:42Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"864071a0b576d16a42df06d66d99a2c3ec09e1ce9ed6b4d69cc4acec131c58a4","cni.projectcalico.org/podIP":"10.233.109.96/32","cni.projectcalico.org/podIPs":"10.233.109.96/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"1589ffc2-3593-4160-8b40-b8484b3a832f","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-12-30T03:51:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1589ffc2-3593-4160-8b40-b8484b3a832f\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2022-12-30T03:51:10Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-12-30T03:51:11Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.109.96\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-mxtwc","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-mxtwc","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"k8s-worker02","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["k8s-worker02"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-30T03:51:09Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-30T03:51:11Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-30T03:51:11Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-30T03:51:09Z"}],"hostIP":"10.78.26.195","podIP":"10.233.109.96","podIPs":[{"ip":"10.233.109.96"}],"startTime":"2022-12-30T03:51:09Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-12-30T03:51:11Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://db4ed38c7e2087eac88d2bce3545270b38b30f7dc3e284041471fa461255008e","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-mkk7b","generateName":"daemon-set-","namespace":"daemonsets-7173","uid":"519591d4-837c-443d-bc7e-b98940f79ed0","resourceVersion":"435494","creationTimestamp":"2022-12-30T03:51:09Z","deletionTimestamp":"2022-12-30T03:51:42Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"6fafdc3148b9d55c995b2c256469d5f9a08a0c80219b4eeb8b29e8f903969dcd","cni.projectcalico.org/podIP":"10.233.112.137/32","cni.projectcalico.org/podIPs":"10.233.112.137/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"1589ffc2-3593-4160-8b40-b8484b3a832f","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-12-30T03:51:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1589ffc2-3593-4160-8b40-b8484b3a832f\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2022-12-30T03:51:10Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-12-30T03:51:10Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.112.137\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-b87m7","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-b87m7","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"k8s-mgmt01","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["k8s-mgmt01"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-30T03:51:09Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-30T03:51:10Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-30T03:51:10Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-30T03:51:09Z"}],"hostIP":"10.78.26.140","podIP":"10.233.112.137","podIPs":[{"ip":"10.233.112.137"}],"startTime":"2022-12-30T03:51:09Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-12-30T03:51:10Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://afd7286b9e0576f3af5374162bd33c9b59388690673d825fff806be6f4575eab","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Dec 30 03:51:12.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-7173" for this suite. 12/30/22 03:51:12.773
  << End Captured GinkgoWriter Output
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:822
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:51:12.784
Dec 30 03:51:12.785: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename kubectl 12/30/22 03:51:12.786
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:51:12.797
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:51:12.8
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:822
STEP: validating api versions 12/30/22 03:51:12.803
Dec 30 03:51:12.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-2817 api-versions'
Dec 30 03:51:12.887: INFO: stderr: ""
Dec 30 03:51:12.887: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Dec 30 03:51:12.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2817" for this suite. 12/30/22 03:51:12.893
{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","completed":134,"skipped":2474,"failed":0}
------------------------------
• [0.116 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:816
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:822

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:51:12.784
    Dec 30 03:51:12.785: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename kubectl 12/30/22 03:51:12.786
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:51:12.797
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:51:12.8
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:822
    STEP: validating api versions 12/30/22 03:51:12.803
    Dec 30 03:51:12.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-2817 api-versions'
    Dec 30 03:51:12.887: INFO: stderr: ""
    Dec 30 03:51:12.887: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Dec 30 03:51:12.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-2817" for this suite. 12/30/22 03:51:12.893
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:51:12.902
Dec 30 03:51:12.902: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename deployment 12/30/22 03:51:12.903
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:51:12.916
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:51:12.919
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
Dec 30 03:51:12.936: INFO: Pod name rollover-pod: Found 0 pods out of 1
Dec 30 03:51:17.942: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 12/30/22 03:51:17.942
Dec 30 03:51:17.943: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Dec 30 03:51:19.948: INFO: Creating deployment "test-rollover-deployment"
Dec 30 03:51:19.958: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Dec 30 03:51:21.967: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Dec 30 03:51:21.976: INFO: Ensure that both replica sets have 1 created replica
Dec 30 03:51:21.983: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Dec 30 03:51:21.994: INFO: Updating deployment test-rollover-deployment
Dec 30 03:51:21.994: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Dec 30 03:51:24.002: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Dec 30 03:51:24.011: INFO: Make sure deployment "test-rollover-deployment" is complete
Dec 30 03:51:24.019: INFO: all replica sets need to contain the pod-template-hash label
Dec 30 03:51:24.019: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.December, 30, 3, 51, 19, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 30, 3, 51, 19, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 30, 3, 51, 22, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 30, 3, 51, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 30 03:51:26.030: INFO: all replica sets need to contain the pod-template-hash label
Dec 30 03:51:26.030: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.December, 30, 3, 51, 19, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 30, 3, 51, 19, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 30, 3, 51, 22, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 30, 3, 51, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 30 03:51:28.030: INFO: all replica sets need to contain the pod-template-hash label
Dec 30 03:51:28.030: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.December, 30, 3, 51, 19, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 30, 3, 51, 19, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 30, 3, 51, 22, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 30, 3, 51, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 30 03:51:30.028: INFO: all replica sets need to contain the pod-template-hash label
Dec 30 03:51:30.028: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.December, 30, 3, 51, 19, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 30, 3, 51, 19, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 30, 3, 51, 22, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 30, 3, 51, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 30 03:51:32.029: INFO: all replica sets need to contain the pod-template-hash label
Dec 30 03:51:32.029: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.December, 30, 3, 51, 19, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 30, 3, 51, 19, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 30, 3, 51, 22, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 30, 3, 51, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 30 03:51:34.030: INFO: 
Dec 30 03:51:34.030: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Dec 30 03:51:34.041: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-6138  86dfee44-0937-49b1-886c-010759c86f65 435730 2 2022-12-30 03:51:19 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2022-12-30 03:51:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-30 03:51:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001feb2d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-12-30 03:51:19 +0000 UTC,LastTransitionTime:2022-12-30 03:51:19 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6d45fd857b" has successfully progressed.,LastUpdateTime:2022-12-30 03:51:33 +0000 UTC,LastTransitionTime:2022-12-30 03:51:19 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Dec 30 03:51:34.047: INFO: New ReplicaSet "test-rollover-deployment-6d45fd857b" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6d45fd857b  deployment-6138  a2a83b57-e03a-4955-8411-7f689fda026c 435719 2 2022-12-30 03:51:21 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 86dfee44-0937-49b1-886c-010759c86f65 0xc00098bd97 0xc00098bd98}] [] [{kube-controller-manager Update apps/v1 2022-12-30 03:51:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"86dfee44-0937-49b1-886c-010759c86f65\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-30 03:51:33 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6d45fd857b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00098be48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Dec 30 03:51:34.047: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Dec 30 03:51:34.047: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-6138  69a23d85-5a92-4eca-8eb2-78345d73b388 435729 2 2022-12-30 03:51:12 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 86dfee44-0937-49b1-886c-010759c86f65 0xc00098ba67 0xc00098ba68}] [] [{e2e.test Update apps/v1 2022-12-30 03:51:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-30 03:51:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"86dfee44-0937-49b1-886c-010759c86f65\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-12-30 03:51:33 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00098bb58 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 30 03:51:34.047: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-59b9df946d  deployment-6138  153bd6ea-4eea-452b-8ab0-07966c9e9f4d 435674 2 2022-12-30 03:51:19 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 86dfee44-0937-49b1-886c-010759c86f65 0xc00098bc37 0xc00098bc38}] [] [{kube-controller-manager Update apps/v1 2022-12-30 03:51:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"86dfee44-0937-49b1-886c-010759c86f65\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-30 03:51:22 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 59b9df946d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00098bd28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 30 03:51:34.052: INFO: Pod "test-rollover-deployment-6d45fd857b-tj92s" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6d45fd857b-tj92s test-rollover-deployment-6d45fd857b- deployment-6138  8e5e5b4b-7d94-445a-9c37-590da330fb77 435687 0 2022-12-30 03:51:22 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[cni.projectcalico.org/containerID:90553224a1cf2b039f4684e55d3bc7f9813045e2bca0429128e96f88badbbc93 cni.projectcalico.org/podIP:10.233.112.148/32 cni.projectcalico.org/podIPs:10.233.112.148/32] [{apps/v1 ReplicaSet test-rollover-deployment-6d45fd857b a2a83b57-e03a-4955-8411-7f689fda026c 0xc0005ea3e7 0xc0005ea3e8}] [] [{Go-http-client Update v1 2022-12-30 03:51:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-12-30 03:51:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2a83b57-e03a-4955-8411-7f689fda026c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-30 03:51:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.112.148\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dkxhq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dkxhq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-mgmt01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:51:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:51:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:51:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:51:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.140,PodIP:10.233.112.148,StartTime:2022-12-30 03:51:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-30 03:51:22 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://8a5e5f892354958ced9655c8526aba94b0ebd289a8e10f3c5d252b37e823745d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.112.148,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Dec 30 03:51:34.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6138" for this suite. 12/30/22 03:51:34.058
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","completed":135,"skipped":2494,"failed":0}
------------------------------
• [SLOW TEST] [21.164 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:51:12.902
    Dec 30 03:51:12.902: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename deployment 12/30/22 03:51:12.903
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:51:12.916
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:51:12.919
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    Dec 30 03:51:12.936: INFO: Pod name rollover-pod: Found 0 pods out of 1
    Dec 30 03:51:17.942: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 12/30/22 03:51:17.942
    Dec 30 03:51:17.943: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    Dec 30 03:51:19.948: INFO: Creating deployment "test-rollover-deployment"
    Dec 30 03:51:19.958: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    Dec 30 03:51:21.967: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    Dec 30 03:51:21.976: INFO: Ensure that both replica sets have 1 created replica
    Dec 30 03:51:21.983: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    Dec 30 03:51:21.994: INFO: Updating deployment test-rollover-deployment
    Dec 30 03:51:21.994: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    Dec 30 03:51:24.002: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    Dec 30 03:51:24.011: INFO: Make sure deployment "test-rollover-deployment" is complete
    Dec 30 03:51:24.019: INFO: all replica sets need to contain the pod-template-hash label
    Dec 30 03:51:24.019: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.December, 30, 3, 51, 19, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 30, 3, 51, 19, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 30, 3, 51, 22, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 30, 3, 51, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Dec 30 03:51:26.030: INFO: all replica sets need to contain the pod-template-hash label
    Dec 30 03:51:26.030: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.December, 30, 3, 51, 19, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 30, 3, 51, 19, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 30, 3, 51, 22, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 30, 3, 51, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Dec 30 03:51:28.030: INFO: all replica sets need to contain the pod-template-hash label
    Dec 30 03:51:28.030: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.December, 30, 3, 51, 19, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 30, 3, 51, 19, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 30, 3, 51, 22, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 30, 3, 51, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Dec 30 03:51:30.028: INFO: all replica sets need to contain the pod-template-hash label
    Dec 30 03:51:30.028: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.December, 30, 3, 51, 19, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 30, 3, 51, 19, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 30, 3, 51, 22, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 30, 3, 51, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Dec 30 03:51:32.029: INFO: all replica sets need to contain the pod-template-hash label
    Dec 30 03:51:32.029: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.December, 30, 3, 51, 19, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 30, 3, 51, 19, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 30, 3, 51, 22, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 30, 3, 51, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Dec 30 03:51:34.030: INFO: 
    Dec 30 03:51:34.030: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Dec 30 03:51:34.041: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-6138  86dfee44-0937-49b1-886c-010759c86f65 435730 2 2022-12-30 03:51:19 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2022-12-30 03:51:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-30 03:51:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001feb2d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-12-30 03:51:19 +0000 UTC,LastTransitionTime:2022-12-30 03:51:19 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6d45fd857b" has successfully progressed.,LastUpdateTime:2022-12-30 03:51:33 +0000 UTC,LastTransitionTime:2022-12-30 03:51:19 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Dec 30 03:51:34.047: INFO: New ReplicaSet "test-rollover-deployment-6d45fd857b" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6d45fd857b  deployment-6138  a2a83b57-e03a-4955-8411-7f689fda026c 435719 2 2022-12-30 03:51:21 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 86dfee44-0937-49b1-886c-010759c86f65 0xc00098bd97 0xc00098bd98}] [] [{kube-controller-manager Update apps/v1 2022-12-30 03:51:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"86dfee44-0937-49b1-886c-010759c86f65\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-30 03:51:33 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6d45fd857b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00098be48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Dec 30 03:51:34.047: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    Dec 30 03:51:34.047: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-6138  69a23d85-5a92-4eca-8eb2-78345d73b388 435729 2 2022-12-30 03:51:12 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 86dfee44-0937-49b1-886c-010759c86f65 0xc00098ba67 0xc00098ba68}] [] [{e2e.test Update apps/v1 2022-12-30 03:51:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-30 03:51:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"86dfee44-0937-49b1-886c-010759c86f65\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-12-30 03:51:33 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00098bb58 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Dec 30 03:51:34.047: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-59b9df946d  deployment-6138  153bd6ea-4eea-452b-8ab0-07966c9e9f4d 435674 2 2022-12-30 03:51:19 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 86dfee44-0937-49b1-886c-010759c86f65 0xc00098bc37 0xc00098bc38}] [] [{kube-controller-manager Update apps/v1 2022-12-30 03:51:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"86dfee44-0937-49b1-886c-010759c86f65\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-30 03:51:22 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 59b9df946d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00098bd28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Dec 30 03:51:34.052: INFO: Pod "test-rollover-deployment-6d45fd857b-tj92s" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6d45fd857b-tj92s test-rollover-deployment-6d45fd857b- deployment-6138  8e5e5b4b-7d94-445a-9c37-590da330fb77 435687 0 2022-12-30 03:51:22 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[cni.projectcalico.org/containerID:90553224a1cf2b039f4684e55d3bc7f9813045e2bca0429128e96f88badbbc93 cni.projectcalico.org/podIP:10.233.112.148/32 cni.projectcalico.org/podIPs:10.233.112.148/32] [{apps/v1 ReplicaSet test-rollover-deployment-6d45fd857b a2a83b57-e03a-4955-8411-7f689fda026c 0xc0005ea3e7 0xc0005ea3e8}] [] [{Go-http-client Update v1 2022-12-30 03:51:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-12-30 03:51:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2a83b57-e03a-4955-8411-7f689fda026c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-30 03:51:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.112.148\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dkxhq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dkxhq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-mgmt01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:51:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:51:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:51:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 03:51:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.140,PodIP:10.233.112.148,StartTime:2022-12-30 03:51:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-30 03:51:22 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://8a5e5f892354958ced9655c8526aba94b0ebd289a8e10f3c5d252b37e823745d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.112.148,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Dec 30 03:51:34.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-6138" for this suite. 12/30/22 03:51:34.058
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:51:34.066
Dec 30 03:51:34.066: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename secrets 12/30/22 03:51:34.068
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:51:34.082
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:51:34.086
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67
STEP: Creating secret with name secret-test-3b5f5b5e-d3c1-4f68-9f11-9c3fa4dadc0b 12/30/22 03:51:34.089
STEP: Creating a pod to test consume secrets 12/30/22 03:51:34.094
Dec 30 03:51:34.109: INFO: Waiting up to 5m0s for pod "pod-secrets-07663343-e227-410a-9ffb-8e5d6f48bddc" in namespace "secrets-3949" to be "Succeeded or Failed"
Dec 30 03:51:34.112: INFO: Pod "pod-secrets-07663343-e227-410a-9ffb-8e5d6f48bddc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.215924ms
Dec 30 03:51:36.116: INFO: Pod "pod-secrets-07663343-e227-410a-9ffb-8e5d6f48bddc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007777052s
Dec 30 03:51:38.118: INFO: Pod "pod-secrets-07663343-e227-410a-9ffb-8e5d6f48bddc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009919673s
STEP: Saw pod success 12/30/22 03:51:38.119
Dec 30 03:51:38.119: INFO: Pod "pod-secrets-07663343-e227-410a-9ffb-8e5d6f48bddc" satisfied condition "Succeeded or Failed"
Dec 30 03:51:38.122: INFO: Trying to get logs from node k8s-mgmt03 pod pod-secrets-07663343-e227-410a-9ffb-8e5d6f48bddc container secret-volume-test: <nil>
STEP: delete the pod 12/30/22 03:51:38.143
Dec 30 03:51:38.157: INFO: Waiting for pod pod-secrets-07663343-e227-410a-9ffb-8e5d6f48bddc to disappear
Dec 30 03:51:38.160: INFO: Pod pod-secrets-07663343-e227-410a-9ffb-8e5d6f48bddc no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Dec 30 03:51:38.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3949" for this suite. 12/30/22 03:51:38.166
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","completed":136,"skipped":2494,"failed":0}
------------------------------
• [4.106 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:51:34.066
    Dec 30 03:51:34.066: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename secrets 12/30/22 03:51:34.068
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:51:34.082
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:51:34.086
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:67
    STEP: Creating secret with name secret-test-3b5f5b5e-d3c1-4f68-9f11-9c3fa4dadc0b 12/30/22 03:51:34.089
    STEP: Creating a pod to test consume secrets 12/30/22 03:51:34.094
    Dec 30 03:51:34.109: INFO: Waiting up to 5m0s for pod "pod-secrets-07663343-e227-410a-9ffb-8e5d6f48bddc" in namespace "secrets-3949" to be "Succeeded or Failed"
    Dec 30 03:51:34.112: INFO: Pod "pod-secrets-07663343-e227-410a-9ffb-8e5d6f48bddc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.215924ms
    Dec 30 03:51:36.116: INFO: Pod "pod-secrets-07663343-e227-410a-9ffb-8e5d6f48bddc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007777052s
    Dec 30 03:51:38.118: INFO: Pod "pod-secrets-07663343-e227-410a-9ffb-8e5d6f48bddc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009919673s
    STEP: Saw pod success 12/30/22 03:51:38.119
    Dec 30 03:51:38.119: INFO: Pod "pod-secrets-07663343-e227-410a-9ffb-8e5d6f48bddc" satisfied condition "Succeeded or Failed"
    Dec 30 03:51:38.122: INFO: Trying to get logs from node k8s-mgmt03 pod pod-secrets-07663343-e227-410a-9ffb-8e5d6f48bddc container secret-volume-test: <nil>
    STEP: delete the pod 12/30/22 03:51:38.143
    Dec 30 03:51:38.157: INFO: Waiting for pod pod-secrets-07663343-e227-410a-9ffb-8e5d6f48bddc to disappear
    Dec 30 03:51:38.160: INFO: Pod pod-secrets-07663343-e227-410a-9ffb-8e5d6f48bddc no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Dec 30 03:51:38.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-3949" for this suite. 12/30/22 03:51:38.166
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:51:38.178
Dec 30 03:51:38.178: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename secrets 12/30/22 03:51:38.18
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:51:38.193
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:51:38.196
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94
STEP: creating secret secrets-3525/secret-test-ee8b2092-582c-43d9-a733-34f2630331be 12/30/22 03:51:38.199
STEP: Creating a pod to test consume secrets 12/30/22 03:51:38.203
Dec 30 03:51:38.212: INFO: Waiting up to 5m0s for pod "pod-configmaps-1f727a40-fbfb-4dd9-99c7-c77b7698ba33" in namespace "secrets-3525" to be "Succeeded or Failed"
Dec 30 03:51:38.216: INFO: Pod "pod-configmaps-1f727a40-fbfb-4dd9-99c7-c77b7698ba33": Phase="Pending", Reason="", readiness=false. Elapsed: 3.953115ms
Dec 30 03:51:40.221: INFO: Pod "pod-configmaps-1f727a40-fbfb-4dd9-99c7-c77b7698ba33": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009442025s
Dec 30 03:51:42.221: INFO: Pod "pod-configmaps-1f727a40-fbfb-4dd9-99c7-c77b7698ba33": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00954306s
STEP: Saw pod success 12/30/22 03:51:42.221
Dec 30 03:51:42.222: INFO: Pod "pod-configmaps-1f727a40-fbfb-4dd9-99c7-c77b7698ba33" satisfied condition "Succeeded or Failed"
Dec 30 03:51:42.226: INFO: Trying to get logs from node k8s-mgmt03 pod pod-configmaps-1f727a40-fbfb-4dd9-99c7-c77b7698ba33 container env-test: <nil>
STEP: delete the pod 12/30/22 03:51:42.236
Dec 30 03:51:42.248: INFO: Waiting for pod pod-configmaps-1f727a40-fbfb-4dd9-99c7-c77b7698ba33 to disappear
Dec 30 03:51:42.252: INFO: Pod pod-configmaps-1f727a40-fbfb-4dd9-99c7-c77b7698ba33 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Dec 30 03:51:42.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3525" for this suite. 12/30/22 03:51:42.257
{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","completed":137,"skipped":2553,"failed":0}
------------------------------
• [4.086 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:51:38.178
    Dec 30 03:51:38.178: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename secrets 12/30/22 03:51:38.18
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:51:38.193
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:51:38.196
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:94
    STEP: creating secret secrets-3525/secret-test-ee8b2092-582c-43d9-a733-34f2630331be 12/30/22 03:51:38.199
    STEP: Creating a pod to test consume secrets 12/30/22 03:51:38.203
    Dec 30 03:51:38.212: INFO: Waiting up to 5m0s for pod "pod-configmaps-1f727a40-fbfb-4dd9-99c7-c77b7698ba33" in namespace "secrets-3525" to be "Succeeded or Failed"
    Dec 30 03:51:38.216: INFO: Pod "pod-configmaps-1f727a40-fbfb-4dd9-99c7-c77b7698ba33": Phase="Pending", Reason="", readiness=false. Elapsed: 3.953115ms
    Dec 30 03:51:40.221: INFO: Pod "pod-configmaps-1f727a40-fbfb-4dd9-99c7-c77b7698ba33": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009442025s
    Dec 30 03:51:42.221: INFO: Pod "pod-configmaps-1f727a40-fbfb-4dd9-99c7-c77b7698ba33": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00954306s
    STEP: Saw pod success 12/30/22 03:51:42.221
    Dec 30 03:51:42.222: INFO: Pod "pod-configmaps-1f727a40-fbfb-4dd9-99c7-c77b7698ba33" satisfied condition "Succeeded or Failed"
    Dec 30 03:51:42.226: INFO: Trying to get logs from node k8s-mgmt03 pod pod-configmaps-1f727a40-fbfb-4dd9-99c7-c77b7698ba33 container env-test: <nil>
    STEP: delete the pod 12/30/22 03:51:42.236
    Dec 30 03:51:42.248: INFO: Waiting for pod pod-configmaps-1f727a40-fbfb-4dd9-99c7-c77b7698ba33 to disappear
    Dec 30 03:51:42.252: INFO: Pod pod-configmaps-1f727a40-fbfb-4dd9-99c7-c77b7698ba33 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Dec 30 03:51:42.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-3525" for this suite. 12/30/22 03:51:42.257
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:51:42.265
Dec 30 03:51:42.265: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename limitrange 12/30/22 03:51:42.266
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:51:42.28
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:51:42.284
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57
STEP: Creating a LimitRange 12/30/22 03:51:42.287
STEP: Setting up watch 12/30/22 03:51:42.287
STEP: Submitting a LimitRange 12/30/22 03:51:42.392
STEP: Verifying LimitRange creation was observed 12/30/22 03:51:42.397
STEP: Fetching the LimitRange to ensure it has proper values 12/30/22 03:51:42.398
Dec 30 03:51:42.402: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Dec 30 03:51:42.402: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 12/30/22 03:51:42.402
STEP: Ensuring Pod has resource requirements applied from LimitRange 12/30/22 03:51:42.408
Dec 30 03:51:42.412: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Dec 30 03:51:42.412: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 12/30/22 03:51:42.412
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 12/30/22 03:51:42.418
Dec 30 03:51:42.422: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Dec 30 03:51:42.422: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 12/30/22 03:51:42.422
STEP: Failing to create a Pod with more than max resources 12/30/22 03:51:42.424
STEP: Updating a LimitRange 12/30/22 03:51:42.427
STEP: Verifying LimitRange updating is effective 12/30/22 03:51:42.433
STEP: Creating a Pod with less than former min resources 12/30/22 03:51:44.438
STEP: Failing to create a Pod with more than max resources 12/30/22 03:51:44.445
STEP: Deleting a LimitRange 12/30/22 03:51:44.448
STEP: Verifying the LimitRange was deleted 12/30/22 03:51:44.454
Dec 30 03:51:49.460: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 12/30/22 03:51:49.46
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:187
Dec 30 03:51:49.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-9004" for this suite. 12/30/22 03:51:49.476
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","completed":138,"skipped":2559,"failed":0}
------------------------------
• [SLOW TEST] [7.217 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:51:42.265
    Dec 30 03:51:42.265: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename limitrange 12/30/22 03:51:42.266
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:51:42.28
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:51:42.284
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:57
    STEP: Creating a LimitRange 12/30/22 03:51:42.287
    STEP: Setting up watch 12/30/22 03:51:42.287
    STEP: Submitting a LimitRange 12/30/22 03:51:42.392
    STEP: Verifying LimitRange creation was observed 12/30/22 03:51:42.397
    STEP: Fetching the LimitRange to ensure it has proper values 12/30/22 03:51:42.398
    Dec 30 03:51:42.402: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Dec 30 03:51:42.402: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 12/30/22 03:51:42.402
    STEP: Ensuring Pod has resource requirements applied from LimitRange 12/30/22 03:51:42.408
    Dec 30 03:51:42.412: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Dec 30 03:51:42.412: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 12/30/22 03:51:42.412
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 12/30/22 03:51:42.418
    Dec 30 03:51:42.422: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    Dec 30 03:51:42.422: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 12/30/22 03:51:42.422
    STEP: Failing to create a Pod with more than max resources 12/30/22 03:51:42.424
    STEP: Updating a LimitRange 12/30/22 03:51:42.427
    STEP: Verifying LimitRange updating is effective 12/30/22 03:51:42.433
    STEP: Creating a Pod with less than former min resources 12/30/22 03:51:44.438
    STEP: Failing to create a Pod with more than max resources 12/30/22 03:51:44.445
    STEP: Deleting a LimitRange 12/30/22 03:51:44.448
    STEP: Verifying the LimitRange was deleted 12/30/22 03:51:44.454
    Dec 30 03:51:49.460: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 12/30/22 03:51:49.46
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/framework.go:187
    Dec 30 03:51:49.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "limitrange-9004" for this suite. 12/30/22 03:51:49.476
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:51:49.483
Dec 30 03:51:49.484: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename gc 12/30/22 03:51:49.485
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:51:49.498
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:51:49.502
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 12/30/22 03:51:49.505
STEP: Wait for the Deployment to create new ReplicaSet 12/30/22 03:51:49.51
STEP: delete the deployment 12/30/22 03:51:50.025
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 12/30/22 03:51:50.033
STEP: Gathering metrics 12/30/22 03:51:50.556
Dec 30 03:51:50.584: INFO: Waiting up to 5m0s for pod "kube-controller-manager-k8s-mgmt03" in namespace "kube-system" to be "running and ready"
Dec 30 03:51:50.588: INFO: Pod "kube-controller-manager-k8s-mgmt03": Phase="Running", Reason="", readiness=true. Elapsed: 4.30669ms
Dec 30 03:51:50.588: INFO: The phase of Pod kube-controller-manager-k8s-mgmt03 is Running (Ready = true)
Dec 30 03:51:50.588: INFO: Pod "kube-controller-manager-k8s-mgmt03" satisfied condition "running and ready"
Dec 30 03:51:50.686: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Dec 30 03:51:50.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7164" for this suite. 12/30/22 03:51:50.691
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","completed":139,"skipped":2569,"failed":0}
------------------------------
• [1.214 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:51:49.483
    Dec 30 03:51:49.484: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename gc 12/30/22 03:51:49.485
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:51:49.498
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:51:49.502
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 12/30/22 03:51:49.505
    STEP: Wait for the Deployment to create new ReplicaSet 12/30/22 03:51:49.51
    STEP: delete the deployment 12/30/22 03:51:50.025
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 12/30/22 03:51:50.033
    STEP: Gathering metrics 12/30/22 03:51:50.556
    Dec 30 03:51:50.584: INFO: Waiting up to 5m0s for pod "kube-controller-manager-k8s-mgmt03" in namespace "kube-system" to be "running and ready"
    Dec 30 03:51:50.588: INFO: Pod "kube-controller-manager-k8s-mgmt03": Phase="Running", Reason="", readiness=true. Elapsed: 4.30669ms
    Dec 30 03:51:50.588: INFO: The phase of Pod kube-controller-manager-k8s-mgmt03 is Running (Ready = true)
    Dec 30 03:51:50.588: INFO: Pod "kube-controller-manager-k8s-mgmt03" satisfied condition "running and ready"
    Dec 30 03:51:50.686: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Dec 30 03:51:50.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-7164" for this suite. 12/30/22 03:51:50.691
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:51:50.702
Dec 30 03:51:50.702: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename podtemplate 12/30/22 03:51:50.704
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:51:50.717
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:51:50.72
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 12/30/22 03:51:50.724
Dec 30 03:51:50.729: INFO: created test-podtemplate-1
Dec 30 03:51:50.733: INFO: created test-podtemplate-2
Dec 30 03:51:50.739: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 12/30/22 03:51:50.739
STEP: delete collection of pod templates 12/30/22 03:51:50.743
Dec 30 03:51:50.743: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 12/30/22 03:51:50.761
Dec 30 03:51:50.762: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Dec 30 03:51:50.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-3557" for this suite. 12/30/22 03:51:50.771
{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","completed":140,"skipped":2606,"failed":0}
------------------------------
• [0.076 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:51:50.702
    Dec 30 03:51:50.702: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename podtemplate 12/30/22 03:51:50.704
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:51:50.717
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:51:50.72
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 12/30/22 03:51:50.724
    Dec 30 03:51:50.729: INFO: created test-podtemplate-1
    Dec 30 03:51:50.733: INFO: created test-podtemplate-2
    Dec 30 03:51:50.739: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 12/30/22 03:51:50.739
    STEP: delete collection of pod templates 12/30/22 03:51:50.743
    Dec 30 03:51:50.743: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 12/30/22 03:51:50.761
    Dec 30 03:51:50.762: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Dec 30 03:51:50.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-3557" for this suite. 12/30/22 03:51:50.771
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:51:50.778
Dec 30 03:51:50.779: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename webhook 12/30/22 03:51:50.78
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:51:50.793
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:51:50.796
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 12/30/22 03:51:50.812
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/30/22 03:51:51.375
STEP: Deploying the webhook pod 12/30/22 03:51:51.384
STEP: Wait for the deployment to be ready 12/30/22 03:51:51.395
Dec 30 03:51:51.403: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 12/30/22 03:51:53.417
STEP: Verifying the service has paired with the endpoint 12/30/22 03:51:53.428
Dec 30 03:51:54.428: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196
STEP: Registering the webhook via the AdmissionRegistration API 12/30/22 03:51:54.434
STEP: create a pod that should be denied by the webhook 12/30/22 03:51:54.455
STEP: create a pod that causes the webhook to hang 12/30/22 03:51:54.474
STEP: create a configmap that should be denied by the webhook 12/30/22 03:52:04.482
STEP: create a configmap that should be admitted by the webhook 12/30/22 03:52:04.495
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 12/30/22 03:52:04.506
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 12/30/22 03:52:04.516
STEP: create a namespace that bypass the webhook 12/30/22 03:52:04.524
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 12/30/22 03:52:04.532
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Dec 30 03:52:04.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3307" for this suite. 12/30/22 03:52:04.563
STEP: Destroying namespace "webhook-3307-markers" for this suite. 12/30/22 03:52:04.571
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","completed":141,"skipped":2606,"failed":0}
------------------------------
• [SLOW TEST] [13.835 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:51:50.778
    Dec 30 03:51:50.779: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename webhook 12/30/22 03:51:50.78
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:51:50.793
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:51:50.796
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 12/30/22 03:51:50.812
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/30/22 03:51:51.375
    STEP: Deploying the webhook pod 12/30/22 03:51:51.384
    STEP: Wait for the deployment to be ready 12/30/22 03:51:51.395
    Dec 30 03:51:51.403: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 12/30/22 03:51:53.417
    STEP: Verifying the service has paired with the endpoint 12/30/22 03:51:53.428
    Dec 30 03:51:54.428: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:196
    STEP: Registering the webhook via the AdmissionRegistration API 12/30/22 03:51:54.434
    STEP: create a pod that should be denied by the webhook 12/30/22 03:51:54.455
    STEP: create a pod that causes the webhook to hang 12/30/22 03:51:54.474
    STEP: create a configmap that should be denied by the webhook 12/30/22 03:52:04.482
    STEP: create a configmap that should be admitted by the webhook 12/30/22 03:52:04.495
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 12/30/22 03:52:04.506
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 12/30/22 03:52:04.516
    STEP: create a namespace that bypass the webhook 12/30/22 03:52:04.524
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 12/30/22 03:52:04.532
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Dec 30 03:52:04.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-3307" for this suite. 12/30/22 03:52:04.563
    STEP: Destroying namespace "webhook-3307-markers" for this suite. 12/30/22 03:52:04.571
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:52:04.615
Dec 30 03:52:04.615: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename pods 12/30/22 03:52:04.617
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:52:04.63
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:52:04.633
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895
STEP: creating a Pod with a static label 12/30/22 03:52:04.648
STEP: watching for Pod to be ready 12/30/22 03:52:04.657
Dec 30 03:52:04.659: INFO: observed Pod pod-test in namespace pods-4422 in phase Pending with labels: map[test-pod-static:true] & conditions []
Dec 30 03:52:04.663: INFO: observed Pod pod-test in namespace pods-4422 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-30 03:52:04 +0000 UTC  }]
Dec 30 03:52:04.678: INFO: observed Pod pod-test in namespace pods-4422 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-30 03:52:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-30 03:52:04 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-30 03:52:04 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-30 03:52:04 +0000 UTC  }]
Dec 30 03:52:05.279: INFO: observed Pod pod-test in namespace pods-4422 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-30 03:52:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-30 03:52:04 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-30 03:52:04 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-30 03:52:04 +0000 UTC  }]
Dec 30 03:52:06.120: INFO: Found Pod pod-test in namespace pods-4422 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-30 03:52:04 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2022-12-30 03:52:06 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2022-12-30 03:52:06 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-30 03:52:04 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 12/30/22 03:52:06.124
STEP: getting the Pod and ensuring that it's patched 12/30/22 03:52:06.135
STEP: replacing the Pod's status Ready condition to False 12/30/22 03:52:06.14
STEP: check the Pod again to ensure its Ready conditions are False 12/30/22 03:52:06.155
STEP: deleting the Pod via a Collection with a LabelSelector 12/30/22 03:52:06.155
STEP: watching for the Pod to be deleted 12/30/22 03:52:06.165
Dec 30 03:52:06.167: INFO: observed event type MODIFIED
Dec 30 03:52:07.813: INFO: observed event type MODIFIED
Dec 30 03:52:08.366: INFO: observed event type MODIFIED
Dec 30 03:52:09.132: INFO: observed event type MODIFIED
Dec 30 03:52:09.138: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Dec 30 03:52:09.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4422" for this suite. 12/30/22 03:52:09.153
{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","completed":142,"skipped":2612,"failed":0}
------------------------------
• [4.545 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:52:04.615
    Dec 30 03:52:04.615: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename pods 12/30/22 03:52:04.617
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:52:04.63
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:52:04.633
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:895
    STEP: creating a Pod with a static label 12/30/22 03:52:04.648
    STEP: watching for Pod to be ready 12/30/22 03:52:04.657
    Dec 30 03:52:04.659: INFO: observed Pod pod-test in namespace pods-4422 in phase Pending with labels: map[test-pod-static:true] & conditions []
    Dec 30 03:52:04.663: INFO: observed Pod pod-test in namespace pods-4422 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-30 03:52:04 +0000 UTC  }]
    Dec 30 03:52:04.678: INFO: observed Pod pod-test in namespace pods-4422 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-30 03:52:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-30 03:52:04 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-30 03:52:04 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-30 03:52:04 +0000 UTC  }]
    Dec 30 03:52:05.279: INFO: observed Pod pod-test in namespace pods-4422 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-30 03:52:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-30 03:52:04 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-30 03:52:04 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-30 03:52:04 +0000 UTC  }]
    Dec 30 03:52:06.120: INFO: Found Pod pod-test in namespace pods-4422 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-30 03:52:04 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2022-12-30 03:52:06 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2022-12-30 03:52:06 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-30 03:52:04 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 12/30/22 03:52:06.124
    STEP: getting the Pod and ensuring that it's patched 12/30/22 03:52:06.135
    STEP: replacing the Pod's status Ready condition to False 12/30/22 03:52:06.14
    STEP: check the Pod again to ensure its Ready conditions are False 12/30/22 03:52:06.155
    STEP: deleting the Pod via a Collection with a LabelSelector 12/30/22 03:52:06.155
    STEP: watching for the Pod to be deleted 12/30/22 03:52:06.165
    Dec 30 03:52:06.167: INFO: observed event type MODIFIED
    Dec 30 03:52:07.813: INFO: observed event type MODIFIED
    Dec 30 03:52:08.366: INFO: observed event type MODIFIED
    Dec 30 03:52:09.132: INFO: observed event type MODIFIED
    Dec 30 03:52:09.138: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Dec 30 03:52:09.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-4422" for this suite. 12/30/22 03:52:09.153
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:52:09.16
Dec 30 03:52:09.160: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename webhook 12/30/22 03:52:09.162
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:52:09.183
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:52:09.186
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 12/30/22 03:52:09.202
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/30/22 03:52:09.625
STEP: Deploying the webhook pod 12/30/22 03:52:09.63
STEP: Wait for the deployment to be ready 12/30/22 03:52:09.639
Dec 30 03:52:09.646: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 12/30/22 03:52:11.66
STEP: Verifying the service has paired with the endpoint 12/30/22 03:52:11.669
Dec 30 03:52:12.670: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220
Dec 30 03:52:12.674: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Registering the custom resource webhook via the AdmissionRegistration API 12/30/22 03:52:18.184
STEP: Creating a custom resource that should be denied by the webhook 12/30/22 03:52:18.204
STEP: Creating a custom resource whose deletion would be denied by the webhook 12/30/22 03:52:20.263
STEP: Updating the custom resource with disallowed data should be denied 12/30/22 03:52:20.274
STEP: Deleting the custom resource should be denied 12/30/22 03:52:20.289
STEP: Remove the offending key and value from the custom resource data 12/30/22 03:52:20.301
STEP: Deleting the updated custom resource should be successful 12/30/22 03:52:20.315
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Dec 30 03:52:20.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9576" for this suite. 12/30/22 03:52:20.847
STEP: Destroying namespace "webhook-9576-markers" for this suite. 12/30/22 03:52:20.854
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","completed":143,"skipped":2613,"failed":0}
------------------------------
• [SLOW TEST] [11.734 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:52:09.16
    Dec 30 03:52:09.160: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename webhook 12/30/22 03:52:09.162
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:52:09.183
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:52:09.186
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 12/30/22 03:52:09.202
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/30/22 03:52:09.625
    STEP: Deploying the webhook pod 12/30/22 03:52:09.63
    STEP: Wait for the deployment to be ready 12/30/22 03:52:09.639
    Dec 30 03:52:09.646: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 12/30/22 03:52:11.66
    STEP: Verifying the service has paired with the endpoint 12/30/22 03:52:11.669
    Dec 30 03:52:12.670: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:220
    Dec 30 03:52:12.674: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 12/30/22 03:52:18.184
    STEP: Creating a custom resource that should be denied by the webhook 12/30/22 03:52:18.204
    STEP: Creating a custom resource whose deletion would be denied by the webhook 12/30/22 03:52:20.263
    STEP: Updating the custom resource with disallowed data should be denied 12/30/22 03:52:20.274
    STEP: Deleting the custom resource should be denied 12/30/22 03:52:20.289
    STEP: Remove the offending key and value from the custom resource data 12/30/22 03:52:20.301
    STEP: Deleting the updated custom resource should be successful 12/30/22 03:52:20.315
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Dec 30 03:52:20.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-9576" for this suite. 12/30/22 03:52:20.847
    STEP: Destroying namespace "webhook-9576-markers" for this suite. 12/30/22 03:52:20.854
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:52:20.901
Dec 30 03:52:20.902: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename projected 12/30/22 03:52:20.902
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:52:20.915
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:52:20.918
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98
STEP: Creating configMap with name projected-configmap-test-volume-map-3c5e8433-01c8-4146-949f-b7f52c197313 12/30/22 03:52:20.921
STEP: Creating a pod to test consume configMaps 12/30/22 03:52:20.935
Dec 30 03:52:20.947: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7a4d2b53-47b6-408a-974c-1ab9d2f3e300" in namespace "projected-4695" to be "Succeeded or Failed"
Dec 30 03:52:20.950: INFO: Pod "pod-projected-configmaps-7a4d2b53-47b6-408a-974c-1ab9d2f3e300": Phase="Pending", Reason="", readiness=false. Elapsed: 3.61578ms
Dec 30 03:52:22.955: INFO: Pod "pod-projected-configmaps-7a4d2b53-47b6-408a-974c-1ab9d2f3e300": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00858891s
Dec 30 03:52:24.956: INFO: Pod "pod-projected-configmaps-7a4d2b53-47b6-408a-974c-1ab9d2f3e300": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009599122s
STEP: Saw pod success 12/30/22 03:52:24.956
Dec 30 03:52:24.957: INFO: Pod "pod-projected-configmaps-7a4d2b53-47b6-408a-974c-1ab9d2f3e300" satisfied condition "Succeeded or Failed"
Dec 30 03:52:24.960: INFO: Trying to get logs from node k8s-mgmt01 pod pod-projected-configmaps-7a4d2b53-47b6-408a-974c-1ab9d2f3e300 container agnhost-container: <nil>
STEP: delete the pod 12/30/22 03:52:24.981
Dec 30 03:52:24.995: INFO: Waiting for pod pod-projected-configmaps-7a4d2b53-47b6-408a-974c-1ab9d2f3e300 to disappear
Dec 30 03:52:24.998: INFO: Pod pod-projected-configmaps-7a4d2b53-47b6-408a-974c-1ab9d2f3e300 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Dec 30 03:52:24.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4695" for this suite. 12/30/22 03:52:25.004
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":144,"skipped":2690,"failed":0}
------------------------------
• [4.109 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:52:20.901
    Dec 30 03:52:20.902: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename projected 12/30/22 03:52:20.902
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:52:20.915
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:52:20.918
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:98
    STEP: Creating configMap with name projected-configmap-test-volume-map-3c5e8433-01c8-4146-949f-b7f52c197313 12/30/22 03:52:20.921
    STEP: Creating a pod to test consume configMaps 12/30/22 03:52:20.935
    Dec 30 03:52:20.947: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7a4d2b53-47b6-408a-974c-1ab9d2f3e300" in namespace "projected-4695" to be "Succeeded or Failed"
    Dec 30 03:52:20.950: INFO: Pod "pod-projected-configmaps-7a4d2b53-47b6-408a-974c-1ab9d2f3e300": Phase="Pending", Reason="", readiness=false. Elapsed: 3.61578ms
    Dec 30 03:52:22.955: INFO: Pod "pod-projected-configmaps-7a4d2b53-47b6-408a-974c-1ab9d2f3e300": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00858891s
    Dec 30 03:52:24.956: INFO: Pod "pod-projected-configmaps-7a4d2b53-47b6-408a-974c-1ab9d2f3e300": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009599122s
    STEP: Saw pod success 12/30/22 03:52:24.956
    Dec 30 03:52:24.957: INFO: Pod "pod-projected-configmaps-7a4d2b53-47b6-408a-974c-1ab9d2f3e300" satisfied condition "Succeeded or Failed"
    Dec 30 03:52:24.960: INFO: Trying to get logs from node k8s-mgmt01 pod pod-projected-configmaps-7a4d2b53-47b6-408a-974c-1ab9d2f3e300 container agnhost-container: <nil>
    STEP: delete the pod 12/30/22 03:52:24.981
    Dec 30 03:52:24.995: INFO: Waiting for pod pod-projected-configmaps-7a4d2b53-47b6-408a-974c-1ab9d2f3e300 to disappear
    Dec 30 03:52:24.998: INFO: Pod pod-projected-configmaps-7a4d2b53-47b6-408a-974c-1ab9d2f3e300 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Dec 30 03:52:24.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4695" for this suite. 12/30/22 03:52:25.004
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:52:25.012
Dec 30 03:52:25.012: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename configmap 12/30/22 03:52:25.013
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:52:25.026
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:52:25.03
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168
STEP: creating a ConfigMap 12/30/22 03:52:25.033
STEP: fetching the ConfigMap 12/30/22 03:52:25.038
STEP: patching the ConfigMap 12/30/22 03:52:25.042
STEP: listing all ConfigMaps in all namespaces with a label selector 12/30/22 03:52:25.047
STEP: deleting the ConfigMap by collection with a label selector 12/30/22 03:52:25.052
STEP: listing all ConfigMaps in test namespace 12/30/22 03:52:25.061
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Dec 30 03:52:25.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4945" for this suite. 12/30/22 03:52:25.07
{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","completed":145,"skipped":2694,"failed":0}
------------------------------
• [0.066 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:52:25.012
    Dec 30 03:52:25.012: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename configmap 12/30/22 03:52:25.013
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:52:25.026
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:52:25.03
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:168
    STEP: creating a ConfigMap 12/30/22 03:52:25.033
    STEP: fetching the ConfigMap 12/30/22 03:52:25.038
    STEP: patching the ConfigMap 12/30/22 03:52:25.042
    STEP: listing all ConfigMaps in all namespaces with a label selector 12/30/22 03:52:25.047
    STEP: deleting the ConfigMap by collection with a label selector 12/30/22 03:52:25.052
    STEP: listing all ConfigMaps in test namespace 12/30/22 03:52:25.061
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Dec 30 03:52:25.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-4945" for this suite. 12/30/22 03:52:25.07
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:52:25.079
Dec 30 03:52:25.079: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename sched-pred 12/30/22 03:52:25.08
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:52:25.093
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:52:25.097
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Dec 30 03:52:25.100: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 30 03:52:25.111: INFO: Waiting for terminating namespaces to be deleted...
Dec 30 03:52:25.116: INFO: 
Logging pods the apiserver thinks is on node k8s-mgmt01 before test
Dec 30 03:52:25.130: INFO: calico-node-6xm98 from kube-system started at 2022-12-28 02:19:33 +0000 UTC (1 container statuses recorded)
Dec 30 03:52:25.130: INFO: 	Container calico-node ready: true, restart count 0
Dec 30 03:52:25.130: INFO: coredns-69dfc8446-pwfn2 from kube-system started at 2022-12-28 02:22:04 +0000 UTC (1 container statuses recorded)
Dec 30 03:52:25.130: INFO: 	Container coredns ready: true, restart count 0
Dec 30 03:52:25.130: INFO: dns-autoscaler-5b9959d7fc-s8g6t from kube-system started at 2022-12-28 02:22:11 +0000 UTC (1 container statuses recorded)
Dec 30 03:52:25.130: INFO: 	Container autoscaler ready: true, restart count 0
Dec 30 03:52:25.130: INFO: kube-apiserver-k8s-mgmt01 from kube-system started at 2022-12-28 02:15:50 +0000 UTC (1 container statuses recorded)
Dec 30 03:52:25.130: INFO: 	Container kube-apiserver ready: true, restart count 1
Dec 30 03:52:25.130: INFO: kube-controller-manager-k8s-mgmt01 from kube-system started at 2022-12-28 02:15:49 +0000 UTC (1 container statuses recorded)
Dec 30 03:52:25.130: INFO: 	Container kube-controller-manager ready: true, restart count 1
Dec 30 03:52:25.130: INFO: kube-proxy-tcvcl from kube-system started at 2022-12-28 02:18:09 +0000 UTC (1 container statuses recorded)
Dec 30 03:52:25.130: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 30 03:52:25.130: INFO: kube-scheduler-k8s-mgmt01 from kube-system started at 2022-12-28 02:15:38 +0000 UTC (1 container statuses recorded)
Dec 30 03:52:25.130: INFO: 	Container kube-scheduler ready: true, restart count 1
Dec 30 03:52:25.130: INFO: nodelocaldns-n9jl8 from kube-system started at 2022-12-28 02:22:14 +0000 UTC (1 container statuses recorded)
Dec 30 03:52:25.130: INFO: 	Container node-cache ready: true, restart count 0
Dec 30 03:52:25.130: INFO: sonobuoy-systemd-logs-daemon-set-4da877126f61426e-qd9td from sonobuoy started at 2022-12-30 03:20:18 +0000 UTC (2 container statuses recorded)
Dec 30 03:52:25.130: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 30 03:52:25.130: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 30 03:52:25.130: INFO: 
Logging pods the apiserver thinks is on node k8s-mgmt02 before test
Dec 30 03:52:25.143: INFO: calico-node-gwlv6 from kube-system started at 2022-12-28 02:19:33 +0000 UTC (1 container statuses recorded)
Dec 30 03:52:25.143: INFO: 	Container calico-node ready: true, restart count 0
Dec 30 03:52:25.143: INFO: kube-apiserver-k8s-mgmt02 from kube-system started at 2022-12-28 02:16:10 +0000 UTC (1 container statuses recorded)
Dec 30 03:52:25.143: INFO: 	Container kube-apiserver ready: true, restart count 1
Dec 30 03:52:25.143: INFO: kube-controller-manager-k8s-mgmt02 from kube-system started at 2022-12-28 02:16:10 +0000 UTC (1 container statuses recorded)
Dec 30 03:52:25.143: INFO: 	Container kube-controller-manager ready: true, restart count 1
Dec 30 03:52:25.143: INFO: kube-proxy-5lvmg from kube-system started at 2022-12-28 02:18:08 +0000 UTC (1 container statuses recorded)
Dec 30 03:52:25.143: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 30 03:52:25.143: INFO: kube-scheduler-k8s-mgmt02 from kube-system started at 2022-12-28 02:16:10 +0000 UTC (1 container statuses recorded)
Dec 30 03:52:25.143: INFO: 	Container kube-scheduler ready: true, restart count 1
Dec 30 03:52:25.143: INFO: metrics-server-67f489ffcf-srn44 from kube-system started at 2022-12-28 02:22:51 +0000 UTC (1 container statuses recorded)
Dec 30 03:52:25.143: INFO: 	Container metrics-server ready: true, restart count 0
Dec 30 03:52:25.143: INFO: nodelocaldns-bwgsf from kube-system started at 2022-12-28 02:22:14 +0000 UTC (1 container statuses recorded)
Dec 30 03:52:25.143: INFO: 	Container node-cache ready: true, restart count 0
Dec 30 03:52:25.143: INFO: sonobuoy from sonobuoy started at 2022-12-30 03:20:11 +0000 UTC (1 container statuses recorded)
Dec 30 03:52:25.143: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec 30 03:52:25.143: INFO: sonobuoy-systemd-logs-daemon-set-4da877126f61426e-llwcr from sonobuoy started at 2022-12-30 03:20:18 +0000 UTC (2 container statuses recorded)
Dec 30 03:52:25.143: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 30 03:52:25.143: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 30 03:52:25.143: INFO: 
Logging pods the apiserver thinks is on node k8s-mgmt03 before test
Dec 30 03:52:25.156: INFO: calico-kube-controllers-d6484b75c-8gdnt from kube-system started at 2022-12-28 02:21:06 +0000 UTC (1 container statuses recorded)
Dec 30 03:52:25.156: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Dec 30 03:52:25.156: INFO: calico-node-lqstj from kube-system started at 2022-12-28 02:19:33 +0000 UTC (1 container statuses recorded)
Dec 30 03:52:25.156: INFO: 	Container calico-node ready: true, restart count 0
Dec 30 03:52:25.156: INFO: coredns-69dfc8446-6gk67 from kube-system started at 2022-12-28 02:22:13 +0000 UTC (1 container statuses recorded)
Dec 30 03:52:25.156: INFO: 	Container coredns ready: true, restart count 0
Dec 30 03:52:25.156: INFO: kube-apiserver-k8s-mgmt03 from kube-system started at 2022-12-28 02:16:20 +0000 UTC (1 container statuses recorded)
Dec 30 03:52:25.156: INFO: 	Container kube-apiserver ready: true, restart count 1
Dec 30 03:52:25.156: INFO: kube-controller-manager-k8s-mgmt03 from kube-system started at 2022-12-28 02:16:20 +0000 UTC (1 container statuses recorded)
Dec 30 03:52:25.156: INFO: 	Container kube-controller-manager ready: true, restart count 1
Dec 30 03:52:25.156: INFO: kube-proxy-rfm26 from kube-system started at 2022-12-28 02:18:08 +0000 UTC (1 container statuses recorded)
Dec 30 03:52:25.156: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 30 03:52:25.156: INFO: kube-scheduler-k8s-mgmt03 from kube-system started at 2022-12-28 02:17:11 +0000 UTC (1 container statuses recorded)
Dec 30 03:52:25.156: INFO: 	Container kube-scheduler ready: true, restart count 1
Dec 30 03:52:25.156: INFO: nodelocaldns-76d47 from kube-system started at 2022-12-28 02:22:14 +0000 UTC (1 container statuses recorded)
Dec 30 03:52:25.156: INFO: 	Container node-cache ready: true, restart count 0
Dec 30 03:52:25.156: INFO: sonobuoy-e2e-job-c5efb7b0ce524286 from sonobuoy started at 2022-12-30 03:20:18 +0000 UTC (2 container statuses recorded)
Dec 30 03:52:25.156: INFO: 	Container e2e ready: true, restart count 0
Dec 30 03:52:25.156: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 30 03:52:25.156: INFO: sonobuoy-systemd-logs-daemon-set-4da877126f61426e-wfvd8 from sonobuoy started at 2022-12-30 03:20:18 +0000 UTC (2 container statuses recorded)
Dec 30 03:52:25.156: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 30 03:52:25.156: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 30 03:52:25.156: INFO: 
Logging pods the apiserver thinks is on node k8s-worker01 before test
Dec 30 03:52:25.168: INFO: calico-node-dtvd6 from kube-system started at 2022-12-28 02:19:33 +0000 UTC (1 container statuses recorded)
Dec 30 03:52:25.168: INFO: 	Container calico-node ready: true, restart count 0
Dec 30 03:52:25.168: INFO: kube-proxy-m86bn from kube-system started at 2022-12-28 02:18:09 +0000 UTC (1 container statuses recorded)
Dec 30 03:52:25.168: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 30 03:52:25.168: INFO: nginx-proxy-k8s-worker01 from kube-system started at 2022-12-28 02:17:47 +0000 UTC (1 container statuses recorded)
Dec 30 03:52:25.168: INFO: 	Container nginx-proxy ready: true, restart count 0
Dec 30 03:52:25.168: INFO: nodelocaldns-8zm2b from kube-system started at 2022-12-28 02:22:14 +0000 UTC (1 container statuses recorded)
Dec 30 03:52:25.168: INFO: 	Container node-cache ready: true, restart count 0
Dec 30 03:52:25.168: INFO: pfpod2 from limitrange-9004 started at 2022-12-30 03:51:49 +0000 UTC (1 container statuses recorded)
Dec 30 03:52:25.168: INFO: 	Container pause ready: false, restart count 0
Dec 30 03:52:25.168: INFO: sonobuoy-systemd-logs-daemon-set-4da877126f61426e-4wpvd from sonobuoy started at 2022-12-30 03:20:18 +0000 UTC (2 container statuses recorded)
Dec 30 03:52:25.168: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 30 03:52:25.168: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 30 03:52:25.168: INFO: 
Logging pods the apiserver thinks is on node k8s-worker02 before test
Dec 30 03:52:25.180: INFO: calico-node-s7npn from kube-system started at 2022-12-28 02:19:33 +0000 UTC (1 container statuses recorded)
Dec 30 03:52:25.180: INFO: 	Container calico-node ready: true, restart count 0
Dec 30 03:52:25.180: INFO: kube-proxy-8cg4d from kube-system started at 2022-12-28 02:18:09 +0000 UTC (1 container statuses recorded)
Dec 30 03:52:25.180: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 30 03:52:25.180: INFO: nginx-proxy-k8s-worker02 from kube-system started at 2022-12-28 02:17:48 +0000 UTC (1 container statuses recorded)
Dec 30 03:52:25.180: INFO: 	Container nginx-proxy ready: true, restart count 0
Dec 30 03:52:25.180: INFO: nodelocaldns-4hlqv from kube-system started at 2022-12-28 02:22:14 +0000 UTC (1 container statuses recorded)
Dec 30 03:52:25.180: INFO: 	Container node-cache ready: true, restart count 0
Dec 30 03:52:25.180: INFO: sonobuoy-systemd-logs-daemon-set-4da877126f61426e-7mpss from sonobuoy started at 2022-12-30 03:20:18 +0000 UTC (2 container statuses recorded)
Dec 30 03:52:25.180: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 30 03:52:25.180: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326
STEP: verifying the node has the label node k8s-mgmt01 12/30/22 03:52:25.214
STEP: verifying the node has the label node k8s-mgmt02 12/30/22 03:52:25.232
STEP: verifying the node has the label node k8s-mgmt03 12/30/22 03:52:25.247
STEP: verifying the node has the label node k8s-worker01 12/30/22 03:52:25.263
STEP: verifying the node has the label node k8s-worker02 12/30/22 03:52:25.279
Dec 30 03:52:25.306: INFO: Pod calico-kube-controllers-d6484b75c-8gdnt requesting resource cpu=30m on Node k8s-mgmt03
Dec 30 03:52:25.306: INFO: Pod calico-node-6xm98 requesting resource cpu=150m on Node k8s-mgmt01
Dec 30 03:52:25.306: INFO: Pod calico-node-dtvd6 requesting resource cpu=150m on Node k8s-worker01
Dec 30 03:52:25.306: INFO: Pod calico-node-gwlv6 requesting resource cpu=150m on Node k8s-mgmt02
Dec 30 03:52:25.306: INFO: Pod calico-node-lqstj requesting resource cpu=150m on Node k8s-mgmt03
Dec 30 03:52:25.306: INFO: Pod calico-node-s7npn requesting resource cpu=150m on Node k8s-worker02
Dec 30 03:52:25.306: INFO: Pod coredns-69dfc8446-6gk67 requesting resource cpu=100m on Node k8s-mgmt03
Dec 30 03:52:25.306: INFO: Pod coredns-69dfc8446-pwfn2 requesting resource cpu=100m on Node k8s-mgmt01
Dec 30 03:52:25.306: INFO: Pod dns-autoscaler-5b9959d7fc-s8g6t requesting resource cpu=20m on Node k8s-mgmt01
Dec 30 03:52:25.306: INFO: Pod kube-apiserver-k8s-mgmt01 requesting resource cpu=250m on Node k8s-mgmt01
Dec 30 03:52:25.306: INFO: Pod kube-apiserver-k8s-mgmt02 requesting resource cpu=250m on Node k8s-mgmt02
Dec 30 03:52:25.306: INFO: Pod kube-apiserver-k8s-mgmt03 requesting resource cpu=250m on Node k8s-mgmt03
Dec 30 03:52:25.306: INFO: Pod kube-controller-manager-k8s-mgmt01 requesting resource cpu=200m on Node k8s-mgmt01
Dec 30 03:52:25.306: INFO: Pod kube-controller-manager-k8s-mgmt02 requesting resource cpu=200m on Node k8s-mgmt02
Dec 30 03:52:25.306: INFO: Pod kube-controller-manager-k8s-mgmt03 requesting resource cpu=200m on Node k8s-mgmt03
Dec 30 03:52:25.306: INFO: Pod kube-proxy-5lvmg requesting resource cpu=0m on Node k8s-mgmt02
Dec 30 03:52:25.306: INFO: Pod kube-proxy-8cg4d requesting resource cpu=0m on Node k8s-worker02
Dec 30 03:52:25.306: INFO: Pod kube-proxy-m86bn requesting resource cpu=0m on Node k8s-worker01
Dec 30 03:52:25.306: INFO: Pod kube-proxy-rfm26 requesting resource cpu=0m on Node k8s-mgmt03
Dec 30 03:52:25.306: INFO: Pod kube-proxy-tcvcl requesting resource cpu=0m on Node k8s-mgmt01
Dec 30 03:52:25.306: INFO: Pod kube-scheduler-k8s-mgmt01 requesting resource cpu=100m on Node k8s-mgmt01
Dec 30 03:52:25.306: INFO: Pod kube-scheduler-k8s-mgmt02 requesting resource cpu=100m on Node k8s-mgmt02
Dec 30 03:52:25.306: INFO: Pod kube-scheduler-k8s-mgmt03 requesting resource cpu=100m on Node k8s-mgmt03
Dec 30 03:52:25.306: INFO: Pod metrics-server-67f489ffcf-srn44 requesting resource cpu=100m on Node k8s-mgmt02
Dec 30 03:52:25.306: INFO: Pod nginx-proxy-k8s-worker01 requesting resource cpu=25m on Node k8s-worker01
Dec 30 03:52:25.306: INFO: Pod nginx-proxy-k8s-worker02 requesting resource cpu=25m on Node k8s-worker02
Dec 30 03:52:25.306: INFO: Pod nodelocaldns-4hlqv requesting resource cpu=100m on Node k8s-worker02
Dec 30 03:52:25.306: INFO: Pod nodelocaldns-76d47 requesting resource cpu=100m on Node k8s-mgmt03
Dec 30 03:52:25.306: INFO: Pod nodelocaldns-8zm2b requesting resource cpu=100m on Node k8s-worker01
Dec 30 03:52:25.306: INFO: Pod nodelocaldns-bwgsf requesting resource cpu=100m on Node k8s-mgmt02
Dec 30 03:52:25.306: INFO: Pod nodelocaldns-n9jl8 requesting resource cpu=100m on Node k8s-mgmt01
Dec 30 03:52:25.306: INFO: Pod pfpod2 requesting resource cpu=600m on Node k8s-worker01
Dec 30 03:52:25.306: INFO: Pod sonobuoy requesting resource cpu=0m on Node k8s-mgmt02
Dec 30 03:52:25.306: INFO: Pod sonobuoy-e2e-job-c5efb7b0ce524286 requesting resource cpu=0m on Node k8s-mgmt03
Dec 30 03:52:25.306: INFO: Pod sonobuoy-systemd-logs-daemon-set-4da877126f61426e-4wpvd requesting resource cpu=0m on Node k8s-worker01
Dec 30 03:52:25.306: INFO: Pod sonobuoy-systemd-logs-daemon-set-4da877126f61426e-7mpss requesting resource cpu=0m on Node k8s-worker02
Dec 30 03:52:25.306: INFO: Pod sonobuoy-systemd-logs-daemon-set-4da877126f61426e-llwcr requesting resource cpu=0m on Node k8s-mgmt02
Dec 30 03:52:25.306: INFO: Pod sonobuoy-systemd-logs-daemon-set-4da877126f61426e-qd9td requesting resource cpu=0m on Node k8s-mgmt01
Dec 30 03:52:25.306: INFO: Pod sonobuoy-systemd-logs-daemon-set-4da877126f61426e-wfvd8 requesting resource cpu=0m on Node k8s-mgmt03
STEP: Starting Pods to consume most of the cluster CPU. 12/30/22 03:52:25.306
Dec 30 03:52:25.307: INFO: Creating a pod which consumes cpu=4917m on Node k8s-worker01
Dec 30 03:52:25.317: INFO: Creating a pod which consumes cpu=5337m on Node k8s-worker02
Dec 30 03:52:25.323: INFO: Creating a pod which consumes cpu=21616m on Node k8s-mgmt01
Dec 30 03:52:25.328: INFO: Creating a pod which consumes cpu=21630m on Node k8s-mgmt02
Dec 30 03:52:25.334: INFO: Creating a pod which consumes cpu=21609m on Node k8s-mgmt03
Dec 30 03:52:25.340: INFO: Waiting up to 5m0s for pod "filler-pod-e4f1eb2e-2754-47d9-bef1-46f62e0352bc" in namespace "sched-pred-1219" to be "running"
Dec 30 03:52:25.343: INFO: Pod "filler-pod-e4f1eb2e-2754-47d9-bef1-46f62e0352bc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.139617ms
Dec 30 03:52:27.348: INFO: Pod "filler-pod-e4f1eb2e-2754-47d9-bef1-46f62e0352bc": Phase="Running", Reason="", readiness=true. Elapsed: 2.008675612s
Dec 30 03:52:27.349: INFO: Pod "filler-pod-e4f1eb2e-2754-47d9-bef1-46f62e0352bc" satisfied condition "running"
Dec 30 03:52:27.349: INFO: Waiting up to 5m0s for pod "filler-pod-22f4c009-577e-4246-8a83-21a2cc4ce9bc" in namespace "sched-pred-1219" to be "running"
Dec 30 03:52:27.352: INFO: Pod "filler-pod-22f4c009-577e-4246-8a83-21a2cc4ce9bc": Phase="Running", Reason="", readiness=true. Elapsed: 3.877858ms
Dec 30 03:52:27.352: INFO: Pod "filler-pod-22f4c009-577e-4246-8a83-21a2cc4ce9bc" satisfied condition "running"
Dec 30 03:52:27.352: INFO: Waiting up to 5m0s for pod "filler-pod-44521eaa-e78c-4919-a997-e9a6a815715e" in namespace "sched-pred-1219" to be "running"
Dec 30 03:52:27.356: INFO: Pod "filler-pod-44521eaa-e78c-4919-a997-e9a6a815715e": Phase="Running", Reason="", readiness=true. Elapsed: 3.307433ms
Dec 30 03:52:27.356: INFO: Pod "filler-pod-44521eaa-e78c-4919-a997-e9a6a815715e" satisfied condition "running"
Dec 30 03:52:27.356: INFO: Waiting up to 5m0s for pod "filler-pod-099548e8-55a5-4754-bc15-fd370b9cb2c4" in namespace "sched-pred-1219" to be "running"
Dec 30 03:52:27.360: INFO: Pod "filler-pod-099548e8-55a5-4754-bc15-fd370b9cb2c4": Phase="Running", Reason="", readiness=true. Elapsed: 4.14338ms
Dec 30 03:52:27.360: INFO: Pod "filler-pod-099548e8-55a5-4754-bc15-fd370b9cb2c4" satisfied condition "running"
Dec 30 03:52:27.360: INFO: Waiting up to 5m0s for pod "filler-pod-7ae1be45-b522-4c00-a01e-e39f3270fe8d" in namespace "sched-pred-1219" to be "running"
Dec 30 03:52:27.364: INFO: Pod "filler-pod-7ae1be45-b522-4c00-a01e-e39f3270fe8d": Phase="Running", Reason="", readiness=true. Elapsed: 3.655583ms
Dec 30 03:52:27.364: INFO: Pod "filler-pod-7ae1be45-b522-4c00-a01e-e39f3270fe8d" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 12/30/22 03:52:27.364
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-099548e8-55a5-4754-bc15-fd370b9cb2c4.173575c99861f17b], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1219/filler-pod-099548e8-55a5-4754-bc15-fd370b9cb2c4 to k8s-mgmt02] 12/30/22 03:52:27.37
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-099548e8-55a5-4754-bc15-fd370b9cb2c4.173575c9c7203005], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 12/30/22 03:52:27.37
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-099548e8-55a5-4754-bc15-fd370b9cb2c4.173575c9c7d5e9db], Reason = [Created], Message = [Created container filler-pod-099548e8-55a5-4754-bc15-fd370b9cb2c4] 12/30/22 03:52:27.37
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-099548e8-55a5-4754-bc15-fd370b9cb2c4.173575c9d064e9d2], Reason = [Started], Message = [Started container filler-pod-099548e8-55a5-4754-bc15-fd370b9cb2c4] 12/30/22 03:52:27.37
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-22f4c009-577e-4246-8a83-21a2cc4ce9bc.173575c997ab3436], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1219/filler-pod-22f4c009-577e-4246-8a83-21a2cc4ce9bc to k8s-worker02] 12/30/22 03:52:27.37
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-22f4c009-577e-4246-8a83-21a2cc4ce9bc.173575c9d37f80e2], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 12/30/22 03:52:27.37
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-22f4c009-577e-4246-8a83-21a2cc4ce9bc.173575c9e4030984], Reason = [Created], Message = [Created container filler-pod-22f4c009-577e-4246-8a83-21a2cc4ce9bc] 12/30/22 03:52:27.371
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-22f4c009-577e-4246-8a83-21a2cc4ce9bc.173575c9ee03d67f], Reason = [Started], Message = [Started container filler-pod-22f4c009-577e-4246-8a83-21a2cc4ce9bc] 12/30/22 03:52:27.371
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-44521eaa-e78c-4919-a997-e9a6a815715e.173575c99800debc], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1219/filler-pod-44521eaa-e78c-4919-a997-e9a6a815715e to k8s-mgmt01] 12/30/22 03:52:27.371
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-44521eaa-e78c-4919-a997-e9a6a815715e.173575c9c6b93d96], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 12/30/22 03:52:27.371
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-44521eaa-e78c-4919-a997-e9a6a815715e.173575c9c77a1e03], Reason = [Created], Message = [Created container filler-pod-44521eaa-e78c-4919-a997-e9a6a815715e] 12/30/22 03:52:27.371
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-44521eaa-e78c-4919-a997-e9a6a815715e.173575c9d043c8f0], Reason = [Started], Message = [Started container filler-pod-44521eaa-e78c-4919-a997-e9a6a815715e] 12/30/22 03:52:27.371
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7ae1be45-b522-4c00-a01e-e39f3270fe8d.173575c998bc7bf3], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1219/filler-pod-7ae1be45-b522-4c00-a01e-e39f3270fe8d to k8s-mgmt03] 12/30/22 03:52:27.371
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7ae1be45-b522-4c00-a01e-e39f3270fe8d.173575c9c9568ac3], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 12/30/22 03:52:27.372
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7ae1be45-b522-4c00-a01e-e39f3270fe8d.173575c9ca0fc15c], Reason = [Created], Message = [Created container filler-pod-7ae1be45-b522-4c00-a01e-e39f3270fe8d] 12/30/22 03:52:27.372
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7ae1be45-b522-4c00-a01e-e39f3270fe8d.173575c9d34b2d6a], Reason = [Started], Message = [Started container filler-pod-7ae1be45-b522-4c00-a01e-e39f3270fe8d] 12/30/22 03:52:27.372
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e4f1eb2e-2754-47d9-bef1-46f62e0352bc.173575c9975956c5], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1219/filler-pod-e4f1eb2e-2754-47d9-bef1-46f62e0352bc to k8s-worker01] 12/30/22 03:52:27.372
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e4f1eb2e-2754-47d9-bef1-46f62e0352bc.173575c9cc6d3e08], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 12/30/22 03:52:27.372
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e4f1eb2e-2754-47d9-bef1-46f62e0352bc.173575c9d92079c0], Reason = [Created], Message = [Created container filler-pod-e4f1eb2e-2754-47d9-bef1-46f62e0352bc] 12/30/22 03:52:27.372
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e4f1eb2e-2754-47d9-bef1-46f62e0352bc.173575c9e19ff3d9], Reason = [Started], Message = [Started container filler-pod-e4f1eb2e-2754-47d9-bef1-46f62e0352bc] 12/30/22 03:52:27.372
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.173575ca11c57714], Reason = [FailedScheduling], Message = [0/5 nodes are available: 5 Insufficient cpu. preemption: 0/5 nodes are available: 5 No preemption victims found for incoming pod.] 12/30/22 03:52:27.381
STEP: removing the label node off the node k8s-mgmt03 12/30/22 03:52:28.385
STEP: verifying the node doesn't have the label node 12/30/22 03:52:28.4
STEP: removing the label node off the node k8s-worker01 12/30/22 03:52:28.404
STEP: verifying the node doesn't have the label node 12/30/22 03:52:28.419
STEP: removing the label node off the node k8s-worker02 12/30/22 03:52:28.424
STEP: verifying the node doesn't have the label node 12/30/22 03:52:28.438
STEP: removing the label node off the node k8s-mgmt01 12/30/22 03:52:28.443
STEP: verifying the node doesn't have the label node 12/30/22 03:52:28.457
STEP: removing the label node off the node k8s-mgmt02 12/30/22 03:52:28.462
STEP: verifying the node doesn't have the label node 12/30/22 03:52:28.476
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Dec 30 03:52:28.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1219" for this suite. 12/30/22 03:52:28.486
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","completed":146,"skipped":2708,"failed":0}
------------------------------
• [3.414 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:52:25.079
    Dec 30 03:52:25.079: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename sched-pred 12/30/22 03:52:25.08
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:52:25.093
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:52:25.097
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Dec 30 03:52:25.100: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Dec 30 03:52:25.111: INFO: Waiting for terminating namespaces to be deleted...
    Dec 30 03:52:25.116: INFO: 
    Logging pods the apiserver thinks is on node k8s-mgmt01 before test
    Dec 30 03:52:25.130: INFO: calico-node-6xm98 from kube-system started at 2022-12-28 02:19:33 +0000 UTC (1 container statuses recorded)
    Dec 30 03:52:25.130: INFO: 	Container calico-node ready: true, restart count 0
    Dec 30 03:52:25.130: INFO: coredns-69dfc8446-pwfn2 from kube-system started at 2022-12-28 02:22:04 +0000 UTC (1 container statuses recorded)
    Dec 30 03:52:25.130: INFO: 	Container coredns ready: true, restart count 0
    Dec 30 03:52:25.130: INFO: dns-autoscaler-5b9959d7fc-s8g6t from kube-system started at 2022-12-28 02:22:11 +0000 UTC (1 container statuses recorded)
    Dec 30 03:52:25.130: INFO: 	Container autoscaler ready: true, restart count 0
    Dec 30 03:52:25.130: INFO: kube-apiserver-k8s-mgmt01 from kube-system started at 2022-12-28 02:15:50 +0000 UTC (1 container statuses recorded)
    Dec 30 03:52:25.130: INFO: 	Container kube-apiserver ready: true, restart count 1
    Dec 30 03:52:25.130: INFO: kube-controller-manager-k8s-mgmt01 from kube-system started at 2022-12-28 02:15:49 +0000 UTC (1 container statuses recorded)
    Dec 30 03:52:25.130: INFO: 	Container kube-controller-manager ready: true, restart count 1
    Dec 30 03:52:25.130: INFO: kube-proxy-tcvcl from kube-system started at 2022-12-28 02:18:09 +0000 UTC (1 container statuses recorded)
    Dec 30 03:52:25.130: INFO: 	Container kube-proxy ready: true, restart count 0
    Dec 30 03:52:25.130: INFO: kube-scheduler-k8s-mgmt01 from kube-system started at 2022-12-28 02:15:38 +0000 UTC (1 container statuses recorded)
    Dec 30 03:52:25.130: INFO: 	Container kube-scheduler ready: true, restart count 1
    Dec 30 03:52:25.130: INFO: nodelocaldns-n9jl8 from kube-system started at 2022-12-28 02:22:14 +0000 UTC (1 container statuses recorded)
    Dec 30 03:52:25.130: INFO: 	Container node-cache ready: true, restart count 0
    Dec 30 03:52:25.130: INFO: sonobuoy-systemd-logs-daemon-set-4da877126f61426e-qd9td from sonobuoy started at 2022-12-30 03:20:18 +0000 UTC (2 container statuses recorded)
    Dec 30 03:52:25.130: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Dec 30 03:52:25.130: INFO: 	Container systemd-logs ready: true, restart count 0
    Dec 30 03:52:25.130: INFO: 
    Logging pods the apiserver thinks is on node k8s-mgmt02 before test
    Dec 30 03:52:25.143: INFO: calico-node-gwlv6 from kube-system started at 2022-12-28 02:19:33 +0000 UTC (1 container statuses recorded)
    Dec 30 03:52:25.143: INFO: 	Container calico-node ready: true, restart count 0
    Dec 30 03:52:25.143: INFO: kube-apiserver-k8s-mgmt02 from kube-system started at 2022-12-28 02:16:10 +0000 UTC (1 container statuses recorded)
    Dec 30 03:52:25.143: INFO: 	Container kube-apiserver ready: true, restart count 1
    Dec 30 03:52:25.143: INFO: kube-controller-manager-k8s-mgmt02 from kube-system started at 2022-12-28 02:16:10 +0000 UTC (1 container statuses recorded)
    Dec 30 03:52:25.143: INFO: 	Container kube-controller-manager ready: true, restart count 1
    Dec 30 03:52:25.143: INFO: kube-proxy-5lvmg from kube-system started at 2022-12-28 02:18:08 +0000 UTC (1 container statuses recorded)
    Dec 30 03:52:25.143: INFO: 	Container kube-proxy ready: true, restart count 0
    Dec 30 03:52:25.143: INFO: kube-scheduler-k8s-mgmt02 from kube-system started at 2022-12-28 02:16:10 +0000 UTC (1 container statuses recorded)
    Dec 30 03:52:25.143: INFO: 	Container kube-scheduler ready: true, restart count 1
    Dec 30 03:52:25.143: INFO: metrics-server-67f489ffcf-srn44 from kube-system started at 2022-12-28 02:22:51 +0000 UTC (1 container statuses recorded)
    Dec 30 03:52:25.143: INFO: 	Container metrics-server ready: true, restart count 0
    Dec 30 03:52:25.143: INFO: nodelocaldns-bwgsf from kube-system started at 2022-12-28 02:22:14 +0000 UTC (1 container statuses recorded)
    Dec 30 03:52:25.143: INFO: 	Container node-cache ready: true, restart count 0
    Dec 30 03:52:25.143: INFO: sonobuoy from sonobuoy started at 2022-12-30 03:20:11 +0000 UTC (1 container statuses recorded)
    Dec 30 03:52:25.143: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Dec 30 03:52:25.143: INFO: sonobuoy-systemd-logs-daemon-set-4da877126f61426e-llwcr from sonobuoy started at 2022-12-30 03:20:18 +0000 UTC (2 container statuses recorded)
    Dec 30 03:52:25.143: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Dec 30 03:52:25.143: INFO: 	Container systemd-logs ready: true, restart count 0
    Dec 30 03:52:25.143: INFO: 
    Logging pods the apiserver thinks is on node k8s-mgmt03 before test
    Dec 30 03:52:25.156: INFO: calico-kube-controllers-d6484b75c-8gdnt from kube-system started at 2022-12-28 02:21:06 +0000 UTC (1 container statuses recorded)
    Dec 30 03:52:25.156: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Dec 30 03:52:25.156: INFO: calico-node-lqstj from kube-system started at 2022-12-28 02:19:33 +0000 UTC (1 container statuses recorded)
    Dec 30 03:52:25.156: INFO: 	Container calico-node ready: true, restart count 0
    Dec 30 03:52:25.156: INFO: coredns-69dfc8446-6gk67 from kube-system started at 2022-12-28 02:22:13 +0000 UTC (1 container statuses recorded)
    Dec 30 03:52:25.156: INFO: 	Container coredns ready: true, restart count 0
    Dec 30 03:52:25.156: INFO: kube-apiserver-k8s-mgmt03 from kube-system started at 2022-12-28 02:16:20 +0000 UTC (1 container statuses recorded)
    Dec 30 03:52:25.156: INFO: 	Container kube-apiserver ready: true, restart count 1
    Dec 30 03:52:25.156: INFO: kube-controller-manager-k8s-mgmt03 from kube-system started at 2022-12-28 02:16:20 +0000 UTC (1 container statuses recorded)
    Dec 30 03:52:25.156: INFO: 	Container kube-controller-manager ready: true, restart count 1
    Dec 30 03:52:25.156: INFO: kube-proxy-rfm26 from kube-system started at 2022-12-28 02:18:08 +0000 UTC (1 container statuses recorded)
    Dec 30 03:52:25.156: INFO: 	Container kube-proxy ready: true, restart count 0
    Dec 30 03:52:25.156: INFO: kube-scheduler-k8s-mgmt03 from kube-system started at 2022-12-28 02:17:11 +0000 UTC (1 container statuses recorded)
    Dec 30 03:52:25.156: INFO: 	Container kube-scheduler ready: true, restart count 1
    Dec 30 03:52:25.156: INFO: nodelocaldns-76d47 from kube-system started at 2022-12-28 02:22:14 +0000 UTC (1 container statuses recorded)
    Dec 30 03:52:25.156: INFO: 	Container node-cache ready: true, restart count 0
    Dec 30 03:52:25.156: INFO: sonobuoy-e2e-job-c5efb7b0ce524286 from sonobuoy started at 2022-12-30 03:20:18 +0000 UTC (2 container statuses recorded)
    Dec 30 03:52:25.156: INFO: 	Container e2e ready: true, restart count 0
    Dec 30 03:52:25.156: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Dec 30 03:52:25.156: INFO: sonobuoy-systemd-logs-daemon-set-4da877126f61426e-wfvd8 from sonobuoy started at 2022-12-30 03:20:18 +0000 UTC (2 container statuses recorded)
    Dec 30 03:52:25.156: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Dec 30 03:52:25.156: INFO: 	Container systemd-logs ready: true, restart count 0
    Dec 30 03:52:25.156: INFO: 
    Logging pods the apiserver thinks is on node k8s-worker01 before test
    Dec 30 03:52:25.168: INFO: calico-node-dtvd6 from kube-system started at 2022-12-28 02:19:33 +0000 UTC (1 container statuses recorded)
    Dec 30 03:52:25.168: INFO: 	Container calico-node ready: true, restart count 0
    Dec 30 03:52:25.168: INFO: kube-proxy-m86bn from kube-system started at 2022-12-28 02:18:09 +0000 UTC (1 container statuses recorded)
    Dec 30 03:52:25.168: INFO: 	Container kube-proxy ready: true, restart count 0
    Dec 30 03:52:25.168: INFO: nginx-proxy-k8s-worker01 from kube-system started at 2022-12-28 02:17:47 +0000 UTC (1 container statuses recorded)
    Dec 30 03:52:25.168: INFO: 	Container nginx-proxy ready: true, restart count 0
    Dec 30 03:52:25.168: INFO: nodelocaldns-8zm2b from kube-system started at 2022-12-28 02:22:14 +0000 UTC (1 container statuses recorded)
    Dec 30 03:52:25.168: INFO: 	Container node-cache ready: true, restart count 0
    Dec 30 03:52:25.168: INFO: pfpod2 from limitrange-9004 started at 2022-12-30 03:51:49 +0000 UTC (1 container statuses recorded)
    Dec 30 03:52:25.168: INFO: 	Container pause ready: false, restart count 0
    Dec 30 03:52:25.168: INFO: sonobuoy-systemd-logs-daemon-set-4da877126f61426e-4wpvd from sonobuoy started at 2022-12-30 03:20:18 +0000 UTC (2 container statuses recorded)
    Dec 30 03:52:25.168: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Dec 30 03:52:25.168: INFO: 	Container systemd-logs ready: true, restart count 0
    Dec 30 03:52:25.168: INFO: 
    Logging pods the apiserver thinks is on node k8s-worker02 before test
    Dec 30 03:52:25.180: INFO: calico-node-s7npn from kube-system started at 2022-12-28 02:19:33 +0000 UTC (1 container statuses recorded)
    Dec 30 03:52:25.180: INFO: 	Container calico-node ready: true, restart count 0
    Dec 30 03:52:25.180: INFO: kube-proxy-8cg4d from kube-system started at 2022-12-28 02:18:09 +0000 UTC (1 container statuses recorded)
    Dec 30 03:52:25.180: INFO: 	Container kube-proxy ready: true, restart count 0
    Dec 30 03:52:25.180: INFO: nginx-proxy-k8s-worker02 from kube-system started at 2022-12-28 02:17:48 +0000 UTC (1 container statuses recorded)
    Dec 30 03:52:25.180: INFO: 	Container nginx-proxy ready: true, restart count 0
    Dec 30 03:52:25.180: INFO: nodelocaldns-4hlqv from kube-system started at 2022-12-28 02:22:14 +0000 UTC (1 container statuses recorded)
    Dec 30 03:52:25.180: INFO: 	Container node-cache ready: true, restart count 0
    Dec 30 03:52:25.180: INFO: sonobuoy-systemd-logs-daemon-set-4da877126f61426e-7mpss from sonobuoy started at 2022-12-30 03:20:18 +0000 UTC (2 container statuses recorded)
    Dec 30 03:52:25.180: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Dec 30 03:52:25.180: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:326
    STEP: verifying the node has the label node k8s-mgmt01 12/30/22 03:52:25.214
    STEP: verifying the node has the label node k8s-mgmt02 12/30/22 03:52:25.232
    STEP: verifying the node has the label node k8s-mgmt03 12/30/22 03:52:25.247
    STEP: verifying the node has the label node k8s-worker01 12/30/22 03:52:25.263
    STEP: verifying the node has the label node k8s-worker02 12/30/22 03:52:25.279
    Dec 30 03:52:25.306: INFO: Pod calico-kube-controllers-d6484b75c-8gdnt requesting resource cpu=30m on Node k8s-mgmt03
    Dec 30 03:52:25.306: INFO: Pod calico-node-6xm98 requesting resource cpu=150m on Node k8s-mgmt01
    Dec 30 03:52:25.306: INFO: Pod calico-node-dtvd6 requesting resource cpu=150m on Node k8s-worker01
    Dec 30 03:52:25.306: INFO: Pod calico-node-gwlv6 requesting resource cpu=150m on Node k8s-mgmt02
    Dec 30 03:52:25.306: INFO: Pod calico-node-lqstj requesting resource cpu=150m on Node k8s-mgmt03
    Dec 30 03:52:25.306: INFO: Pod calico-node-s7npn requesting resource cpu=150m on Node k8s-worker02
    Dec 30 03:52:25.306: INFO: Pod coredns-69dfc8446-6gk67 requesting resource cpu=100m on Node k8s-mgmt03
    Dec 30 03:52:25.306: INFO: Pod coredns-69dfc8446-pwfn2 requesting resource cpu=100m on Node k8s-mgmt01
    Dec 30 03:52:25.306: INFO: Pod dns-autoscaler-5b9959d7fc-s8g6t requesting resource cpu=20m on Node k8s-mgmt01
    Dec 30 03:52:25.306: INFO: Pod kube-apiserver-k8s-mgmt01 requesting resource cpu=250m on Node k8s-mgmt01
    Dec 30 03:52:25.306: INFO: Pod kube-apiserver-k8s-mgmt02 requesting resource cpu=250m on Node k8s-mgmt02
    Dec 30 03:52:25.306: INFO: Pod kube-apiserver-k8s-mgmt03 requesting resource cpu=250m on Node k8s-mgmt03
    Dec 30 03:52:25.306: INFO: Pod kube-controller-manager-k8s-mgmt01 requesting resource cpu=200m on Node k8s-mgmt01
    Dec 30 03:52:25.306: INFO: Pod kube-controller-manager-k8s-mgmt02 requesting resource cpu=200m on Node k8s-mgmt02
    Dec 30 03:52:25.306: INFO: Pod kube-controller-manager-k8s-mgmt03 requesting resource cpu=200m on Node k8s-mgmt03
    Dec 30 03:52:25.306: INFO: Pod kube-proxy-5lvmg requesting resource cpu=0m on Node k8s-mgmt02
    Dec 30 03:52:25.306: INFO: Pod kube-proxy-8cg4d requesting resource cpu=0m on Node k8s-worker02
    Dec 30 03:52:25.306: INFO: Pod kube-proxy-m86bn requesting resource cpu=0m on Node k8s-worker01
    Dec 30 03:52:25.306: INFO: Pod kube-proxy-rfm26 requesting resource cpu=0m on Node k8s-mgmt03
    Dec 30 03:52:25.306: INFO: Pod kube-proxy-tcvcl requesting resource cpu=0m on Node k8s-mgmt01
    Dec 30 03:52:25.306: INFO: Pod kube-scheduler-k8s-mgmt01 requesting resource cpu=100m on Node k8s-mgmt01
    Dec 30 03:52:25.306: INFO: Pod kube-scheduler-k8s-mgmt02 requesting resource cpu=100m on Node k8s-mgmt02
    Dec 30 03:52:25.306: INFO: Pod kube-scheduler-k8s-mgmt03 requesting resource cpu=100m on Node k8s-mgmt03
    Dec 30 03:52:25.306: INFO: Pod metrics-server-67f489ffcf-srn44 requesting resource cpu=100m on Node k8s-mgmt02
    Dec 30 03:52:25.306: INFO: Pod nginx-proxy-k8s-worker01 requesting resource cpu=25m on Node k8s-worker01
    Dec 30 03:52:25.306: INFO: Pod nginx-proxy-k8s-worker02 requesting resource cpu=25m on Node k8s-worker02
    Dec 30 03:52:25.306: INFO: Pod nodelocaldns-4hlqv requesting resource cpu=100m on Node k8s-worker02
    Dec 30 03:52:25.306: INFO: Pod nodelocaldns-76d47 requesting resource cpu=100m on Node k8s-mgmt03
    Dec 30 03:52:25.306: INFO: Pod nodelocaldns-8zm2b requesting resource cpu=100m on Node k8s-worker01
    Dec 30 03:52:25.306: INFO: Pod nodelocaldns-bwgsf requesting resource cpu=100m on Node k8s-mgmt02
    Dec 30 03:52:25.306: INFO: Pod nodelocaldns-n9jl8 requesting resource cpu=100m on Node k8s-mgmt01
    Dec 30 03:52:25.306: INFO: Pod pfpod2 requesting resource cpu=600m on Node k8s-worker01
    Dec 30 03:52:25.306: INFO: Pod sonobuoy requesting resource cpu=0m on Node k8s-mgmt02
    Dec 30 03:52:25.306: INFO: Pod sonobuoy-e2e-job-c5efb7b0ce524286 requesting resource cpu=0m on Node k8s-mgmt03
    Dec 30 03:52:25.306: INFO: Pod sonobuoy-systemd-logs-daemon-set-4da877126f61426e-4wpvd requesting resource cpu=0m on Node k8s-worker01
    Dec 30 03:52:25.306: INFO: Pod sonobuoy-systemd-logs-daemon-set-4da877126f61426e-7mpss requesting resource cpu=0m on Node k8s-worker02
    Dec 30 03:52:25.306: INFO: Pod sonobuoy-systemd-logs-daemon-set-4da877126f61426e-llwcr requesting resource cpu=0m on Node k8s-mgmt02
    Dec 30 03:52:25.306: INFO: Pod sonobuoy-systemd-logs-daemon-set-4da877126f61426e-qd9td requesting resource cpu=0m on Node k8s-mgmt01
    Dec 30 03:52:25.306: INFO: Pod sonobuoy-systemd-logs-daemon-set-4da877126f61426e-wfvd8 requesting resource cpu=0m on Node k8s-mgmt03
    STEP: Starting Pods to consume most of the cluster CPU. 12/30/22 03:52:25.306
    Dec 30 03:52:25.307: INFO: Creating a pod which consumes cpu=4917m on Node k8s-worker01
    Dec 30 03:52:25.317: INFO: Creating a pod which consumes cpu=5337m on Node k8s-worker02
    Dec 30 03:52:25.323: INFO: Creating a pod which consumes cpu=21616m on Node k8s-mgmt01
    Dec 30 03:52:25.328: INFO: Creating a pod which consumes cpu=21630m on Node k8s-mgmt02
    Dec 30 03:52:25.334: INFO: Creating a pod which consumes cpu=21609m on Node k8s-mgmt03
    Dec 30 03:52:25.340: INFO: Waiting up to 5m0s for pod "filler-pod-e4f1eb2e-2754-47d9-bef1-46f62e0352bc" in namespace "sched-pred-1219" to be "running"
    Dec 30 03:52:25.343: INFO: Pod "filler-pod-e4f1eb2e-2754-47d9-bef1-46f62e0352bc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.139617ms
    Dec 30 03:52:27.348: INFO: Pod "filler-pod-e4f1eb2e-2754-47d9-bef1-46f62e0352bc": Phase="Running", Reason="", readiness=true. Elapsed: 2.008675612s
    Dec 30 03:52:27.349: INFO: Pod "filler-pod-e4f1eb2e-2754-47d9-bef1-46f62e0352bc" satisfied condition "running"
    Dec 30 03:52:27.349: INFO: Waiting up to 5m0s for pod "filler-pod-22f4c009-577e-4246-8a83-21a2cc4ce9bc" in namespace "sched-pred-1219" to be "running"
    Dec 30 03:52:27.352: INFO: Pod "filler-pod-22f4c009-577e-4246-8a83-21a2cc4ce9bc": Phase="Running", Reason="", readiness=true. Elapsed: 3.877858ms
    Dec 30 03:52:27.352: INFO: Pod "filler-pod-22f4c009-577e-4246-8a83-21a2cc4ce9bc" satisfied condition "running"
    Dec 30 03:52:27.352: INFO: Waiting up to 5m0s for pod "filler-pod-44521eaa-e78c-4919-a997-e9a6a815715e" in namespace "sched-pred-1219" to be "running"
    Dec 30 03:52:27.356: INFO: Pod "filler-pod-44521eaa-e78c-4919-a997-e9a6a815715e": Phase="Running", Reason="", readiness=true. Elapsed: 3.307433ms
    Dec 30 03:52:27.356: INFO: Pod "filler-pod-44521eaa-e78c-4919-a997-e9a6a815715e" satisfied condition "running"
    Dec 30 03:52:27.356: INFO: Waiting up to 5m0s for pod "filler-pod-099548e8-55a5-4754-bc15-fd370b9cb2c4" in namespace "sched-pred-1219" to be "running"
    Dec 30 03:52:27.360: INFO: Pod "filler-pod-099548e8-55a5-4754-bc15-fd370b9cb2c4": Phase="Running", Reason="", readiness=true. Elapsed: 4.14338ms
    Dec 30 03:52:27.360: INFO: Pod "filler-pod-099548e8-55a5-4754-bc15-fd370b9cb2c4" satisfied condition "running"
    Dec 30 03:52:27.360: INFO: Waiting up to 5m0s for pod "filler-pod-7ae1be45-b522-4c00-a01e-e39f3270fe8d" in namespace "sched-pred-1219" to be "running"
    Dec 30 03:52:27.364: INFO: Pod "filler-pod-7ae1be45-b522-4c00-a01e-e39f3270fe8d": Phase="Running", Reason="", readiness=true. Elapsed: 3.655583ms
    Dec 30 03:52:27.364: INFO: Pod "filler-pod-7ae1be45-b522-4c00-a01e-e39f3270fe8d" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 12/30/22 03:52:27.364
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-099548e8-55a5-4754-bc15-fd370b9cb2c4.173575c99861f17b], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1219/filler-pod-099548e8-55a5-4754-bc15-fd370b9cb2c4 to k8s-mgmt02] 12/30/22 03:52:27.37
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-099548e8-55a5-4754-bc15-fd370b9cb2c4.173575c9c7203005], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 12/30/22 03:52:27.37
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-099548e8-55a5-4754-bc15-fd370b9cb2c4.173575c9c7d5e9db], Reason = [Created], Message = [Created container filler-pod-099548e8-55a5-4754-bc15-fd370b9cb2c4] 12/30/22 03:52:27.37
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-099548e8-55a5-4754-bc15-fd370b9cb2c4.173575c9d064e9d2], Reason = [Started], Message = [Started container filler-pod-099548e8-55a5-4754-bc15-fd370b9cb2c4] 12/30/22 03:52:27.37
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-22f4c009-577e-4246-8a83-21a2cc4ce9bc.173575c997ab3436], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1219/filler-pod-22f4c009-577e-4246-8a83-21a2cc4ce9bc to k8s-worker02] 12/30/22 03:52:27.37
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-22f4c009-577e-4246-8a83-21a2cc4ce9bc.173575c9d37f80e2], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 12/30/22 03:52:27.37
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-22f4c009-577e-4246-8a83-21a2cc4ce9bc.173575c9e4030984], Reason = [Created], Message = [Created container filler-pod-22f4c009-577e-4246-8a83-21a2cc4ce9bc] 12/30/22 03:52:27.371
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-22f4c009-577e-4246-8a83-21a2cc4ce9bc.173575c9ee03d67f], Reason = [Started], Message = [Started container filler-pod-22f4c009-577e-4246-8a83-21a2cc4ce9bc] 12/30/22 03:52:27.371
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-44521eaa-e78c-4919-a997-e9a6a815715e.173575c99800debc], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1219/filler-pod-44521eaa-e78c-4919-a997-e9a6a815715e to k8s-mgmt01] 12/30/22 03:52:27.371
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-44521eaa-e78c-4919-a997-e9a6a815715e.173575c9c6b93d96], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 12/30/22 03:52:27.371
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-44521eaa-e78c-4919-a997-e9a6a815715e.173575c9c77a1e03], Reason = [Created], Message = [Created container filler-pod-44521eaa-e78c-4919-a997-e9a6a815715e] 12/30/22 03:52:27.371
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-44521eaa-e78c-4919-a997-e9a6a815715e.173575c9d043c8f0], Reason = [Started], Message = [Started container filler-pod-44521eaa-e78c-4919-a997-e9a6a815715e] 12/30/22 03:52:27.371
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-7ae1be45-b522-4c00-a01e-e39f3270fe8d.173575c998bc7bf3], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1219/filler-pod-7ae1be45-b522-4c00-a01e-e39f3270fe8d to k8s-mgmt03] 12/30/22 03:52:27.371
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-7ae1be45-b522-4c00-a01e-e39f3270fe8d.173575c9c9568ac3], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 12/30/22 03:52:27.372
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-7ae1be45-b522-4c00-a01e-e39f3270fe8d.173575c9ca0fc15c], Reason = [Created], Message = [Created container filler-pod-7ae1be45-b522-4c00-a01e-e39f3270fe8d] 12/30/22 03:52:27.372
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-7ae1be45-b522-4c00-a01e-e39f3270fe8d.173575c9d34b2d6a], Reason = [Started], Message = [Started container filler-pod-7ae1be45-b522-4c00-a01e-e39f3270fe8d] 12/30/22 03:52:27.372
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-e4f1eb2e-2754-47d9-bef1-46f62e0352bc.173575c9975956c5], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1219/filler-pod-e4f1eb2e-2754-47d9-bef1-46f62e0352bc to k8s-worker01] 12/30/22 03:52:27.372
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-e4f1eb2e-2754-47d9-bef1-46f62e0352bc.173575c9cc6d3e08], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 12/30/22 03:52:27.372
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-e4f1eb2e-2754-47d9-bef1-46f62e0352bc.173575c9d92079c0], Reason = [Created], Message = [Created container filler-pod-e4f1eb2e-2754-47d9-bef1-46f62e0352bc] 12/30/22 03:52:27.372
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-e4f1eb2e-2754-47d9-bef1-46f62e0352bc.173575c9e19ff3d9], Reason = [Started], Message = [Started container filler-pod-e4f1eb2e-2754-47d9-bef1-46f62e0352bc] 12/30/22 03:52:27.372
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.173575ca11c57714], Reason = [FailedScheduling], Message = [0/5 nodes are available: 5 Insufficient cpu. preemption: 0/5 nodes are available: 5 No preemption victims found for incoming pod.] 12/30/22 03:52:27.381
    STEP: removing the label node off the node k8s-mgmt03 12/30/22 03:52:28.385
    STEP: verifying the node doesn't have the label node 12/30/22 03:52:28.4
    STEP: removing the label node off the node k8s-worker01 12/30/22 03:52:28.404
    STEP: verifying the node doesn't have the label node 12/30/22 03:52:28.419
    STEP: removing the label node off the node k8s-worker02 12/30/22 03:52:28.424
    STEP: verifying the node doesn't have the label node 12/30/22 03:52:28.438
    STEP: removing the label node off the node k8s-mgmt01 12/30/22 03:52:28.443
    STEP: verifying the node doesn't have the label node 12/30/22 03:52:28.457
    STEP: removing the label node off the node k8s-mgmt02 12/30/22 03:52:28.462
    STEP: verifying the node doesn't have the label node 12/30/22 03:52:28.476
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Dec 30 03:52:28.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-1219" for this suite. 12/30/22 03:52:28.486
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:52:28.494
Dec 30 03:52:28.494: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename disruption 12/30/22 03:52:28.495
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:52:28.508
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:52:28.511
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140
STEP: Waiting for the pdb to be processed 12/30/22 03:52:28.52
STEP: Waiting for all pods to be running 12/30/22 03:52:30.547
Dec 30 03:52:30.552: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Dec 30 03:52:32.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-5442" for this suite. 12/30/22 03:52:32.567
{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","completed":147,"skipped":2711,"failed":0}
------------------------------
• [4.081 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:52:28.494
    Dec 30 03:52:28.494: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename disruption 12/30/22 03:52:28.495
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:52:28.508
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:52:28.511
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:140
    STEP: Waiting for the pdb to be processed 12/30/22 03:52:28.52
    STEP: Waiting for all pods to be running 12/30/22 03:52:30.547
    Dec 30 03:52:30.552: INFO: running pods: 0 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Dec 30 03:52:32.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-5442" for this suite. 12/30/22 03:52:32.567
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:52:32.576
Dec 30 03:52:32.576: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename emptydir 12/30/22 03:52:32.577
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:52:32.592
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:52:32.595
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216
STEP: Creating a pod to test emptydir 0777 on node default medium 12/30/22 03:52:32.599
Dec 30 03:52:32.608: INFO: Waiting up to 5m0s for pod "pod-a516dbd9-404e-438b-9bf3-12beeb8d448d" in namespace "emptydir-1355" to be "Succeeded or Failed"
Dec 30 03:52:32.612: INFO: Pod "pod-a516dbd9-404e-438b-9bf3-12beeb8d448d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.684007ms
Dec 30 03:52:34.617: INFO: Pod "pod-a516dbd9-404e-438b-9bf3-12beeb8d448d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008585661s
Dec 30 03:52:36.617: INFO: Pod "pod-a516dbd9-404e-438b-9bf3-12beeb8d448d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008895108s
STEP: Saw pod success 12/30/22 03:52:36.617
Dec 30 03:52:36.617: INFO: Pod "pod-a516dbd9-404e-438b-9bf3-12beeb8d448d" satisfied condition "Succeeded or Failed"
Dec 30 03:52:36.622: INFO: Trying to get logs from node k8s-mgmt01 pod pod-a516dbd9-404e-438b-9bf3-12beeb8d448d container test-container: <nil>
STEP: delete the pod 12/30/22 03:52:36.631
Dec 30 03:52:36.657: INFO: Waiting for pod pod-a516dbd9-404e-438b-9bf3-12beeb8d448d to disappear
Dec 30 03:52:36.660: INFO: Pod pod-a516dbd9-404e-438b-9bf3-12beeb8d448d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Dec 30 03:52:36.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1355" for this suite. 12/30/22 03:52:36.665
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":148,"skipped":2721,"failed":0}
------------------------------
• [4.096 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:52:32.576
    Dec 30 03:52:32.576: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename emptydir 12/30/22 03:52:32.577
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:52:32.592
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:52:32.595
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:216
    STEP: Creating a pod to test emptydir 0777 on node default medium 12/30/22 03:52:32.599
    Dec 30 03:52:32.608: INFO: Waiting up to 5m0s for pod "pod-a516dbd9-404e-438b-9bf3-12beeb8d448d" in namespace "emptydir-1355" to be "Succeeded or Failed"
    Dec 30 03:52:32.612: INFO: Pod "pod-a516dbd9-404e-438b-9bf3-12beeb8d448d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.684007ms
    Dec 30 03:52:34.617: INFO: Pod "pod-a516dbd9-404e-438b-9bf3-12beeb8d448d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008585661s
    Dec 30 03:52:36.617: INFO: Pod "pod-a516dbd9-404e-438b-9bf3-12beeb8d448d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008895108s
    STEP: Saw pod success 12/30/22 03:52:36.617
    Dec 30 03:52:36.617: INFO: Pod "pod-a516dbd9-404e-438b-9bf3-12beeb8d448d" satisfied condition "Succeeded or Failed"
    Dec 30 03:52:36.622: INFO: Trying to get logs from node k8s-mgmt01 pod pod-a516dbd9-404e-438b-9bf3-12beeb8d448d container test-container: <nil>
    STEP: delete the pod 12/30/22 03:52:36.631
    Dec 30 03:52:36.657: INFO: Waiting for pod pod-a516dbd9-404e-438b-9bf3-12beeb8d448d to disappear
    Dec 30 03:52:36.660: INFO: Pod pod-a516dbd9-404e-438b-9bf3-12beeb8d448d no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Dec 30 03:52:36.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-1355" for this suite. 12/30/22 03:52:36.665
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:52:36.674
Dec 30 03:52:36.674: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename emptydir 12/30/22 03:52:36.675
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:52:36.689
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:52:36.693
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96
STEP: Creating a pod to test emptydir 0644 on tmpfs 12/30/22 03:52:36.696
Dec 30 03:52:36.706: INFO: Waiting up to 5m0s for pod "pod-daef81a8-860c-4b28-8e08-123dc6c193ad" in namespace "emptydir-7614" to be "Succeeded or Failed"
Dec 30 03:52:36.709: INFO: Pod "pod-daef81a8-860c-4b28-8e08-123dc6c193ad": Phase="Pending", Reason="", readiness=false. Elapsed: 3.657183ms
Dec 30 03:52:38.714: INFO: Pod "pod-daef81a8-860c-4b28-8e08-123dc6c193ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008298782s
Dec 30 03:52:40.715: INFO: Pod "pod-daef81a8-860c-4b28-8e08-123dc6c193ad": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009255615s
Dec 30 03:52:42.715: INFO: Pod "pod-daef81a8-860c-4b28-8e08-123dc6c193ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008773313s
STEP: Saw pod success 12/30/22 03:52:42.715
Dec 30 03:52:42.715: INFO: Pod "pod-daef81a8-860c-4b28-8e08-123dc6c193ad" satisfied condition "Succeeded or Failed"
Dec 30 03:52:42.719: INFO: Trying to get logs from node k8s-mgmt01 pod pod-daef81a8-860c-4b28-8e08-123dc6c193ad container test-container: <nil>
STEP: delete the pod 12/30/22 03:52:42.728
Dec 30 03:52:42.739: INFO: Waiting for pod pod-daef81a8-860c-4b28-8e08-123dc6c193ad to disappear
Dec 30 03:52:42.743: INFO: Pod pod-daef81a8-860c-4b28-8e08-123dc6c193ad no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Dec 30 03:52:42.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7614" for this suite. 12/30/22 03:52:42.748
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":149,"skipped":2732,"failed":0}
------------------------------
• [SLOW TEST] [6.081 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:52:36.674
    Dec 30 03:52:36.674: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename emptydir 12/30/22 03:52:36.675
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:52:36.689
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:52:36.693
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:96
    STEP: Creating a pod to test emptydir 0644 on tmpfs 12/30/22 03:52:36.696
    Dec 30 03:52:36.706: INFO: Waiting up to 5m0s for pod "pod-daef81a8-860c-4b28-8e08-123dc6c193ad" in namespace "emptydir-7614" to be "Succeeded or Failed"
    Dec 30 03:52:36.709: INFO: Pod "pod-daef81a8-860c-4b28-8e08-123dc6c193ad": Phase="Pending", Reason="", readiness=false. Elapsed: 3.657183ms
    Dec 30 03:52:38.714: INFO: Pod "pod-daef81a8-860c-4b28-8e08-123dc6c193ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008298782s
    Dec 30 03:52:40.715: INFO: Pod "pod-daef81a8-860c-4b28-8e08-123dc6c193ad": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009255615s
    Dec 30 03:52:42.715: INFO: Pod "pod-daef81a8-860c-4b28-8e08-123dc6c193ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008773313s
    STEP: Saw pod success 12/30/22 03:52:42.715
    Dec 30 03:52:42.715: INFO: Pod "pod-daef81a8-860c-4b28-8e08-123dc6c193ad" satisfied condition "Succeeded or Failed"
    Dec 30 03:52:42.719: INFO: Trying to get logs from node k8s-mgmt01 pod pod-daef81a8-860c-4b28-8e08-123dc6c193ad container test-container: <nil>
    STEP: delete the pod 12/30/22 03:52:42.728
    Dec 30 03:52:42.739: INFO: Waiting for pod pod-daef81a8-860c-4b28-8e08-123dc6c193ad to disappear
    Dec 30 03:52:42.743: INFO: Pod pod-daef81a8-860c-4b28-8e08-123dc6c193ad no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Dec 30 03:52:42.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-7614" for this suite. 12/30/22 03:52:42.748
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:52:42.756
Dec 30 03:52:42.756: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename server-version 12/30/22 03:52:42.758
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:52:42.772
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:52:42.775
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 12/30/22 03:52:42.778
STEP: Confirm major version 12/30/22 03:52:42.78
Dec 30 03:52:42.780: INFO: Major version: 1
STEP: Confirm minor version 12/30/22 03:52:42.78
Dec 30 03:52:42.780: INFO: cleanMinorVersion: 25
Dec 30 03:52:42.780: INFO: Minor version: 25
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:187
Dec 30 03:52:42.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-1637" for this suite. 12/30/22 03:52:42.785
{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","completed":150,"skipped":2737,"failed":0}
------------------------------
• [0.035 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:52:42.756
    Dec 30 03:52:42.756: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename server-version 12/30/22 03:52:42.758
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:52:42.772
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:52:42.775
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 12/30/22 03:52:42.778
    STEP: Confirm major version 12/30/22 03:52:42.78
    Dec 30 03:52:42.780: INFO: Major version: 1
    STEP: Confirm minor version 12/30/22 03:52:42.78
    Dec 30 03:52:42.780: INFO: cleanMinorVersion: 25
    Dec 30 03:52:42.780: INFO: Minor version: 25
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/framework.go:187
    Dec 30 03:52:42.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "server-version-1637" for this suite. 12/30/22 03:52:42.785
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:52:42.791
Dec 30 03:52:42.791: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename subpath 12/30/22 03:52:42.793
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:52:42.806
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:52:42.809
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 12/30/22 03:52:42.812
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-lf7p 12/30/22 03:52:42.821
STEP: Creating a pod to test atomic-volume-subpath 12/30/22 03:52:42.821
Dec 30 03:52:42.831: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-lf7p" in namespace "subpath-8646" to be "Succeeded or Failed"
Dec 30 03:52:42.834: INFO: Pod "pod-subpath-test-downwardapi-lf7p": Phase="Pending", Reason="", readiness=false. Elapsed: 3.588948ms
Dec 30 03:52:44.840: INFO: Pod "pod-subpath-test-downwardapi-lf7p": Phase="Running", Reason="", readiness=true. Elapsed: 2.008800427s
Dec 30 03:52:46.840: INFO: Pod "pod-subpath-test-downwardapi-lf7p": Phase="Running", Reason="", readiness=true. Elapsed: 4.008794801s
Dec 30 03:52:48.840: INFO: Pod "pod-subpath-test-downwardapi-lf7p": Phase="Running", Reason="", readiness=true. Elapsed: 6.009458896s
Dec 30 03:52:50.839: INFO: Pod "pod-subpath-test-downwardapi-lf7p": Phase="Running", Reason="", readiness=true. Elapsed: 8.008651297s
Dec 30 03:52:52.840: INFO: Pod "pod-subpath-test-downwardapi-lf7p": Phase="Running", Reason="", readiness=true. Elapsed: 10.008849915s
Dec 30 03:52:54.839: INFO: Pod "pod-subpath-test-downwardapi-lf7p": Phase="Running", Reason="", readiness=true. Elapsed: 12.008666303s
Dec 30 03:52:56.840: INFO: Pod "pod-subpath-test-downwardapi-lf7p": Phase="Running", Reason="", readiness=true. Elapsed: 14.009646956s
Dec 30 03:52:58.841: INFO: Pod "pod-subpath-test-downwardapi-lf7p": Phase="Running", Reason="", readiness=true. Elapsed: 16.010515271s
Dec 30 03:53:00.839: INFO: Pod "pod-subpath-test-downwardapi-lf7p": Phase="Running", Reason="", readiness=true. Elapsed: 18.008662363s
Dec 30 03:53:02.841: INFO: Pod "pod-subpath-test-downwardapi-lf7p": Phase="Running", Reason="", readiness=true. Elapsed: 20.010146891s
Dec 30 03:53:04.839: INFO: Pod "pod-subpath-test-downwardapi-lf7p": Phase="Running", Reason="", readiness=false. Elapsed: 22.008442812s
Dec 30 03:53:06.839: INFO: Pod "pod-subpath-test-downwardapi-lf7p": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.008499054s
STEP: Saw pod success 12/30/22 03:53:06.839
Dec 30 03:53:06.840: INFO: Pod "pod-subpath-test-downwardapi-lf7p" satisfied condition "Succeeded or Failed"
Dec 30 03:53:06.843: INFO: Trying to get logs from node k8s-mgmt01 pod pod-subpath-test-downwardapi-lf7p container test-container-subpath-downwardapi-lf7p: <nil>
STEP: delete the pod 12/30/22 03:53:06.852
Dec 30 03:53:06.865: INFO: Waiting for pod pod-subpath-test-downwardapi-lf7p to disappear
Dec 30 03:53:06.869: INFO: Pod pod-subpath-test-downwardapi-lf7p no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-lf7p 12/30/22 03:53:06.869
Dec 30 03:53:06.869: INFO: Deleting pod "pod-subpath-test-downwardapi-lf7p" in namespace "subpath-8646"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Dec 30 03:53:06.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8646" for this suite. 12/30/22 03:53:06.877
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]","completed":151,"skipped":2737,"failed":0}
------------------------------
• [SLOW TEST] [24.092 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:52:42.791
    Dec 30 03:52:42.791: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename subpath 12/30/22 03:52:42.793
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:52:42.806
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:52:42.809
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 12/30/22 03:52:42.812
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-lf7p 12/30/22 03:52:42.821
    STEP: Creating a pod to test atomic-volume-subpath 12/30/22 03:52:42.821
    Dec 30 03:52:42.831: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-lf7p" in namespace "subpath-8646" to be "Succeeded or Failed"
    Dec 30 03:52:42.834: INFO: Pod "pod-subpath-test-downwardapi-lf7p": Phase="Pending", Reason="", readiness=false. Elapsed: 3.588948ms
    Dec 30 03:52:44.840: INFO: Pod "pod-subpath-test-downwardapi-lf7p": Phase="Running", Reason="", readiness=true. Elapsed: 2.008800427s
    Dec 30 03:52:46.840: INFO: Pod "pod-subpath-test-downwardapi-lf7p": Phase="Running", Reason="", readiness=true. Elapsed: 4.008794801s
    Dec 30 03:52:48.840: INFO: Pod "pod-subpath-test-downwardapi-lf7p": Phase="Running", Reason="", readiness=true. Elapsed: 6.009458896s
    Dec 30 03:52:50.839: INFO: Pod "pod-subpath-test-downwardapi-lf7p": Phase="Running", Reason="", readiness=true. Elapsed: 8.008651297s
    Dec 30 03:52:52.840: INFO: Pod "pod-subpath-test-downwardapi-lf7p": Phase="Running", Reason="", readiness=true. Elapsed: 10.008849915s
    Dec 30 03:52:54.839: INFO: Pod "pod-subpath-test-downwardapi-lf7p": Phase="Running", Reason="", readiness=true. Elapsed: 12.008666303s
    Dec 30 03:52:56.840: INFO: Pod "pod-subpath-test-downwardapi-lf7p": Phase="Running", Reason="", readiness=true. Elapsed: 14.009646956s
    Dec 30 03:52:58.841: INFO: Pod "pod-subpath-test-downwardapi-lf7p": Phase="Running", Reason="", readiness=true. Elapsed: 16.010515271s
    Dec 30 03:53:00.839: INFO: Pod "pod-subpath-test-downwardapi-lf7p": Phase="Running", Reason="", readiness=true. Elapsed: 18.008662363s
    Dec 30 03:53:02.841: INFO: Pod "pod-subpath-test-downwardapi-lf7p": Phase="Running", Reason="", readiness=true. Elapsed: 20.010146891s
    Dec 30 03:53:04.839: INFO: Pod "pod-subpath-test-downwardapi-lf7p": Phase="Running", Reason="", readiness=false. Elapsed: 22.008442812s
    Dec 30 03:53:06.839: INFO: Pod "pod-subpath-test-downwardapi-lf7p": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.008499054s
    STEP: Saw pod success 12/30/22 03:53:06.839
    Dec 30 03:53:06.840: INFO: Pod "pod-subpath-test-downwardapi-lf7p" satisfied condition "Succeeded or Failed"
    Dec 30 03:53:06.843: INFO: Trying to get logs from node k8s-mgmt01 pod pod-subpath-test-downwardapi-lf7p container test-container-subpath-downwardapi-lf7p: <nil>
    STEP: delete the pod 12/30/22 03:53:06.852
    Dec 30 03:53:06.865: INFO: Waiting for pod pod-subpath-test-downwardapi-lf7p to disappear
    Dec 30 03:53:06.869: INFO: Pod pod-subpath-test-downwardapi-lf7p no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-lf7p 12/30/22 03:53:06.869
    Dec 30 03:53:06.869: INFO: Deleting pod "pod-subpath-test-downwardapi-lf7p" in namespace "subpath-8646"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Dec 30 03:53:06.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-8646" for this suite. 12/30/22 03:53:06.877
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:53:06.885
Dec 30 03:53:06.885: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename job 12/30/22 03:53:06.886
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:53:06.9
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:53:06.904
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464
STEP: Creating a job 12/30/22 03:53:06.907
STEP: Ensure pods equal to paralellism count is attached to the job 12/30/22 03:53:06.912
STEP: patching /status 12/30/22 03:53:08.917
STEP: updating /status 12/30/22 03:53:08.928
STEP: get /status 12/30/22 03:53:08.965
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Dec 30 03:53:08.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-3791" for this suite. 12/30/22 03:53:08.975
{"msg":"PASSED [sig-apps] Job should apply changes to a job status [Conformance]","completed":152,"skipped":2746,"failed":0}
------------------------------
• [2.097 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:53:06.885
    Dec 30 03:53:06.885: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename job 12/30/22 03:53:06.886
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:53:06.9
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:53:06.904
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:464
    STEP: Creating a job 12/30/22 03:53:06.907
    STEP: Ensure pods equal to paralellism count is attached to the job 12/30/22 03:53:06.912
    STEP: patching /status 12/30/22 03:53:08.917
    STEP: updating /status 12/30/22 03:53:08.928
    STEP: get /status 12/30/22 03:53:08.965
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Dec 30 03:53:08.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-3791" for this suite. 12/30/22 03:53:08.975
  << End Captured GinkgoWriter Output
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1683
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:53:08.982
Dec 30 03:53:08.982: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename kubectl 12/30/22 03:53:08.984
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:53:08.996
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:53:08.999
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1683
Dec 30 03:53:09.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-8360 version'
Dec 30 03:53:09.088: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Dec 30 03:53:09.088: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.3\", GitCommit:\"434bfd82814af038ad94d62ebe59b133fcb50506\", GitTreeState:\"clean\", BuildDate:\"2022-10-12T10:57:26Z\", GoVersion:\"go1.19.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.3\", GitCommit:\"434bfd82814af038ad94d62ebe59b133fcb50506\", GitTreeState:\"clean\", BuildDate:\"2022-10-12T10:49:09Z\", GoVersion:\"go1.19.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Dec 30 03:53:09.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8360" for this suite. 12/30/22 03:53:09.098
{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","completed":153,"skipped":2746,"failed":0}
------------------------------
• [0.123 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1677
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1683

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:53:08.982
    Dec 30 03:53:08.982: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename kubectl 12/30/22 03:53:08.984
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:53:08.996
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:53:08.999
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1683
    Dec 30 03:53:09.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-8360 version'
    Dec 30 03:53:09.088: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    Dec 30 03:53:09.088: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.3\", GitCommit:\"434bfd82814af038ad94d62ebe59b133fcb50506\", GitTreeState:\"clean\", BuildDate:\"2022-10-12T10:57:26Z\", GoVersion:\"go1.19.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.3\", GitCommit:\"434bfd82814af038ad94d62ebe59b133fcb50506\", GitTreeState:\"clean\", BuildDate:\"2022-10-12T10:49:09Z\", GoVersion:\"go1.19.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Dec 30 03:53:09.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-8360" for this suite. 12/30/22 03:53:09.098
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:53:09.105
Dec 30 03:53:09.105: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename secrets 12/30/22 03:53:09.106
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:53:09.118
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:53:09.121
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139
STEP: Creating projection with secret that has name secret-emptykey-test-14442828-0468-4703-bdc8-345342cf9f31 12/30/22 03:53:09.124
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Dec 30 03:53:09.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7297" for this suite. 12/30/22 03:53:09.131
{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","completed":154,"skipped":2746,"failed":0}
------------------------------
• [0.033 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:53:09.105
    Dec 30 03:53:09.105: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename secrets 12/30/22 03:53:09.106
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:53:09.118
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:53:09.121
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:139
    STEP: Creating projection with secret that has name secret-emptykey-test-14442828-0468-4703-bdc8-345342cf9f31 12/30/22 03:53:09.124
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Dec 30 03:53:09.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-7297" for this suite. 12/30/22 03:53:09.131
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:53:09.138
Dec 30 03:53:09.138: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename crd-publish-openapi 12/30/22 03:53:09.139
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:53:09.153
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:53:09.156
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152
Dec 30 03:53:09.159: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 12/30/22 03:53:19.429
Dec 30 03:53:19.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=crd-publish-openapi-720 --namespace=crd-publish-openapi-720 create -f -'
Dec 30 03:53:20.289: INFO: stderr: ""
Dec 30 03:53:20.289: INFO: stdout: "e2e-test-crd-publish-openapi-5806-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Dec 30 03:53:20.289: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=crd-publish-openapi-720 --namespace=crd-publish-openapi-720 delete e2e-test-crd-publish-openapi-5806-crds test-cr'
Dec 30 03:53:20.432: INFO: stderr: ""
Dec 30 03:53:20.432: INFO: stdout: "e2e-test-crd-publish-openapi-5806-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Dec 30 03:53:20.432: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=crd-publish-openapi-720 --namespace=crd-publish-openapi-720 apply -f -'
Dec 30 03:53:21.338: INFO: stderr: ""
Dec 30 03:53:21.338: INFO: stdout: "e2e-test-crd-publish-openapi-5806-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Dec 30 03:53:21.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=crd-publish-openapi-720 --namespace=crd-publish-openapi-720 delete e2e-test-crd-publish-openapi-5806-crds test-cr'
Dec 30 03:53:21.441: INFO: stderr: ""
Dec 30 03:53:21.441: INFO: stdout: "e2e-test-crd-publish-openapi-5806-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 12/30/22 03:53:21.441
Dec 30 03:53:21.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=crd-publish-openapi-720 explain e2e-test-crd-publish-openapi-5806-crds'
Dec 30 03:53:22.092: INFO: stderr: ""
Dec 30 03:53:22.092: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5806-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Dec 30 03:53:26.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-720" for this suite. 12/30/22 03:53:26.927
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","completed":155,"skipped":2746,"failed":0}
------------------------------
• [SLOW TEST] [17.797 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:53:09.138
    Dec 30 03:53:09.138: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename crd-publish-openapi 12/30/22 03:53:09.139
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:53:09.153
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:53:09.156
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:152
    Dec 30 03:53:09.159: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 12/30/22 03:53:19.429
    Dec 30 03:53:19.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=crd-publish-openapi-720 --namespace=crd-publish-openapi-720 create -f -'
    Dec 30 03:53:20.289: INFO: stderr: ""
    Dec 30 03:53:20.289: INFO: stdout: "e2e-test-crd-publish-openapi-5806-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Dec 30 03:53:20.289: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=crd-publish-openapi-720 --namespace=crd-publish-openapi-720 delete e2e-test-crd-publish-openapi-5806-crds test-cr'
    Dec 30 03:53:20.432: INFO: stderr: ""
    Dec 30 03:53:20.432: INFO: stdout: "e2e-test-crd-publish-openapi-5806-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    Dec 30 03:53:20.432: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=crd-publish-openapi-720 --namespace=crd-publish-openapi-720 apply -f -'
    Dec 30 03:53:21.338: INFO: stderr: ""
    Dec 30 03:53:21.338: INFO: stdout: "e2e-test-crd-publish-openapi-5806-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Dec 30 03:53:21.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=crd-publish-openapi-720 --namespace=crd-publish-openapi-720 delete e2e-test-crd-publish-openapi-5806-crds test-cr'
    Dec 30 03:53:21.441: INFO: stderr: ""
    Dec 30 03:53:21.441: INFO: stdout: "e2e-test-crd-publish-openapi-5806-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 12/30/22 03:53:21.441
    Dec 30 03:53:21.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=crd-publish-openapi-720 explain e2e-test-crd-publish-openapi-5806-crds'
    Dec 30 03:53:22.092: INFO: stderr: ""
    Dec 30 03:53:22.092: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5806-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Dec 30 03:53:26.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-720" for this suite. 12/30/22 03:53:26.927
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1413
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:53:26.936
Dec 30 03:53:26.936: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename kubectl 12/30/22 03:53:26.937
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:53:26.955
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:53:26.958
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1413
STEP: creating Agnhost RC 12/30/22 03:53:26.961
Dec 30 03:53:26.962: INFO: namespace kubectl-7938
Dec 30 03:53:26.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-7938 create -f -'
Dec 30 03:53:27.756: INFO: stderr: ""
Dec 30 03:53:27.756: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 12/30/22 03:53:27.756
Dec 30 03:53:28.762: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 30 03:53:28.763: INFO: Found 0 / 1
Dec 30 03:53:29.762: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 30 03:53:29.762: INFO: Found 1 / 1
Dec 30 03:53:29.762: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec 30 03:53:29.766: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 30 03:53:29.766: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec 30 03:53:29.766: INFO: wait on agnhost-primary startup in kubectl-7938 
Dec 30 03:53:29.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-7938 logs agnhost-primary-x7prb agnhost-primary'
Dec 30 03:53:29.882: INFO: stderr: ""
Dec 30 03:53:29.882: INFO: stdout: "Paused\n"
STEP: exposing RC 12/30/22 03:53:29.882
Dec 30 03:53:29.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-7938 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Dec 30 03:53:29.997: INFO: stderr: ""
Dec 30 03:53:29.997: INFO: stdout: "service/rm2 exposed\n"
Dec 30 03:53:30.001: INFO: Service rm2 in namespace kubectl-7938 found.
STEP: exposing service 12/30/22 03:53:32.01
Dec 30 03:53:32.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-7938 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Dec 30 03:53:32.144: INFO: stderr: ""
Dec 30 03:53:32.144: INFO: stdout: "service/rm3 exposed\n"
Dec 30 03:53:32.148: INFO: Service rm3 in namespace kubectl-7938 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Dec 30 03:53:34.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7938" for this suite. 12/30/22 03:53:34.162
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","completed":156,"skipped":2755,"failed":0}
------------------------------
• [SLOW TEST] [7.232 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1407
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:53:26.936
    Dec 30 03:53:26.936: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename kubectl 12/30/22 03:53:26.937
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:53:26.955
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:53:26.958
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1413
    STEP: creating Agnhost RC 12/30/22 03:53:26.961
    Dec 30 03:53:26.962: INFO: namespace kubectl-7938
    Dec 30 03:53:26.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-7938 create -f -'
    Dec 30 03:53:27.756: INFO: stderr: ""
    Dec 30 03:53:27.756: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 12/30/22 03:53:27.756
    Dec 30 03:53:28.762: INFO: Selector matched 1 pods for map[app:agnhost]
    Dec 30 03:53:28.763: INFO: Found 0 / 1
    Dec 30 03:53:29.762: INFO: Selector matched 1 pods for map[app:agnhost]
    Dec 30 03:53:29.762: INFO: Found 1 / 1
    Dec 30 03:53:29.762: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Dec 30 03:53:29.766: INFO: Selector matched 1 pods for map[app:agnhost]
    Dec 30 03:53:29.766: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Dec 30 03:53:29.766: INFO: wait on agnhost-primary startup in kubectl-7938 
    Dec 30 03:53:29.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-7938 logs agnhost-primary-x7prb agnhost-primary'
    Dec 30 03:53:29.882: INFO: stderr: ""
    Dec 30 03:53:29.882: INFO: stdout: "Paused\n"
    STEP: exposing RC 12/30/22 03:53:29.882
    Dec 30 03:53:29.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-7938 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    Dec 30 03:53:29.997: INFO: stderr: ""
    Dec 30 03:53:29.997: INFO: stdout: "service/rm2 exposed\n"
    Dec 30 03:53:30.001: INFO: Service rm2 in namespace kubectl-7938 found.
    STEP: exposing service 12/30/22 03:53:32.01
    Dec 30 03:53:32.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-7938 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    Dec 30 03:53:32.144: INFO: stderr: ""
    Dec 30 03:53:32.144: INFO: stdout: "service/rm3 exposed\n"
    Dec 30 03:53:32.148: INFO: Service rm3 in namespace kubectl-7938 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Dec 30 03:53:34.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-7938" for this suite. 12/30/22 03:53:34.162
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:53:34.17
Dec 30 03:53:34.170: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename sysctl 12/30/22 03:53:34.171
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:53:34.188
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:53:34.191
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 12/30/22 03:53:34.195
STEP: Watching for error events or started pod 12/30/22 03:53:34.205
STEP: Waiting for pod completion 12/30/22 03:53:36.211
Dec 30 03:53:36.211: INFO: Waiting up to 3m0s for pod "sysctl-20fdc543-8bc3-49cc-8df2-02da8cf1cd40" in namespace "sysctl-9214" to be "completed"
Dec 30 03:53:36.215: INFO: Pod "sysctl-20fdc543-8bc3-49cc-8df2-02da8cf1cd40": Phase="Pending", Reason="", readiness=false. Elapsed: 3.622847ms
Dec 30 03:53:38.220: INFO: Pod "sysctl-20fdc543-8bc3-49cc-8df2-02da8cf1cd40": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008813318s
Dec 30 03:53:38.220: INFO: Pod "sysctl-20fdc543-8bc3-49cc-8df2-02da8cf1cd40" satisfied condition "completed"
STEP: Checking that the pod succeeded 12/30/22 03:53:38.224
STEP: Getting logs from the pod 12/30/22 03:53:38.224
STEP: Checking that the sysctl is actually updated 12/30/22 03:53:38.244
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
Dec 30 03:53:38.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-9214" for this suite. 12/30/22 03:53:38.25
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","completed":157,"skipped":2764,"failed":0}
------------------------------
• [4.088 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:53:34.17
    Dec 30 03:53:34.170: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename sysctl 12/30/22 03:53:34.171
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:53:34.188
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:53:34.191
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 12/30/22 03:53:34.195
    STEP: Watching for error events or started pod 12/30/22 03:53:34.205
    STEP: Waiting for pod completion 12/30/22 03:53:36.211
    Dec 30 03:53:36.211: INFO: Waiting up to 3m0s for pod "sysctl-20fdc543-8bc3-49cc-8df2-02da8cf1cd40" in namespace "sysctl-9214" to be "completed"
    Dec 30 03:53:36.215: INFO: Pod "sysctl-20fdc543-8bc3-49cc-8df2-02da8cf1cd40": Phase="Pending", Reason="", readiness=false. Elapsed: 3.622847ms
    Dec 30 03:53:38.220: INFO: Pod "sysctl-20fdc543-8bc3-49cc-8df2-02da8cf1cd40": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008813318s
    Dec 30 03:53:38.220: INFO: Pod "sysctl-20fdc543-8bc3-49cc-8df2-02da8cf1cd40" satisfied condition "completed"
    STEP: Checking that the pod succeeded 12/30/22 03:53:38.224
    STEP: Getting logs from the pod 12/30/22 03:53:38.224
    STEP: Checking that the sysctl is actually updated 12/30/22 03:53:38.244
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:187
    Dec 30 03:53:38.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sysctl-9214" for this suite. 12/30/22 03:53:38.25
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:53:38.259
Dec 30 03:53:38.260: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename resourcequota 12/30/22 03:53:38.261
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:53:38.28
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:53:38.283
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65
STEP: Counting existing ResourceQuota 12/30/22 03:53:38.287
STEP: Creating a ResourceQuota 12/30/22 03:53:43.291
STEP: Ensuring resource quota status is calculated 12/30/22 03:53:43.298
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Dec 30 03:53:45.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6921" for this suite. 12/30/22 03:53:45.314
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","completed":158,"skipped":2786,"failed":0}
------------------------------
• [SLOW TEST] [7.062 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:53:38.259
    Dec 30 03:53:38.260: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename resourcequota 12/30/22 03:53:38.261
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:53:38.28
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:53:38.283
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:65
    STEP: Counting existing ResourceQuota 12/30/22 03:53:38.287
    STEP: Creating a ResourceQuota 12/30/22 03:53:43.291
    STEP: Ensuring resource quota status is calculated 12/30/22 03:53:43.298
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Dec 30 03:53:45.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-6921" for this suite. 12/30/22 03:53:45.314
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:53:45.322
Dec 30 03:53:45.323: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename projected 12/30/22 03:53:45.324
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:53:45.34
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:53:45.344
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73
STEP: Creating configMap with name projected-configmap-test-volume-1ccf08f3-eed7-4822-a325-5bda55f489cf 12/30/22 03:53:45.348
STEP: Creating a pod to test consume configMaps 12/30/22 03:53:45.353
Dec 30 03:53:45.363: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-429a3316-d79d-4655-b20e-cd7ecf99d378" in namespace "projected-2998" to be "Succeeded or Failed"
Dec 30 03:53:45.367: INFO: Pod "pod-projected-configmaps-429a3316-d79d-4655-b20e-cd7ecf99d378": Phase="Pending", Reason="", readiness=false. Elapsed: 3.733488ms
Dec 30 03:53:47.372: INFO: Pod "pod-projected-configmaps-429a3316-d79d-4655-b20e-cd7ecf99d378": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008498157s
Dec 30 03:53:49.372: INFO: Pod "pod-projected-configmaps-429a3316-d79d-4655-b20e-cd7ecf99d378": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008875562s
STEP: Saw pod success 12/30/22 03:53:49.372
Dec 30 03:53:49.372: INFO: Pod "pod-projected-configmaps-429a3316-d79d-4655-b20e-cd7ecf99d378" satisfied condition "Succeeded or Failed"
Dec 30 03:53:49.376: INFO: Trying to get logs from node k8s-mgmt02 pod pod-projected-configmaps-429a3316-d79d-4655-b20e-cd7ecf99d378 container agnhost-container: <nil>
STEP: delete the pod 12/30/22 03:53:49.396
Dec 30 03:53:49.409: INFO: Waiting for pod pod-projected-configmaps-429a3316-d79d-4655-b20e-cd7ecf99d378 to disappear
Dec 30 03:53:49.413: INFO: Pod pod-projected-configmaps-429a3316-d79d-4655-b20e-cd7ecf99d378 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Dec 30 03:53:49.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2998" for this suite. 12/30/22 03:53:49.419
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","completed":159,"skipped":2794,"failed":0}
------------------------------
• [4.104 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:53:45.322
    Dec 30 03:53:45.323: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename projected 12/30/22 03:53:45.324
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:53:45.34
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:53:45.344
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:73
    STEP: Creating configMap with name projected-configmap-test-volume-1ccf08f3-eed7-4822-a325-5bda55f489cf 12/30/22 03:53:45.348
    STEP: Creating a pod to test consume configMaps 12/30/22 03:53:45.353
    Dec 30 03:53:45.363: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-429a3316-d79d-4655-b20e-cd7ecf99d378" in namespace "projected-2998" to be "Succeeded or Failed"
    Dec 30 03:53:45.367: INFO: Pod "pod-projected-configmaps-429a3316-d79d-4655-b20e-cd7ecf99d378": Phase="Pending", Reason="", readiness=false. Elapsed: 3.733488ms
    Dec 30 03:53:47.372: INFO: Pod "pod-projected-configmaps-429a3316-d79d-4655-b20e-cd7ecf99d378": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008498157s
    Dec 30 03:53:49.372: INFO: Pod "pod-projected-configmaps-429a3316-d79d-4655-b20e-cd7ecf99d378": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008875562s
    STEP: Saw pod success 12/30/22 03:53:49.372
    Dec 30 03:53:49.372: INFO: Pod "pod-projected-configmaps-429a3316-d79d-4655-b20e-cd7ecf99d378" satisfied condition "Succeeded or Failed"
    Dec 30 03:53:49.376: INFO: Trying to get logs from node k8s-mgmt02 pod pod-projected-configmaps-429a3316-d79d-4655-b20e-cd7ecf99d378 container agnhost-container: <nil>
    STEP: delete the pod 12/30/22 03:53:49.396
    Dec 30 03:53:49.409: INFO: Waiting for pod pod-projected-configmaps-429a3316-d79d-4655-b20e-cd7ecf99d378 to disappear
    Dec 30 03:53:49.413: INFO: Pod pod-projected-configmaps-429a3316-d79d-4655-b20e-cd7ecf99d378 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Dec 30 03:53:49.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2998" for this suite. 12/30/22 03:53:49.419
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:53:49.428
Dec 30 03:53:49.428: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename var-expansion 12/30/22 03:53:49.429
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:53:49.448
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:53:49.451
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296
STEP: creating the pod 12/30/22 03:53:49.454
STEP: waiting for pod running 12/30/22 03:53:49.463
Dec 30 03:53:49.463: INFO: Waiting up to 2m0s for pod "var-expansion-d19092e4-a375-42d2-9149-ef80e2eb4686" in namespace "var-expansion-1224" to be "running"
Dec 30 03:53:49.467: INFO: Pod "var-expansion-d19092e4-a375-42d2-9149-ef80e2eb4686": Phase="Pending", Reason="", readiness=false. Elapsed: 3.958692ms
Dec 30 03:53:51.473: INFO: Pod "var-expansion-d19092e4-a375-42d2-9149-ef80e2eb4686": Phase="Running", Reason="", readiness=true. Elapsed: 2.009443592s
Dec 30 03:53:51.473: INFO: Pod "var-expansion-d19092e4-a375-42d2-9149-ef80e2eb4686" satisfied condition "running"
STEP: creating a file in subpath 12/30/22 03:53:51.473
Dec 30 03:53:51.477: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-1224 PodName:var-expansion-d19092e4-a375-42d2-9149-ef80e2eb4686 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 30 03:53:51.477: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
Dec 30 03:53:51.478: INFO: ExecWithOptions: Clientset creation
Dec 30 03:53:51.478: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-1224/pods/var-expansion-d19092e4-a375-42d2-9149-ef80e2eb4686/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 12/30/22 03:53:51.588
Dec 30 03:53:51.593: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-1224 PodName:var-expansion-d19092e4-a375-42d2-9149-ef80e2eb4686 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 30 03:53:51.593: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
Dec 30 03:53:51.594: INFO: ExecWithOptions: Clientset creation
Dec 30 03:53:51.594: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-1224/pods/var-expansion-d19092e4-a375-42d2-9149-ef80e2eb4686/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 12/30/22 03:53:51.688
Dec 30 03:53:52.204: INFO: Successfully updated pod "var-expansion-d19092e4-a375-42d2-9149-ef80e2eb4686"
STEP: waiting for annotated pod running 12/30/22 03:53:52.204
Dec 30 03:53:52.204: INFO: Waiting up to 2m0s for pod "var-expansion-d19092e4-a375-42d2-9149-ef80e2eb4686" in namespace "var-expansion-1224" to be "running"
Dec 30 03:53:52.208: INFO: Pod "var-expansion-d19092e4-a375-42d2-9149-ef80e2eb4686": Phase="Running", Reason="", readiness=true. Elapsed: 3.76835ms
Dec 30 03:53:52.208: INFO: Pod "var-expansion-d19092e4-a375-42d2-9149-ef80e2eb4686" satisfied condition "running"
STEP: deleting the pod gracefully 12/30/22 03:53:52.208
Dec 30 03:53:52.208: INFO: Deleting pod "var-expansion-d19092e4-a375-42d2-9149-ef80e2eb4686" in namespace "var-expansion-1224"
Dec 30 03:53:52.216: INFO: Wait up to 5m0s for pod "var-expansion-d19092e4-a375-42d2-9149-ef80e2eb4686" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Dec 30 03:54:26.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1224" for this suite. 12/30/22 03:54:26.232
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","completed":160,"skipped":2810,"failed":0}
------------------------------
• [SLOW TEST] [36.811 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:53:49.428
    Dec 30 03:53:49.428: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename var-expansion 12/30/22 03:53:49.429
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:53:49.448
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:53:49.451
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:296
    STEP: creating the pod 12/30/22 03:53:49.454
    STEP: waiting for pod running 12/30/22 03:53:49.463
    Dec 30 03:53:49.463: INFO: Waiting up to 2m0s for pod "var-expansion-d19092e4-a375-42d2-9149-ef80e2eb4686" in namespace "var-expansion-1224" to be "running"
    Dec 30 03:53:49.467: INFO: Pod "var-expansion-d19092e4-a375-42d2-9149-ef80e2eb4686": Phase="Pending", Reason="", readiness=false. Elapsed: 3.958692ms
    Dec 30 03:53:51.473: INFO: Pod "var-expansion-d19092e4-a375-42d2-9149-ef80e2eb4686": Phase="Running", Reason="", readiness=true. Elapsed: 2.009443592s
    Dec 30 03:53:51.473: INFO: Pod "var-expansion-d19092e4-a375-42d2-9149-ef80e2eb4686" satisfied condition "running"
    STEP: creating a file in subpath 12/30/22 03:53:51.473
    Dec 30 03:53:51.477: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-1224 PodName:var-expansion-d19092e4-a375-42d2-9149-ef80e2eb4686 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 30 03:53:51.477: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    Dec 30 03:53:51.478: INFO: ExecWithOptions: Clientset creation
    Dec 30 03:53:51.478: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-1224/pods/var-expansion-d19092e4-a375-42d2-9149-ef80e2eb4686/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 12/30/22 03:53:51.588
    Dec 30 03:53:51.593: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-1224 PodName:var-expansion-d19092e4-a375-42d2-9149-ef80e2eb4686 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 30 03:53:51.593: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    Dec 30 03:53:51.594: INFO: ExecWithOptions: Clientset creation
    Dec 30 03:53:51.594: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-1224/pods/var-expansion-d19092e4-a375-42d2-9149-ef80e2eb4686/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 12/30/22 03:53:51.688
    Dec 30 03:53:52.204: INFO: Successfully updated pod "var-expansion-d19092e4-a375-42d2-9149-ef80e2eb4686"
    STEP: waiting for annotated pod running 12/30/22 03:53:52.204
    Dec 30 03:53:52.204: INFO: Waiting up to 2m0s for pod "var-expansion-d19092e4-a375-42d2-9149-ef80e2eb4686" in namespace "var-expansion-1224" to be "running"
    Dec 30 03:53:52.208: INFO: Pod "var-expansion-d19092e4-a375-42d2-9149-ef80e2eb4686": Phase="Running", Reason="", readiness=true. Elapsed: 3.76835ms
    Dec 30 03:53:52.208: INFO: Pod "var-expansion-d19092e4-a375-42d2-9149-ef80e2eb4686" satisfied condition "running"
    STEP: deleting the pod gracefully 12/30/22 03:53:52.208
    Dec 30 03:53:52.208: INFO: Deleting pod "var-expansion-d19092e4-a375-42d2-9149-ef80e2eb4686" in namespace "var-expansion-1224"
    Dec 30 03:53:52.216: INFO: Wait up to 5m0s for pod "var-expansion-d19092e4-a375-42d2-9149-ef80e2eb4686" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Dec 30 03:54:26.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-1224" for this suite. 12/30/22 03:54:26.232
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:54:26.242
Dec 30 03:54:26.243: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename dns 12/30/22 03:54:26.244
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:54:26.262
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:54:26.266
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 12/30/22 03:54:26.269
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3237.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3237.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3237.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3237.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3237.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-3237.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3237.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-3237.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3237.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-3237.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3237.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-3237.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 122.41.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.41.122_udp@PTR;check="$$(dig +tcp +noall +answer +search 122.41.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.41.122_tcp@PTR;sleep 1; done
 12/30/22 03:54:26.285
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3237.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3237.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3237.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3237.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3237.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-3237.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3237.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-3237.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3237.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-3237.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3237.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-3237.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 122.41.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.41.122_udp@PTR;check="$$(dig +tcp +noall +answer +search 122.41.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.41.122_tcp@PTR;sleep 1; done
 12/30/22 03:54:26.286
STEP: creating a pod to probe DNS 12/30/22 03:54:26.286
STEP: submitting the pod to kubernetes 12/30/22 03:54:26.286
Dec 30 03:54:26.296: INFO: Waiting up to 15m0s for pod "dns-test-3ffbece1-bbf2-4265-8451-809c6687f933" in namespace "dns-3237" to be "running"
Dec 30 03:54:26.301: INFO: Pod "dns-test-3ffbece1-bbf2-4265-8451-809c6687f933": Phase="Pending", Reason="", readiness=false. Elapsed: 4.396323ms
Dec 30 03:54:28.311: INFO: Pod "dns-test-3ffbece1-bbf2-4265-8451-809c6687f933": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014556427s
Dec 30 03:54:30.312: INFO: Pod "dns-test-3ffbece1-bbf2-4265-8451-809c6687f933": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015830254s
Dec 30 03:54:32.311: INFO: Pod "dns-test-3ffbece1-bbf2-4265-8451-809c6687f933": Phase="Pending", Reason="", readiness=false. Elapsed: 6.015187498s
Dec 30 03:54:34.311: INFO: Pod "dns-test-3ffbece1-bbf2-4265-8451-809c6687f933": Phase="Pending", Reason="", readiness=false. Elapsed: 8.015323961s
Dec 30 03:54:36.306: INFO: Pod "dns-test-3ffbece1-bbf2-4265-8451-809c6687f933": Phase="Running", Reason="", readiness=true. Elapsed: 10.009718568s
Dec 30 03:54:36.306: INFO: Pod "dns-test-3ffbece1-bbf2-4265-8451-809c6687f933" satisfied condition "running"
STEP: retrieving the pod 12/30/22 03:54:36.306
STEP: looking for the results for each expected name from probers 12/30/22 03:54:36.311
Dec 30 03:54:36.316: INFO: Unable to read wheezy_udp@dns-test-service.dns-3237.svc.cluster.local from pod dns-3237/dns-test-3ffbece1-bbf2-4265-8451-809c6687f933: the server could not find the requested resource (get pods dns-test-3ffbece1-bbf2-4265-8451-809c6687f933)
Dec 30 03:54:36.321: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3237.svc.cluster.local from pod dns-3237/dns-test-3ffbece1-bbf2-4265-8451-809c6687f933: the server could not find the requested resource (get pods dns-test-3ffbece1-bbf2-4265-8451-809c6687f933)
Dec 30 03:54:36.326: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3237.svc.cluster.local from pod dns-3237/dns-test-3ffbece1-bbf2-4265-8451-809c6687f933: the server could not find the requested resource (get pods dns-test-3ffbece1-bbf2-4265-8451-809c6687f933)
Dec 30 03:54:36.331: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3237.svc.cluster.local from pod dns-3237/dns-test-3ffbece1-bbf2-4265-8451-809c6687f933: the server could not find the requested resource (get pods dns-test-3ffbece1-bbf2-4265-8451-809c6687f933)
Dec 30 03:54:36.356: INFO: Unable to read jessie_udp@dns-test-service.dns-3237.svc.cluster.local from pod dns-3237/dns-test-3ffbece1-bbf2-4265-8451-809c6687f933: the server could not find the requested resource (get pods dns-test-3ffbece1-bbf2-4265-8451-809c6687f933)
Dec 30 03:54:36.361: INFO: Unable to read jessie_tcp@dns-test-service.dns-3237.svc.cluster.local from pod dns-3237/dns-test-3ffbece1-bbf2-4265-8451-809c6687f933: the server could not find the requested resource (get pods dns-test-3ffbece1-bbf2-4265-8451-809c6687f933)
Dec 30 03:54:36.366: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3237.svc.cluster.local from pod dns-3237/dns-test-3ffbece1-bbf2-4265-8451-809c6687f933: the server could not find the requested resource (get pods dns-test-3ffbece1-bbf2-4265-8451-809c6687f933)
Dec 30 03:54:36.371: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3237.svc.cluster.local from pod dns-3237/dns-test-3ffbece1-bbf2-4265-8451-809c6687f933: the server could not find the requested resource (get pods dns-test-3ffbece1-bbf2-4265-8451-809c6687f933)
Dec 30 03:54:36.390: INFO: Lookups using dns-3237/dns-test-3ffbece1-bbf2-4265-8451-809c6687f933 failed for: [wheezy_udp@dns-test-service.dns-3237.svc.cluster.local wheezy_tcp@dns-test-service.dns-3237.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3237.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3237.svc.cluster.local jessie_udp@dns-test-service.dns-3237.svc.cluster.local jessie_tcp@dns-test-service.dns-3237.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3237.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3237.svc.cluster.local]

Dec 30 03:54:41.466: INFO: DNS probes using dns-3237/dns-test-3ffbece1-bbf2-4265-8451-809c6687f933 succeeded

STEP: deleting the pod 12/30/22 03:54:41.466
STEP: deleting the test service 12/30/22 03:54:41.481
STEP: deleting the test headless service 12/30/22 03:54:41.497
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Dec 30 03:54:41.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3237" for this suite. 12/30/22 03:54:41.514
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","completed":161,"skipped":2828,"failed":0}
------------------------------
• [SLOW TEST] [15.278 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:54:26.242
    Dec 30 03:54:26.243: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename dns 12/30/22 03:54:26.244
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:54:26.262
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:54:26.266
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 12/30/22 03:54:26.269
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3237.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3237.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3237.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3237.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3237.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-3237.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3237.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-3237.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3237.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-3237.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3237.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-3237.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 122.41.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.41.122_udp@PTR;check="$$(dig +tcp +noall +answer +search 122.41.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.41.122_tcp@PTR;sleep 1; done
     12/30/22 03:54:26.285
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3237.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3237.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3237.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3237.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3237.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-3237.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3237.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-3237.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3237.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-3237.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3237.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-3237.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 122.41.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.41.122_udp@PTR;check="$$(dig +tcp +noall +answer +search 122.41.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.41.122_tcp@PTR;sleep 1; done
     12/30/22 03:54:26.286
    STEP: creating a pod to probe DNS 12/30/22 03:54:26.286
    STEP: submitting the pod to kubernetes 12/30/22 03:54:26.286
    Dec 30 03:54:26.296: INFO: Waiting up to 15m0s for pod "dns-test-3ffbece1-bbf2-4265-8451-809c6687f933" in namespace "dns-3237" to be "running"
    Dec 30 03:54:26.301: INFO: Pod "dns-test-3ffbece1-bbf2-4265-8451-809c6687f933": Phase="Pending", Reason="", readiness=false. Elapsed: 4.396323ms
    Dec 30 03:54:28.311: INFO: Pod "dns-test-3ffbece1-bbf2-4265-8451-809c6687f933": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014556427s
    Dec 30 03:54:30.312: INFO: Pod "dns-test-3ffbece1-bbf2-4265-8451-809c6687f933": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015830254s
    Dec 30 03:54:32.311: INFO: Pod "dns-test-3ffbece1-bbf2-4265-8451-809c6687f933": Phase="Pending", Reason="", readiness=false. Elapsed: 6.015187498s
    Dec 30 03:54:34.311: INFO: Pod "dns-test-3ffbece1-bbf2-4265-8451-809c6687f933": Phase="Pending", Reason="", readiness=false. Elapsed: 8.015323961s
    Dec 30 03:54:36.306: INFO: Pod "dns-test-3ffbece1-bbf2-4265-8451-809c6687f933": Phase="Running", Reason="", readiness=true. Elapsed: 10.009718568s
    Dec 30 03:54:36.306: INFO: Pod "dns-test-3ffbece1-bbf2-4265-8451-809c6687f933" satisfied condition "running"
    STEP: retrieving the pod 12/30/22 03:54:36.306
    STEP: looking for the results for each expected name from probers 12/30/22 03:54:36.311
    Dec 30 03:54:36.316: INFO: Unable to read wheezy_udp@dns-test-service.dns-3237.svc.cluster.local from pod dns-3237/dns-test-3ffbece1-bbf2-4265-8451-809c6687f933: the server could not find the requested resource (get pods dns-test-3ffbece1-bbf2-4265-8451-809c6687f933)
    Dec 30 03:54:36.321: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3237.svc.cluster.local from pod dns-3237/dns-test-3ffbece1-bbf2-4265-8451-809c6687f933: the server could not find the requested resource (get pods dns-test-3ffbece1-bbf2-4265-8451-809c6687f933)
    Dec 30 03:54:36.326: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3237.svc.cluster.local from pod dns-3237/dns-test-3ffbece1-bbf2-4265-8451-809c6687f933: the server could not find the requested resource (get pods dns-test-3ffbece1-bbf2-4265-8451-809c6687f933)
    Dec 30 03:54:36.331: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3237.svc.cluster.local from pod dns-3237/dns-test-3ffbece1-bbf2-4265-8451-809c6687f933: the server could not find the requested resource (get pods dns-test-3ffbece1-bbf2-4265-8451-809c6687f933)
    Dec 30 03:54:36.356: INFO: Unable to read jessie_udp@dns-test-service.dns-3237.svc.cluster.local from pod dns-3237/dns-test-3ffbece1-bbf2-4265-8451-809c6687f933: the server could not find the requested resource (get pods dns-test-3ffbece1-bbf2-4265-8451-809c6687f933)
    Dec 30 03:54:36.361: INFO: Unable to read jessie_tcp@dns-test-service.dns-3237.svc.cluster.local from pod dns-3237/dns-test-3ffbece1-bbf2-4265-8451-809c6687f933: the server could not find the requested resource (get pods dns-test-3ffbece1-bbf2-4265-8451-809c6687f933)
    Dec 30 03:54:36.366: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3237.svc.cluster.local from pod dns-3237/dns-test-3ffbece1-bbf2-4265-8451-809c6687f933: the server could not find the requested resource (get pods dns-test-3ffbece1-bbf2-4265-8451-809c6687f933)
    Dec 30 03:54:36.371: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3237.svc.cluster.local from pod dns-3237/dns-test-3ffbece1-bbf2-4265-8451-809c6687f933: the server could not find the requested resource (get pods dns-test-3ffbece1-bbf2-4265-8451-809c6687f933)
    Dec 30 03:54:36.390: INFO: Lookups using dns-3237/dns-test-3ffbece1-bbf2-4265-8451-809c6687f933 failed for: [wheezy_udp@dns-test-service.dns-3237.svc.cluster.local wheezy_tcp@dns-test-service.dns-3237.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3237.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3237.svc.cluster.local jessie_udp@dns-test-service.dns-3237.svc.cluster.local jessie_tcp@dns-test-service.dns-3237.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3237.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3237.svc.cluster.local]

    Dec 30 03:54:41.466: INFO: DNS probes using dns-3237/dns-test-3ffbece1-bbf2-4265-8451-809c6687f933 succeeded

    STEP: deleting the pod 12/30/22 03:54:41.466
    STEP: deleting the test service 12/30/22 03:54:41.481
    STEP: deleting the test headless service 12/30/22 03:54:41.497
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Dec 30 03:54:41.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-3237" for this suite. 12/30/22 03:54:41.514
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:54:41.524
Dec 30 03:54:41.524: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename subpath 12/30/22 03:54:41.526
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:54:41.547
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:54:41.551
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 12/30/22 03:54:41.554
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-djk9 12/30/22 03:54:41.626
STEP: Creating a pod to test atomic-volume-subpath 12/30/22 03:54:41.626
Dec 30 03:54:41.635: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-djk9" in namespace "subpath-8563" to be "Succeeded or Failed"
Dec 30 03:54:41.764: INFO: Pod "pod-subpath-test-secret-djk9": Phase="Pending", Reason="", readiness=false. Elapsed: 128.719464ms
Dec 30 03:54:43.769: INFO: Pod "pod-subpath-test-secret-djk9": Phase="Running", Reason="", readiness=true. Elapsed: 2.134142607s
Dec 30 03:54:45.771: INFO: Pod "pod-subpath-test-secret-djk9": Phase="Running", Reason="", readiness=true. Elapsed: 4.135230602s
Dec 30 03:54:47.771: INFO: Pod "pod-subpath-test-secret-djk9": Phase="Running", Reason="", readiness=true. Elapsed: 6.135201678s
Dec 30 03:54:49.770: INFO: Pod "pod-subpath-test-secret-djk9": Phase="Running", Reason="", readiness=true. Elapsed: 8.134898462s
Dec 30 03:54:51.770: INFO: Pod "pod-subpath-test-secret-djk9": Phase="Running", Reason="", readiness=true. Elapsed: 10.1345095s
Dec 30 03:54:53.771: INFO: Pod "pod-subpath-test-secret-djk9": Phase="Running", Reason="", readiness=true. Elapsed: 12.135796574s
Dec 30 03:54:55.771: INFO: Pod "pod-subpath-test-secret-djk9": Phase="Running", Reason="", readiness=true. Elapsed: 14.135454977s
Dec 30 03:54:57.771: INFO: Pod "pod-subpath-test-secret-djk9": Phase="Running", Reason="", readiness=true. Elapsed: 16.135211079s
Dec 30 03:54:59.770: INFO: Pod "pod-subpath-test-secret-djk9": Phase="Running", Reason="", readiness=true. Elapsed: 18.134264059s
Dec 30 03:55:01.769: INFO: Pod "pod-subpath-test-secret-djk9": Phase="Running", Reason="", readiness=true. Elapsed: 20.133783755s
Dec 30 03:55:03.770: INFO: Pod "pod-subpath-test-secret-djk9": Phase="Running", Reason="", readiness=false. Elapsed: 22.134642119s
Dec 30 03:55:05.771: INFO: Pod "pod-subpath-test-secret-djk9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.135264106s
STEP: Saw pod success 12/30/22 03:55:05.771
Dec 30 03:55:05.771: INFO: Pod "pod-subpath-test-secret-djk9" satisfied condition "Succeeded or Failed"
Dec 30 03:55:05.775: INFO: Trying to get logs from node k8s-mgmt01 pod pod-subpath-test-secret-djk9 container test-container-subpath-secret-djk9: <nil>
STEP: delete the pod 12/30/22 03:55:05.796
Dec 30 03:55:05.810: INFO: Waiting for pod pod-subpath-test-secret-djk9 to disappear
Dec 30 03:55:05.814: INFO: Pod pod-subpath-test-secret-djk9 no longer exists
STEP: Deleting pod pod-subpath-test-secret-djk9 12/30/22 03:55:05.814
Dec 30 03:55:05.814: INFO: Deleting pod "pod-subpath-test-secret-djk9" in namespace "subpath-8563"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Dec 30 03:55:05.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8563" for this suite. 12/30/22 03:55:05.824
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]","completed":162,"skipped":2860,"failed":0}
------------------------------
• [SLOW TEST] [24.307 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:54:41.524
    Dec 30 03:54:41.524: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename subpath 12/30/22 03:54:41.526
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:54:41.547
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:54:41.551
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 12/30/22 03:54:41.554
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-djk9 12/30/22 03:54:41.626
    STEP: Creating a pod to test atomic-volume-subpath 12/30/22 03:54:41.626
    Dec 30 03:54:41.635: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-djk9" in namespace "subpath-8563" to be "Succeeded or Failed"
    Dec 30 03:54:41.764: INFO: Pod "pod-subpath-test-secret-djk9": Phase="Pending", Reason="", readiness=false. Elapsed: 128.719464ms
    Dec 30 03:54:43.769: INFO: Pod "pod-subpath-test-secret-djk9": Phase="Running", Reason="", readiness=true. Elapsed: 2.134142607s
    Dec 30 03:54:45.771: INFO: Pod "pod-subpath-test-secret-djk9": Phase="Running", Reason="", readiness=true. Elapsed: 4.135230602s
    Dec 30 03:54:47.771: INFO: Pod "pod-subpath-test-secret-djk9": Phase="Running", Reason="", readiness=true. Elapsed: 6.135201678s
    Dec 30 03:54:49.770: INFO: Pod "pod-subpath-test-secret-djk9": Phase="Running", Reason="", readiness=true. Elapsed: 8.134898462s
    Dec 30 03:54:51.770: INFO: Pod "pod-subpath-test-secret-djk9": Phase="Running", Reason="", readiness=true. Elapsed: 10.1345095s
    Dec 30 03:54:53.771: INFO: Pod "pod-subpath-test-secret-djk9": Phase="Running", Reason="", readiness=true. Elapsed: 12.135796574s
    Dec 30 03:54:55.771: INFO: Pod "pod-subpath-test-secret-djk9": Phase="Running", Reason="", readiness=true. Elapsed: 14.135454977s
    Dec 30 03:54:57.771: INFO: Pod "pod-subpath-test-secret-djk9": Phase="Running", Reason="", readiness=true. Elapsed: 16.135211079s
    Dec 30 03:54:59.770: INFO: Pod "pod-subpath-test-secret-djk9": Phase="Running", Reason="", readiness=true. Elapsed: 18.134264059s
    Dec 30 03:55:01.769: INFO: Pod "pod-subpath-test-secret-djk9": Phase="Running", Reason="", readiness=true. Elapsed: 20.133783755s
    Dec 30 03:55:03.770: INFO: Pod "pod-subpath-test-secret-djk9": Phase="Running", Reason="", readiness=false. Elapsed: 22.134642119s
    Dec 30 03:55:05.771: INFO: Pod "pod-subpath-test-secret-djk9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.135264106s
    STEP: Saw pod success 12/30/22 03:55:05.771
    Dec 30 03:55:05.771: INFO: Pod "pod-subpath-test-secret-djk9" satisfied condition "Succeeded or Failed"
    Dec 30 03:55:05.775: INFO: Trying to get logs from node k8s-mgmt01 pod pod-subpath-test-secret-djk9 container test-container-subpath-secret-djk9: <nil>
    STEP: delete the pod 12/30/22 03:55:05.796
    Dec 30 03:55:05.810: INFO: Waiting for pod pod-subpath-test-secret-djk9 to disappear
    Dec 30 03:55:05.814: INFO: Pod pod-subpath-test-secret-djk9 no longer exists
    STEP: Deleting pod pod-subpath-test-secret-djk9 12/30/22 03:55:05.814
    Dec 30 03:55:05.814: INFO: Deleting pod "pod-subpath-test-secret-djk9" in namespace "subpath-8563"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Dec 30 03:55:05.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-8563" for this suite. 12/30/22 03:55:05.824
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:55:05.836
Dec 30 03:55:05.836: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename services 12/30/22 03:55:05.838
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:55:05.856
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:55:05.861
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173
STEP: creating service in namespace services-3775 12/30/22 03:55:05.865
Dec 30 03:55:05.875: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-3775" to be "running and ready"
Dec 30 03:55:05.878: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 3.489049ms
Dec 30 03:55:05.878: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Dec 30 03:55:07.884: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.009490417s
Dec 30 03:55:07.884: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
Dec 30 03:55:07.884: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
Dec 30 03:55:07.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-3775 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Dec 30 03:55:08.109: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Dec 30 03:55:08.109: INFO: stdout: "ipvs"
Dec 30 03:55:08.109: INFO: proxyMode: ipvs
Dec 30 03:55:08.122: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Dec 30 03:55:08.126: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-3775 12/30/22 03:55:08.126
STEP: creating replication controller affinity-clusterip-timeout in namespace services-3775 12/30/22 03:55:08.136
I1230 03:55:08.143401      25 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-3775, replica count: 3
I1230 03:55:11.195484      25 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 30 03:55:11.203: INFO: Creating new exec pod
Dec 30 03:55:11.208: INFO: Waiting up to 5m0s for pod "execpod-affinitynxhst" in namespace "services-3775" to be "running"
Dec 30 03:55:11.212: INFO: Pod "execpod-affinitynxhst": Phase="Pending", Reason="", readiness=false. Elapsed: 4.135348ms
Dec 30 03:55:13.218: INFO: Pod "execpod-affinitynxhst": Phase="Running", Reason="", readiness=true. Elapsed: 2.009613853s
Dec 30 03:55:13.218: INFO: Pod "execpod-affinitynxhst" satisfied condition "running"
Dec 30 03:55:14.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-3775 exec execpod-affinitynxhst -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
Dec 30 03:55:14.449: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Dec 30 03:55:14.449: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 30 03:55:14.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-3775 exec execpod-affinitynxhst -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.16.238 80'
Dec 30 03:55:14.639: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.16.238 80\nConnection to 10.233.16.238 80 port [tcp/http] succeeded!\n"
Dec 30 03:55:14.639: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 30 03:55:14.639: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-3775 exec execpod-affinitynxhst -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.16.238:80/ ; done'
Dec 30 03:55:14.949: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.16.238:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.16.238:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.16.238:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.16.238:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.16.238:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.16.238:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.16.238:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.16.238:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.16.238:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.16.238:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.16.238:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.16.238:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.16.238:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.16.238:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.16.238:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.16.238:80/\n"
Dec 30 03:55:14.949: INFO: stdout: "\naffinity-clusterip-timeout-v55kf\naffinity-clusterip-timeout-v55kf\naffinity-clusterip-timeout-v55kf\naffinity-clusterip-timeout-v55kf\naffinity-clusterip-timeout-v55kf\naffinity-clusterip-timeout-v55kf\naffinity-clusterip-timeout-v55kf\naffinity-clusterip-timeout-v55kf\naffinity-clusterip-timeout-v55kf\naffinity-clusterip-timeout-v55kf\naffinity-clusterip-timeout-v55kf\naffinity-clusterip-timeout-v55kf\naffinity-clusterip-timeout-v55kf\naffinity-clusterip-timeout-v55kf\naffinity-clusterip-timeout-v55kf\naffinity-clusterip-timeout-v55kf"
Dec 30 03:55:14.949: INFO: Received response from host: affinity-clusterip-timeout-v55kf
Dec 30 03:55:14.949: INFO: Received response from host: affinity-clusterip-timeout-v55kf
Dec 30 03:55:14.949: INFO: Received response from host: affinity-clusterip-timeout-v55kf
Dec 30 03:55:14.949: INFO: Received response from host: affinity-clusterip-timeout-v55kf
Dec 30 03:55:14.949: INFO: Received response from host: affinity-clusterip-timeout-v55kf
Dec 30 03:55:14.949: INFO: Received response from host: affinity-clusterip-timeout-v55kf
Dec 30 03:55:14.949: INFO: Received response from host: affinity-clusterip-timeout-v55kf
Dec 30 03:55:14.949: INFO: Received response from host: affinity-clusterip-timeout-v55kf
Dec 30 03:55:14.949: INFO: Received response from host: affinity-clusterip-timeout-v55kf
Dec 30 03:55:14.949: INFO: Received response from host: affinity-clusterip-timeout-v55kf
Dec 30 03:55:14.949: INFO: Received response from host: affinity-clusterip-timeout-v55kf
Dec 30 03:55:14.949: INFO: Received response from host: affinity-clusterip-timeout-v55kf
Dec 30 03:55:14.949: INFO: Received response from host: affinity-clusterip-timeout-v55kf
Dec 30 03:55:14.949: INFO: Received response from host: affinity-clusterip-timeout-v55kf
Dec 30 03:55:14.949: INFO: Received response from host: affinity-clusterip-timeout-v55kf
Dec 30 03:55:14.949: INFO: Received response from host: affinity-clusterip-timeout-v55kf
Dec 30 03:55:14.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-3775 exec execpod-affinitynxhst -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.233.16.238:80/'
Dec 30 03:55:15.144: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.233.16.238:80/\n"
Dec 30 03:55:15.144: INFO: stdout: "affinity-clusterip-timeout-v55kf"
Dec 30 03:57:25.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-3775 exec execpod-affinitynxhst -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.233.16.238:80/'
Dec 30 03:57:25.374: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.233.16.238:80/\n"
Dec 30 03:57:25.374: INFO: stdout: "affinity-clusterip-timeout-wbt68"
Dec 30 03:57:25.374: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-3775, will wait for the garbage collector to delete the pods 12/30/22 03:57:25.395
Dec 30 03:57:25.457: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 6.936934ms
Dec 30 03:57:25.558: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 100.765787ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Dec 30 03:57:27.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3775" for this suite. 12/30/22 03:57:27.683
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","completed":163,"skipped":2914,"failed":0}
------------------------------
• [SLOW TEST] [141.854 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:55:05.836
    Dec 30 03:55:05.836: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename services 12/30/22 03:55:05.838
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:55:05.856
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:55:05.861
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2173
    STEP: creating service in namespace services-3775 12/30/22 03:55:05.865
    Dec 30 03:55:05.875: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-3775" to be "running and ready"
    Dec 30 03:55:05.878: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 3.489049ms
    Dec 30 03:55:05.878: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
    Dec 30 03:55:07.884: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.009490417s
    Dec 30 03:55:07.884: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
    Dec 30 03:55:07.884: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
    Dec 30 03:55:07.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-3775 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
    Dec 30 03:55:08.109: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
    Dec 30 03:55:08.109: INFO: stdout: "ipvs"
    Dec 30 03:55:08.109: INFO: proxyMode: ipvs
    Dec 30 03:55:08.122: INFO: Waiting for pod kube-proxy-mode-detector to disappear
    Dec 30 03:55:08.126: INFO: Pod kube-proxy-mode-detector no longer exists
    STEP: creating service affinity-clusterip-timeout in namespace services-3775 12/30/22 03:55:08.126
    STEP: creating replication controller affinity-clusterip-timeout in namespace services-3775 12/30/22 03:55:08.136
    I1230 03:55:08.143401      25 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-3775, replica count: 3
    I1230 03:55:11.195484      25 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Dec 30 03:55:11.203: INFO: Creating new exec pod
    Dec 30 03:55:11.208: INFO: Waiting up to 5m0s for pod "execpod-affinitynxhst" in namespace "services-3775" to be "running"
    Dec 30 03:55:11.212: INFO: Pod "execpod-affinitynxhst": Phase="Pending", Reason="", readiness=false. Elapsed: 4.135348ms
    Dec 30 03:55:13.218: INFO: Pod "execpod-affinitynxhst": Phase="Running", Reason="", readiness=true. Elapsed: 2.009613853s
    Dec 30 03:55:13.218: INFO: Pod "execpod-affinitynxhst" satisfied condition "running"
    Dec 30 03:55:14.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-3775 exec execpod-affinitynxhst -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
    Dec 30 03:55:14.449: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
    Dec 30 03:55:14.449: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Dec 30 03:55:14.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-3775 exec execpod-affinitynxhst -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.16.238 80'
    Dec 30 03:55:14.639: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.16.238 80\nConnection to 10.233.16.238 80 port [tcp/http] succeeded!\n"
    Dec 30 03:55:14.639: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Dec 30 03:55:14.639: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-3775 exec execpod-affinitynxhst -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.16.238:80/ ; done'
    Dec 30 03:55:14.949: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.16.238:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.16.238:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.16.238:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.16.238:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.16.238:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.16.238:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.16.238:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.16.238:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.16.238:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.16.238:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.16.238:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.16.238:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.16.238:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.16.238:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.16.238:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.16.238:80/\n"
    Dec 30 03:55:14.949: INFO: stdout: "\naffinity-clusterip-timeout-v55kf\naffinity-clusterip-timeout-v55kf\naffinity-clusterip-timeout-v55kf\naffinity-clusterip-timeout-v55kf\naffinity-clusterip-timeout-v55kf\naffinity-clusterip-timeout-v55kf\naffinity-clusterip-timeout-v55kf\naffinity-clusterip-timeout-v55kf\naffinity-clusterip-timeout-v55kf\naffinity-clusterip-timeout-v55kf\naffinity-clusterip-timeout-v55kf\naffinity-clusterip-timeout-v55kf\naffinity-clusterip-timeout-v55kf\naffinity-clusterip-timeout-v55kf\naffinity-clusterip-timeout-v55kf\naffinity-clusterip-timeout-v55kf"
    Dec 30 03:55:14.949: INFO: Received response from host: affinity-clusterip-timeout-v55kf
    Dec 30 03:55:14.949: INFO: Received response from host: affinity-clusterip-timeout-v55kf
    Dec 30 03:55:14.949: INFO: Received response from host: affinity-clusterip-timeout-v55kf
    Dec 30 03:55:14.949: INFO: Received response from host: affinity-clusterip-timeout-v55kf
    Dec 30 03:55:14.949: INFO: Received response from host: affinity-clusterip-timeout-v55kf
    Dec 30 03:55:14.949: INFO: Received response from host: affinity-clusterip-timeout-v55kf
    Dec 30 03:55:14.949: INFO: Received response from host: affinity-clusterip-timeout-v55kf
    Dec 30 03:55:14.949: INFO: Received response from host: affinity-clusterip-timeout-v55kf
    Dec 30 03:55:14.949: INFO: Received response from host: affinity-clusterip-timeout-v55kf
    Dec 30 03:55:14.949: INFO: Received response from host: affinity-clusterip-timeout-v55kf
    Dec 30 03:55:14.949: INFO: Received response from host: affinity-clusterip-timeout-v55kf
    Dec 30 03:55:14.949: INFO: Received response from host: affinity-clusterip-timeout-v55kf
    Dec 30 03:55:14.949: INFO: Received response from host: affinity-clusterip-timeout-v55kf
    Dec 30 03:55:14.949: INFO: Received response from host: affinity-clusterip-timeout-v55kf
    Dec 30 03:55:14.949: INFO: Received response from host: affinity-clusterip-timeout-v55kf
    Dec 30 03:55:14.949: INFO: Received response from host: affinity-clusterip-timeout-v55kf
    Dec 30 03:55:14.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-3775 exec execpod-affinitynxhst -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.233.16.238:80/'
    Dec 30 03:55:15.144: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.233.16.238:80/\n"
    Dec 30 03:55:15.144: INFO: stdout: "affinity-clusterip-timeout-v55kf"
    Dec 30 03:57:25.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-3775 exec execpod-affinitynxhst -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.233.16.238:80/'
    Dec 30 03:57:25.374: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.233.16.238:80/\n"
    Dec 30 03:57:25.374: INFO: stdout: "affinity-clusterip-timeout-wbt68"
    Dec 30 03:57:25.374: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-3775, will wait for the garbage collector to delete the pods 12/30/22 03:57:25.395
    Dec 30 03:57:25.457: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 6.936934ms
    Dec 30 03:57:25.558: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 100.765787ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Dec 30 03:57:27.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-3775" for this suite. 12/30/22 03:57:27.683
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:57:27.692
Dec 30 03:57:27.692: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename webhook 12/30/22 03:57:27.693
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:57:27.711
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:57:27.715
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 12/30/22 03:57:27.734
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/30/22 03:57:28.475
STEP: Deploying the webhook pod 12/30/22 03:57:28.483
STEP: Wait for the deployment to be ready 12/30/22 03:57:28.495
Dec 30 03:57:28.504: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 12/30/22 03:57:30.519
STEP: Verifying the service has paired with the endpoint 12/30/22 03:57:30.531
Dec 30 03:57:31.531: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655
STEP: Listing all of the created validation webhooks 12/30/22 03:57:31.596
STEP: Creating a configMap that should be mutated 12/30/22 03:57:31.615
STEP: Deleting the collection of validation webhooks 12/30/22 03:57:31.649
STEP: Creating a configMap that should not be mutated 12/30/22 03:57:31.705
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Dec 30 03:57:31.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6941" for this suite. 12/30/22 03:57:31.722
STEP: Destroying namespace "webhook-6941-markers" for this suite. 12/30/22 03:57:31.729
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","completed":164,"skipped":2938,"failed":0}
------------------------------
• [4.079 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:57:27.692
    Dec 30 03:57:27.692: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename webhook 12/30/22 03:57:27.693
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:57:27.711
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:57:27.715
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 12/30/22 03:57:27.734
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/30/22 03:57:28.475
    STEP: Deploying the webhook pod 12/30/22 03:57:28.483
    STEP: Wait for the deployment to be ready 12/30/22 03:57:28.495
    Dec 30 03:57:28.504: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 12/30/22 03:57:30.519
    STEP: Verifying the service has paired with the endpoint 12/30/22 03:57:30.531
    Dec 30 03:57:31.531: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:655
    STEP: Listing all of the created validation webhooks 12/30/22 03:57:31.596
    STEP: Creating a configMap that should be mutated 12/30/22 03:57:31.615
    STEP: Deleting the collection of validation webhooks 12/30/22 03:57:31.649
    STEP: Creating a configMap that should not be mutated 12/30/22 03:57:31.705
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Dec 30 03:57:31.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-6941" for this suite. 12/30/22 03:57:31.722
    STEP: Destroying namespace "webhook-6941-markers" for this suite. 12/30/22 03:57:31.729
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:57:31.772
Dec 30 03:57:31.772: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename secrets 12/30/22 03:57:31.774
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:57:31.791
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:57:31.794
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124
STEP: Creating secret with name secret-test-f9e3007b-f3ed-4a99-bb0f-8dabf9650e2e 12/30/22 03:57:31.797
STEP: Creating a pod to test consume secrets 12/30/22 03:57:31.801
Dec 30 03:57:31.811: INFO: Waiting up to 5m0s for pod "pod-secrets-307c05f5-2222-46aa-a6bf-76aea2a680b6" in namespace "secrets-7795" to be "Succeeded or Failed"
Dec 30 03:57:31.815: INFO: Pod "pod-secrets-307c05f5-2222-46aa-a6bf-76aea2a680b6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.999946ms
Dec 30 03:57:33.821: INFO: Pod "pod-secrets-307c05f5-2222-46aa-a6bf-76aea2a680b6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010217588s
Dec 30 03:57:35.820: INFO: Pod "pod-secrets-307c05f5-2222-46aa-a6bf-76aea2a680b6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009168965s
STEP: Saw pod success 12/30/22 03:57:35.82
Dec 30 03:57:35.821: INFO: Pod "pod-secrets-307c05f5-2222-46aa-a6bf-76aea2a680b6" satisfied condition "Succeeded or Failed"
Dec 30 03:57:35.824: INFO: Trying to get logs from node k8s-mgmt01 pod pod-secrets-307c05f5-2222-46aa-a6bf-76aea2a680b6 container secret-volume-test: <nil>
STEP: delete the pod 12/30/22 03:57:35.845
Dec 30 03:57:35.886: INFO: Waiting for pod pod-secrets-307c05f5-2222-46aa-a6bf-76aea2a680b6 to disappear
Dec 30 03:57:35.889: INFO: Pod pod-secrets-307c05f5-2222-46aa-a6bf-76aea2a680b6 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Dec 30 03:57:35.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7795" for this suite. 12/30/22 03:57:35.896
{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","completed":165,"skipped":2941,"failed":0}
------------------------------
• [4.278 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:57:31.772
    Dec 30 03:57:31.772: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename secrets 12/30/22 03:57:31.774
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:57:31.791
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:57:31.794
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:124
    STEP: Creating secret with name secret-test-f9e3007b-f3ed-4a99-bb0f-8dabf9650e2e 12/30/22 03:57:31.797
    STEP: Creating a pod to test consume secrets 12/30/22 03:57:31.801
    Dec 30 03:57:31.811: INFO: Waiting up to 5m0s for pod "pod-secrets-307c05f5-2222-46aa-a6bf-76aea2a680b6" in namespace "secrets-7795" to be "Succeeded or Failed"
    Dec 30 03:57:31.815: INFO: Pod "pod-secrets-307c05f5-2222-46aa-a6bf-76aea2a680b6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.999946ms
    Dec 30 03:57:33.821: INFO: Pod "pod-secrets-307c05f5-2222-46aa-a6bf-76aea2a680b6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010217588s
    Dec 30 03:57:35.820: INFO: Pod "pod-secrets-307c05f5-2222-46aa-a6bf-76aea2a680b6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009168965s
    STEP: Saw pod success 12/30/22 03:57:35.82
    Dec 30 03:57:35.821: INFO: Pod "pod-secrets-307c05f5-2222-46aa-a6bf-76aea2a680b6" satisfied condition "Succeeded or Failed"
    Dec 30 03:57:35.824: INFO: Trying to get logs from node k8s-mgmt01 pod pod-secrets-307c05f5-2222-46aa-a6bf-76aea2a680b6 container secret-volume-test: <nil>
    STEP: delete the pod 12/30/22 03:57:35.845
    Dec 30 03:57:35.886: INFO: Waiting for pod pod-secrets-307c05f5-2222-46aa-a6bf-76aea2a680b6 to disappear
    Dec 30 03:57:35.889: INFO: Pod pod-secrets-307c05f5-2222-46aa-a6bf-76aea2a680b6 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Dec 30 03:57:35.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-7795" for this suite. 12/30/22 03:57:35.896
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:57:36.052
Dec 30 03:57:36.052: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename downward-api 12/30/22 03:57:36.053
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:57:36.07
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:57:36.074
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161
STEP: Creating the pod 12/30/22 03:57:36.077
Dec 30 03:57:36.087: INFO: Waiting up to 5m0s for pod "annotationupdate3b359a17-a521-4d0a-a938-5c9498445783" in namespace "downward-api-3921" to be "running and ready"
Dec 30 03:57:36.091: INFO: Pod "annotationupdate3b359a17-a521-4d0a-a938-5c9498445783": Phase="Pending", Reason="", readiness=false. Elapsed: 4.209098ms
Dec 30 03:57:36.092: INFO: The phase of Pod annotationupdate3b359a17-a521-4d0a-a938-5c9498445783 is Pending, waiting for it to be Running (with Ready = true)
Dec 30 03:57:38.097: INFO: Pod "annotationupdate3b359a17-a521-4d0a-a938-5c9498445783": Phase="Running", Reason="", readiness=true. Elapsed: 2.009958382s
Dec 30 03:57:38.097: INFO: The phase of Pod annotationupdate3b359a17-a521-4d0a-a938-5c9498445783 is Running (Ready = true)
Dec 30 03:57:38.097: INFO: Pod "annotationupdate3b359a17-a521-4d0a-a938-5c9498445783" satisfied condition "running and ready"
Dec 30 03:57:38.624: INFO: Successfully updated pod "annotationupdate3b359a17-a521-4d0a-a938-5c9498445783"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Dec 30 03:57:42.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3921" for this suite. 12/30/22 03:57:42.659
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","completed":166,"skipped":2968,"failed":0}
------------------------------
• [SLOW TEST] [6.623 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:57:36.052
    Dec 30 03:57:36.052: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename downward-api 12/30/22 03:57:36.053
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:57:36.07
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:57:36.074
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:161
    STEP: Creating the pod 12/30/22 03:57:36.077
    Dec 30 03:57:36.087: INFO: Waiting up to 5m0s for pod "annotationupdate3b359a17-a521-4d0a-a938-5c9498445783" in namespace "downward-api-3921" to be "running and ready"
    Dec 30 03:57:36.091: INFO: Pod "annotationupdate3b359a17-a521-4d0a-a938-5c9498445783": Phase="Pending", Reason="", readiness=false. Elapsed: 4.209098ms
    Dec 30 03:57:36.092: INFO: The phase of Pod annotationupdate3b359a17-a521-4d0a-a938-5c9498445783 is Pending, waiting for it to be Running (with Ready = true)
    Dec 30 03:57:38.097: INFO: Pod "annotationupdate3b359a17-a521-4d0a-a938-5c9498445783": Phase="Running", Reason="", readiness=true. Elapsed: 2.009958382s
    Dec 30 03:57:38.097: INFO: The phase of Pod annotationupdate3b359a17-a521-4d0a-a938-5c9498445783 is Running (Ready = true)
    Dec 30 03:57:38.097: INFO: Pod "annotationupdate3b359a17-a521-4d0a-a938-5c9498445783" satisfied condition "running and ready"
    Dec 30 03:57:38.624: INFO: Successfully updated pod "annotationupdate3b359a17-a521-4d0a-a938-5c9498445783"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Dec 30 03:57:42.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-3921" for this suite. 12/30/22 03:57:42.659
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:51
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:57:42.676
Dec 30 03:57:42.676: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename container-runtime 12/30/22 03:57:42.678
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:57:42.695
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:57:42.699
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:51
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 12/30/22 03:57:42.711
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 12/30/22 03:58:01.809
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 12/30/22 03:58:01.814
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 12/30/22 03:58:01.822
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 12/30/22 03:58:01.822
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 12/30/22 03:58:01.843
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 12/30/22 03:58:04.862
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 12/30/22 03:58:06.877
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 12/30/22 03:58:06.885
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 12/30/22 03:58:06.885
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 12/30/22 03:58:06.912
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 12/30/22 03:58:07.921
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 12/30/22 03:58:10.94
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 12/30/22 03:58:10.949
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 12/30/22 03:58:10.949
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Dec 30 03:58:10.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3259" for this suite. 12/30/22 03:58:10.988
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","completed":167,"skipped":2978,"failed":0}
------------------------------
• [SLOW TEST] [28.319 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    when starting a container that exits
    test/e2e/common/node/runtime.go:44
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:57:42.676
    Dec 30 03:57:42.676: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename container-runtime 12/30/22 03:57:42.678
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:57:42.695
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:57:42.699
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:51
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 12/30/22 03:57:42.711
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 12/30/22 03:58:01.809
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 12/30/22 03:58:01.814
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 12/30/22 03:58:01.822
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 12/30/22 03:58:01.822
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 12/30/22 03:58:01.843
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 12/30/22 03:58:04.862
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 12/30/22 03:58:06.877
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 12/30/22 03:58:06.885
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 12/30/22 03:58:06.885
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 12/30/22 03:58:06.912
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 12/30/22 03:58:07.921
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 12/30/22 03:58:10.94
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 12/30/22 03:58:10.949
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 12/30/22 03:58:10.949
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Dec 30 03:58:10.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-3259" for this suite. 12/30/22 03:58:10.988
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 03:58:10.996
Dec 30 03:58:10.996: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename container-probe 12/30/22 03:58:10.997
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:58:11.014
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:58:11.018
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195
STEP: Creating pod liveness-6d7ab90c-6960-4c95-a850-355bd1a5646c in namespace container-probe-3746 12/30/22 03:58:11.022
Dec 30 03:58:11.032: INFO: Waiting up to 5m0s for pod "liveness-6d7ab90c-6960-4c95-a850-355bd1a5646c" in namespace "container-probe-3746" to be "not pending"
Dec 30 03:58:11.035: INFO: Pod "liveness-6d7ab90c-6960-4c95-a850-355bd1a5646c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.554356ms
Dec 30 03:58:13.040: INFO: Pod "liveness-6d7ab90c-6960-4c95-a850-355bd1a5646c": Phase="Running", Reason="", readiness=true. Elapsed: 2.008650804s
Dec 30 03:58:13.040: INFO: Pod "liveness-6d7ab90c-6960-4c95-a850-355bd1a5646c" satisfied condition "not pending"
Dec 30 03:58:13.040: INFO: Started pod liveness-6d7ab90c-6960-4c95-a850-355bd1a5646c in namespace container-probe-3746
STEP: checking the pod's current state and verifying that restartCount is present 12/30/22 03:58:13.04
Dec 30 03:58:13.044: INFO: Initial restart count of pod liveness-6d7ab90c-6960-4c95-a850-355bd1a5646c is 0
Dec 30 03:58:33.109: INFO: Restart count of pod container-probe-3746/liveness-6d7ab90c-6960-4c95-a850-355bd1a5646c is now 1 (20.064232484s elapsed)
Dec 30 03:58:53.164: INFO: Restart count of pod container-probe-3746/liveness-6d7ab90c-6960-4c95-a850-355bd1a5646c is now 2 (40.119954361s elapsed)
Dec 30 03:59:13.225: INFO: Restart count of pod container-probe-3746/liveness-6d7ab90c-6960-4c95-a850-355bd1a5646c is now 3 (1m0.180788438s elapsed)
Dec 30 03:59:33.281: INFO: Restart count of pod container-probe-3746/liveness-6d7ab90c-6960-4c95-a850-355bd1a5646c is now 4 (1m20.237063187s elapsed)
Dec 30 04:00:45.481: INFO: Restart count of pod container-probe-3746/liveness-6d7ab90c-6960-4c95-a850-355bd1a5646c is now 5 (2m32.436956233s elapsed)
STEP: deleting the pod 12/30/22 04:00:45.481
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Dec 30 04:00:45.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3746" for this suite. 12/30/22 04:00:45.499
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","completed":168,"skipped":2979,"failed":0}
------------------------------
• [SLOW TEST] [154.510 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 03:58:10.996
    Dec 30 03:58:10.996: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename container-probe 12/30/22 03:58:10.997
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 03:58:11.014
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 03:58:11.018
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:195
    STEP: Creating pod liveness-6d7ab90c-6960-4c95-a850-355bd1a5646c in namespace container-probe-3746 12/30/22 03:58:11.022
    Dec 30 03:58:11.032: INFO: Waiting up to 5m0s for pod "liveness-6d7ab90c-6960-4c95-a850-355bd1a5646c" in namespace "container-probe-3746" to be "not pending"
    Dec 30 03:58:11.035: INFO: Pod "liveness-6d7ab90c-6960-4c95-a850-355bd1a5646c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.554356ms
    Dec 30 03:58:13.040: INFO: Pod "liveness-6d7ab90c-6960-4c95-a850-355bd1a5646c": Phase="Running", Reason="", readiness=true. Elapsed: 2.008650804s
    Dec 30 03:58:13.040: INFO: Pod "liveness-6d7ab90c-6960-4c95-a850-355bd1a5646c" satisfied condition "not pending"
    Dec 30 03:58:13.040: INFO: Started pod liveness-6d7ab90c-6960-4c95-a850-355bd1a5646c in namespace container-probe-3746
    STEP: checking the pod's current state and verifying that restartCount is present 12/30/22 03:58:13.04
    Dec 30 03:58:13.044: INFO: Initial restart count of pod liveness-6d7ab90c-6960-4c95-a850-355bd1a5646c is 0
    Dec 30 03:58:33.109: INFO: Restart count of pod container-probe-3746/liveness-6d7ab90c-6960-4c95-a850-355bd1a5646c is now 1 (20.064232484s elapsed)
    Dec 30 03:58:53.164: INFO: Restart count of pod container-probe-3746/liveness-6d7ab90c-6960-4c95-a850-355bd1a5646c is now 2 (40.119954361s elapsed)
    Dec 30 03:59:13.225: INFO: Restart count of pod container-probe-3746/liveness-6d7ab90c-6960-4c95-a850-355bd1a5646c is now 3 (1m0.180788438s elapsed)
    Dec 30 03:59:33.281: INFO: Restart count of pod container-probe-3746/liveness-6d7ab90c-6960-4c95-a850-355bd1a5646c is now 4 (1m20.237063187s elapsed)
    Dec 30 04:00:45.481: INFO: Restart count of pod container-probe-3746/liveness-6d7ab90c-6960-4c95-a850-355bd1a5646c is now 5 (2m32.436956233s elapsed)
    STEP: deleting the pod 12/30/22 04:00:45.481
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Dec 30 04:00:45.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-3746" for this suite. 12/30/22 04:00:45.499
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:00:45.508
Dec 30 04:00:45.508: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename emptydir 12/30/22 04:00:45.509
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:00:45.527
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:00:45.532
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156
STEP: Creating a pod to test emptydir volume type on node default medium 12/30/22 04:00:45.536
Dec 30 04:00:45.545: INFO: Waiting up to 5m0s for pod "pod-d7a01df1-9b59-481c-9894-369d26928923" in namespace "emptydir-2352" to be "Succeeded or Failed"
Dec 30 04:00:45.549: INFO: Pod "pod-d7a01df1-9b59-481c-9894-369d26928923": Phase="Pending", Reason="", readiness=false. Elapsed: 4.171093ms
Dec 30 04:00:47.556: INFO: Pod "pod-d7a01df1-9b59-481c-9894-369d26928923": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010810749s
Dec 30 04:00:49.555: INFO: Pod "pod-d7a01df1-9b59-481c-9894-369d26928923": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010533577s
STEP: Saw pod success 12/30/22 04:00:49.555
Dec 30 04:00:49.556: INFO: Pod "pod-d7a01df1-9b59-481c-9894-369d26928923" satisfied condition "Succeeded or Failed"
Dec 30 04:00:49.560: INFO: Trying to get logs from node k8s-mgmt01 pod pod-d7a01df1-9b59-481c-9894-369d26928923 container test-container: <nil>
STEP: delete the pod 12/30/22 04:00:49.58
Dec 30 04:00:49.593: INFO: Waiting for pod pod-d7a01df1-9b59-481c-9894-369d26928923 to disappear
Dec 30 04:00:49.598: INFO: Pod pod-d7a01df1-9b59-481c-9894-369d26928923 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Dec 30 04:00:49.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2352" for this suite. 12/30/22 04:00:49.603
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","completed":169,"skipped":3004,"failed":0}
------------------------------
• [4.102 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:00:45.508
    Dec 30 04:00:45.508: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename emptydir 12/30/22 04:00:45.509
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:00:45.527
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:00:45.532
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:156
    STEP: Creating a pod to test emptydir volume type on node default medium 12/30/22 04:00:45.536
    Dec 30 04:00:45.545: INFO: Waiting up to 5m0s for pod "pod-d7a01df1-9b59-481c-9894-369d26928923" in namespace "emptydir-2352" to be "Succeeded or Failed"
    Dec 30 04:00:45.549: INFO: Pod "pod-d7a01df1-9b59-481c-9894-369d26928923": Phase="Pending", Reason="", readiness=false. Elapsed: 4.171093ms
    Dec 30 04:00:47.556: INFO: Pod "pod-d7a01df1-9b59-481c-9894-369d26928923": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010810749s
    Dec 30 04:00:49.555: INFO: Pod "pod-d7a01df1-9b59-481c-9894-369d26928923": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010533577s
    STEP: Saw pod success 12/30/22 04:00:49.555
    Dec 30 04:00:49.556: INFO: Pod "pod-d7a01df1-9b59-481c-9894-369d26928923" satisfied condition "Succeeded or Failed"
    Dec 30 04:00:49.560: INFO: Trying to get logs from node k8s-mgmt01 pod pod-d7a01df1-9b59-481c-9894-369d26928923 container test-container: <nil>
    STEP: delete the pod 12/30/22 04:00:49.58
    Dec 30 04:00:49.593: INFO: Waiting for pod pod-d7a01df1-9b59-481c-9894-369d26928923 to disappear
    Dec 30 04:00:49.598: INFO: Pod pod-d7a01df1-9b59-481c-9894-369d26928923 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Dec 30 04:00:49.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-2352" for this suite. 12/30/22 04:00:49.603
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:00:49.612
Dec 30 04:00:49.612: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename configmap 12/30/22 04:00:49.613
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:00:49.63
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:00:49.633
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88
STEP: Creating configMap with name configmap-test-volume-map-1ac8360f-861d-4fd2-a80d-e8d80ac2934d 12/30/22 04:00:49.637
STEP: Creating a pod to test consume configMaps 12/30/22 04:00:49.641
Dec 30 04:00:49.650: INFO: Waiting up to 5m0s for pod "pod-configmaps-1e13a280-5dd1-4bfc-9d62-2f6927c20a24" in namespace "configmap-6055" to be "Succeeded or Failed"
Dec 30 04:00:49.654: INFO: Pod "pod-configmaps-1e13a280-5dd1-4bfc-9d62-2f6927c20a24": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030526ms
Dec 30 04:00:51.659: INFO: Pod "pod-configmaps-1e13a280-5dd1-4bfc-9d62-2f6927c20a24": Phase="Running", Reason="", readiness=false. Elapsed: 2.009141127s
Dec 30 04:00:53.661: INFO: Pod "pod-configmaps-1e13a280-5dd1-4bfc-9d62-2f6927c20a24": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010756802s
STEP: Saw pod success 12/30/22 04:00:53.661
Dec 30 04:00:53.661: INFO: Pod "pod-configmaps-1e13a280-5dd1-4bfc-9d62-2f6927c20a24" satisfied condition "Succeeded or Failed"
Dec 30 04:00:53.664: INFO: Trying to get logs from node k8s-mgmt01 pod pod-configmaps-1e13a280-5dd1-4bfc-9d62-2f6927c20a24 container agnhost-container: <nil>
STEP: delete the pod 12/30/22 04:00:53.673
Dec 30 04:00:53.686: INFO: Waiting for pod pod-configmaps-1e13a280-5dd1-4bfc-9d62-2f6927c20a24 to disappear
Dec 30 04:00:53.690: INFO: Pod pod-configmaps-1e13a280-5dd1-4bfc-9d62-2f6927c20a24 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Dec 30 04:00:53.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6055" for this suite. 12/30/22 04:00:53.696
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":170,"skipped":3022,"failed":0}
------------------------------
• [4.091 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:00:49.612
    Dec 30 04:00:49.612: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename configmap 12/30/22 04:00:49.613
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:00:49.63
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:00:49.633
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:88
    STEP: Creating configMap with name configmap-test-volume-map-1ac8360f-861d-4fd2-a80d-e8d80ac2934d 12/30/22 04:00:49.637
    STEP: Creating a pod to test consume configMaps 12/30/22 04:00:49.641
    Dec 30 04:00:49.650: INFO: Waiting up to 5m0s for pod "pod-configmaps-1e13a280-5dd1-4bfc-9d62-2f6927c20a24" in namespace "configmap-6055" to be "Succeeded or Failed"
    Dec 30 04:00:49.654: INFO: Pod "pod-configmaps-1e13a280-5dd1-4bfc-9d62-2f6927c20a24": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030526ms
    Dec 30 04:00:51.659: INFO: Pod "pod-configmaps-1e13a280-5dd1-4bfc-9d62-2f6927c20a24": Phase="Running", Reason="", readiness=false. Elapsed: 2.009141127s
    Dec 30 04:00:53.661: INFO: Pod "pod-configmaps-1e13a280-5dd1-4bfc-9d62-2f6927c20a24": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010756802s
    STEP: Saw pod success 12/30/22 04:00:53.661
    Dec 30 04:00:53.661: INFO: Pod "pod-configmaps-1e13a280-5dd1-4bfc-9d62-2f6927c20a24" satisfied condition "Succeeded or Failed"
    Dec 30 04:00:53.664: INFO: Trying to get logs from node k8s-mgmt01 pod pod-configmaps-1e13a280-5dd1-4bfc-9d62-2f6927c20a24 container agnhost-container: <nil>
    STEP: delete the pod 12/30/22 04:00:53.673
    Dec 30 04:00:53.686: INFO: Waiting for pod pod-configmaps-1e13a280-5dd1-4bfc-9d62-2f6927c20a24 to disappear
    Dec 30 04:00:53.690: INFO: Pod pod-configmaps-1e13a280-5dd1-4bfc-9d62-2f6927c20a24 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Dec 30 04:00:53.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-6055" for this suite. 12/30/22 04:00:53.696
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:00:53.705
Dec 30 04:00:53.705: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename runtimeclass 12/30/22 04:00:53.707
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:00:53.722
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:00:53.725
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
Dec 30 04:00:53.742: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-9550 to be scheduled
Dec 30 04:00:53.746: INFO: 1 pods are not scheduled: [runtimeclass-9550/test-runtimeclass-runtimeclass-9550-preconfigured-handler-tx7ln(fc82410c-cddd-420d-b052-17a5b59ff68d)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Dec 30 04:00:55.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-9550" for this suite. 12/30/22 04:00:55.764
{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]","completed":171,"skipped":3043,"failed":0}
------------------------------
• [2.065 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:00:53.705
    Dec 30 04:00:53.705: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename runtimeclass 12/30/22 04:00:53.707
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:00:53.722
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:00:53.725
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    Dec 30 04:00:53.742: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-9550 to be scheduled
    Dec 30 04:00:53.746: INFO: 1 pods are not scheduled: [runtimeclass-9550/test-runtimeclass-runtimeclass-9550-preconfigured-handler-tx7ln(fc82410c-cddd-420d-b052-17a5b59ff68d)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Dec 30 04:00:55.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-9550" for this suite. 12/30/22 04:00:55.764
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:00:55.774
Dec 30 04:00:55.774: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename downward-api 12/30/22 04:00:55.776
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:00:55.793
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:00:55.797
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129
STEP: Creating the pod 12/30/22 04:00:55.8
Dec 30 04:00:55.811: INFO: Waiting up to 5m0s for pod "labelsupdate0d585632-07e7-443d-ac4e-81d79596a0ad" in namespace "downward-api-9078" to be "running and ready"
Dec 30 04:00:55.814: INFO: Pod "labelsupdate0d585632-07e7-443d-ac4e-81d79596a0ad": Phase="Pending", Reason="", readiness=false. Elapsed: 3.426361ms
Dec 30 04:00:55.814: INFO: The phase of Pod labelsupdate0d585632-07e7-443d-ac4e-81d79596a0ad is Pending, waiting for it to be Running (with Ready = true)
Dec 30 04:00:57.820: INFO: Pod "labelsupdate0d585632-07e7-443d-ac4e-81d79596a0ad": Phase="Running", Reason="", readiness=true. Elapsed: 2.009156396s
Dec 30 04:00:57.820: INFO: The phase of Pod labelsupdate0d585632-07e7-443d-ac4e-81d79596a0ad is Running (Ready = true)
Dec 30 04:00:57.820: INFO: Pod "labelsupdate0d585632-07e7-443d-ac4e-81d79596a0ad" satisfied condition "running and ready"
Dec 30 04:00:58.361: INFO: Successfully updated pod "labelsupdate0d585632-07e7-443d-ac4e-81d79596a0ad"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Dec 30 04:01:00.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9078" for this suite. 12/30/22 04:01:00.383
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","completed":172,"skipped":3084,"failed":0}
------------------------------
• [4.616 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:00:55.774
    Dec 30 04:00:55.774: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename downward-api 12/30/22 04:00:55.776
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:00:55.793
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:00:55.797
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:129
    STEP: Creating the pod 12/30/22 04:00:55.8
    Dec 30 04:00:55.811: INFO: Waiting up to 5m0s for pod "labelsupdate0d585632-07e7-443d-ac4e-81d79596a0ad" in namespace "downward-api-9078" to be "running and ready"
    Dec 30 04:00:55.814: INFO: Pod "labelsupdate0d585632-07e7-443d-ac4e-81d79596a0ad": Phase="Pending", Reason="", readiness=false. Elapsed: 3.426361ms
    Dec 30 04:00:55.814: INFO: The phase of Pod labelsupdate0d585632-07e7-443d-ac4e-81d79596a0ad is Pending, waiting for it to be Running (with Ready = true)
    Dec 30 04:00:57.820: INFO: Pod "labelsupdate0d585632-07e7-443d-ac4e-81d79596a0ad": Phase="Running", Reason="", readiness=true. Elapsed: 2.009156396s
    Dec 30 04:00:57.820: INFO: The phase of Pod labelsupdate0d585632-07e7-443d-ac4e-81d79596a0ad is Running (Ready = true)
    Dec 30 04:00:57.820: INFO: Pod "labelsupdate0d585632-07e7-443d-ac4e-81d79596a0ad" satisfied condition "running and ready"
    Dec 30 04:00:58.361: INFO: Successfully updated pod "labelsupdate0d585632-07e7-443d-ac4e-81d79596a0ad"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Dec 30 04:01:00.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-9078" for this suite. 12/30/22 04:01:00.383
  << End Captured GinkgoWriter Output
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:01:00.391
Dec 30 04:01:00.391: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename svcaccounts 12/30/22 04:01:00.392
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:01:00.41
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:01:00.414
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646
STEP: creating a ServiceAccount 12/30/22 04:01:00.418
STEP: watching for the ServiceAccount to be added 12/30/22 04:01:00.426
STEP: patching the ServiceAccount 12/30/22 04:01:00.428
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 12/30/22 04:01:00.435
STEP: deleting the ServiceAccount 12/30/22 04:01:00.44
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Dec 30 04:01:00.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2025" for this suite. 12/30/22 04:01:00.46
{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","completed":173,"skipped":3084,"failed":0}
------------------------------
• [0.075 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:01:00.391
    Dec 30 04:01:00.391: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename svcaccounts 12/30/22 04:01:00.392
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:01:00.41
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:01:00.414
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:646
    STEP: creating a ServiceAccount 12/30/22 04:01:00.418
    STEP: watching for the ServiceAccount to be added 12/30/22 04:01:00.426
    STEP: patching the ServiceAccount 12/30/22 04:01:00.428
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 12/30/22 04:01:00.435
    STEP: deleting the ServiceAccount 12/30/22 04:01:00.44
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Dec 30 04:01:00.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-2025" for this suite. 12/30/22 04:01:00.46
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:01:00.468
Dec 30 04:01:00.468: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename crd-webhook 12/30/22 04:01:00.47
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:01:00.485
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:01:00.488
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 12/30/22 04:01:00.491
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 12/30/22 04:01:01.33
STEP: Deploying the custom resource conversion webhook pod 12/30/22 04:01:01.339
STEP: Wait for the deployment to be ready 12/30/22 04:01:01.35
Dec 30 04:01:01.358: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 12/30/22 04:01:03.373
STEP: Verifying the service has paired with the endpoint 12/30/22 04:01:03.385
Dec 30 04:01:04.386: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
Dec 30 04:01:04.390: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Creating a v1 custom resource 12/30/22 04:01:12.002
STEP: Create a v2 custom resource 12/30/22 04:01:12.023
STEP: List CRs in v1 12/30/22 04:01:12.097
STEP: List CRs in v2 12/30/22 04:01:12.104
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Dec 30 04:01:12.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-4385" for this suite. 12/30/22 04:01:12.631
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","completed":174,"skipped":3103,"failed":0}
------------------------------
• [SLOW TEST] [12.205 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:01:00.468
    Dec 30 04:01:00.468: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename crd-webhook 12/30/22 04:01:00.47
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:01:00.485
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:01:00.488
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 12/30/22 04:01:00.491
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 12/30/22 04:01:01.33
    STEP: Deploying the custom resource conversion webhook pod 12/30/22 04:01:01.339
    STEP: Wait for the deployment to be ready 12/30/22 04:01:01.35
    Dec 30 04:01:01.358: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 12/30/22 04:01:03.373
    STEP: Verifying the service has paired with the endpoint 12/30/22 04:01:03.385
    Dec 30 04:01:04.386: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    Dec 30 04:01:04.390: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Creating a v1 custom resource 12/30/22 04:01:12.002
    STEP: Create a v2 custom resource 12/30/22 04:01:12.023
    STEP: List CRs in v1 12/30/22 04:01:12.097
    STEP: List CRs in v2 12/30/22 04:01:12.104
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Dec 30 04:01:12.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-webhook-4385" for this suite. 12/30/22 04:01:12.631
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:01:12.673
Dec 30 04:01:12.674: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename controllerrevisions 12/30/22 04:01:12.675
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:01:12.692
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:01:12.695
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-k6v8v-daemon-set" 12/30/22 04:01:12.73
STEP: Check that daemon pods launch on every node of the cluster. 12/30/22 04:01:12.737
Dec 30 04:01:12.746: INFO: Number of nodes with available pods controlled by daemonset e2e-k6v8v-daemon-set: 0
Dec 30 04:01:12.746: INFO: Node k8s-mgmt01 is running 0 daemon pod, expected 1
Dec 30 04:01:13.758: INFO: Number of nodes with available pods controlled by daemonset e2e-k6v8v-daemon-set: 1
Dec 30 04:01:13.758: INFO: Node k8s-mgmt01 is running 0 daemon pod, expected 1
Dec 30 04:01:14.759: INFO: Number of nodes with available pods controlled by daemonset e2e-k6v8v-daemon-set: 4
Dec 30 04:01:14.759: INFO: Node k8s-worker02 is running 0 daemon pod, expected 1
Dec 30 04:01:15.758: INFO: Number of nodes with available pods controlled by daemonset e2e-k6v8v-daemon-set: 5
Dec 30 04:01:15.758: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset e2e-k6v8v-daemon-set
STEP: Confirm DaemonSet "e2e-k6v8v-daemon-set" successfully created with "daemonset-name=e2e-k6v8v-daemon-set" label 12/30/22 04:01:15.762
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-k6v8v-daemon-set" 12/30/22 04:01:15.771
Dec 30 04:01:15.776: INFO: Located ControllerRevision: "e2e-k6v8v-daemon-set-54585dfdb8"
STEP: Patching ControllerRevision "e2e-k6v8v-daemon-set-54585dfdb8" 12/30/22 04:01:15.78
Dec 30 04:01:15.788: INFO: e2e-k6v8v-daemon-set-54585dfdb8 has been patched
STEP: Create a new ControllerRevision 12/30/22 04:01:15.788
Dec 30 04:01:15.794: INFO: Created ControllerRevision: e2e-k6v8v-daemon-set-59cc65b95d
STEP: Confirm that there are two ControllerRevisions 12/30/22 04:01:15.794
Dec 30 04:01:15.794: INFO: Requesting list of ControllerRevisions to confirm quantity
Dec 30 04:01:15.799: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-k6v8v-daemon-set-54585dfdb8" 12/30/22 04:01:15.799
STEP: Confirm that there is only one ControllerRevision 12/30/22 04:01:15.805
Dec 30 04:01:15.805: INFO: Requesting list of ControllerRevisions to confirm quantity
Dec 30 04:01:15.810: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-k6v8v-daemon-set-59cc65b95d" 12/30/22 04:01:15.813
Dec 30 04:01:15.823: INFO: e2e-k6v8v-daemon-set-59cc65b95d has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 12/30/22 04:01:15.823
W1230 04:01:15.836179      25 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 12/30/22 04:01:15.836
Dec 30 04:01:15.836: INFO: Requesting list of ControllerRevisions to confirm quantity
Dec 30 04:01:16.839: INFO: Requesting list of ControllerRevisions to confirm quantity
Dec 30 04:01:16.844: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-k6v8v-daemon-set-59cc65b95d=updated" 12/30/22 04:01:16.844
STEP: Confirm that there is only one ControllerRevision 12/30/22 04:01:16.853
Dec 30 04:01:16.854: INFO: Requesting list of ControllerRevisions to confirm quantity
Dec 30 04:01:16.858: INFO: Found 1 ControllerRevisions
Dec 30 04:01:16.861: INFO: ControllerRevision "e2e-k6v8v-daemon-set-758697855f" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-k6v8v-daemon-set" 12/30/22 04:01:16.865
STEP: deleting DaemonSet.extensions e2e-k6v8v-daemon-set in namespace controllerrevisions-606, will wait for the garbage collector to delete the pods 12/30/22 04:01:16.866
Dec 30 04:01:16.936: INFO: Deleting DaemonSet.extensions e2e-k6v8v-daemon-set took: 14.830872ms
Dec 30 04:01:17.036: INFO: Terminating DaemonSet.extensions e2e-k6v8v-daemon-set pods took: 100.593609ms
Dec 30 04:01:19.241: INFO: Number of nodes with available pods controlled by daemonset e2e-k6v8v-daemon-set: 0
Dec 30 04:01:19.241: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-k6v8v-daemon-set
Dec 30 04:01:19.245: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"439189"},"items":null}

Dec 30 04:01:19.249: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"439189"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/framework.go:187
Dec 30 04:01:19.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "controllerrevisions-606" for this suite. 12/30/22 04:01:19.282
{"msg":"PASSED [sig-apps] ControllerRevision [Serial] should manage the lifecycle of a ControllerRevision [Conformance]","completed":175,"skipped":3104,"failed":0}
------------------------------
• [SLOW TEST] [6.616 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:01:12.673
    Dec 30 04:01:12.674: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename controllerrevisions 12/30/22 04:01:12.675
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:01:12.692
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:01:12.695
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-k6v8v-daemon-set" 12/30/22 04:01:12.73
    STEP: Check that daemon pods launch on every node of the cluster. 12/30/22 04:01:12.737
    Dec 30 04:01:12.746: INFO: Number of nodes with available pods controlled by daemonset e2e-k6v8v-daemon-set: 0
    Dec 30 04:01:12.746: INFO: Node k8s-mgmt01 is running 0 daemon pod, expected 1
    Dec 30 04:01:13.758: INFO: Number of nodes with available pods controlled by daemonset e2e-k6v8v-daemon-set: 1
    Dec 30 04:01:13.758: INFO: Node k8s-mgmt01 is running 0 daemon pod, expected 1
    Dec 30 04:01:14.759: INFO: Number of nodes with available pods controlled by daemonset e2e-k6v8v-daemon-set: 4
    Dec 30 04:01:14.759: INFO: Node k8s-worker02 is running 0 daemon pod, expected 1
    Dec 30 04:01:15.758: INFO: Number of nodes with available pods controlled by daemonset e2e-k6v8v-daemon-set: 5
    Dec 30 04:01:15.758: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset e2e-k6v8v-daemon-set
    STEP: Confirm DaemonSet "e2e-k6v8v-daemon-set" successfully created with "daemonset-name=e2e-k6v8v-daemon-set" label 12/30/22 04:01:15.762
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-k6v8v-daemon-set" 12/30/22 04:01:15.771
    Dec 30 04:01:15.776: INFO: Located ControllerRevision: "e2e-k6v8v-daemon-set-54585dfdb8"
    STEP: Patching ControllerRevision "e2e-k6v8v-daemon-set-54585dfdb8" 12/30/22 04:01:15.78
    Dec 30 04:01:15.788: INFO: e2e-k6v8v-daemon-set-54585dfdb8 has been patched
    STEP: Create a new ControllerRevision 12/30/22 04:01:15.788
    Dec 30 04:01:15.794: INFO: Created ControllerRevision: e2e-k6v8v-daemon-set-59cc65b95d
    STEP: Confirm that there are two ControllerRevisions 12/30/22 04:01:15.794
    Dec 30 04:01:15.794: INFO: Requesting list of ControllerRevisions to confirm quantity
    Dec 30 04:01:15.799: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-k6v8v-daemon-set-54585dfdb8" 12/30/22 04:01:15.799
    STEP: Confirm that there is only one ControllerRevision 12/30/22 04:01:15.805
    Dec 30 04:01:15.805: INFO: Requesting list of ControllerRevisions to confirm quantity
    Dec 30 04:01:15.810: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-k6v8v-daemon-set-59cc65b95d" 12/30/22 04:01:15.813
    Dec 30 04:01:15.823: INFO: e2e-k6v8v-daemon-set-59cc65b95d has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 12/30/22 04:01:15.823
    W1230 04:01:15.836179      25 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 12/30/22 04:01:15.836
    Dec 30 04:01:15.836: INFO: Requesting list of ControllerRevisions to confirm quantity
    Dec 30 04:01:16.839: INFO: Requesting list of ControllerRevisions to confirm quantity
    Dec 30 04:01:16.844: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-k6v8v-daemon-set-59cc65b95d=updated" 12/30/22 04:01:16.844
    STEP: Confirm that there is only one ControllerRevision 12/30/22 04:01:16.853
    Dec 30 04:01:16.854: INFO: Requesting list of ControllerRevisions to confirm quantity
    Dec 30 04:01:16.858: INFO: Found 1 ControllerRevisions
    Dec 30 04:01:16.861: INFO: ControllerRevision "e2e-k6v8v-daemon-set-758697855f" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-k6v8v-daemon-set" 12/30/22 04:01:16.865
    STEP: deleting DaemonSet.extensions e2e-k6v8v-daemon-set in namespace controllerrevisions-606, will wait for the garbage collector to delete the pods 12/30/22 04:01:16.866
    Dec 30 04:01:16.936: INFO: Deleting DaemonSet.extensions e2e-k6v8v-daemon-set took: 14.830872ms
    Dec 30 04:01:17.036: INFO: Terminating DaemonSet.extensions e2e-k6v8v-daemon-set pods took: 100.593609ms
    Dec 30 04:01:19.241: INFO: Number of nodes with available pods controlled by daemonset e2e-k6v8v-daemon-set: 0
    Dec 30 04:01:19.241: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-k6v8v-daemon-set
    Dec 30 04:01:19.245: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"439189"},"items":null}

    Dec 30 04:01:19.249: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"439189"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/framework.go:187
    Dec 30 04:01:19.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "controllerrevisions-606" for this suite. 12/30/22 04:01:19.282
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:86
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:01:19.29
Dec 30 04:01:19.291: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename disruption 12/30/22 04:01:19.292
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:01:19.309
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:01:19.313
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:01:19.317
Dec 30 04:01:19.317: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename disruption-2 12/30/22 04:01:19.318
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:01:19.334
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:01:19.337
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:86
STEP: Waiting for the pdb to be processed 12/30/22 04:01:19.346
STEP: Waiting for the pdb to be processed 12/30/22 04:01:21.36
STEP: Waiting for the pdb to be processed 12/30/22 04:01:23.374
STEP: listing a collection of PDBs across all namespaces 12/30/22 04:01:25.384
STEP: listing a collection of PDBs in namespace disruption-311 12/30/22 04:01:25.388
STEP: deleting a collection of PDBs 12/30/22 04:01:25.393
STEP: Waiting for the PDB collection to be deleted 12/30/22 04:01:25.407
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:187
Dec 30 04:01:25.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-6559" for this suite. 12/30/22 04:01:25.416
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Dec 30 04:01:25.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-311" for this suite. 12/30/22 04:01:25.429
{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","completed":176,"skipped":3110,"failed":0}
------------------------------
• [SLOW TEST] [6.145 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:77
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:01:19.29
    Dec 30 04:01:19.291: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename disruption 12/30/22 04:01:19.292
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:01:19.309
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:01:19.313
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:01:19.317
    Dec 30 04:01:19.317: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename disruption-2 12/30/22 04:01:19.318
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:01:19.334
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:01:19.337
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:86
    STEP: Waiting for the pdb to be processed 12/30/22 04:01:19.346
    STEP: Waiting for the pdb to be processed 12/30/22 04:01:21.36
    STEP: Waiting for the pdb to be processed 12/30/22 04:01:23.374
    STEP: listing a collection of PDBs across all namespaces 12/30/22 04:01:25.384
    STEP: listing a collection of PDBs in namespace disruption-311 12/30/22 04:01:25.388
    STEP: deleting a collection of PDBs 12/30/22 04:01:25.393
    STEP: Waiting for the PDB collection to be deleted 12/30/22 04:01:25.407
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/framework.go:187
    Dec 30 04:01:25.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-2-6559" for this suite. 12/30/22 04:01:25.416
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Dec 30 04:01:25.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-311" for this suite. 12/30/22 04:01:25.429
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:01:25.444
Dec 30 04:01:25.444: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename crd-watch 12/30/22 04:01:25.445
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:01:25.461
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:01:25.465
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
Dec 30 04:01:25.468: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Creating first CR  12/30/22 04:01:33.045
Dec 30 04:01:33.051: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-12-30T04:01:33Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-12-30T04:01:33Z]] name:name1 resourceVersion:439308 uid:24674aea-a709-4f2c-bbed-d5bcec6eb0af] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 12/30/22 04:01:43.053
Dec 30 04:01:43.059: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-12-30T04:01:43Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-12-30T04:01:43Z]] name:name2 resourceVersion:439336 uid:f1e23e8c-848e-46ae-a196-4610140d8747] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 12/30/22 04:01:53.061
Dec 30 04:01:53.069: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-12-30T04:01:33Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-12-30T04:01:53Z]] name:name1 resourceVersion:439362 uid:24674aea-a709-4f2c-bbed-d5bcec6eb0af] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 12/30/22 04:02:03.072
Dec 30 04:02:03.080: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-12-30T04:01:43Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-12-30T04:02:03Z]] name:name2 resourceVersion:439389 uid:f1e23e8c-848e-46ae-a196-4610140d8747] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 12/30/22 04:02:13.082
Dec 30 04:02:13.091: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-12-30T04:01:33Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-12-30T04:01:53Z]] name:name1 resourceVersion:439415 uid:24674aea-a709-4f2c-bbed-d5bcec6eb0af] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 12/30/22 04:02:23.093
Dec 30 04:02:23.102: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-12-30T04:01:43Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-12-30T04:02:03Z]] name:name2 resourceVersion:439438 uid:f1e23e8c-848e-46ae-a196-4610140d8747] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Dec 30 04:02:33.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-7375" for this suite. 12/30/22 04:02:33.624
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","completed":177,"skipped":3160,"failed":0}
------------------------------
• [SLOW TEST] [68.187 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:01:25.444
    Dec 30 04:01:25.444: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename crd-watch 12/30/22 04:01:25.445
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:01:25.461
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:01:25.465
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    Dec 30 04:01:25.468: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Creating first CR  12/30/22 04:01:33.045
    Dec 30 04:01:33.051: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-12-30T04:01:33Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-12-30T04:01:33Z]] name:name1 resourceVersion:439308 uid:24674aea-a709-4f2c-bbed-d5bcec6eb0af] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 12/30/22 04:01:43.053
    Dec 30 04:01:43.059: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-12-30T04:01:43Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-12-30T04:01:43Z]] name:name2 resourceVersion:439336 uid:f1e23e8c-848e-46ae-a196-4610140d8747] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 12/30/22 04:01:53.061
    Dec 30 04:01:53.069: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-12-30T04:01:33Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-12-30T04:01:53Z]] name:name1 resourceVersion:439362 uid:24674aea-a709-4f2c-bbed-d5bcec6eb0af] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 12/30/22 04:02:03.072
    Dec 30 04:02:03.080: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-12-30T04:01:43Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-12-30T04:02:03Z]] name:name2 resourceVersion:439389 uid:f1e23e8c-848e-46ae-a196-4610140d8747] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 12/30/22 04:02:13.082
    Dec 30 04:02:13.091: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-12-30T04:01:33Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-12-30T04:01:53Z]] name:name1 resourceVersion:439415 uid:24674aea-a709-4f2c-bbed-d5bcec6eb0af] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 12/30/22 04:02:23.093
    Dec 30 04:02:23.102: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-12-30T04:01:43Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-12-30T04:02:03Z]] name:name2 resourceVersion:439438 uid:f1e23e8c-848e-46ae-a196-4610140d8747] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Dec 30 04:02:33.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-watch-7375" for this suite. 12/30/22 04:02:33.624
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:02:33.631
Dec 30 04:02:33.632: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename downward-api 12/30/22 04:02:33.633
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:02:33.65
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:02:33.654
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192
STEP: Creating a pod to test downward API volume plugin 12/30/22 04:02:33.657
Dec 30 04:02:33.667: INFO: Waiting up to 5m0s for pod "downwardapi-volume-35e3649b-a12b-4cda-a222-e9e4f25d1e87" in namespace "downward-api-5797" to be "Succeeded or Failed"
Dec 30 04:02:33.671: INFO: Pod "downwardapi-volume-35e3649b-a12b-4cda-a222-e9e4f25d1e87": Phase="Pending", Reason="", readiness=false. Elapsed: 3.86821ms
Dec 30 04:02:35.677: INFO: Pod "downwardapi-volume-35e3649b-a12b-4cda-a222-e9e4f25d1e87": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010335784s
Dec 30 04:02:37.677: INFO: Pod "downwardapi-volume-35e3649b-a12b-4cda-a222-e9e4f25d1e87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009772194s
STEP: Saw pod success 12/30/22 04:02:37.677
Dec 30 04:02:37.677: INFO: Pod "downwardapi-volume-35e3649b-a12b-4cda-a222-e9e4f25d1e87" satisfied condition "Succeeded or Failed"
Dec 30 04:02:37.681: INFO: Trying to get logs from node k8s-mgmt01 pod downwardapi-volume-35e3649b-a12b-4cda-a222-e9e4f25d1e87 container client-container: <nil>
STEP: delete the pod 12/30/22 04:02:37.702
Dec 30 04:02:37.715: INFO: Waiting for pod downwardapi-volume-35e3649b-a12b-4cda-a222-e9e4f25d1e87 to disappear
Dec 30 04:02:37.719: INFO: Pod downwardapi-volume-35e3649b-a12b-4cda-a222-e9e4f25d1e87 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Dec 30 04:02:37.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5797" for this suite. 12/30/22 04:02:37.725
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","completed":178,"skipped":3164,"failed":0}
------------------------------
• [4.100 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:02:33.631
    Dec 30 04:02:33.632: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename downward-api 12/30/22 04:02:33.633
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:02:33.65
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:02:33.654
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:192
    STEP: Creating a pod to test downward API volume plugin 12/30/22 04:02:33.657
    Dec 30 04:02:33.667: INFO: Waiting up to 5m0s for pod "downwardapi-volume-35e3649b-a12b-4cda-a222-e9e4f25d1e87" in namespace "downward-api-5797" to be "Succeeded or Failed"
    Dec 30 04:02:33.671: INFO: Pod "downwardapi-volume-35e3649b-a12b-4cda-a222-e9e4f25d1e87": Phase="Pending", Reason="", readiness=false. Elapsed: 3.86821ms
    Dec 30 04:02:35.677: INFO: Pod "downwardapi-volume-35e3649b-a12b-4cda-a222-e9e4f25d1e87": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010335784s
    Dec 30 04:02:37.677: INFO: Pod "downwardapi-volume-35e3649b-a12b-4cda-a222-e9e4f25d1e87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009772194s
    STEP: Saw pod success 12/30/22 04:02:37.677
    Dec 30 04:02:37.677: INFO: Pod "downwardapi-volume-35e3649b-a12b-4cda-a222-e9e4f25d1e87" satisfied condition "Succeeded or Failed"
    Dec 30 04:02:37.681: INFO: Trying to get logs from node k8s-mgmt01 pod downwardapi-volume-35e3649b-a12b-4cda-a222-e9e4f25d1e87 container client-container: <nil>
    STEP: delete the pod 12/30/22 04:02:37.702
    Dec 30 04:02:37.715: INFO: Waiting for pod downwardapi-volume-35e3649b-a12b-4cda-a222-e9e4f25d1e87 to disappear
    Dec 30 04:02:37.719: INFO: Pod downwardapi-volume-35e3649b-a12b-4cda-a222-e9e4f25d1e87 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Dec 30 04:02:37.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-5797" for this suite. 12/30/22 04:02:37.725
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:02:37.733
Dec 30 04:02:37.733: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename webhook 12/30/22 04:02:37.735
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:02:37.751
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:02:37.754
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 12/30/22 04:02:37.771
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/30/22 04:02:38.284
STEP: Deploying the webhook pod 12/30/22 04:02:38.293
STEP: Wait for the deployment to be ready 12/30/22 04:02:38.304
Dec 30 04:02:38.312: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 12/30/22 04:02:40.325
STEP: Verifying the service has paired with the endpoint 12/30/22 04:02:40.335
Dec 30 04:02:41.336: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340
Dec 30 04:02:41.340: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6715-crds.webhook.example.com via the AdmissionRegistration API 12/30/22 04:02:46.853
STEP: Creating a custom resource that should be mutated by the webhook 12/30/22 04:02:46.872
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Dec 30 04:02:49.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6730" for this suite. 12/30/22 04:02:49.502
STEP: Destroying namespace "webhook-6730-markers" for this suite. 12/30/22 04:02:49.509
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","completed":179,"skipped":3181,"failed":0}
------------------------------
• [SLOW TEST] [11.820 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:02:37.733
    Dec 30 04:02:37.733: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename webhook 12/30/22 04:02:37.735
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:02:37.751
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:02:37.754
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 12/30/22 04:02:37.771
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/30/22 04:02:38.284
    STEP: Deploying the webhook pod 12/30/22 04:02:38.293
    STEP: Wait for the deployment to be ready 12/30/22 04:02:38.304
    Dec 30 04:02:38.312: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 12/30/22 04:02:40.325
    STEP: Verifying the service has paired with the endpoint 12/30/22 04:02:40.335
    Dec 30 04:02:41.336: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:340
    Dec 30 04:02:41.340: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6715-crds.webhook.example.com via the AdmissionRegistration API 12/30/22 04:02:46.853
    STEP: Creating a custom resource that should be mutated by the webhook 12/30/22 04:02:46.872
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Dec 30 04:02:49.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-6730" for this suite. 12/30/22 04:02:49.502
    STEP: Destroying namespace "webhook-6730-markers" for this suite. 12/30/22 04:02:49.509
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:02:49.556
Dec 30 04:02:49.556: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename configmap 12/30/22 04:02:49.558
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:02:49.574
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:02:49.578
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137
STEP: Creating configMap that has name configmap-test-emptyKey-055874cb-cb69-40ed-a8ed-195421f1377b 12/30/22 04:02:49.581
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Dec 30 04:02:49.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1932" for this suite. 12/30/22 04:02:49.59
{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","completed":180,"skipped":3211,"failed":0}
------------------------------
• [0.040 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:02:49.556
    Dec 30 04:02:49.556: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename configmap 12/30/22 04:02:49.558
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:02:49.574
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:02:49.578
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:137
    STEP: Creating configMap that has name configmap-test-emptyKey-055874cb-cb69-40ed-a8ed-195421f1377b 12/30/22 04:02:49.581
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Dec 30 04:02:49.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-1932" for this suite. 12/30/22 04:02:49.59
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:02:49.6
Dec 30 04:02:49.600: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename secrets 12/30/22 04:02:49.601
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:02:49.618
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:02:49.621
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204
STEP: Creating secret with name s-test-opt-del-0014287b-1ed1-4fc3-83d7-976ea1810104 12/30/22 04:02:49.629
STEP: Creating secret with name s-test-opt-upd-b0fd490f-332a-4007-989f-9ecee1dda3e5 12/30/22 04:02:49.634
STEP: Creating the pod 12/30/22 04:02:49.638
Dec 30 04:02:49.649: INFO: Waiting up to 5m0s for pod "pod-secrets-52652189-ad0c-4220-a611-d5221f25de82" in namespace "secrets-8618" to be "running and ready"
Dec 30 04:02:49.653: INFO: Pod "pod-secrets-52652189-ad0c-4220-a611-d5221f25de82": Phase="Pending", Reason="", readiness=false. Elapsed: 3.490924ms
Dec 30 04:02:49.653: INFO: The phase of Pod pod-secrets-52652189-ad0c-4220-a611-d5221f25de82 is Pending, waiting for it to be Running (with Ready = true)
Dec 30 04:02:51.658: INFO: Pod "pod-secrets-52652189-ad0c-4220-a611-d5221f25de82": Phase="Running", Reason="", readiness=true. Elapsed: 2.008587568s
Dec 30 04:02:51.658: INFO: The phase of Pod pod-secrets-52652189-ad0c-4220-a611-d5221f25de82 is Running (Ready = true)
Dec 30 04:02:51.658: INFO: Pod "pod-secrets-52652189-ad0c-4220-a611-d5221f25de82" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-0014287b-1ed1-4fc3-83d7-976ea1810104 12/30/22 04:02:51.699
STEP: Updating secret s-test-opt-upd-b0fd490f-332a-4007-989f-9ecee1dda3e5 12/30/22 04:02:51.706
STEP: Creating secret with name s-test-opt-create-279740c2-16ae-48e8-8b5d-2bae6b14392e 12/30/22 04:02:51.711
STEP: waiting to observe update in volume 12/30/22 04:02:51.717
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Dec 30 04:02:53.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8618" for this suite. 12/30/22 04:02:53.758
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":181,"skipped":3248,"failed":0}
------------------------------
• [4.165 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:02:49.6
    Dec 30 04:02:49.600: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename secrets 12/30/22 04:02:49.601
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:02:49.618
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:02:49.621
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:204
    STEP: Creating secret with name s-test-opt-del-0014287b-1ed1-4fc3-83d7-976ea1810104 12/30/22 04:02:49.629
    STEP: Creating secret with name s-test-opt-upd-b0fd490f-332a-4007-989f-9ecee1dda3e5 12/30/22 04:02:49.634
    STEP: Creating the pod 12/30/22 04:02:49.638
    Dec 30 04:02:49.649: INFO: Waiting up to 5m0s for pod "pod-secrets-52652189-ad0c-4220-a611-d5221f25de82" in namespace "secrets-8618" to be "running and ready"
    Dec 30 04:02:49.653: INFO: Pod "pod-secrets-52652189-ad0c-4220-a611-d5221f25de82": Phase="Pending", Reason="", readiness=false. Elapsed: 3.490924ms
    Dec 30 04:02:49.653: INFO: The phase of Pod pod-secrets-52652189-ad0c-4220-a611-d5221f25de82 is Pending, waiting for it to be Running (with Ready = true)
    Dec 30 04:02:51.658: INFO: Pod "pod-secrets-52652189-ad0c-4220-a611-d5221f25de82": Phase="Running", Reason="", readiness=true. Elapsed: 2.008587568s
    Dec 30 04:02:51.658: INFO: The phase of Pod pod-secrets-52652189-ad0c-4220-a611-d5221f25de82 is Running (Ready = true)
    Dec 30 04:02:51.658: INFO: Pod "pod-secrets-52652189-ad0c-4220-a611-d5221f25de82" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-0014287b-1ed1-4fc3-83d7-976ea1810104 12/30/22 04:02:51.699
    STEP: Updating secret s-test-opt-upd-b0fd490f-332a-4007-989f-9ecee1dda3e5 12/30/22 04:02:51.706
    STEP: Creating secret with name s-test-opt-create-279740c2-16ae-48e8-8b5d-2bae6b14392e 12/30/22 04:02:51.711
    STEP: waiting to observe update in volume 12/30/22 04:02:51.717
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Dec 30 04:02:53.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-8618" for this suite. 12/30/22 04:02:53.758
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:02:53.769
Dec 30 04:02:53.769: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename crd-publish-openapi 12/30/22 04:02:53.771
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:02:53.787
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:02:53.79
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193
Dec 30 04:02:53.794: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 12/30/22 04:03:02.237
Dec 30 04:03:02.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=crd-publish-openapi-3318 --namespace=crd-publish-openapi-3318 create -f -'
Dec 30 04:03:03.094: INFO: stderr: ""
Dec 30 04:03:03.094: INFO: stdout: "e2e-test-crd-publish-openapi-712-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Dec 30 04:03:03.094: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=crd-publish-openapi-3318 --namespace=crd-publish-openapi-3318 delete e2e-test-crd-publish-openapi-712-crds test-cr'
Dec 30 04:03:03.245: INFO: stderr: ""
Dec 30 04:03:03.245: INFO: stdout: "e2e-test-crd-publish-openapi-712-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Dec 30 04:03:03.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=crd-publish-openapi-3318 --namespace=crd-publish-openapi-3318 apply -f -'
Dec 30 04:03:03.945: INFO: stderr: ""
Dec 30 04:03:03.945: INFO: stdout: "e2e-test-crd-publish-openapi-712-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Dec 30 04:03:03.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=crd-publish-openapi-3318 --namespace=crd-publish-openapi-3318 delete e2e-test-crd-publish-openapi-712-crds test-cr'
Dec 30 04:03:04.049: INFO: stderr: ""
Dec 30 04:03:04.049: INFO: stdout: "e2e-test-crd-publish-openapi-712-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 12/30/22 04:03:04.049
Dec 30 04:03:04.049: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=crd-publish-openapi-3318 explain e2e-test-crd-publish-openapi-712-crds'
Dec 30 04:03:04.749: INFO: stderr: ""
Dec 30 04:03:04.749: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-712-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Dec 30 04:03:09.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3318" for this suite. 12/30/22 04:03:09.434
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","completed":182,"skipped":3295,"failed":0}
------------------------------
• [SLOW TEST] [15.672 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:02:53.769
    Dec 30 04:02:53.769: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename crd-publish-openapi 12/30/22 04:02:53.771
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:02:53.787
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:02:53.79
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:193
    Dec 30 04:02:53.794: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 12/30/22 04:03:02.237
    Dec 30 04:03:02.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=crd-publish-openapi-3318 --namespace=crd-publish-openapi-3318 create -f -'
    Dec 30 04:03:03.094: INFO: stderr: ""
    Dec 30 04:03:03.094: INFO: stdout: "e2e-test-crd-publish-openapi-712-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Dec 30 04:03:03.094: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=crd-publish-openapi-3318 --namespace=crd-publish-openapi-3318 delete e2e-test-crd-publish-openapi-712-crds test-cr'
    Dec 30 04:03:03.245: INFO: stderr: ""
    Dec 30 04:03:03.245: INFO: stdout: "e2e-test-crd-publish-openapi-712-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    Dec 30 04:03:03.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=crd-publish-openapi-3318 --namespace=crd-publish-openapi-3318 apply -f -'
    Dec 30 04:03:03.945: INFO: stderr: ""
    Dec 30 04:03:03.945: INFO: stdout: "e2e-test-crd-publish-openapi-712-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Dec 30 04:03:03.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=crd-publish-openapi-3318 --namespace=crd-publish-openapi-3318 delete e2e-test-crd-publish-openapi-712-crds test-cr'
    Dec 30 04:03:04.049: INFO: stderr: ""
    Dec 30 04:03:04.049: INFO: stdout: "e2e-test-crd-publish-openapi-712-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 12/30/22 04:03:04.049
    Dec 30 04:03:04.049: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=crd-publish-openapi-3318 explain e2e-test-crd-publish-openapi-712-crds'
    Dec 30 04:03:04.749: INFO: stderr: ""
    Dec 30 04:03:04.749: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-712-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Dec 30 04:03:09.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-3318" for this suite. 12/30/22 04:03:09.434
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:03:09.442
Dec 30 04:03:09.442: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename configmap 12/30/22 04:03:09.444
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:03:09.46
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:03:09.463
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123
STEP: Creating configMap with name configmap-test-upd-9d343550-7247-40d7-9a8b-0494b313f918 12/30/22 04:03:09.472
STEP: Creating the pod 12/30/22 04:03:09.477
Dec 30 04:03:09.486: INFO: Waiting up to 5m0s for pod "pod-configmaps-13342ad2-d9af-49d9-b621-58ecc7664108" in namespace "configmap-411" to be "running and ready"
Dec 30 04:03:09.489: INFO: Pod "pod-configmaps-13342ad2-d9af-49d9-b621-58ecc7664108": Phase="Pending", Reason="", readiness=false. Elapsed: 3.319534ms
Dec 30 04:03:09.489: INFO: The phase of Pod pod-configmaps-13342ad2-d9af-49d9-b621-58ecc7664108 is Pending, waiting for it to be Running (with Ready = true)
Dec 30 04:03:11.495: INFO: Pod "pod-configmaps-13342ad2-d9af-49d9-b621-58ecc7664108": Phase="Running", Reason="", readiness=true. Elapsed: 2.009020034s
Dec 30 04:03:11.495: INFO: The phase of Pod pod-configmaps-13342ad2-d9af-49d9-b621-58ecc7664108 is Running (Ready = true)
Dec 30 04:03:11.495: INFO: Pod "pod-configmaps-13342ad2-d9af-49d9-b621-58ecc7664108" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-9d343550-7247-40d7-9a8b-0494b313f918 12/30/22 04:03:11.508
STEP: waiting to observe update in volume 12/30/22 04:03:11.514
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Dec 30 04:03:13.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-411" for this suite. 12/30/22 04:03:13.537
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","completed":183,"skipped":3303,"failed":0}
------------------------------
• [4.102 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:03:09.442
    Dec 30 04:03:09.442: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename configmap 12/30/22 04:03:09.444
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:03:09.46
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:03:09.463
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:123
    STEP: Creating configMap with name configmap-test-upd-9d343550-7247-40d7-9a8b-0494b313f918 12/30/22 04:03:09.472
    STEP: Creating the pod 12/30/22 04:03:09.477
    Dec 30 04:03:09.486: INFO: Waiting up to 5m0s for pod "pod-configmaps-13342ad2-d9af-49d9-b621-58ecc7664108" in namespace "configmap-411" to be "running and ready"
    Dec 30 04:03:09.489: INFO: Pod "pod-configmaps-13342ad2-d9af-49d9-b621-58ecc7664108": Phase="Pending", Reason="", readiness=false. Elapsed: 3.319534ms
    Dec 30 04:03:09.489: INFO: The phase of Pod pod-configmaps-13342ad2-d9af-49d9-b621-58ecc7664108 is Pending, waiting for it to be Running (with Ready = true)
    Dec 30 04:03:11.495: INFO: Pod "pod-configmaps-13342ad2-d9af-49d9-b621-58ecc7664108": Phase="Running", Reason="", readiness=true. Elapsed: 2.009020034s
    Dec 30 04:03:11.495: INFO: The phase of Pod pod-configmaps-13342ad2-d9af-49d9-b621-58ecc7664108 is Running (Ready = true)
    Dec 30 04:03:11.495: INFO: Pod "pod-configmaps-13342ad2-d9af-49d9-b621-58ecc7664108" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-9d343550-7247-40d7-9a8b-0494b313f918 12/30/22 04:03:11.508
    STEP: waiting to observe update in volume 12/30/22 04:03:11.514
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Dec 30 04:03:13.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-411" for this suite. 12/30/22 04:03:13.537
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:03:13.545
Dec 30 04:03:13.546: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename namespaces 12/30/22 04:03:13.547
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:03:13.564
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:03:13.568
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242
STEP: Creating a test namespace 12/30/22 04:03:13.571
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:03:13.586
STEP: Creating a pod in the namespace 12/30/22 04:03:13.59
STEP: Waiting for the pod to have running status 12/30/22 04:03:13.6
Dec 30 04:03:13.600: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-4387" to be "running"
Dec 30 04:03:13.604: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.115818ms
Dec 30 04:03:15.610: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009961444s
Dec 30 04:03:15.610: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 12/30/22 04:03:15.61
STEP: Waiting for the namespace to be removed. 12/30/22 04:03:15.617
STEP: Recreating the namespace 12/30/22 04:03:26.622
STEP: Verifying there are no pods in the namespace 12/30/22 04:03:26.639
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Dec 30 04:03:26.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-2074" for this suite. 12/30/22 04:03:26.649
STEP: Destroying namespace "nsdeletetest-4387" for this suite. 12/30/22 04:03:26.656
Dec 30 04:03:26.660: INFO: Namespace nsdeletetest-4387 was already deleted
STEP: Destroying namespace "nsdeletetest-7117" for this suite. 12/30/22 04:03:26.66
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","completed":184,"skipped":3313,"failed":0}
------------------------------
• [SLOW TEST] [13.121 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:03:13.545
    Dec 30 04:03:13.546: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename namespaces 12/30/22 04:03:13.547
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:03:13.564
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:03:13.568
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:242
    STEP: Creating a test namespace 12/30/22 04:03:13.571
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:03:13.586
    STEP: Creating a pod in the namespace 12/30/22 04:03:13.59
    STEP: Waiting for the pod to have running status 12/30/22 04:03:13.6
    Dec 30 04:03:13.600: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-4387" to be "running"
    Dec 30 04:03:13.604: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.115818ms
    Dec 30 04:03:15.610: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009961444s
    Dec 30 04:03:15.610: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 12/30/22 04:03:15.61
    STEP: Waiting for the namespace to be removed. 12/30/22 04:03:15.617
    STEP: Recreating the namespace 12/30/22 04:03:26.622
    STEP: Verifying there are no pods in the namespace 12/30/22 04:03:26.639
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Dec 30 04:03:26.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-2074" for this suite. 12/30/22 04:03:26.649
    STEP: Destroying namespace "nsdeletetest-4387" for this suite. 12/30/22 04:03:26.656
    Dec 30 04:03:26.660: INFO: Namespace nsdeletetest-4387 was already deleted
    STEP: Destroying namespace "nsdeletetest-7117" for this suite. 12/30/22 04:03:26.66
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:03:26.667
Dec 30 04:03:26.668: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename security-context 12/30/22 04:03:26.669
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:03:26.684
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:03:26.688
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 12/30/22 04:03:26.692
Dec 30 04:03:26.700: INFO: Waiting up to 5m0s for pod "security-context-46265451-23a1-40dd-953d-b4540f372bfa" in namespace "security-context-9433" to be "Succeeded or Failed"
Dec 30 04:03:26.704: INFO: Pod "security-context-46265451-23a1-40dd-953d-b4540f372bfa": Phase="Pending", Reason="", readiness=false. Elapsed: 3.954268ms
Dec 30 04:03:28.710: INFO: Pod "security-context-46265451-23a1-40dd-953d-b4540f372bfa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009713557s
Dec 30 04:03:30.709: INFO: Pod "security-context-46265451-23a1-40dd-953d-b4540f372bfa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008517086s
STEP: Saw pod success 12/30/22 04:03:30.709
Dec 30 04:03:30.709: INFO: Pod "security-context-46265451-23a1-40dd-953d-b4540f372bfa" satisfied condition "Succeeded or Failed"
Dec 30 04:03:30.714: INFO: Trying to get logs from node k8s-mgmt01 pod security-context-46265451-23a1-40dd-953d-b4540f372bfa container test-container: <nil>
STEP: delete the pod 12/30/22 04:03:30.721
Dec 30 04:03:30.735: INFO: Waiting for pod security-context-46265451-23a1-40dd-953d-b4540f372bfa to disappear
Dec 30 04:03:30.739: INFO: Pod security-context-46265451-23a1-40dd-953d-b4540f372bfa no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Dec 30 04:03:30.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-9433" for this suite. 12/30/22 04:03:30.745
{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","completed":185,"skipped":3315,"failed":0}
------------------------------
• [4.084 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:03:26.667
    Dec 30 04:03:26.668: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename security-context 12/30/22 04:03:26.669
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:03:26.684
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:03:26.688
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:97
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 12/30/22 04:03:26.692
    Dec 30 04:03:26.700: INFO: Waiting up to 5m0s for pod "security-context-46265451-23a1-40dd-953d-b4540f372bfa" in namespace "security-context-9433" to be "Succeeded or Failed"
    Dec 30 04:03:26.704: INFO: Pod "security-context-46265451-23a1-40dd-953d-b4540f372bfa": Phase="Pending", Reason="", readiness=false. Elapsed: 3.954268ms
    Dec 30 04:03:28.710: INFO: Pod "security-context-46265451-23a1-40dd-953d-b4540f372bfa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009713557s
    Dec 30 04:03:30.709: INFO: Pod "security-context-46265451-23a1-40dd-953d-b4540f372bfa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008517086s
    STEP: Saw pod success 12/30/22 04:03:30.709
    Dec 30 04:03:30.709: INFO: Pod "security-context-46265451-23a1-40dd-953d-b4540f372bfa" satisfied condition "Succeeded or Failed"
    Dec 30 04:03:30.714: INFO: Trying to get logs from node k8s-mgmt01 pod security-context-46265451-23a1-40dd-953d-b4540f372bfa container test-container: <nil>
    STEP: delete the pod 12/30/22 04:03:30.721
    Dec 30 04:03:30.735: INFO: Waiting for pod security-context-46265451-23a1-40dd-953d-b4540f372bfa to disappear
    Dec 30 04:03:30.739: INFO: Pod security-context-46265451-23a1-40dd-953d-b4540f372bfa no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Dec 30 04:03:30.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-9433" for this suite. 12/30/22 04:03:30.745
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:03:30.753
Dec 30 04:03:30.753: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename job 12/30/22 04:03:30.754
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:03:30.771
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:03:30.774
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254
STEP: Creating a job 12/30/22 04:03:30.778
STEP: Ensuring job reaches completions 12/30/22 04:03:30.785
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Dec 30 04:03:42.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-174" for this suite. 12/30/22 04:03:42.797
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","completed":186,"skipped":3321,"failed":0}
------------------------------
• [SLOW TEST] [12.051 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:03:30.753
    Dec 30 04:03:30.753: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename job 12/30/22 04:03:30.754
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:03:30.771
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:03:30.774
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:254
    STEP: Creating a job 12/30/22 04:03:30.778
    STEP: Ensuring job reaches completions 12/30/22 04:03:30.785
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Dec 30 04:03:42.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-174" for this suite. 12/30/22 04:03:42.797
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:03:42.805
Dec 30 04:03:42.805: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename projected 12/30/22 04:03:42.807
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:03:42.824
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:03:42.827
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55
STEP: Creating projection with secret that has name projected-secret-test-7543f3c7-774d-4bb8-8459-6a7fe6e31847 12/30/22 04:03:42.831
STEP: Creating a pod to test consume secrets 12/30/22 04:03:42.836
Dec 30 04:03:42.845: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0c4a53ac-e476-4802-9f96-b78dd540cb8c" in namespace "projected-6532" to be "Succeeded or Failed"
Dec 30 04:03:42.849: INFO: Pod "pod-projected-secrets-0c4a53ac-e476-4802-9f96-b78dd540cb8c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.21695ms
Dec 30 04:03:44.855: INFO: Pod "pod-projected-secrets-0c4a53ac-e476-4802-9f96-b78dd540cb8c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009432619s
Dec 30 04:03:46.855: INFO: Pod "pod-projected-secrets-0c4a53ac-e476-4802-9f96-b78dd540cb8c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009834873s
STEP: Saw pod success 12/30/22 04:03:46.855
Dec 30 04:03:46.855: INFO: Pod "pod-projected-secrets-0c4a53ac-e476-4802-9f96-b78dd540cb8c" satisfied condition "Succeeded or Failed"
Dec 30 04:03:46.860: INFO: Trying to get logs from node k8s-mgmt01 pod pod-projected-secrets-0c4a53ac-e476-4802-9f96-b78dd540cb8c container projected-secret-volume-test: <nil>
STEP: delete the pod 12/30/22 04:03:46.867
Dec 30 04:03:46.881: INFO: Waiting for pod pod-projected-secrets-0c4a53ac-e476-4802-9f96-b78dd540cb8c to disappear
Dec 30 04:03:46.885: INFO: Pod pod-projected-secrets-0c4a53ac-e476-4802-9f96-b78dd540cb8c no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Dec 30 04:03:46.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6532" for this suite. 12/30/22 04:03:46.891
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":187,"skipped":3329,"failed":0}
------------------------------
• [4.093 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:03:42.805
    Dec 30 04:03:42.805: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename projected 12/30/22 04:03:42.807
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:03:42.824
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:03:42.827
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:55
    STEP: Creating projection with secret that has name projected-secret-test-7543f3c7-774d-4bb8-8459-6a7fe6e31847 12/30/22 04:03:42.831
    STEP: Creating a pod to test consume secrets 12/30/22 04:03:42.836
    Dec 30 04:03:42.845: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0c4a53ac-e476-4802-9f96-b78dd540cb8c" in namespace "projected-6532" to be "Succeeded or Failed"
    Dec 30 04:03:42.849: INFO: Pod "pod-projected-secrets-0c4a53ac-e476-4802-9f96-b78dd540cb8c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.21695ms
    Dec 30 04:03:44.855: INFO: Pod "pod-projected-secrets-0c4a53ac-e476-4802-9f96-b78dd540cb8c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009432619s
    Dec 30 04:03:46.855: INFO: Pod "pod-projected-secrets-0c4a53ac-e476-4802-9f96-b78dd540cb8c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009834873s
    STEP: Saw pod success 12/30/22 04:03:46.855
    Dec 30 04:03:46.855: INFO: Pod "pod-projected-secrets-0c4a53ac-e476-4802-9f96-b78dd540cb8c" satisfied condition "Succeeded or Failed"
    Dec 30 04:03:46.860: INFO: Trying to get logs from node k8s-mgmt01 pod pod-projected-secrets-0c4a53ac-e476-4802-9f96-b78dd540cb8c container projected-secret-volume-test: <nil>
    STEP: delete the pod 12/30/22 04:03:46.867
    Dec 30 04:03:46.881: INFO: Waiting for pod pod-projected-secrets-0c4a53ac-e476-4802-9f96-b78dd540cb8c to disappear
    Dec 30 04:03:46.885: INFO: Pod pod-projected-secrets-0c4a53ac-e476-4802-9f96-b78dd540cb8c no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Dec 30 04:03:46.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6532" for this suite. 12/30/22 04:03:46.891
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:03:46.9
Dec 30 04:03:46.900: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename replication-controller 12/30/22 04:03:46.901
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:03:46.918
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:03:46.921
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91
STEP: Given a Pod with a 'name' label pod-adoption is created 12/30/22 04:03:46.925
Dec 30 04:03:46.933: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-6381" to be "running and ready"
Dec 30 04:03:46.937: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 3.921734ms
Dec 30 04:03:46.937: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Dec 30 04:03:48.943: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.010176946s
Dec 30 04:03:48.944: INFO: The phase of Pod pod-adoption is Running (Ready = true)
Dec 30 04:03:48.944: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 12/30/22 04:03:48.948
STEP: Then the orphan pod is adopted 12/30/22 04:03:48.953
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Dec 30 04:03:49.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6381" for this suite. 12/30/22 04:03:49.972
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","completed":188,"skipped":3350,"failed":0}
------------------------------
• [3.079 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:03:46.9
    Dec 30 04:03:46.900: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename replication-controller 12/30/22 04:03:46.901
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:03:46.918
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:03:46.921
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:91
    STEP: Given a Pod with a 'name' label pod-adoption is created 12/30/22 04:03:46.925
    Dec 30 04:03:46.933: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-6381" to be "running and ready"
    Dec 30 04:03:46.937: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 3.921734ms
    Dec 30 04:03:46.937: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Dec 30 04:03:48.943: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.010176946s
    Dec 30 04:03:48.944: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    Dec 30 04:03:48.944: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 12/30/22 04:03:48.948
    STEP: Then the orphan pod is adopted 12/30/22 04:03:48.953
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Dec 30 04:03:49.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-6381" for this suite. 12/30/22 04:03:49.972
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:03:49.98
Dec 30 04:03:49.980: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename emptydir 12/30/22 04:03:49.981
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:03:49.997
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:03:50.001
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146
STEP: Creating a pod to test emptydir 0777 on tmpfs 12/30/22 04:03:50.004
Dec 30 04:03:50.014: INFO: Waiting up to 5m0s for pod "pod-fd1dcf89-96bf-42db-8ca8-b50e33a567ca" in namespace "emptydir-2853" to be "Succeeded or Failed"
Dec 30 04:03:50.018: INFO: Pod "pod-fd1dcf89-96bf-42db-8ca8-b50e33a567ca": Phase="Pending", Reason="", readiness=false. Elapsed: 4.132219ms
Dec 30 04:03:52.022: INFO: Pod "pod-fd1dcf89-96bf-42db-8ca8-b50e33a567ca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008545139s
Dec 30 04:03:54.023: INFO: Pod "pod-fd1dcf89-96bf-42db-8ca8-b50e33a567ca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008803788s
STEP: Saw pod success 12/30/22 04:03:54.023
Dec 30 04:03:54.023: INFO: Pod "pod-fd1dcf89-96bf-42db-8ca8-b50e33a567ca" satisfied condition "Succeeded or Failed"
Dec 30 04:03:54.029: INFO: Trying to get logs from node k8s-mgmt03 pod pod-fd1dcf89-96bf-42db-8ca8-b50e33a567ca container test-container: <nil>
STEP: delete the pod 12/30/22 04:03:54.061
Dec 30 04:03:54.071: INFO: Waiting for pod pod-fd1dcf89-96bf-42db-8ca8-b50e33a567ca to disappear
Dec 30 04:03:54.074: INFO: Pod pod-fd1dcf89-96bf-42db-8ca8-b50e33a567ca no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Dec 30 04:03:54.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2853" for this suite. 12/30/22 04:03:54.08
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":189,"skipped":3357,"failed":0}
------------------------------
• [4.108 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:03:49.98
    Dec 30 04:03:49.980: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename emptydir 12/30/22 04:03:49.981
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:03:49.997
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:03:50.001
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:146
    STEP: Creating a pod to test emptydir 0777 on tmpfs 12/30/22 04:03:50.004
    Dec 30 04:03:50.014: INFO: Waiting up to 5m0s for pod "pod-fd1dcf89-96bf-42db-8ca8-b50e33a567ca" in namespace "emptydir-2853" to be "Succeeded or Failed"
    Dec 30 04:03:50.018: INFO: Pod "pod-fd1dcf89-96bf-42db-8ca8-b50e33a567ca": Phase="Pending", Reason="", readiness=false. Elapsed: 4.132219ms
    Dec 30 04:03:52.022: INFO: Pod "pod-fd1dcf89-96bf-42db-8ca8-b50e33a567ca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008545139s
    Dec 30 04:03:54.023: INFO: Pod "pod-fd1dcf89-96bf-42db-8ca8-b50e33a567ca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008803788s
    STEP: Saw pod success 12/30/22 04:03:54.023
    Dec 30 04:03:54.023: INFO: Pod "pod-fd1dcf89-96bf-42db-8ca8-b50e33a567ca" satisfied condition "Succeeded or Failed"
    Dec 30 04:03:54.029: INFO: Trying to get logs from node k8s-mgmt03 pod pod-fd1dcf89-96bf-42db-8ca8-b50e33a567ca container test-container: <nil>
    STEP: delete the pod 12/30/22 04:03:54.061
    Dec 30 04:03:54.071: INFO: Waiting for pod pod-fd1dcf89-96bf-42db-8ca8-b50e33a567ca to disappear
    Dec 30 04:03:54.074: INFO: Pod pod-fd1dcf89-96bf-42db-8ca8-b50e33a567ca no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Dec 30 04:03:54.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-2853" for this suite. 12/30/22 04:03:54.08
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:03:54.089
Dec 30 04:03:54.090: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename cronjob 12/30/22 04:03:54.091
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:03:54.107
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:03:54.111
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 12/30/22 04:03:54.114
STEP: Ensuring no jobs are scheduled 12/30/22 04:03:54.121
STEP: Ensuring no job exists by listing jobs explicitly 12/30/22 04:08:54.13
STEP: Removing cronjob 12/30/22 04:08:54.135
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Dec 30 04:08:54.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-5578" for this suite. 12/30/22 04:08:54.148
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","completed":190,"skipped":3379,"failed":0}
------------------------------
• [SLOW TEST] [300.066 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:03:54.089
    Dec 30 04:03:54.090: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename cronjob 12/30/22 04:03:54.091
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:03:54.107
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:03:54.111
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 12/30/22 04:03:54.114
    STEP: Ensuring no jobs are scheduled 12/30/22 04:03:54.121
    STEP: Ensuring no job exists by listing jobs explicitly 12/30/22 04:08:54.13
    STEP: Removing cronjob 12/30/22 04:08:54.135
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Dec 30 04:08:54.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-5578" for this suite. 12/30/22 04:08:54.148
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:08:54.158
Dec 30 04:08:54.158: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename svcaccounts 12/30/22 04:08:54.16
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:08:54.19
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:08:54.194
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272
STEP: Creating a pod to test service account token:  12/30/22 04:08:54.197
Dec 30 04:08:54.207: INFO: Waiting up to 5m0s for pod "test-pod-b62d4808-1ce2-4031-b3ab-cb4ed5935318" in namespace "svcaccounts-2358" to be "Succeeded or Failed"
Dec 30 04:08:54.211: INFO: Pod "test-pod-b62d4808-1ce2-4031-b3ab-cb4ed5935318": Phase="Pending", Reason="", readiness=false. Elapsed: 3.958739ms
Dec 30 04:08:56.216: INFO: Pod "test-pod-b62d4808-1ce2-4031-b3ab-cb4ed5935318": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009250652s
Dec 30 04:08:58.217: INFO: Pod "test-pod-b62d4808-1ce2-4031-b3ab-cb4ed5935318": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009846543s
STEP: Saw pod success 12/30/22 04:08:58.217
Dec 30 04:08:58.217: INFO: Pod "test-pod-b62d4808-1ce2-4031-b3ab-cb4ed5935318" satisfied condition "Succeeded or Failed"
Dec 30 04:08:58.221: INFO: Trying to get logs from node k8s-mgmt01 pod test-pod-b62d4808-1ce2-4031-b3ab-cb4ed5935318 container agnhost-container: <nil>
STEP: delete the pod 12/30/22 04:08:58.242
Dec 30 04:08:58.255: INFO: Waiting for pod test-pod-b62d4808-1ce2-4031-b3ab-cb4ed5935318 to disappear
Dec 30 04:08:58.259: INFO: Pod test-pod-b62d4808-1ce2-4031-b3ab-cb4ed5935318 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Dec 30 04:08:58.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2358" for this suite. 12/30/22 04:08:58.265
{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","completed":191,"skipped":3402,"failed":0}
------------------------------
• [4.113 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:08:54.158
    Dec 30 04:08:54.158: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename svcaccounts 12/30/22 04:08:54.16
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:08:54.19
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:08:54.194
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:272
    STEP: Creating a pod to test service account token:  12/30/22 04:08:54.197
    Dec 30 04:08:54.207: INFO: Waiting up to 5m0s for pod "test-pod-b62d4808-1ce2-4031-b3ab-cb4ed5935318" in namespace "svcaccounts-2358" to be "Succeeded or Failed"
    Dec 30 04:08:54.211: INFO: Pod "test-pod-b62d4808-1ce2-4031-b3ab-cb4ed5935318": Phase="Pending", Reason="", readiness=false. Elapsed: 3.958739ms
    Dec 30 04:08:56.216: INFO: Pod "test-pod-b62d4808-1ce2-4031-b3ab-cb4ed5935318": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009250652s
    Dec 30 04:08:58.217: INFO: Pod "test-pod-b62d4808-1ce2-4031-b3ab-cb4ed5935318": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009846543s
    STEP: Saw pod success 12/30/22 04:08:58.217
    Dec 30 04:08:58.217: INFO: Pod "test-pod-b62d4808-1ce2-4031-b3ab-cb4ed5935318" satisfied condition "Succeeded or Failed"
    Dec 30 04:08:58.221: INFO: Trying to get logs from node k8s-mgmt01 pod test-pod-b62d4808-1ce2-4031-b3ab-cb4ed5935318 container agnhost-container: <nil>
    STEP: delete the pod 12/30/22 04:08:58.242
    Dec 30 04:08:58.255: INFO: Waiting for pod test-pod-b62d4808-1ce2-4031-b3ab-cb4ed5935318 to disappear
    Dec 30 04:08:58.259: INFO: Pod test-pod-b62d4808-1ce2-4031-b3ab-cb4ed5935318 no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Dec 30 04:08:58.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-2358" for this suite. 12/30/22 04:08:58.265
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:08:58.276
Dec 30 04:08:58.276: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename cronjob 12/30/22 04:08:58.277
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:08:58.294
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:08:58.298
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 12/30/22 04:08:58.301
STEP: Ensuring more than one job is running at a time 12/30/22 04:08:58.307
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 12/30/22 04:10:00.313
STEP: Removing cronjob 12/30/22 04:10:00.318
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Dec 30 04:10:00.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-1493" for this suite. 12/30/22 04:10:00.331
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","completed":192,"skipped":3457,"failed":0}
------------------------------
• [SLOW TEST] [62.063 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:08:58.276
    Dec 30 04:08:58.276: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename cronjob 12/30/22 04:08:58.277
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:08:58.294
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:08:58.298
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 12/30/22 04:08:58.301
    STEP: Ensuring more than one job is running at a time 12/30/22 04:08:58.307
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 12/30/22 04:10:00.313
    STEP: Removing cronjob 12/30/22 04:10:00.318
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Dec 30 04:10:00.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-1493" for this suite. 12/30/22 04:10:00.331
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:10:00.341
Dec 30 04:10:00.342: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename pods 12/30/22 04:10:00.343
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:10:00.359
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:10:00.362
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343
STEP: creating the pod 12/30/22 04:10:00.366
STEP: submitting the pod to kubernetes 12/30/22 04:10:00.366
Dec 30 04:10:00.376: INFO: Waiting up to 5m0s for pod "pod-update-bb10a626-681e-4c99-9fcd-37995e854a71" in namespace "pods-4734" to be "running and ready"
Dec 30 04:10:00.380: INFO: Pod "pod-update-bb10a626-681e-4c99-9fcd-37995e854a71": Phase="Pending", Reason="", readiness=false. Elapsed: 3.280184ms
Dec 30 04:10:00.380: INFO: The phase of Pod pod-update-bb10a626-681e-4c99-9fcd-37995e854a71 is Pending, waiting for it to be Running (with Ready = true)
Dec 30 04:10:02.385: INFO: Pod "pod-update-bb10a626-681e-4c99-9fcd-37995e854a71": Phase="Running", Reason="", readiness=true. Elapsed: 2.009085237s
Dec 30 04:10:02.385: INFO: The phase of Pod pod-update-bb10a626-681e-4c99-9fcd-37995e854a71 is Running (Ready = true)
Dec 30 04:10:02.385: INFO: Pod "pod-update-bb10a626-681e-4c99-9fcd-37995e854a71" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 12/30/22 04:10:02.39
STEP: updating the pod 12/30/22 04:10:02.394
Dec 30 04:10:02.909: INFO: Successfully updated pod "pod-update-bb10a626-681e-4c99-9fcd-37995e854a71"
Dec 30 04:10:02.909: INFO: Waiting up to 5m0s for pod "pod-update-bb10a626-681e-4c99-9fcd-37995e854a71" in namespace "pods-4734" to be "running"
Dec 30 04:10:02.913: INFO: Pod "pod-update-bb10a626-681e-4c99-9fcd-37995e854a71": Phase="Running", Reason="", readiness=true. Elapsed: 3.745643ms
Dec 30 04:10:02.913: INFO: Pod "pod-update-bb10a626-681e-4c99-9fcd-37995e854a71" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 12/30/22 04:10:02.913
Dec 30 04:10:02.917: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Dec 30 04:10:02.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4734" for this suite. 12/30/22 04:10:02.925
{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","completed":193,"skipped":3473,"failed":0}
------------------------------
• [2.591 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:10:00.341
    Dec 30 04:10:00.342: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename pods 12/30/22 04:10:00.343
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:10:00.359
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:10:00.362
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:343
    STEP: creating the pod 12/30/22 04:10:00.366
    STEP: submitting the pod to kubernetes 12/30/22 04:10:00.366
    Dec 30 04:10:00.376: INFO: Waiting up to 5m0s for pod "pod-update-bb10a626-681e-4c99-9fcd-37995e854a71" in namespace "pods-4734" to be "running and ready"
    Dec 30 04:10:00.380: INFO: Pod "pod-update-bb10a626-681e-4c99-9fcd-37995e854a71": Phase="Pending", Reason="", readiness=false. Elapsed: 3.280184ms
    Dec 30 04:10:00.380: INFO: The phase of Pod pod-update-bb10a626-681e-4c99-9fcd-37995e854a71 is Pending, waiting for it to be Running (with Ready = true)
    Dec 30 04:10:02.385: INFO: Pod "pod-update-bb10a626-681e-4c99-9fcd-37995e854a71": Phase="Running", Reason="", readiness=true. Elapsed: 2.009085237s
    Dec 30 04:10:02.385: INFO: The phase of Pod pod-update-bb10a626-681e-4c99-9fcd-37995e854a71 is Running (Ready = true)
    Dec 30 04:10:02.385: INFO: Pod "pod-update-bb10a626-681e-4c99-9fcd-37995e854a71" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 12/30/22 04:10:02.39
    STEP: updating the pod 12/30/22 04:10:02.394
    Dec 30 04:10:02.909: INFO: Successfully updated pod "pod-update-bb10a626-681e-4c99-9fcd-37995e854a71"
    Dec 30 04:10:02.909: INFO: Waiting up to 5m0s for pod "pod-update-bb10a626-681e-4c99-9fcd-37995e854a71" in namespace "pods-4734" to be "running"
    Dec 30 04:10:02.913: INFO: Pod "pod-update-bb10a626-681e-4c99-9fcd-37995e854a71": Phase="Running", Reason="", readiness=true. Elapsed: 3.745643ms
    Dec 30 04:10:02.913: INFO: Pod "pod-update-bb10a626-681e-4c99-9fcd-37995e854a71" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 12/30/22 04:10:02.913
    Dec 30 04:10:02.917: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Dec 30 04:10:02.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-4734" for this suite. 12/30/22 04:10:02.925
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:10:02.933
Dec 30 04:10:02.933: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename resourcequota 12/30/22 04:10:02.935
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:10:02.952
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:10:02.956
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150
STEP: Discovering how many secrets are in namespace by default 12/30/22 04:10:02.96
STEP: Counting existing ResourceQuota 12/30/22 04:10:07.965
STEP: Creating a ResourceQuota 12/30/22 04:10:12.968
STEP: Ensuring resource quota status is calculated 12/30/22 04:10:12.974
STEP: Creating a Secret 12/30/22 04:10:14.981
STEP: Ensuring resource quota status captures secret creation 12/30/22 04:10:14.993
STEP: Deleting a secret 12/30/22 04:10:16.999
STEP: Ensuring resource quota status released usage 12/30/22 04:10:17.006
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Dec 30 04:10:19.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1180" for this suite. 12/30/22 04:10:19.019
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","completed":194,"skipped":3488,"failed":0}
------------------------------
• [SLOW TEST] [16.094 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:10:02.933
    Dec 30 04:10:02.933: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename resourcequota 12/30/22 04:10:02.935
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:10:02.952
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:10:02.956
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:150
    STEP: Discovering how many secrets are in namespace by default 12/30/22 04:10:02.96
    STEP: Counting existing ResourceQuota 12/30/22 04:10:07.965
    STEP: Creating a ResourceQuota 12/30/22 04:10:12.968
    STEP: Ensuring resource quota status is calculated 12/30/22 04:10:12.974
    STEP: Creating a Secret 12/30/22 04:10:14.981
    STEP: Ensuring resource quota status captures secret creation 12/30/22 04:10:14.993
    STEP: Deleting a secret 12/30/22 04:10:16.999
    STEP: Ensuring resource quota status released usage 12/30/22 04:10:17.006
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Dec 30 04:10:19.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-1180" for this suite. 12/30/22 04:10:19.019
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:10:19.028
Dec 30 04:10:19.028: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename projected 12/30/22 04:10:19.029
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:10:19.047
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:10:19.05
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260
STEP: Creating a pod to test downward API volume plugin 12/30/22 04:10:19.054
Dec 30 04:10:19.062: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8baff4d1-ed08-4830-bb43-111ca4fbd05e" in namespace "projected-7908" to be "Succeeded or Failed"
Dec 30 04:10:19.067: INFO: Pod "downwardapi-volume-8baff4d1-ed08-4830-bb43-111ca4fbd05e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.442175ms
Dec 30 04:10:21.072: INFO: Pod "downwardapi-volume-8baff4d1-ed08-4830-bb43-111ca4fbd05e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009519362s
Dec 30 04:10:23.073: INFO: Pod "downwardapi-volume-8baff4d1-ed08-4830-bb43-111ca4fbd05e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011135989s
STEP: Saw pod success 12/30/22 04:10:23.073
Dec 30 04:10:23.074: INFO: Pod "downwardapi-volume-8baff4d1-ed08-4830-bb43-111ca4fbd05e" satisfied condition "Succeeded or Failed"
Dec 30 04:10:23.078: INFO: Trying to get logs from node k8s-mgmt02 pod downwardapi-volume-8baff4d1-ed08-4830-bb43-111ca4fbd05e container client-container: <nil>
STEP: delete the pod 12/30/22 04:10:23.099
Dec 30 04:10:23.110: INFO: Waiting for pod downwardapi-volume-8baff4d1-ed08-4830-bb43-111ca4fbd05e to disappear
Dec 30 04:10:23.114: INFO: Pod downwardapi-volume-8baff4d1-ed08-4830-bb43-111ca4fbd05e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Dec 30 04:10:23.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7908" for this suite. 12/30/22 04:10:23.12
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","completed":195,"skipped":3489,"failed":0}
------------------------------
• [4.100 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:10:19.028
    Dec 30 04:10:19.028: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename projected 12/30/22 04:10:19.029
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:10:19.047
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:10:19.05
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:260
    STEP: Creating a pod to test downward API volume plugin 12/30/22 04:10:19.054
    Dec 30 04:10:19.062: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8baff4d1-ed08-4830-bb43-111ca4fbd05e" in namespace "projected-7908" to be "Succeeded or Failed"
    Dec 30 04:10:19.067: INFO: Pod "downwardapi-volume-8baff4d1-ed08-4830-bb43-111ca4fbd05e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.442175ms
    Dec 30 04:10:21.072: INFO: Pod "downwardapi-volume-8baff4d1-ed08-4830-bb43-111ca4fbd05e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009519362s
    Dec 30 04:10:23.073: INFO: Pod "downwardapi-volume-8baff4d1-ed08-4830-bb43-111ca4fbd05e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011135989s
    STEP: Saw pod success 12/30/22 04:10:23.073
    Dec 30 04:10:23.074: INFO: Pod "downwardapi-volume-8baff4d1-ed08-4830-bb43-111ca4fbd05e" satisfied condition "Succeeded or Failed"
    Dec 30 04:10:23.078: INFO: Trying to get logs from node k8s-mgmt02 pod downwardapi-volume-8baff4d1-ed08-4830-bb43-111ca4fbd05e container client-container: <nil>
    STEP: delete the pod 12/30/22 04:10:23.099
    Dec 30 04:10:23.110: INFO: Waiting for pod downwardapi-volume-8baff4d1-ed08-4830-bb43-111ca4fbd05e to disappear
    Dec 30 04:10:23.114: INFO: Pod downwardapi-volume-8baff4d1-ed08-4830-bb43-111ca4fbd05e no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Dec 30 04:10:23.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7908" for this suite. 12/30/22 04:10:23.12
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1785
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:10:23.129
Dec 30 04:10:23.129: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename kubectl 12/30/22 04:10:23.13
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:10:23.147
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:10:23.151
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1785
STEP: starting the proxy server 12/30/22 04:10:23.155
Dec 30 04:10:23.155: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-689 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 12/30/22 04:10:23.229
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Dec 30 04:10:23.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-689" for this suite. 12/30/22 04:10:23.248
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","completed":196,"skipped":3510,"failed":0}
------------------------------
• [0.126 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1778
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1785

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:10:23.129
    Dec 30 04:10:23.129: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename kubectl 12/30/22 04:10:23.13
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:10:23.147
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:10:23.151
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1785
    STEP: starting the proxy server 12/30/22 04:10:23.155
    Dec 30 04:10:23.155: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-689 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 12/30/22 04:10:23.229
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Dec 30 04:10:23.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-689" for this suite. 12/30/22 04:10:23.248
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:10:23.256
Dec 30 04:10:23.256: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename configmap 12/30/22 04:10:23.257
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:10:23.275
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:10:23.278
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239
STEP: Creating configMap with name cm-test-opt-del-b1b0b470-b163-4a1b-b9ac-d9f911be1244 12/30/22 04:10:23.287
STEP: Creating configMap with name cm-test-opt-upd-6d05b43f-d375-4f9b-b377-432bd6ef2649 12/30/22 04:10:23.292
STEP: Creating the pod 12/30/22 04:10:23.297
Dec 30 04:10:23.308: INFO: Waiting up to 5m0s for pod "pod-configmaps-2eda0630-a5f1-4bb8-b701-223fdba2a7b3" in namespace "configmap-5001" to be "running and ready"
Dec 30 04:10:23.311: INFO: Pod "pod-configmaps-2eda0630-a5f1-4bb8-b701-223fdba2a7b3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.494131ms
Dec 30 04:10:23.311: INFO: The phase of Pod pod-configmaps-2eda0630-a5f1-4bb8-b701-223fdba2a7b3 is Pending, waiting for it to be Running (with Ready = true)
Dec 30 04:10:25.318: INFO: Pod "pod-configmaps-2eda0630-a5f1-4bb8-b701-223fdba2a7b3": Phase="Running", Reason="", readiness=true. Elapsed: 2.010081628s
Dec 30 04:10:25.318: INFO: The phase of Pod pod-configmaps-2eda0630-a5f1-4bb8-b701-223fdba2a7b3 is Running (Ready = true)
Dec 30 04:10:25.318: INFO: Pod "pod-configmaps-2eda0630-a5f1-4bb8-b701-223fdba2a7b3" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-b1b0b470-b163-4a1b-b9ac-d9f911be1244 12/30/22 04:10:25.347
STEP: Updating configmap cm-test-opt-upd-6d05b43f-d375-4f9b-b377-432bd6ef2649 12/30/22 04:10:25.354
STEP: Creating configMap with name cm-test-opt-create-d2187ceb-c922-45e5-a2a0-45487b059b7f 12/30/22 04:10:25.36
STEP: waiting to observe update in volume 12/30/22 04:10:25.364
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Dec 30 04:10:27.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5001" for this suite. 12/30/22 04:10:27.405
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":197,"skipped":3512,"failed":0}
------------------------------
• [4.156 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:10:23.256
    Dec 30 04:10:23.256: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename configmap 12/30/22 04:10:23.257
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:10:23.275
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:10:23.278
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:239
    STEP: Creating configMap with name cm-test-opt-del-b1b0b470-b163-4a1b-b9ac-d9f911be1244 12/30/22 04:10:23.287
    STEP: Creating configMap with name cm-test-opt-upd-6d05b43f-d375-4f9b-b377-432bd6ef2649 12/30/22 04:10:23.292
    STEP: Creating the pod 12/30/22 04:10:23.297
    Dec 30 04:10:23.308: INFO: Waiting up to 5m0s for pod "pod-configmaps-2eda0630-a5f1-4bb8-b701-223fdba2a7b3" in namespace "configmap-5001" to be "running and ready"
    Dec 30 04:10:23.311: INFO: Pod "pod-configmaps-2eda0630-a5f1-4bb8-b701-223fdba2a7b3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.494131ms
    Dec 30 04:10:23.311: INFO: The phase of Pod pod-configmaps-2eda0630-a5f1-4bb8-b701-223fdba2a7b3 is Pending, waiting for it to be Running (with Ready = true)
    Dec 30 04:10:25.318: INFO: Pod "pod-configmaps-2eda0630-a5f1-4bb8-b701-223fdba2a7b3": Phase="Running", Reason="", readiness=true. Elapsed: 2.010081628s
    Dec 30 04:10:25.318: INFO: The phase of Pod pod-configmaps-2eda0630-a5f1-4bb8-b701-223fdba2a7b3 is Running (Ready = true)
    Dec 30 04:10:25.318: INFO: Pod "pod-configmaps-2eda0630-a5f1-4bb8-b701-223fdba2a7b3" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-b1b0b470-b163-4a1b-b9ac-d9f911be1244 12/30/22 04:10:25.347
    STEP: Updating configmap cm-test-opt-upd-6d05b43f-d375-4f9b-b377-432bd6ef2649 12/30/22 04:10:25.354
    STEP: Creating configMap with name cm-test-opt-create-d2187ceb-c922-45e5-a2a0-45487b059b7f 12/30/22 04:10:25.36
    STEP: waiting to observe update in volume 12/30/22 04:10:25.364
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Dec 30 04:10:27.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-5001" for this suite. 12/30/22 04:10:27.405
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:10:27.414
Dec 30 04:10:27.414: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename downward-api 12/30/22 04:10:27.416
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:10:27.432
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:10:27.436
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83
STEP: Creating a pod to test downward API volume plugin 12/30/22 04:10:27.439
Dec 30 04:10:27.449: INFO: Waiting up to 5m0s for pod "downwardapi-volume-786e0e86-98c3-4fac-9950-6b6defa84592" in namespace "downward-api-2822" to be "Succeeded or Failed"
Dec 30 04:10:27.453: INFO: Pod "downwardapi-volume-786e0e86-98c3-4fac-9950-6b6defa84592": Phase="Pending", Reason="", readiness=false. Elapsed: 3.43163ms
Dec 30 04:10:29.458: INFO: Pod "downwardapi-volume-786e0e86-98c3-4fac-9950-6b6defa84592": Phase="Running", Reason="", readiness=true. Elapsed: 2.008998287s
Dec 30 04:10:31.458: INFO: Pod "downwardapi-volume-786e0e86-98c3-4fac-9950-6b6defa84592": Phase="Running", Reason="", readiness=false. Elapsed: 4.008392916s
Dec 30 04:10:33.457: INFO: Pod "downwardapi-volume-786e0e86-98c3-4fac-9950-6b6defa84592": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008373696s
STEP: Saw pod success 12/30/22 04:10:33.458
Dec 30 04:10:33.458: INFO: Pod "downwardapi-volume-786e0e86-98c3-4fac-9950-6b6defa84592" satisfied condition "Succeeded or Failed"
Dec 30 04:10:33.462: INFO: Trying to get logs from node k8s-mgmt02 pod downwardapi-volume-786e0e86-98c3-4fac-9950-6b6defa84592 container client-container: <nil>
STEP: delete the pod 12/30/22 04:10:33.47
Dec 30 04:10:33.481: INFO: Waiting for pod downwardapi-volume-786e0e86-98c3-4fac-9950-6b6defa84592 to disappear
Dec 30 04:10:33.485: INFO: Pod downwardapi-volume-786e0e86-98c3-4fac-9950-6b6defa84592 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Dec 30 04:10:33.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2822" for this suite. 12/30/22 04:10:33.491
{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","completed":198,"skipped":3544,"failed":0}
------------------------------
• [SLOW TEST] [6.083 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:10:27.414
    Dec 30 04:10:27.414: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename downward-api 12/30/22 04:10:27.416
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:10:27.432
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:10:27.436
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:83
    STEP: Creating a pod to test downward API volume plugin 12/30/22 04:10:27.439
    Dec 30 04:10:27.449: INFO: Waiting up to 5m0s for pod "downwardapi-volume-786e0e86-98c3-4fac-9950-6b6defa84592" in namespace "downward-api-2822" to be "Succeeded or Failed"
    Dec 30 04:10:27.453: INFO: Pod "downwardapi-volume-786e0e86-98c3-4fac-9950-6b6defa84592": Phase="Pending", Reason="", readiness=false. Elapsed: 3.43163ms
    Dec 30 04:10:29.458: INFO: Pod "downwardapi-volume-786e0e86-98c3-4fac-9950-6b6defa84592": Phase="Running", Reason="", readiness=true. Elapsed: 2.008998287s
    Dec 30 04:10:31.458: INFO: Pod "downwardapi-volume-786e0e86-98c3-4fac-9950-6b6defa84592": Phase="Running", Reason="", readiness=false. Elapsed: 4.008392916s
    Dec 30 04:10:33.457: INFO: Pod "downwardapi-volume-786e0e86-98c3-4fac-9950-6b6defa84592": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008373696s
    STEP: Saw pod success 12/30/22 04:10:33.458
    Dec 30 04:10:33.458: INFO: Pod "downwardapi-volume-786e0e86-98c3-4fac-9950-6b6defa84592" satisfied condition "Succeeded or Failed"
    Dec 30 04:10:33.462: INFO: Trying to get logs from node k8s-mgmt02 pod downwardapi-volume-786e0e86-98c3-4fac-9950-6b6defa84592 container client-container: <nil>
    STEP: delete the pod 12/30/22 04:10:33.47
    Dec 30 04:10:33.481: INFO: Waiting for pod downwardapi-volume-786e0e86-98c3-4fac-9950-6b6defa84592 to disappear
    Dec 30 04:10:33.485: INFO: Pod downwardapi-volume-786e0e86-98c3-4fac-9950-6b6defa84592 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Dec 30 04:10:33.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-2822" for this suite. 12/30/22 04:10:33.491
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:10:33.5
Dec 30 04:10:33.500: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename emptydir-wrapper 12/30/22 04:10:33.502
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:10:33.52
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:10:33.523
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 12/30/22 04:10:33.527
STEP: Creating RC which spawns configmap-volume pods 12/30/22 04:10:33.763
Dec 30 04:10:33.859: INFO: Pod name wrapped-volume-race-e7961bb5-5b64-48d5-adfb-9f85d45397a3: Found 1 pods out of 5
Dec 30 04:10:38.871: INFO: Pod name wrapped-volume-race-e7961bb5-5b64-48d5-adfb-9f85d45397a3: Found 5 pods out of 5
STEP: Ensuring each pod is running 12/30/22 04:10:38.871
Dec 30 04:10:38.871: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-e7961bb5-5b64-48d5-adfb-9f85d45397a3-777cw" in namespace "emptydir-wrapper-5256" to be "running"
Dec 30 04:10:38.876: INFO: Pod "wrapped-volume-race-e7961bb5-5b64-48d5-adfb-9f85d45397a3-777cw": Phase="Pending", Reason="", readiness=false. Elapsed: 5.194446ms
Dec 30 04:10:40.883: INFO: Pod "wrapped-volume-race-e7961bb5-5b64-48d5-adfb-9f85d45397a3-777cw": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011625942s
Dec 30 04:10:42.882: INFO: Pod "wrapped-volume-race-e7961bb5-5b64-48d5-adfb-9f85d45397a3-777cw": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01116009s
Dec 30 04:10:44.883: INFO: Pod "wrapped-volume-race-e7961bb5-5b64-48d5-adfb-9f85d45397a3-777cw": Phase="Pending", Reason="", readiness=false. Elapsed: 6.011437259s
Dec 30 04:10:46.885: INFO: Pod "wrapped-volume-race-e7961bb5-5b64-48d5-adfb-9f85d45397a3-777cw": Phase="Pending", Reason="", readiness=false. Elapsed: 8.014386359s
Dec 30 04:10:48.884: INFO: Pod "wrapped-volume-race-e7961bb5-5b64-48d5-adfb-9f85d45397a3-777cw": Phase="Running", Reason="", readiness=true. Elapsed: 10.01261589s
Dec 30 04:10:48.884: INFO: Pod "wrapped-volume-race-e7961bb5-5b64-48d5-adfb-9f85d45397a3-777cw" satisfied condition "running"
Dec 30 04:10:48.884: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-e7961bb5-5b64-48d5-adfb-9f85d45397a3-h5vm8" in namespace "emptydir-wrapper-5256" to be "running"
Dec 30 04:10:48.889: INFO: Pod "wrapped-volume-race-e7961bb5-5b64-48d5-adfb-9f85d45397a3-h5vm8": Phase="Running", Reason="", readiness=true. Elapsed: 5.157017ms
Dec 30 04:10:48.889: INFO: Pod "wrapped-volume-race-e7961bb5-5b64-48d5-adfb-9f85d45397a3-h5vm8" satisfied condition "running"
Dec 30 04:10:48.889: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-e7961bb5-5b64-48d5-adfb-9f85d45397a3-mf9b6" in namespace "emptydir-wrapper-5256" to be "running"
Dec 30 04:10:48.895: INFO: Pod "wrapped-volume-race-e7961bb5-5b64-48d5-adfb-9f85d45397a3-mf9b6": Phase="Running", Reason="", readiness=true. Elapsed: 5.667345ms
Dec 30 04:10:48.895: INFO: Pod "wrapped-volume-race-e7961bb5-5b64-48d5-adfb-9f85d45397a3-mf9b6" satisfied condition "running"
Dec 30 04:10:48.895: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-e7961bb5-5b64-48d5-adfb-9f85d45397a3-pmbcs" in namespace "emptydir-wrapper-5256" to be "running"
Dec 30 04:10:48.901: INFO: Pod "wrapped-volume-race-e7961bb5-5b64-48d5-adfb-9f85d45397a3-pmbcs": Phase="Running", Reason="", readiness=true. Elapsed: 5.924069ms
Dec 30 04:10:48.901: INFO: Pod "wrapped-volume-race-e7961bb5-5b64-48d5-adfb-9f85d45397a3-pmbcs" satisfied condition "running"
Dec 30 04:10:48.901: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-e7961bb5-5b64-48d5-adfb-9f85d45397a3-v9zk8" in namespace "emptydir-wrapper-5256" to be "running"
Dec 30 04:10:48.906: INFO: Pod "wrapped-volume-race-e7961bb5-5b64-48d5-adfb-9f85d45397a3-v9zk8": Phase="Running", Reason="", readiness=true. Elapsed: 5.021068ms
Dec 30 04:10:48.906: INFO: Pod "wrapped-volume-race-e7961bb5-5b64-48d5-adfb-9f85d45397a3-v9zk8" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-e7961bb5-5b64-48d5-adfb-9f85d45397a3 in namespace emptydir-wrapper-5256, will wait for the garbage collector to delete the pods 12/30/22 04:10:48.906
Dec 30 04:10:48.971: INFO: Deleting ReplicationController wrapped-volume-race-e7961bb5-5b64-48d5-adfb-9f85d45397a3 took: 9.031606ms
Dec 30 04:10:49.072: INFO: Terminating ReplicationController wrapped-volume-race-e7961bb5-5b64-48d5-adfb-9f85d45397a3 pods took: 101.159793ms
STEP: Creating RC which spawns configmap-volume pods 12/30/22 04:10:51.379
Dec 30 04:10:51.396: INFO: Pod name wrapped-volume-race-68b98df8-bcac-47c0-9316-55d2a8a13f9e: Found 0 pods out of 5
Dec 30 04:10:56.407: INFO: Pod name wrapped-volume-race-68b98df8-bcac-47c0-9316-55d2a8a13f9e: Found 5 pods out of 5
STEP: Ensuring each pod is running 12/30/22 04:10:56.407
Dec 30 04:10:56.407: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-68b98df8-bcac-47c0-9316-55d2a8a13f9e-4h98t" in namespace "emptydir-wrapper-5256" to be "running"
Dec 30 04:10:56.412: INFO: Pod "wrapped-volume-race-68b98df8-bcac-47c0-9316-55d2a8a13f9e-4h98t": Phase="Pending", Reason="", readiness=false. Elapsed: 5.596368ms
Dec 30 04:10:58.419: INFO: Pod "wrapped-volume-race-68b98df8-bcac-47c0-9316-55d2a8a13f9e-4h98t": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01258489s
Dec 30 04:11:00.420: INFO: Pod "wrapped-volume-race-68b98df8-bcac-47c0-9316-55d2a8a13f9e-4h98t": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013282825s
Dec 30 04:11:02.420: INFO: Pod "wrapped-volume-race-68b98df8-bcac-47c0-9316-55d2a8a13f9e-4h98t": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013008215s
Dec 30 04:11:04.420: INFO: Pod "wrapped-volume-race-68b98df8-bcac-47c0-9316-55d2a8a13f9e-4h98t": Phase="Pending", Reason="", readiness=false. Elapsed: 8.013290835s
Dec 30 04:11:06.419: INFO: Pod "wrapped-volume-race-68b98df8-bcac-47c0-9316-55d2a8a13f9e-4h98t": Phase="Running", Reason="", readiness=true. Elapsed: 10.011916543s
Dec 30 04:11:06.419: INFO: Pod "wrapped-volume-race-68b98df8-bcac-47c0-9316-55d2a8a13f9e-4h98t" satisfied condition "running"
Dec 30 04:11:06.419: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-68b98df8-bcac-47c0-9316-55d2a8a13f9e-5bx87" in namespace "emptydir-wrapper-5256" to be "running"
Dec 30 04:11:06.425: INFO: Pod "wrapped-volume-race-68b98df8-bcac-47c0-9316-55d2a8a13f9e-5bx87": Phase="Running", Reason="", readiness=true. Elapsed: 6.043299ms
Dec 30 04:11:06.425: INFO: Pod "wrapped-volume-race-68b98df8-bcac-47c0-9316-55d2a8a13f9e-5bx87" satisfied condition "running"
Dec 30 04:11:06.425: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-68b98df8-bcac-47c0-9316-55d2a8a13f9e-dj9sw" in namespace "emptydir-wrapper-5256" to be "running"
Dec 30 04:11:06.430: INFO: Pod "wrapped-volume-race-68b98df8-bcac-47c0-9316-55d2a8a13f9e-dj9sw": Phase="Running", Reason="", readiness=true. Elapsed: 5.300048ms
Dec 30 04:11:06.430: INFO: Pod "wrapped-volume-race-68b98df8-bcac-47c0-9316-55d2a8a13f9e-dj9sw" satisfied condition "running"
Dec 30 04:11:06.430: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-68b98df8-bcac-47c0-9316-55d2a8a13f9e-fvp4d" in namespace "emptydir-wrapper-5256" to be "running"
Dec 30 04:11:06.436: INFO: Pod "wrapped-volume-race-68b98df8-bcac-47c0-9316-55d2a8a13f9e-fvp4d": Phase="Running", Reason="", readiness=true. Elapsed: 5.340509ms
Dec 30 04:11:06.436: INFO: Pod "wrapped-volume-race-68b98df8-bcac-47c0-9316-55d2a8a13f9e-fvp4d" satisfied condition "running"
Dec 30 04:11:06.436: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-68b98df8-bcac-47c0-9316-55d2a8a13f9e-svc89" in namespace "emptydir-wrapper-5256" to be "running"
Dec 30 04:11:06.441: INFO: Pod "wrapped-volume-race-68b98df8-bcac-47c0-9316-55d2a8a13f9e-svc89": Phase="Running", Reason="", readiness=true. Elapsed: 5.607517ms
Dec 30 04:11:06.441: INFO: Pod "wrapped-volume-race-68b98df8-bcac-47c0-9316-55d2a8a13f9e-svc89" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-68b98df8-bcac-47c0-9316-55d2a8a13f9e in namespace emptydir-wrapper-5256, will wait for the garbage collector to delete the pods 12/30/22 04:11:06.441
Dec 30 04:11:06.505: INFO: Deleting ReplicationController wrapped-volume-race-68b98df8-bcac-47c0-9316-55d2a8a13f9e took: 7.750385ms
Dec 30 04:11:06.606: INFO: Terminating ReplicationController wrapped-volume-race-68b98df8-bcac-47c0-9316-55d2a8a13f9e pods took: 100.832042ms
STEP: Creating RC which spawns configmap-volume pods 12/30/22 04:11:09.313
Dec 30 04:11:09.334: INFO: Pod name wrapped-volume-race-cdde8b3c-16c8-4987-b808-3691cd427abe: Found 0 pods out of 5
Dec 30 04:11:14.346: INFO: Pod name wrapped-volume-race-cdde8b3c-16c8-4987-b808-3691cd427abe: Found 5 pods out of 5
STEP: Ensuring each pod is running 12/30/22 04:11:14.346
Dec 30 04:11:14.347: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-cdde8b3c-16c8-4987-b808-3691cd427abe-46g58" in namespace "emptydir-wrapper-5256" to be "running"
Dec 30 04:11:14.351: INFO: Pod "wrapped-volume-race-cdde8b3c-16c8-4987-b808-3691cd427abe-46g58": Phase="Pending", Reason="", readiness=false. Elapsed: 4.566206ms
Dec 30 04:11:16.358: INFO: Pod "wrapped-volume-race-cdde8b3c-16c8-4987-b808-3691cd427abe-46g58": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011462945s
Dec 30 04:11:18.357: INFO: Pod "wrapped-volume-race-cdde8b3c-16c8-4987-b808-3691cd427abe-46g58": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009917305s
Dec 30 04:11:20.359: INFO: Pod "wrapped-volume-race-cdde8b3c-16c8-4987-b808-3691cd427abe-46g58": Phase="Pending", Reason="", readiness=false. Elapsed: 6.01207372s
Dec 30 04:11:22.359: INFO: Pod "wrapped-volume-race-cdde8b3c-16c8-4987-b808-3691cd427abe-46g58": Phase="Pending", Reason="", readiness=false. Elapsed: 8.012426477s
Dec 30 04:11:24.359: INFO: Pod "wrapped-volume-race-cdde8b3c-16c8-4987-b808-3691cd427abe-46g58": Phase="Running", Reason="", readiness=true. Elapsed: 10.011814169s
Dec 30 04:11:24.359: INFO: Pod "wrapped-volume-race-cdde8b3c-16c8-4987-b808-3691cd427abe-46g58" satisfied condition "running"
Dec 30 04:11:24.359: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-cdde8b3c-16c8-4987-b808-3691cd427abe-bq6n6" in namespace "emptydir-wrapper-5256" to be "running"
Dec 30 04:11:24.364: INFO: Pod "wrapped-volume-race-cdde8b3c-16c8-4987-b808-3691cd427abe-bq6n6": Phase="Running", Reason="", readiness=true. Elapsed: 5.404259ms
Dec 30 04:11:24.364: INFO: Pod "wrapped-volume-race-cdde8b3c-16c8-4987-b808-3691cd427abe-bq6n6" satisfied condition "running"
Dec 30 04:11:24.364: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-cdde8b3c-16c8-4987-b808-3691cd427abe-fgcnr" in namespace "emptydir-wrapper-5256" to be "running"
Dec 30 04:11:24.369: INFO: Pod "wrapped-volume-race-cdde8b3c-16c8-4987-b808-3691cd427abe-fgcnr": Phase="Running", Reason="", readiness=true. Elapsed: 4.760607ms
Dec 30 04:11:24.369: INFO: Pod "wrapped-volume-race-cdde8b3c-16c8-4987-b808-3691cd427abe-fgcnr" satisfied condition "running"
Dec 30 04:11:24.369: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-cdde8b3c-16c8-4987-b808-3691cd427abe-l8gg2" in namespace "emptydir-wrapper-5256" to be "running"
Dec 30 04:11:24.374: INFO: Pod "wrapped-volume-race-cdde8b3c-16c8-4987-b808-3691cd427abe-l8gg2": Phase="Running", Reason="", readiness=true. Elapsed: 5.068522ms
Dec 30 04:11:24.374: INFO: Pod "wrapped-volume-race-cdde8b3c-16c8-4987-b808-3691cd427abe-l8gg2" satisfied condition "running"
Dec 30 04:11:24.374: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-cdde8b3c-16c8-4987-b808-3691cd427abe-s8q52" in namespace "emptydir-wrapper-5256" to be "running"
Dec 30 04:11:24.379: INFO: Pod "wrapped-volume-race-cdde8b3c-16c8-4987-b808-3691cd427abe-s8q52": Phase="Running", Reason="", readiness=true. Elapsed: 5.28888ms
Dec 30 04:11:24.379: INFO: Pod "wrapped-volume-race-cdde8b3c-16c8-4987-b808-3691cd427abe-s8q52" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-cdde8b3c-16c8-4987-b808-3691cd427abe in namespace emptydir-wrapper-5256, will wait for the garbage collector to delete the pods 12/30/22 04:11:24.379
Dec 30 04:11:24.443: INFO: Deleting ReplicationController wrapped-volume-race-cdde8b3c-16c8-4987-b808-3691cd427abe took: 8.156051ms
Dec 30 04:11:24.544: INFO: Terminating ReplicationController wrapped-volume-race-cdde8b3c-16c8-4987-b808-3691cd427abe pods took: 100.630621ms
STEP: Cleaning up the configMaps 12/30/22 04:11:27.445
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
Dec 30 04:11:27.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-5256" for this suite. 12/30/22 04:11:27.776
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","completed":199,"skipped":3575,"failed":0}
------------------------------
• [SLOW TEST] [54.283 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:10:33.5
    Dec 30 04:10:33.500: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename emptydir-wrapper 12/30/22 04:10:33.502
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:10:33.52
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:10:33.523
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 12/30/22 04:10:33.527
    STEP: Creating RC which spawns configmap-volume pods 12/30/22 04:10:33.763
    Dec 30 04:10:33.859: INFO: Pod name wrapped-volume-race-e7961bb5-5b64-48d5-adfb-9f85d45397a3: Found 1 pods out of 5
    Dec 30 04:10:38.871: INFO: Pod name wrapped-volume-race-e7961bb5-5b64-48d5-adfb-9f85d45397a3: Found 5 pods out of 5
    STEP: Ensuring each pod is running 12/30/22 04:10:38.871
    Dec 30 04:10:38.871: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-e7961bb5-5b64-48d5-adfb-9f85d45397a3-777cw" in namespace "emptydir-wrapper-5256" to be "running"
    Dec 30 04:10:38.876: INFO: Pod "wrapped-volume-race-e7961bb5-5b64-48d5-adfb-9f85d45397a3-777cw": Phase="Pending", Reason="", readiness=false. Elapsed: 5.194446ms
    Dec 30 04:10:40.883: INFO: Pod "wrapped-volume-race-e7961bb5-5b64-48d5-adfb-9f85d45397a3-777cw": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011625942s
    Dec 30 04:10:42.882: INFO: Pod "wrapped-volume-race-e7961bb5-5b64-48d5-adfb-9f85d45397a3-777cw": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01116009s
    Dec 30 04:10:44.883: INFO: Pod "wrapped-volume-race-e7961bb5-5b64-48d5-adfb-9f85d45397a3-777cw": Phase="Pending", Reason="", readiness=false. Elapsed: 6.011437259s
    Dec 30 04:10:46.885: INFO: Pod "wrapped-volume-race-e7961bb5-5b64-48d5-adfb-9f85d45397a3-777cw": Phase="Pending", Reason="", readiness=false. Elapsed: 8.014386359s
    Dec 30 04:10:48.884: INFO: Pod "wrapped-volume-race-e7961bb5-5b64-48d5-adfb-9f85d45397a3-777cw": Phase="Running", Reason="", readiness=true. Elapsed: 10.01261589s
    Dec 30 04:10:48.884: INFO: Pod "wrapped-volume-race-e7961bb5-5b64-48d5-adfb-9f85d45397a3-777cw" satisfied condition "running"
    Dec 30 04:10:48.884: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-e7961bb5-5b64-48d5-adfb-9f85d45397a3-h5vm8" in namespace "emptydir-wrapper-5256" to be "running"
    Dec 30 04:10:48.889: INFO: Pod "wrapped-volume-race-e7961bb5-5b64-48d5-adfb-9f85d45397a3-h5vm8": Phase="Running", Reason="", readiness=true. Elapsed: 5.157017ms
    Dec 30 04:10:48.889: INFO: Pod "wrapped-volume-race-e7961bb5-5b64-48d5-adfb-9f85d45397a3-h5vm8" satisfied condition "running"
    Dec 30 04:10:48.889: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-e7961bb5-5b64-48d5-adfb-9f85d45397a3-mf9b6" in namespace "emptydir-wrapper-5256" to be "running"
    Dec 30 04:10:48.895: INFO: Pod "wrapped-volume-race-e7961bb5-5b64-48d5-adfb-9f85d45397a3-mf9b6": Phase="Running", Reason="", readiness=true. Elapsed: 5.667345ms
    Dec 30 04:10:48.895: INFO: Pod "wrapped-volume-race-e7961bb5-5b64-48d5-adfb-9f85d45397a3-mf9b6" satisfied condition "running"
    Dec 30 04:10:48.895: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-e7961bb5-5b64-48d5-adfb-9f85d45397a3-pmbcs" in namespace "emptydir-wrapper-5256" to be "running"
    Dec 30 04:10:48.901: INFO: Pod "wrapped-volume-race-e7961bb5-5b64-48d5-adfb-9f85d45397a3-pmbcs": Phase="Running", Reason="", readiness=true. Elapsed: 5.924069ms
    Dec 30 04:10:48.901: INFO: Pod "wrapped-volume-race-e7961bb5-5b64-48d5-adfb-9f85d45397a3-pmbcs" satisfied condition "running"
    Dec 30 04:10:48.901: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-e7961bb5-5b64-48d5-adfb-9f85d45397a3-v9zk8" in namespace "emptydir-wrapper-5256" to be "running"
    Dec 30 04:10:48.906: INFO: Pod "wrapped-volume-race-e7961bb5-5b64-48d5-adfb-9f85d45397a3-v9zk8": Phase="Running", Reason="", readiness=true. Elapsed: 5.021068ms
    Dec 30 04:10:48.906: INFO: Pod "wrapped-volume-race-e7961bb5-5b64-48d5-adfb-9f85d45397a3-v9zk8" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-e7961bb5-5b64-48d5-adfb-9f85d45397a3 in namespace emptydir-wrapper-5256, will wait for the garbage collector to delete the pods 12/30/22 04:10:48.906
    Dec 30 04:10:48.971: INFO: Deleting ReplicationController wrapped-volume-race-e7961bb5-5b64-48d5-adfb-9f85d45397a3 took: 9.031606ms
    Dec 30 04:10:49.072: INFO: Terminating ReplicationController wrapped-volume-race-e7961bb5-5b64-48d5-adfb-9f85d45397a3 pods took: 101.159793ms
    STEP: Creating RC which spawns configmap-volume pods 12/30/22 04:10:51.379
    Dec 30 04:10:51.396: INFO: Pod name wrapped-volume-race-68b98df8-bcac-47c0-9316-55d2a8a13f9e: Found 0 pods out of 5
    Dec 30 04:10:56.407: INFO: Pod name wrapped-volume-race-68b98df8-bcac-47c0-9316-55d2a8a13f9e: Found 5 pods out of 5
    STEP: Ensuring each pod is running 12/30/22 04:10:56.407
    Dec 30 04:10:56.407: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-68b98df8-bcac-47c0-9316-55d2a8a13f9e-4h98t" in namespace "emptydir-wrapper-5256" to be "running"
    Dec 30 04:10:56.412: INFO: Pod "wrapped-volume-race-68b98df8-bcac-47c0-9316-55d2a8a13f9e-4h98t": Phase="Pending", Reason="", readiness=false. Elapsed: 5.596368ms
    Dec 30 04:10:58.419: INFO: Pod "wrapped-volume-race-68b98df8-bcac-47c0-9316-55d2a8a13f9e-4h98t": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01258489s
    Dec 30 04:11:00.420: INFO: Pod "wrapped-volume-race-68b98df8-bcac-47c0-9316-55d2a8a13f9e-4h98t": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013282825s
    Dec 30 04:11:02.420: INFO: Pod "wrapped-volume-race-68b98df8-bcac-47c0-9316-55d2a8a13f9e-4h98t": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013008215s
    Dec 30 04:11:04.420: INFO: Pod "wrapped-volume-race-68b98df8-bcac-47c0-9316-55d2a8a13f9e-4h98t": Phase="Pending", Reason="", readiness=false. Elapsed: 8.013290835s
    Dec 30 04:11:06.419: INFO: Pod "wrapped-volume-race-68b98df8-bcac-47c0-9316-55d2a8a13f9e-4h98t": Phase="Running", Reason="", readiness=true. Elapsed: 10.011916543s
    Dec 30 04:11:06.419: INFO: Pod "wrapped-volume-race-68b98df8-bcac-47c0-9316-55d2a8a13f9e-4h98t" satisfied condition "running"
    Dec 30 04:11:06.419: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-68b98df8-bcac-47c0-9316-55d2a8a13f9e-5bx87" in namespace "emptydir-wrapper-5256" to be "running"
    Dec 30 04:11:06.425: INFO: Pod "wrapped-volume-race-68b98df8-bcac-47c0-9316-55d2a8a13f9e-5bx87": Phase="Running", Reason="", readiness=true. Elapsed: 6.043299ms
    Dec 30 04:11:06.425: INFO: Pod "wrapped-volume-race-68b98df8-bcac-47c0-9316-55d2a8a13f9e-5bx87" satisfied condition "running"
    Dec 30 04:11:06.425: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-68b98df8-bcac-47c0-9316-55d2a8a13f9e-dj9sw" in namespace "emptydir-wrapper-5256" to be "running"
    Dec 30 04:11:06.430: INFO: Pod "wrapped-volume-race-68b98df8-bcac-47c0-9316-55d2a8a13f9e-dj9sw": Phase="Running", Reason="", readiness=true. Elapsed: 5.300048ms
    Dec 30 04:11:06.430: INFO: Pod "wrapped-volume-race-68b98df8-bcac-47c0-9316-55d2a8a13f9e-dj9sw" satisfied condition "running"
    Dec 30 04:11:06.430: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-68b98df8-bcac-47c0-9316-55d2a8a13f9e-fvp4d" in namespace "emptydir-wrapper-5256" to be "running"
    Dec 30 04:11:06.436: INFO: Pod "wrapped-volume-race-68b98df8-bcac-47c0-9316-55d2a8a13f9e-fvp4d": Phase="Running", Reason="", readiness=true. Elapsed: 5.340509ms
    Dec 30 04:11:06.436: INFO: Pod "wrapped-volume-race-68b98df8-bcac-47c0-9316-55d2a8a13f9e-fvp4d" satisfied condition "running"
    Dec 30 04:11:06.436: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-68b98df8-bcac-47c0-9316-55d2a8a13f9e-svc89" in namespace "emptydir-wrapper-5256" to be "running"
    Dec 30 04:11:06.441: INFO: Pod "wrapped-volume-race-68b98df8-bcac-47c0-9316-55d2a8a13f9e-svc89": Phase="Running", Reason="", readiness=true. Elapsed: 5.607517ms
    Dec 30 04:11:06.441: INFO: Pod "wrapped-volume-race-68b98df8-bcac-47c0-9316-55d2a8a13f9e-svc89" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-68b98df8-bcac-47c0-9316-55d2a8a13f9e in namespace emptydir-wrapper-5256, will wait for the garbage collector to delete the pods 12/30/22 04:11:06.441
    Dec 30 04:11:06.505: INFO: Deleting ReplicationController wrapped-volume-race-68b98df8-bcac-47c0-9316-55d2a8a13f9e took: 7.750385ms
    Dec 30 04:11:06.606: INFO: Terminating ReplicationController wrapped-volume-race-68b98df8-bcac-47c0-9316-55d2a8a13f9e pods took: 100.832042ms
    STEP: Creating RC which spawns configmap-volume pods 12/30/22 04:11:09.313
    Dec 30 04:11:09.334: INFO: Pod name wrapped-volume-race-cdde8b3c-16c8-4987-b808-3691cd427abe: Found 0 pods out of 5
    Dec 30 04:11:14.346: INFO: Pod name wrapped-volume-race-cdde8b3c-16c8-4987-b808-3691cd427abe: Found 5 pods out of 5
    STEP: Ensuring each pod is running 12/30/22 04:11:14.346
    Dec 30 04:11:14.347: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-cdde8b3c-16c8-4987-b808-3691cd427abe-46g58" in namespace "emptydir-wrapper-5256" to be "running"
    Dec 30 04:11:14.351: INFO: Pod "wrapped-volume-race-cdde8b3c-16c8-4987-b808-3691cd427abe-46g58": Phase="Pending", Reason="", readiness=false. Elapsed: 4.566206ms
    Dec 30 04:11:16.358: INFO: Pod "wrapped-volume-race-cdde8b3c-16c8-4987-b808-3691cd427abe-46g58": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011462945s
    Dec 30 04:11:18.357: INFO: Pod "wrapped-volume-race-cdde8b3c-16c8-4987-b808-3691cd427abe-46g58": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009917305s
    Dec 30 04:11:20.359: INFO: Pod "wrapped-volume-race-cdde8b3c-16c8-4987-b808-3691cd427abe-46g58": Phase="Pending", Reason="", readiness=false. Elapsed: 6.01207372s
    Dec 30 04:11:22.359: INFO: Pod "wrapped-volume-race-cdde8b3c-16c8-4987-b808-3691cd427abe-46g58": Phase="Pending", Reason="", readiness=false. Elapsed: 8.012426477s
    Dec 30 04:11:24.359: INFO: Pod "wrapped-volume-race-cdde8b3c-16c8-4987-b808-3691cd427abe-46g58": Phase="Running", Reason="", readiness=true. Elapsed: 10.011814169s
    Dec 30 04:11:24.359: INFO: Pod "wrapped-volume-race-cdde8b3c-16c8-4987-b808-3691cd427abe-46g58" satisfied condition "running"
    Dec 30 04:11:24.359: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-cdde8b3c-16c8-4987-b808-3691cd427abe-bq6n6" in namespace "emptydir-wrapper-5256" to be "running"
    Dec 30 04:11:24.364: INFO: Pod "wrapped-volume-race-cdde8b3c-16c8-4987-b808-3691cd427abe-bq6n6": Phase="Running", Reason="", readiness=true. Elapsed: 5.404259ms
    Dec 30 04:11:24.364: INFO: Pod "wrapped-volume-race-cdde8b3c-16c8-4987-b808-3691cd427abe-bq6n6" satisfied condition "running"
    Dec 30 04:11:24.364: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-cdde8b3c-16c8-4987-b808-3691cd427abe-fgcnr" in namespace "emptydir-wrapper-5256" to be "running"
    Dec 30 04:11:24.369: INFO: Pod "wrapped-volume-race-cdde8b3c-16c8-4987-b808-3691cd427abe-fgcnr": Phase="Running", Reason="", readiness=true. Elapsed: 4.760607ms
    Dec 30 04:11:24.369: INFO: Pod "wrapped-volume-race-cdde8b3c-16c8-4987-b808-3691cd427abe-fgcnr" satisfied condition "running"
    Dec 30 04:11:24.369: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-cdde8b3c-16c8-4987-b808-3691cd427abe-l8gg2" in namespace "emptydir-wrapper-5256" to be "running"
    Dec 30 04:11:24.374: INFO: Pod "wrapped-volume-race-cdde8b3c-16c8-4987-b808-3691cd427abe-l8gg2": Phase="Running", Reason="", readiness=true. Elapsed: 5.068522ms
    Dec 30 04:11:24.374: INFO: Pod "wrapped-volume-race-cdde8b3c-16c8-4987-b808-3691cd427abe-l8gg2" satisfied condition "running"
    Dec 30 04:11:24.374: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-cdde8b3c-16c8-4987-b808-3691cd427abe-s8q52" in namespace "emptydir-wrapper-5256" to be "running"
    Dec 30 04:11:24.379: INFO: Pod "wrapped-volume-race-cdde8b3c-16c8-4987-b808-3691cd427abe-s8q52": Phase="Running", Reason="", readiness=true. Elapsed: 5.28888ms
    Dec 30 04:11:24.379: INFO: Pod "wrapped-volume-race-cdde8b3c-16c8-4987-b808-3691cd427abe-s8q52" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-cdde8b3c-16c8-4987-b808-3691cd427abe in namespace emptydir-wrapper-5256, will wait for the garbage collector to delete the pods 12/30/22 04:11:24.379
    Dec 30 04:11:24.443: INFO: Deleting ReplicationController wrapped-volume-race-cdde8b3c-16c8-4987-b808-3691cd427abe took: 8.156051ms
    Dec 30 04:11:24.544: INFO: Terminating ReplicationController wrapped-volume-race-cdde8b3c-16c8-4987-b808-3691cd427abe pods took: 100.630621ms
    STEP: Cleaning up the configMaps 12/30/22 04:11:27.445
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:187
    Dec 30 04:11:27.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-wrapper-5256" for this suite. 12/30/22 04:11:27.776
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:11:27.79
Dec 30 04:11:27.790: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename endpointslice 12/30/22 04:11:27.791
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:11:27.809
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:11:27.812
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204
STEP: referencing a single matching pod 12/30/22 04:11:32.871
STEP: referencing matching pods with named port 12/30/22 04:11:37.88
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 12/30/22 04:11:42.89
STEP: recreating EndpointSlices after they've been deleted 12/30/22 04:11:47.902
Dec 30 04:11:47.926: INFO: EndpointSlice for Service endpointslice-6637/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Dec 30 04:11:57.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-6637" for this suite. 12/30/22 04:11:57.95
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","completed":200,"skipped":3642,"failed":0}
------------------------------
• [SLOW TEST] [30.168 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:11:27.79
    Dec 30 04:11:27.790: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename endpointslice 12/30/22 04:11:27.791
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:11:27.809
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:11:27.812
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:204
    STEP: referencing a single matching pod 12/30/22 04:11:32.871
    STEP: referencing matching pods with named port 12/30/22 04:11:37.88
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 12/30/22 04:11:42.89
    STEP: recreating EndpointSlices after they've been deleted 12/30/22 04:11:47.902
    Dec 30 04:11:47.926: INFO: EndpointSlice for Service endpointslice-6637/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Dec 30 04:11:57.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-6637" for this suite. 12/30/22 04:11:57.95
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:231
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:11:57.958
Dec 30 04:11:57.958: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename container-runtime 12/30/22 04:11:57.959
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:11:57.977
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:11:57.981
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:231
STEP: create the container 12/30/22 04:11:57.984
STEP: wait for the container to reach Succeeded 12/30/22 04:11:57.994
STEP: get the container status 12/30/22 04:12:02.014
STEP: the container should be terminated 12/30/22 04:12:02.019
STEP: the termination message should be set 12/30/22 04:12:02.019
Dec 30 04:12:02.019: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 12/30/22 04:12:02.019
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Dec 30 04:12:02.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8650" for this suite. 12/30/22 04:12:02.042
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":201,"skipped":3642,"failed":0}
------------------------------
• [4.091 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:231

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:11:57.958
    Dec 30 04:11:57.958: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename container-runtime 12/30/22 04:11:57.959
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:11:57.977
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:11:57.981
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:231
    STEP: create the container 12/30/22 04:11:57.984
    STEP: wait for the container to reach Succeeded 12/30/22 04:11:57.994
    STEP: get the container status 12/30/22 04:12:02.014
    STEP: the container should be terminated 12/30/22 04:12:02.019
    STEP: the termination message should be set 12/30/22 04:12:02.019
    Dec 30 04:12:02.019: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 12/30/22 04:12:02.019
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Dec 30 04:12:02.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-8650" for this suite. 12/30/22 04:12:02.042
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:12:02.05
Dec 30 04:12:02.050: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename disruption 12/30/22 04:12:02.052
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:12:02.069
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:12:02.073
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107
STEP: creating the pdb 12/30/22 04:12:02.077
STEP: Waiting for the pdb to be processed 12/30/22 04:12:02.082
STEP: updating the pdb 12/30/22 04:12:04.092
STEP: Waiting for the pdb to be processed 12/30/22 04:12:04.103
STEP: patching the pdb 12/30/22 04:12:06.111
STEP: Waiting for the pdb to be processed 12/30/22 04:12:06.123
STEP: Waiting for the pdb to be deleted 12/30/22 04:12:08.139
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Dec 30 04:12:08.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-4547" for this suite. 12/30/22 04:12:08.149
{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","completed":202,"skipped":3652,"failed":0}
------------------------------
• [SLOW TEST] [6.106 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:12:02.05
    Dec 30 04:12:02.050: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename disruption 12/30/22 04:12:02.052
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:12:02.069
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:12:02.073
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:107
    STEP: creating the pdb 12/30/22 04:12:02.077
    STEP: Waiting for the pdb to be processed 12/30/22 04:12:02.082
    STEP: updating the pdb 12/30/22 04:12:04.092
    STEP: Waiting for the pdb to be processed 12/30/22 04:12:04.103
    STEP: patching the pdb 12/30/22 04:12:06.111
    STEP: Waiting for the pdb to be processed 12/30/22 04:12:06.123
    STEP: Waiting for the pdb to be deleted 12/30/22 04:12:08.139
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Dec 30 04:12:08.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-4547" for this suite. 12/30/22 04:12:08.149
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1274
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:12:08.157
Dec 30 04:12:08.157: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename kubectl 12/30/22 04:12:08.158
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:12:08.178
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:12:08.181
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1274
Dec 30 04:12:08.185: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-9224 create -f -'
Dec 30 04:12:09.055: INFO: stderr: ""
Dec 30 04:12:09.055: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Dec 30 04:12:09.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-9224 create -f -'
Dec 30 04:12:09.858: INFO: stderr: ""
Dec 30 04:12:09.858: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 12/30/22 04:12:09.858
Dec 30 04:12:10.864: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 30 04:12:10.864: INFO: Found 1 / 1
Dec 30 04:12:10.864: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec 30 04:12:10.868: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 30 04:12:10.868: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec 30 04:12:10.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-9224 describe pod agnhost-primary-97nph'
Dec 30 04:12:10.984: INFO: stderr: ""
Dec 30 04:12:10.984: INFO: stdout: "Name:             agnhost-primary-97nph\nNamespace:        kubectl-9224\nPriority:         0\nService Account:  default\nNode:             k8s-mgmt01/10.78.26.140\nStart Time:       Fri, 30 Dec 2022 04:12:09 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: 6da1c22de88c90af3261fb470873431312ac0fcc8fed1cfcaab8602ad89f0106\n                  cni.projectcalico.org/podIP: 10.233.112.182/32\n                  cni.projectcalico.org/podIPs: 10.233.112.182/32\nStatus:           Running\nIP:               10.233.112.182\nIPs:\n  IP:           10.233.112.182\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://4b987b7f14e7c471c56075e53e22cb992b1074d68d4115fb550c15183852a262\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.40\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Fri, 30 Dec 2022 04:12:09 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-pjlhj (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-pjlhj:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  1s    default-scheduler  Successfully assigned kubectl-9224/agnhost-primary-97nph to k8s-mgmt01\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.40\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
Dec 30 04:12:10.985: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-9224 describe rc agnhost-primary'
Dec 30 04:12:11.102: INFO: stderr: ""
Dec 30 04:12:11.102: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-9224\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.40\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-97nph\n"
Dec 30 04:12:11.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-9224 describe service agnhost-primary'
Dec 30 04:12:11.214: INFO: stderr: ""
Dec 30 04:12:11.214: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-9224\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.233.43.43\nIPs:               10.233.43.43\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.233.112.182:6379\nSession Affinity:  None\nEvents:            <none>\n"
Dec 30 04:12:11.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-9224 describe node k8s-mgmt01'
Dec 30 04:12:11.372: INFO: stderr: ""
Dec 30 04:12:11.372: INFO: stdout: "Name:               k8s-mgmt01\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=k8s-mgmt01\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.78.26.140/24\n                    projectcalico.org/IPv4VXLANTunnelAddr: 10.233.112.128\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 28 Dec 2022 02:15:45 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  k8s-mgmt01\n  AcquireTime:     <unset>\n  RenewTime:       Fri, 30 Dec 2022 04:12:02 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Wed, 28 Dec 2022 02:19:44 +0000   Wed, 28 Dec 2022 02:19:44 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Fri, 30 Dec 2022 04:12:09 +0000   Wed, 28 Dec 2022 02:15:41 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Fri, 30 Dec 2022 04:12:09 +0000   Wed, 28 Dec 2022 02:15:41 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Fri, 30 Dec 2022 04:12:09 +0000   Wed, 28 Dec 2022 02:15:41 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Fri, 30 Dec 2022 04:12:09 +0000   Wed, 28 Dec 2022 02:24:37 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.78.26.140\n  Hostname:    k8s-mgmt01\nCapacity:\n  cpu:                  32\n  ephemeral-storage:    205375464Ki\n  example.com/fakecpu:  1k\n  hugepages-1Gi:        0\n  hugepages-2Mi:        0\n  memory:               65860368Ki\n  pods:                 110\nAllocatable:\n  cpu:                  31800m\n  ephemeral-storage:    189274027310\n  example.com/fakecpu:  1k\n  hugepages-1Gi:        0\n  hugepages-2Mi:        0\n  memory:               65233680Ki\n  pods:                 110\nSystem Info:\n  Machine ID:                 9d88e66ce2f24aa9af976ec76a77d1f8\n  System UUID:                03f7ed2c-b137-11d0-ea99-2cfda13422cf\n  Boot ID:                    383757f4-3611-444e-ab6f-160cfe364e3d\n  Kernel Version:             5.4.0-81-generic\n  OS Image:                   Ubuntu 20.04.3 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.9\n  Kubelet Version:            v1.25.3\n  Kube-Proxy Version:         v1.25.3\nPodCIDR:                      10.233.64.0/24\nPodCIDRs:                     10.233.64.0/24\nNon-terminated Pods:          (10 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-node-6xm98                                          150m (0%)     300m (0%)   64M (0%)         500M (0%)      2d1h\n  kube-system                 coredns-69dfc8446-pwfn2                                    100m (0%)     0 (0%)      70Mi (0%)        300Mi (0%)     2d1h\n  kube-system                 dns-autoscaler-5b9959d7fc-s8g6t                            20m (0%)      0 (0%)      10Mi (0%)        0 (0%)         2d1h\n  kube-system                 kube-apiserver-k8s-mgmt01                                  250m (0%)     0 (0%)      0 (0%)           0 (0%)         2d1h\n  kube-system                 kube-controller-manager-k8s-mgmt01                         200m (0%)     0 (0%)      0 (0%)           0 (0%)         2d1h\n  kube-system                 kube-proxy-tcvcl                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         2d1h\n  kube-system                 kube-scheduler-k8s-mgmt01                                  100m (0%)     0 (0%)      0 (0%)           0 (0%)         2d1h\n  kube-system                 nodelocaldns-n9jl8                                         100m (0%)     0 (0%)      70Mi (0%)        200Mi (0%)     2d1h\n  kubectl-9224                agnhost-primary-97nph                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         2s\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-4da877126f61426e-qd9td    0 (0%)        0 (0%)      0 (0%)           0 (0%)         51m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource             Requests        Limits\n  --------             --------        ------\n  cpu                  920m (2%)       300m (0%)\n  memory               221286400 (0%)  1024288k (1%)\n  ephemeral-storage    0 (0%)          0 (0%)\n  hugepages-1Gi        0 (0%)          0 (0%)\n  hugepages-2Mi        0 (0%)          0 (0%)\n  example.com/fakecpu  0               0\nEvents:                <none>\n"
Dec 30 04:12:11.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-9224 describe namespace kubectl-9224'
Dec 30 04:12:11.480: INFO: stderr: ""
Dec 30 04:12:11.480: INFO: stdout: "Name:         kubectl-9224\nLabels:       e2e-framework=kubectl\n              e2e-run=d6d95677-1855-4e28-8bac-3e4b37116b4e\n              kubernetes.io/metadata.name=kubectl-9224\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Dec 30 04:12:11.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9224" for this suite. 12/30/22 04:12:11.486
{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","completed":203,"skipped":3654,"failed":0}
------------------------------
• [3.337 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1268
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1274

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:12:08.157
    Dec 30 04:12:08.157: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename kubectl 12/30/22 04:12:08.158
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:12:08.178
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:12:08.181
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1274
    Dec 30 04:12:08.185: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-9224 create -f -'
    Dec 30 04:12:09.055: INFO: stderr: ""
    Dec 30 04:12:09.055: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    Dec 30 04:12:09.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-9224 create -f -'
    Dec 30 04:12:09.858: INFO: stderr: ""
    Dec 30 04:12:09.858: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 12/30/22 04:12:09.858
    Dec 30 04:12:10.864: INFO: Selector matched 1 pods for map[app:agnhost]
    Dec 30 04:12:10.864: INFO: Found 1 / 1
    Dec 30 04:12:10.864: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Dec 30 04:12:10.868: INFO: Selector matched 1 pods for map[app:agnhost]
    Dec 30 04:12:10.868: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Dec 30 04:12:10.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-9224 describe pod agnhost-primary-97nph'
    Dec 30 04:12:10.984: INFO: stderr: ""
    Dec 30 04:12:10.984: INFO: stdout: "Name:             agnhost-primary-97nph\nNamespace:        kubectl-9224\nPriority:         0\nService Account:  default\nNode:             k8s-mgmt01/10.78.26.140\nStart Time:       Fri, 30 Dec 2022 04:12:09 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: 6da1c22de88c90af3261fb470873431312ac0fcc8fed1cfcaab8602ad89f0106\n                  cni.projectcalico.org/podIP: 10.233.112.182/32\n                  cni.projectcalico.org/podIPs: 10.233.112.182/32\nStatus:           Running\nIP:               10.233.112.182\nIPs:\n  IP:           10.233.112.182\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://4b987b7f14e7c471c56075e53e22cb992b1074d68d4115fb550c15183852a262\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.40\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Fri, 30 Dec 2022 04:12:09 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-pjlhj (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-pjlhj:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  1s    default-scheduler  Successfully assigned kubectl-9224/agnhost-primary-97nph to k8s-mgmt01\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.40\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
    Dec 30 04:12:10.985: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-9224 describe rc agnhost-primary'
    Dec 30 04:12:11.102: INFO: stderr: ""
    Dec 30 04:12:11.102: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-9224\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.40\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-97nph\n"
    Dec 30 04:12:11.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-9224 describe service agnhost-primary'
    Dec 30 04:12:11.214: INFO: stderr: ""
    Dec 30 04:12:11.214: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-9224\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.233.43.43\nIPs:               10.233.43.43\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.233.112.182:6379\nSession Affinity:  None\nEvents:            <none>\n"
    Dec 30 04:12:11.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-9224 describe node k8s-mgmt01'
    Dec 30 04:12:11.372: INFO: stderr: ""
    Dec 30 04:12:11.372: INFO: stdout: "Name:               k8s-mgmt01\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=k8s-mgmt01\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.78.26.140/24\n                    projectcalico.org/IPv4VXLANTunnelAddr: 10.233.112.128\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 28 Dec 2022 02:15:45 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  k8s-mgmt01\n  AcquireTime:     <unset>\n  RenewTime:       Fri, 30 Dec 2022 04:12:02 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Wed, 28 Dec 2022 02:19:44 +0000   Wed, 28 Dec 2022 02:19:44 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Fri, 30 Dec 2022 04:12:09 +0000   Wed, 28 Dec 2022 02:15:41 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Fri, 30 Dec 2022 04:12:09 +0000   Wed, 28 Dec 2022 02:15:41 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Fri, 30 Dec 2022 04:12:09 +0000   Wed, 28 Dec 2022 02:15:41 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Fri, 30 Dec 2022 04:12:09 +0000   Wed, 28 Dec 2022 02:24:37 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.78.26.140\n  Hostname:    k8s-mgmt01\nCapacity:\n  cpu:                  32\n  ephemeral-storage:    205375464Ki\n  example.com/fakecpu:  1k\n  hugepages-1Gi:        0\n  hugepages-2Mi:        0\n  memory:               65860368Ki\n  pods:                 110\nAllocatable:\n  cpu:                  31800m\n  ephemeral-storage:    189274027310\n  example.com/fakecpu:  1k\n  hugepages-1Gi:        0\n  hugepages-2Mi:        0\n  memory:               65233680Ki\n  pods:                 110\nSystem Info:\n  Machine ID:                 9d88e66ce2f24aa9af976ec76a77d1f8\n  System UUID:                03f7ed2c-b137-11d0-ea99-2cfda13422cf\n  Boot ID:                    383757f4-3611-444e-ab6f-160cfe364e3d\n  Kernel Version:             5.4.0-81-generic\n  OS Image:                   Ubuntu 20.04.3 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.9\n  Kubelet Version:            v1.25.3\n  Kube-Proxy Version:         v1.25.3\nPodCIDR:                      10.233.64.0/24\nPodCIDRs:                     10.233.64.0/24\nNon-terminated Pods:          (10 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-node-6xm98                                          150m (0%)     300m (0%)   64M (0%)         500M (0%)      2d1h\n  kube-system                 coredns-69dfc8446-pwfn2                                    100m (0%)     0 (0%)      70Mi (0%)        300Mi (0%)     2d1h\n  kube-system                 dns-autoscaler-5b9959d7fc-s8g6t                            20m (0%)      0 (0%)      10Mi (0%)        0 (0%)         2d1h\n  kube-system                 kube-apiserver-k8s-mgmt01                                  250m (0%)     0 (0%)      0 (0%)           0 (0%)         2d1h\n  kube-system                 kube-controller-manager-k8s-mgmt01                         200m (0%)     0 (0%)      0 (0%)           0 (0%)         2d1h\n  kube-system                 kube-proxy-tcvcl                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         2d1h\n  kube-system                 kube-scheduler-k8s-mgmt01                                  100m (0%)     0 (0%)      0 (0%)           0 (0%)         2d1h\n  kube-system                 nodelocaldns-n9jl8                                         100m (0%)     0 (0%)      70Mi (0%)        200Mi (0%)     2d1h\n  kubectl-9224                agnhost-primary-97nph                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         2s\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-4da877126f61426e-qd9td    0 (0%)        0 (0%)      0 (0%)           0 (0%)         51m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource             Requests        Limits\n  --------             --------        ------\n  cpu                  920m (2%)       300m (0%)\n  memory               221286400 (0%)  1024288k (1%)\n  ephemeral-storage    0 (0%)          0 (0%)\n  hugepages-1Gi        0 (0%)          0 (0%)\n  hugepages-2Mi        0 (0%)          0 (0%)\n  example.com/fakecpu  0               0\nEvents:                <none>\n"
    Dec 30 04:12:11.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-9224 describe namespace kubectl-9224'
    Dec 30 04:12:11.480: INFO: stderr: ""
    Dec 30 04:12:11.480: INFO: stdout: "Name:         kubectl-9224\nLabels:       e2e-framework=kubectl\n              e2e-run=d6d95677-1855-4e28-8bac-3e4b37116b4e\n              kubernetes.io/metadata.name=kubectl-9224\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Dec 30 04:12:11.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-9224" for this suite. 12/30/22 04:12:11.486
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:12:11.495
Dec 30 04:12:11.495: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename pods 12/30/22 04:12:11.496
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:12:11.513
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:12:11.517
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225
STEP: creating the pod 12/30/22 04:12:11.52
STEP: setting up watch 12/30/22 04:12:11.52
STEP: submitting the pod to kubernetes 12/30/22 04:12:11.625
STEP: verifying the pod is in kubernetes 12/30/22 04:12:11.634
STEP: verifying pod creation was observed 12/30/22 04:12:11.638
Dec 30 04:12:11.638: INFO: Waiting up to 5m0s for pod "pod-submit-remove-3dffd4d7-2862-4d80-ba71-58bf34fc671e" in namespace "pods-718" to be "running"
Dec 30 04:12:11.642: INFO: Pod "pod-submit-remove-3dffd4d7-2862-4d80-ba71-58bf34fc671e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.91925ms
Dec 30 04:12:13.647: INFO: Pod "pod-submit-remove-3dffd4d7-2862-4d80-ba71-58bf34fc671e": Phase="Running", Reason="", readiness=true. Elapsed: 2.008452552s
Dec 30 04:12:13.647: INFO: Pod "pod-submit-remove-3dffd4d7-2862-4d80-ba71-58bf34fc671e" satisfied condition "running"
STEP: deleting the pod gracefully 12/30/22 04:12:13.651
STEP: verifying pod deletion was observed 12/30/22 04:12:13.66
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Dec 30 04:12:15.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-718" for this suite. 12/30/22 04:12:15.82
{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","completed":204,"skipped":3660,"failed":0}
------------------------------
• [4.332 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:12:11.495
    Dec 30 04:12:11.495: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename pods 12/30/22 04:12:11.496
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:12:11.513
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:12:11.517
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:225
    STEP: creating the pod 12/30/22 04:12:11.52
    STEP: setting up watch 12/30/22 04:12:11.52
    STEP: submitting the pod to kubernetes 12/30/22 04:12:11.625
    STEP: verifying the pod is in kubernetes 12/30/22 04:12:11.634
    STEP: verifying pod creation was observed 12/30/22 04:12:11.638
    Dec 30 04:12:11.638: INFO: Waiting up to 5m0s for pod "pod-submit-remove-3dffd4d7-2862-4d80-ba71-58bf34fc671e" in namespace "pods-718" to be "running"
    Dec 30 04:12:11.642: INFO: Pod "pod-submit-remove-3dffd4d7-2862-4d80-ba71-58bf34fc671e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.91925ms
    Dec 30 04:12:13.647: INFO: Pod "pod-submit-remove-3dffd4d7-2862-4d80-ba71-58bf34fc671e": Phase="Running", Reason="", readiness=true. Elapsed: 2.008452552s
    Dec 30 04:12:13.647: INFO: Pod "pod-submit-remove-3dffd4d7-2862-4d80-ba71-58bf34fc671e" satisfied condition "running"
    STEP: deleting the pod gracefully 12/30/22 04:12:13.651
    STEP: verifying pod deletion was observed 12/30/22 04:12:13.66
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Dec 30 04:12:15.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-718" for this suite. 12/30/22 04:12:15.82
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:12:15.828
Dec 30 04:12:15.829: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename namespaces 12/30/22 04:12:15.83
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:12:15.846
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:12:15.85
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298
STEP: Read namespace status 12/30/22 04:12:15.854
Dec 30 04:12:15.859: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 12/30/22 04:12:15.859
Dec 30 04:12:15.864: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 12/30/22 04:12:15.864
Dec 30 04:12:15.875: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Dec 30 04:12:15.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-6971" for this suite. 12/30/22 04:12:15.88
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should apply changes to a namespace status [Conformance]","completed":205,"skipped":3674,"failed":0}
------------------------------
• [0.058 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:12:15.828
    Dec 30 04:12:15.829: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename namespaces 12/30/22 04:12:15.83
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:12:15.846
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:12:15.85
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:298
    STEP: Read namespace status 12/30/22 04:12:15.854
    Dec 30 04:12:15.859: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 12/30/22 04:12:15.859
    Dec 30 04:12:15.864: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 12/30/22 04:12:15.864
    Dec 30 04:12:15.875: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Dec 30 04:12:15.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-6971" for this suite. 12/30/22 04:12:15.88
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1248
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:12:15.888
Dec 30 04:12:15.888: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename kubectl 12/30/22 04:12:15.889
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:12:15.911
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:12:15.915
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1248
STEP: validating cluster-info 12/30/22 04:12:15.918
Dec 30 04:12:15.918: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-9350 cluster-info'
Dec 30 04:12:16.024: INFO: stderr: ""
Dec 30 04:12:16.024: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.233.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Dec 30 04:12:16.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9350" for this suite. 12/30/22 04:12:16.03
{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","completed":206,"skipped":3691,"failed":0}
------------------------------
• [0.148 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1242
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:12:15.888
    Dec 30 04:12:15.888: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename kubectl 12/30/22 04:12:15.889
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:12:15.911
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:12:15.915
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1248
    STEP: validating cluster-info 12/30/22 04:12:15.918
    Dec 30 04:12:15.918: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-9350 cluster-info'
    Dec 30 04:12:16.024: INFO: stderr: ""
    Dec 30 04:12:16.024: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.233.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Dec 30 04:12:16.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-9350" for this suite. 12/30/22 04:12:16.03
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:12:16.038
Dec 30 04:12:16.038: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename init-container 12/30/22 04:12:16.039
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:12:16.057
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:12:16.061
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254
STEP: creating the pod 12/30/22 04:12:16.065
Dec 30 04:12:16.065: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Dec 30 04:12:19.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1144" for this suite. 12/30/22 04:12:19.824
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","completed":207,"skipped":3708,"failed":0}
------------------------------
• [3.793 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:12:16.038
    Dec 30 04:12:16.038: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename init-container 12/30/22 04:12:16.039
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:12:16.057
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:12:16.061
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:254
    STEP: creating the pod 12/30/22 04:12:16.065
    Dec 30 04:12:16.065: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Dec 30 04:12:19.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-1144" for this suite. 12/30/22 04:12:19.824
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:12:19.834
Dec 30 04:12:19.834: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename deployment 12/30/22 04:12:19.836
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:12:19.853
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:12:19.857
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
Dec 30 04:12:19.860: INFO: Creating simple deployment test-new-deployment
Dec 30 04:12:19.874: INFO: new replicaset for deployment "test-new-deployment" is yet to be created
STEP: getting scale subresource 12/30/22 04:12:21.89
STEP: updating a scale subresource 12/30/22 04:12:21.894
STEP: verifying the deployment Spec.Replicas was modified 12/30/22 04:12:21.902
STEP: Patch a scale subresource 12/30/22 04:12:21.905
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Dec 30 04:12:21.922: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-6200  606c2763-e6f2-40ee-9c9d-2e27d54b4f5c 442910 3 2022-12-30 04:12:19 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2022-12-30 04:12:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-30 04:12:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003c716f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-12-30 04:12:21 +0000 UTC,LastTransitionTime:2022-12-30 04:12:21 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-845c8977d9" has successfully progressed.,LastUpdateTime:2022-12-30 04:12:21 +0000 UTC,LastTransitionTime:2022-12-30 04:12:19 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Dec 30 04:12:21.926: INFO: New ReplicaSet "test-new-deployment-845c8977d9" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-845c8977d9  deployment-6200  b496da34-6398-4371-9b41-1dc672b957b4 442915 2 2022-12-30 04:12:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 606c2763-e6f2-40ee-9c9d-2e27d54b4f5c 0xc004973ba7 0xc004973ba8}] [] [{kube-controller-manager Update apps/v1 2022-12-30 04:12:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"606c2763-e6f2-40ee-9c9d-2e27d54b4f5c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-30 04:12:21 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004973c38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Dec 30 04:12:21.931: INFO: Pod "test-new-deployment-845c8977d9-h557b" is not available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-h557b test-new-deployment-845c8977d9- deployment-6200  5c571d45-951b-42d8-9d8f-cb17a7d00f00 442914 0 2022-12-30 04:12:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 b496da34-6398-4371-9b41-1dc672b957b4 0xc004973ff7 0xc004973ff8}] [] [{kube-controller-manager Update v1 2022-12-30 04:12:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b496da34-6398-4371-9b41-1dc672b957b4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j2vm8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j2vm8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-mgmt03,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 04:12:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 30 04:12:21.931: INFO: Pod "test-new-deployment-845c8977d9-q92f9" is available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-q92f9 test-new-deployment-845c8977d9- deployment-6200  51bd3d64-84ce-410e-bd08-2f539f6be556 442904 0 2022-12-30 04:12:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:8a330eab80c38684d5476edd12830c5d0db010aa2729a42893bb9a8626eabee5 cni.projectcalico.org/podIP:10.233.112.181/32 cni.projectcalico.org/podIPs:10.233.112.181/32] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 b496da34-6398-4371-9b41-1dc672b957b4 0xc000361b40 0xc000361b41}] [] [{kube-controller-manager Update v1 2022-12-30 04:12:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b496da34-6398-4371-9b41-1dc672b957b4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-12-30 04:12:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-30 04:12:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.112.181\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j48nq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j48nq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-mgmt01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 04:12:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 04:12:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 04:12:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 04:12:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.140,PodIP:10.233.112.181,StartTime:2022-12-30 04:12:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-30 04:12:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://44a3b28b881bb51e4cfb70ce4c05be5236aa7b6139a4656574d6b2bde92bcd8b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.112.181,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Dec 30 04:12:21.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6200" for this suite. 12/30/22 04:12:21.941
{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","completed":208,"skipped":3743,"failed":0}
------------------------------
• [2.113 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:12:19.834
    Dec 30 04:12:19.834: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename deployment 12/30/22 04:12:19.836
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:12:19.853
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:12:19.857
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    Dec 30 04:12:19.860: INFO: Creating simple deployment test-new-deployment
    Dec 30 04:12:19.874: INFO: new replicaset for deployment "test-new-deployment" is yet to be created
    STEP: getting scale subresource 12/30/22 04:12:21.89
    STEP: updating a scale subresource 12/30/22 04:12:21.894
    STEP: verifying the deployment Spec.Replicas was modified 12/30/22 04:12:21.902
    STEP: Patch a scale subresource 12/30/22 04:12:21.905
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Dec 30 04:12:21.922: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-6200  606c2763-e6f2-40ee-9c9d-2e27d54b4f5c 442910 3 2022-12-30 04:12:19 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2022-12-30 04:12:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-30 04:12:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003c716f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-12-30 04:12:21 +0000 UTC,LastTransitionTime:2022-12-30 04:12:21 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-845c8977d9" has successfully progressed.,LastUpdateTime:2022-12-30 04:12:21 +0000 UTC,LastTransitionTime:2022-12-30 04:12:19 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Dec 30 04:12:21.926: INFO: New ReplicaSet "test-new-deployment-845c8977d9" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-845c8977d9  deployment-6200  b496da34-6398-4371-9b41-1dc672b957b4 442915 2 2022-12-30 04:12:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 606c2763-e6f2-40ee-9c9d-2e27d54b4f5c 0xc004973ba7 0xc004973ba8}] [] [{kube-controller-manager Update apps/v1 2022-12-30 04:12:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"606c2763-e6f2-40ee-9c9d-2e27d54b4f5c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-30 04:12:21 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004973c38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Dec 30 04:12:21.931: INFO: Pod "test-new-deployment-845c8977d9-h557b" is not available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-h557b test-new-deployment-845c8977d9- deployment-6200  5c571d45-951b-42d8-9d8f-cb17a7d00f00 442914 0 2022-12-30 04:12:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 b496da34-6398-4371-9b41-1dc672b957b4 0xc004973ff7 0xc004973ff8}] [] [{kube-controller-manager Update v1 2022-12-30 04:12:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b496da34-6398-4371-9b41-1dc672b957b4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j2vm8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j2vm8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-mgmt03,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 04:12:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Dec 30 04:12:21.931: INFO: Pod "test-new-deployment-845c8977d9-q92f9" is available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-q92f9 test-new-deployment-845c8977d9- deployment-6200  51bd3d64-84ce-410e-bd08-2f539f6be556 442904 0 2022-12-30 04:12:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:8a330eab80c38684d5476edd12830c5d0db010aa2729a42893bb9a8626eabee5 cni.projectcalico.org/podIP:10.233.112.181/32 cni.projectcalico.org/podIPs:10.233.112.181/32] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 b496da34-6398-4371-9b41-1dc672b957b4 0xc000361b40 0xc000361b41}] [] [{kube-controller-manager Update v1 2022-12-30 04:12:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b496da34-6398-4371-9b41-1dc672b957b4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-12-30 04:12:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-30 04:12:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.112.181\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j48nq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j48nq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-mgmt01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 04:12:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 04:12:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 04:12:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 04:12:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.140,PodIP:10.233.112.181,StartTime:2022-12-30 04:12:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-30 04:12:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://44a3b28b881bb51e4cfb70ce4c05be5236aa7b6139a4656574d6b2bde92bcd8b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.112.181,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Dec 30 04:12:21.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-6200" for this suite. 12/30/22 04:12:21.941
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:12:21.951
Dec 30 04:12:21.951: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename pod-network-test 12/30/22 04:12:21.952
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:12:21.972
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:12:21.975
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-2858 12/30/22 04:12:21.979
STEP: creating a selector 12/30/22 04:12:21.979
STEP: Creating the service pods in kubernetes 12/30/22 04:12:21.979
Dec 30 04:12:21.979: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Dec 30 04:12:22.019: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-2858" to be "running and ready"
Dec 30 04:12:22.023: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.762431ms
Dec 30 04:12:22.024: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec 30 04:12:24.029: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.010283973s
Dec 30 04:12:24.029: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 30 04:12:26.029: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.010609077s
Dec 30 04:12:26.029: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 30 04:12:28.030: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.010806619s
Dec 30 04:12:28.030: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 30 04:12:30.029: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.010632881s
Dec 30 04:12:30.029: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 30 04:12:32.028: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.009526919s
Dec 30 04:12:32.028: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 30 04:12:34.030: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.010934067s
Dec 30 04:12:34.030: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Dec 30 04:12:34.030: INFO: Pod "netserver-0" satisfied condition "running and ready"
Dec 30 04:12:34.033: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-2858" to be "running and ready"
Dec 30 04:12:34.037: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.947363ms
Dec 30 04:12:34.037: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Dec 30 04:12:34.037: INFO: Pod "netserver-1" satisfied condition "running and ready"
Dec 30 04:12:34.041: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-2858" to be "running and ready"
Dec 30 04:12:34.045: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.51983ms
Dec 30 04:12:34.045: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Dec 30 04:12:34.045: INFO: Pod "netserver-2" satisfied condition "running and ready"
Dec 30 04:12:34.049: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-2858" to be "running and ready"
Dec 30 04:12:34.053: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=false. Elapsed: 4.111391ms
Dec 30 04:12:34.053: INFO: The phase of Pod netserver-3 is Running (Ready = false)
Dec 30 04:12:36.059: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=false. Elapsed: 2.00954786s
Dec 30 04:12:36.059: INFO: The phase of Pod netserver-3 is Running (Ready = false)
Dec 30 04:12:38.059: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=false. Elapsed: 4.009565954s
Dec 30 04:12:38.059: INFO: The phase of Pod netserver-3 is Running (Ready = false)
Dec 30 04:12:40.059: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=false. Elapsed: 6.010208818s
Dec 30 04:12:40.059: INFO: The phase of Pod netserver-3 is Running (Ready = false)
Dec 30 04:12:42.059: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=false. Elapsed: 8.009630987s
Dec 30 04:12:42.059: INFO: The phase of Pod netserver-3 is Running (Ready = false)
Dec 30 04:12:44.059: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 10.010271432s
Dec 30 04:12:44.059: INFO: The phase of Pod netserver-3 is Running (Ready = true)
Dec 30 04:12:44.059: INFO: Pod "netserver-3" satisfied condition "running and ready"
Dec 30 04:12:44.064: INFO: Waiting up to 5m0s for pod "netserver-4" in namespace "pod-network-test-2858" to be "running and ready"
Dec 30 04:12:44.067: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=true. Elapsed: 3.580503ms
Dec 30 04:12:44.067: INFO: The phase of Pod netserver-4 is Running (Ready = true)
Dec 30 04:12:44.067: INFO: Pod "netserver-4" satisfied condition "running and ready"
STEP: Creating test pods 12/30/22 04:12:44.071
Dec 30 04:12:44.084: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-2858" to be "running"
Dec 30 04:12:44.088: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.936347ms
Dec 30 04:12:46.094: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.01013151s
Dec 30 04:12:46.094: INFO: Pod "test-container-pod" satisfied condition "running"
Dec 30 04:12:46.098: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-2858" to be "running"
Dec 30 04:12:46.102: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.593035ms
Dec 30 04:12:46.102: INFO: Pod "host-test-container-pod" satisfied condition "running"
Dec 30 04:12:46.106: INFO: Setting MaxTries for pod polling to 55 for networking test based on endpoint count 5
Dec 30 04:12:46.106: INFO: Going to poll 10.233.112.183 on port 8083 at least 0 times, with a maximum of 55 tries before failing
Dec 30 04:12:46.110: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.112.183:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2858 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 30 04:12:46.110: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
Dec 30 04:12:46.111: INFO: ExecWithOptions: Clientset creation
Dec 30 04:12:46.111: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-2858/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.112.183%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Dec 30 04:12:46.228: INFO: Found all 1 expected endpoints: [netserver-0]
Dec 30 04:12:46.228: INFO: Going to poll 10.233.125.231 on port 8083 at least 0 times, with a maximum of 55 tries before failing
Dec 30 04:12:46.231: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.125.231:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2858 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 30 04:12:46.231: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
Dec 30 04:12:46.232: INFO: ExecWithOptions: Clientset creation
Dec 30 04:12:46.232: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-2858/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.125.231%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Dec 30 04:12:46.350: INFO: Found all 1 expected endpoints: [netserver-1]
Dec 30 04:12:46.350: INFO: Going to poll 10.233.78.151 on port 8083 at least 0 times, with a maximum of 55 tries before failing
Dec 30 04:12:46.354: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.78.151:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2858 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 30 04:12:46.355: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
Dec 30 04:12:46.355: INFO: ExecWithOptions: Clientset creation
Dec 30 04:12:46.355: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-2858/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.78.151%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Dec 30 04:12:46.450: INFO: Found all 1 expected endpoints: [netserver-2]
Dec 30 04:12:46.450: INFO: Going to poll 10.233.79.103 on port 8083 at least 0 times, with a maximum of 55 tries before failing
Dec 30 04:12:46.453: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.79.103:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2858 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 30 04:12:46.453: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
Dec 30 04:12:46.454: INFO: ExecWithOptions: Clientset creation
Dec 30 04:12:46.454: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-2858/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.79.103%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Dec 30 04:12:46.549: INFO: Found all 1 expected endpoints: [netserver-3]
Dec 30 04:12:46.549: INFO: Going to poll 10.233.109.100 on port 8083 at least 0 times, with a maximum of 55 tries before failing
Dec 30 04:12:46.553: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.109.100:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2858 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 30 04:12:46.553: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
Dec 30 04:12:46.554: INFO: ExecWithOptions: Clientset creation
Dec 30 04:12:46.554: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-2858/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.109.100%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Dec 30 04:12:46.658: INFO: Found all 1 expected endpoints: [netserver-4]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Dec 30 04:12:46.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2858" for this suite. 12/30/22 04:12:46.663
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","completed":209,"skipped":3812,"failed":0}
------------------------------
• [SLOW TEST] [24.721 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:12:21.951
    Dec 30 04:12:21.951: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename pod-network-test 12/30/22 04:12:21.952
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:12:21.972
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:12:21.975
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-2858 12/30/22 04:12:21.979
    STEP: creating a selector 12/30/22 04:12:21.979
    STEP: Creating the service pods in kubernetes 12/30/22 04:12:21.979
    Dec 30 04:12:21.979: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Dec 30 04:12:22.019: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-2858" to be "running and ready"
    Dec 30 04:12:22.023: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.762431ms
    Dec 30 04:12:22.024: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Dec 30 04:12:24.029: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.010283973s
    Dec 30 04:12:24.029: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 30 04:12:26.029: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.010609077s
    Dec 30 04:12:26.029: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 30 04:12:28.030: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.010806619s
    Dec 30 04:12:28.030: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 30 04:12:30.029: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.010632881s
    Dec 30 04:12:30.029: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 30 04:12:32.028: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.009526919s
    Dec 30 04:12:32.028: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 30 04:12:34.030: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.010934067s
    Dec 30 04:12:34.030: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Dec 30 04:12:34.030: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Dec 30 04:12:34.033: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-2858" to be "running and ready"
    Dec 30 04:12:34.037: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.947363ms
    Dec 30 04:12:34.037: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Dec 30 04:12:34.037: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Dec 30 04:12:34.041: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-2858" to be "running and ready"
    Dec 30 04:12:34.045: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.51983ms
    Dec 30 04:12:34.045: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Dec 30 04:12:34.045: INFO: Pod "netserver-2" satisfied condition "running and ready"
    Dec 30 04:12:34.049: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-2858" to be "running and ready"
    Dec 30 04:12:34.053: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=false. Elapsed: 4.111391ms
    Dec 30 04:12:34.053: INFO: The phase of Pod netserver-3 is Running (Ready = false)
    Dec 30 04:12:36.059: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=false. Elapsed: 2.00954786s
    Dec 30 04:12:36.059: INFO: The phase of Pod netserver-3 is Running (Ready = false)
    Dec 30 04:12:38.059: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=false. Elapsed: 4.009565954s
    Dec 30 04:12:38.059: INFO: The phase of Pod netserver-3 is Running (Ready = false)
    Dec 30 04:12:40.059: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=false. Elapsed: 6.010208818s
    Dec 30 04:12:40.059: INFO: The phase of Pod netserver-3 is Running (Ready = false)
    Dec 30 04:12:42.059: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=false. Elapsed: 8.009630987s
    Dec 30 04:12:42.059: INFO: The phase of Pod netserver-3 is Running (Ready = false)
    Dec 30 04:12:44.059: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 10.010271432s
    Dec 30 04:12:44.059: INFO: The phase of Pod netserver-3 is Running (Ready = true)
    Dec 30 04:12:44.059: INFO: Pod "netserver-3" satisfied condition "running and ready"
    Dec 30 04:12:44.064: INFO: Waiting up to 5m0s for pod "netserver-4" in namespace "pod-network-test-2858" to be "running and ready"
    Dec 30 04:12:44.067: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=true. Elapsed: 3.580503ms
    Dec 30 04:12:44.067: INFO: The phase of Pod netserver-4 is Running (Ready = true)
    Dec 30 04:12:44.067: INFO: Pod "netserver-4" satisfied condition "running and ready"
    STEP: Creating test pods 12/30/22 04:12:44.071
    Dec 30 04:12:44.084: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-2858" to be "running"
    Dec 30 04:12:44.088: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.936347ms
    Dec 30 04:12:46.094: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.01013151s
    Dec 30 04:12:46.094: INFO: Pod "test-container-pod" satisfied condition "running"
    Dec 30 04:12:46.098: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-2858" to be "running"
    Dec 30 04:12:46.102: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.593035ms
    Dec 30 04:12:46.102: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Dec 30 04:12:46.106: INFO: Setting MaxTries for pod polling to 55 for networking test based on endpoint count 5
    Dec 30 04:12:46.106: INFO: Going to poll 10.233.112.183 on port 8083 at least 0 times, with a maximum of 55 tries before failing
    Dec 30 04:12:46.110: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.112.183:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2858 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 30 04:12:46.110: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    Dec 30 04:12:46.111: INFO: ExecWithOptions: Clientset creation
    Dec 30 04:12:46.111: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-2858/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.112.183%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Dec 30 04:12:46.228: INFO: Found all 1 expected endpoints: [netserver-0]
    Dec 30 04:12:46.228: INFO: Going to poll 10.233.125.231 on port 8083 at least 0 times, with a maximum of 55 tries before failing
    Dec 30 04:12:46.231: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.125.231:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2858 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 30 04:12:46.231: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    Dec 30 04:12:46.232: INFO: ExecWithOptions: Clientset creation
    Dec 30 04:12:46.232: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-2858/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.125.231%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Dec 30 04:12:46.350: INFO: Found all 1 expected endpoints: [netserver-1]
    Dec 30 04:12:46.350: INFO: Going to poll 10.233.78.151 on port 8083 at least 0 times, with a maximum of 55 tries before failing
    Dec 30 04:12:46.354: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.78.151:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2858 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 30 04:12:46.355: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    Dec 30 04:12:46.355: INFO: ExecWithOptions: Clientset creation
    Dec 30 04:12:46.355: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-2858/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.78.151%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Dec 30 04:12:46.450: INFO: Found all 1 expected endpoints: [netserver-2]
    Dec 30 04:12:46.450: INFO: Going to poll 10.233.79.103 on port 8083 at least 0 times, with a maximum of 55 tries before failing
    Dec 30 04:12:46.453: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.79.103:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2858 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 30 04:12:46.453: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    Dec 30 04:12:46.454: INFO: ExecWithOptions: Clientset creation
    Dec 30 04:12:46.454: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-2858/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.79.103%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Dec 30 04:12:46.549: INFO: Found all 1 expected endpoints: [netserver-3]
    Dec 30 04:12:46.549: INFO: Going to poll 10.233.109.100 on port 8083 at least 0 times, with a maximum of 55 tries before failing
    Dec 30 04:12:46.553: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.109.100:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2858 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 30 04:12:46.553: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    Dec 30 04:12:46.554: INFO: ExecWithOptions: Clientset creation
    Dec 30 04:12:46.554: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-2858/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.109.100%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Dec 30 04:12:46.658: INFO: Found all 1 expected endpoints: [netserver-4]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Dec 30 04:12:46.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-2858" for this suite. 12/30/22 04:12:46.663
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:12:46.675
Dec 30 04:12:46.675: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename dns 12/30/22 04:12:46.676
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:12:46.694
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:12:46.697
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-3055.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-3055.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 12/30/22 04:12:46.701
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-3055.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-3055.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 12/30/22 04:12:46.701
STEP: creating a pod to probe /etc/hosts 12/30/22 04:12:46.701
STEP: submitting the pod to kubernetes 12/30/22 04:12:46.701
Dec 30 04:12:46.712: INFO: Waiting up to 15m0s for pod "dns-test-8d95ecdb-6ec7-45fb-8db2-30cf59f457fc" in namespace "dns-3055" to be "running"
Dec 30 04:12:46.716: INFO: Pod "dns-test-8d95ecdb-6ec7-45fb-8db2-30cf59f457fc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.585841ms
Dec 30 04:12:48.722: INFO: Pod "dns-test-8d95ecdb-6ec7-45fb-8db2-30cf59f457fc": Phase="Running", Reason="", readiness=true. Elapsed: 2.009863022s
Dec 30 04:12:48.722: INFO: Pod "dns-test-8d95ecdb-6ec7-45fb-8db2-30cf59f457fc" satisfied condition "running"
STEP: retrieving the pod 12/30/22 04:12:48.722
STEP: looking for the results for each expected name from probers 12/30/22 04:12:48.726
Dec 30 04:12:48.746: INFO: DNS probes using dns-3055/dns-test-8d95ecdb-6ec7-45fb-8db2-30cf59f457fc succeeded

STEP: deleting the pod 12/30/22 04:12:48.746
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Dec 30 04:12:48.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3055" for this suite. 12/30/22 04:12:48.766
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]","completed":210,"skipped":3862,"failed":0}
------------------------------
• [2.098 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:12:46.675
    Dec 30 04:12:46.675: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename dns 12/30/22 04:12:46.676
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:12:46.694
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:12:46.697
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-3055.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-3055.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     12/30/22 04:12:46.701
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-3055.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-3055.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     12/30/22 04:12:46.701
    STEP: creating a pod to probe /etc/hosts 12/30/22 04:12:46.701
    STEP: submitting the pod to kubernetes 12/30/22 04:12:46.701
    Dec 30 04:12:46.712: INFO: Waiting up to 15m0s for pod "dns-test-8d95ecdb-6ec7-45fb-8db2-30cf59f457fc" in namespace "dns-3055" to be "running"
    Dec 30 04:12:46.716: INFO: Pod "dns-test-8d95ecdb-6ec7-45fb-8db2-30cf59f457fc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.585841ms
    Dec 30 04:12:48.722: INFO: Pod "dns-test-8d95ecdb-6ec7-45fb-8db2-30cf59f457fc": Phase="Running", Reason="", readiness=true. Elapsed: 2.009863022s
    Dec 30 04:12:48.722: INFO: Pod "dns-test-8d95ecdb-6ec7-45fb-8db2-30cf59f457fc" satisfied condition "running"
    STEP: retrieving the pod 12/30/22 04:12:48.722
    STEP: looking for the results for each expected name from probers 12/30/22 04:12:48.726
    Dec 30 04:12:48.746: INFO: DNS probes using dns-3055/dns-test-8d95ecdb-6ec7-45fb-8db2-30cf59f457fc succeeded

    STEP: deleting the pod 12/30/22 04:12:48.746
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Dec 30 04:12:48.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-3055" for this suite. 12/30/22 04:12:48.766
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:12:48.775
Dec 30 04:12:48.775: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename webhook 12/30/22 04:12:48.776
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:12:48.794
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:12:48.797
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 12/30/22 04:12:48.814
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/30/22 04:12:49.401
STEP: Deploying the webhook pod 12/30/22 04:12:49.409
STEP: Wait for the deployment to be ready 12/30/22 04:12:49.422
Dec 30 04:12:49.430: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 12/30/22 04:12:51.442
STEP: Verifying the service has paired with the endpoint 12/30/22 04:12:51.452
Dec 30 04:12:52.453: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116
STEP: fetching the /apis discovery document 12/30/22 04:12:52.457
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 12/30/22 04:12:52.459
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 12/30/22 04:12:52.459
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 12/30/22 04:12:52.459
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 12/30/22 04:12:52.461
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 12/30/22 04:12:52.461
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 12/30/22 04:12:52.463
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Dec 30 04:12:52.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-806" for this suite. 12/30/22 04:12:52.469
STEP: Destroying namespace "webhook-806-markers" for this suite. 12/30/22 04:12:52.475
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","completed":211,"skipped":3884,"failed":0}
------------------------------
• [3.747 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:12:48.775
    Dec 30 04:12:48.775: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename webhook 12/30/22 04:12:48.776
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:12:48.794
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:12:48.797
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 12/30/22 04:12:48.814
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/30/22 04:12:49.401
    STEP: Deploying the webhook pod 12/30/22 04:12:49.409
    STEP: Wait for the deployment to be ready 12/30/22 04:12:49.422
    Dec 30 04:12:49.430: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 12/30/22 04:12:51.442
    STEP: Verifying the service has paired with the endpoint 12/30/22 04:12:51.452
    Dec 30 04:12:52.453: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:116
    STEP: fetching the /apis discovery document 12/30/22 04:12:52.457
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 12/30/22 04:12:52.459
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 12/30/22 04:12:52.459
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 12/30/22 04:12:52.459
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 12/30/22 04:12:52.461
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 12/30/22 04:12:52.461
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 12/30/22 04:12:52.463
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Dec 30 04:12:52.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-806" for this suite. 12/30/22 04:12:52.469
    STEP: Destroying namespace "webhook-806-markers" for this suite. 12/30/22 04:12:52.475
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:12:52.524
Dec 30 04:12:52.524: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename services 12/30/22 04:12:52.525
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:12:52.541
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:12:52.544
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237
STEP: creating service in namespace services-2756 12/30/22 04:12:52.548
STEP: creating service affinity-nodeport-transition in namespace services-2756 12/30/22 04:12:52.548
STEP: creating replication controller affinity-nodeport-transition in namespace services-2756 12/30/22 04:12:52.562
I1230 04:12:52.567979      25 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-2756, replica count: 3
I1230 04:12:55.619877      25 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 30 04:12:55.633: INFO: Creating new exec pod
Dec 30 04:12:55.643: INFO: Waiting up to 5m0s for pod "execpod-affinityxh4g4" in namespace "services-2756" to be "running"
Dec 30 04:12:55.646: INFO: Pod "execpod-affinityxh4g4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.316397ms
Dec 30 04:12:57.651: INFO: Pod "execpod-affinityxh4g4": Phase="Running", Reason="", readiness=true. Elapsed: 2.007565204s
Dec 30 04:12:57.651: INFO: Pod "execpod-affinityxh4g4" satisfied condition "running"
Dec 30 04:12:58.657: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-2756 exec execpod-affinityxh4g4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
Dec 30 04:12:58.840: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Dec 30 04:12:58.841: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 30 04:12:58.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-2756 exec execpod-affinityxh4g4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.19.217 80'
Dec 30 04:12:59.037: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.19.217 80\nConnection to 10.233.19.217 80 port [tcp/http] succeeded!\n"
Dec 30 04:12:59.037: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 30 04:12:59.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-2756 exec execpod-affinityxh4g4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.78.26.141 30683'
Dec 30 04:12:59.229: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.78.26.141 30683\nConnection to 10.78.26.141 30683 port [tcp/*] succeeded!\n"
Dec 30 04:12:59.229: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 30 04:12:59.229: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-2756 exec execpod-affinityxh4g4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.78.26.195 30683'
Dec 30 04:12:59.409: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.78.26.195 30683\nConnection to 10.78.26.195 30683 port [tcp/*] succeeded!\n"
Dec 30 04:12:59.409: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 30 04:12:59.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-2756 exec execpod-affinityxh4g4 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.78.26.140:30683/ ; done'
Dec 30 04:12:59.735: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30683/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30683/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30683/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30683/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30683/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30683/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30683/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30683/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30683/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30683/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30683/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30683/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30683/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30683/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30683/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30683/\n"
Dec 30 04:12:59.735: INFO: stdout: "\naffinity-nodeport-transition-h9ssx\naffinity-nodeport-transition-s8hm5\naffinity-nodeport-transition-g956s\naffinity-nodeport-transition-h9ssx\naffinity-nodeport-transition-s8hm5\naffinity-nodeport-transition-g956s\naffinity-nodeport-transition-h9ssx\naffinity-nodeport-transition-s8hm5\naffinity-nodeport-transition-g956s\naffinity-nodeport-transition-h9ssx\naffinity-nodeport-transition-s8hm5\naffinity-nodeport-transition-g956s\naffinity-nodeport-transition-h9ssx\naffinity-nodeport-transition-s8hm5\naffinity-nodeport-transition-g956s\naffinity-nodeport-transition-h9ssx"
Dec 30 04:12:59.735: INFO: Received response from host: affinity-nodeport-transition-h9ssx
Dec 30 04:12:59.735: INFO: Received response from host: affinity-nodeport-transition-s8hm5
Dec 30 04:12:59.735: INFO: Received response from host: affinity-nodeport-transition-g956s
Dec 30 04:12:59.735: INFO: Received response from host: affinity-nodeport-transition-h9ssx
Dec 30 04:12:59.735: INFO: Received response from host: affinity-nodeport-transition-s8hm5
Dec 30 04:12:59.735: INFO: Received response from host: affinity-nodeport-transition-g956s
Dec 30 04:12:59.735: INFO: Received response from host: affinity-nodeport-transition-h9ssx
Dec 30 04:12:59.735: INFO: Received response from host: affinity-nodeport-transition-s8hm5
Dec 30 04:12:59.735: INFO: Received response from host: affinity-nodeport-transition-g956s
Dec 30 04:12:59.735: INFO: Received response from host: affinity-nodeport-transition-h9ssx
Dec 30 04:12:59.735: INFO: Received response from host: affinity-nodeport-transition-s8hm5
Dec 30 04:12:59.735: INFO: Received response from host: affinity-nodeport-transition-g956s
Dec 30 04:12:59.735: INFO: Received response from host: affinity-nodeport-transition-h9ssx
Dec 30 04:12:59.735: INFO: Received response from host: affinity-nodeport-transition-s8hm5
Dec 30 04:12:59.735: INFO: Received response from host: affinity-nodeport-transition-g956s
Dec 30 04:12:59.735: INFO: Received response from host: affinity-nodeport-transition-h9ssx
Dec 30 04:12:59.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-2756 exec execpod-affinityxh4g4 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.78.26.140:30683/ ; done'
Dec 30 04:13:00.046: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30683/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30683/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30683/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30683/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30683/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30683/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30683/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30683/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30683/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30683/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30683/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30683/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30683/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30683/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30683/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30683/\n"
Dec 30 04:13:00.046: INFO: stdout: "\naffinity-nodeport-transition-s8hm5\naffinity-nodeport-transition-s8hm5\naffinity-nodeport-transition-s8hm5\naffinity-nodeport-transition-s8hm5\naffinity-nodeport-transition-s8hm5\naffinity-nodeport-transition-s8hm5\naffinity-nodeport-transition-s8hm5\naffinity-nodeport-transition-s8hm5\naffinity-nodeport-transition-s8hm5\naffinity-nodeport-transition-s8hm5\naffinity-nodeport-transition-s8hm5\naffinity-nodeport-transition-s8hm5\naffinity-nodeport-transition-s8hm5\naffinity-nodeport-transition-s8hm5\naffinity-nodeport-transition-s8hm5\naffinity-nodeport-transition-s8hm5"
Dec 30 04:13:00.046: INFO: Received response from host: affinity-nodeport-transition-s8hm5
Dec 30 04:13:00.046: INFO: Received response from host: affinity-nodeport-transition-s8hm5
Dec 30 04:13:00.046: INFO: Received response from host: affinity-nodeport-transition-s8hm5
Dec 30 04:13:00.046: INFO: Received response from host: affinity-nodeport-transition-s8hm5
Dec 30 04:13:00.046: INFO: Received response from host: affinity-nodeport-transition-s8hm5
Dec 30 04:13:00.046: INFO: Received response from host: affinity-nodeport-transition-s8hm5
Dec 30 04:13:00.046: INFO: Received response from host: affinity-nodeport-transition-s8hm5
Dec 30 04:13:00.046: INFO: Received response from host: affinity-nodeport-transition-s8hm5
Dec 30 04:13:00.046: INFO: Received response from host: affinity-nodeport-transition-s8hm5
Dec 30 04:13:00.046: INFO: Received response from host: affinity-nodeport-transition-s8hm5
Dec 30 04:13:00.046: INFO: Received response from host: affinity-nodeport-transition-s8hm5
Dec 30 04:13:00.046: INFO: Received response from host: affinity-nodeport-transition-s8hm5
Dec 30 04:13:00.046: INFO: Received response from host: affinity-nodeport-transition-s8hm5
Dec 30 04:13:00.046: INFO: Received response from host: affinity-nodeport-transition-s8hm5
Dec 30 04:13:00.046: INFO: Received response from host: affinity-nodeport-transition-s8hm5
Dec 30 04:13:00.046: INFO: Received response from host: affinity-nodeport-transition-s8hm5
Dec 30 04:13:00.046: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-2756, will wait for the garbage collector to delete the pods 12/30/22 04:13:00.06
Dec 30 04:13:00.121: INFO: Deleting ReplicationController affinity-nodeport-transition took: 6.496761ms
Dec 30 04:13:00.222: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.549998ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Dec 30 04:13:02.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2756" for this suite. 12/30/22 04:13:02.648
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","completed":212,"skipped":3912,"failed":0}
------------------------------
• [SLOW TEST] [10.132 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:12:52.524
    Dec 30 04:12:52.524: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename services 12/30/22 04:12:52.525
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:12:52.541
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:12:52.544
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2237
    STEP: creating service in namespace services-2756 12/30/22 04:12:52.548
    STEP: creating service affinity-nodeport-transition in namespace services-2756 12/30/22 04:12:52.548
    STEP: creating replication controller affinity-nodeport-transition in namespace services-2756 12/30/22 04:12:52.562
    I1230 04:12:52.567979      25 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-2756, replica count: 3
    I1230 04:12:55.619877      25 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Dec 30 04:12:55.633: INFO: Creating new exec pod
    Dec 30 04:12:55.643: INFO: Waiting up to 5m0s for pod "execpod-affinityxh4g4" in namespace "services-2756" to be "running"
    Dec 30 04:12:55.646: INFO: Pod "execpod-affinityxh4g4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.316397ms
    Dec 30 04:12:57.651: INFO: Pod "execpod-affinityxh4g4": Phase="Running", Reason="", readiness=true. Elapsed: 2.007565204s
    Dec 30 04:12:57.651: INFO: Pod "execpod-affinityxh4g4" satisfied condition "running"
    Dec 30 04:12:58.657: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-2756 exec execpod-affinityxh4g4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
    Dec 30 04:12:58.840: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    Dec 30 04:12:58.841: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Dec 30 04:12:58.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-2756 exec execpod-affinityxh4g4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.19.217 80'
    Dec 30 04:12:59.037: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.19.217 80\nConnection to 10.233.19.217 80 port [tcp/http] succeeded!\n"
    Dec 30 04:12:59.037: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Dec 30 04:12:59.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-2756 exec execpod-affinityxh4g4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.78.26.141 30683'
    Dec 30 04:12:59.229: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.78.26.141 30683\nConnection to 10.78.26.141 30683 port [tcp/*] succeeded!\n"
    Dec 30 04:12:59.229: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Dec 30 04:12:59.229: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-2756 exec execpod-affinityxh4g4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.78.26.195 30683'
    Dec 30 04:12:59.409: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.78.26.195 30683\nConnection to 10.78.26.195 30683 port [tcp/*] succeeded!\n"
    Dec 30 04:12:59.409: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Dec 30 04:12:59.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-2756 exec execpod-affinityxh4g4 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.78.26.140:30683/ ; done'
    Dec 30 04:12:59.735: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30683/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30683/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30683/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30683/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30683/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30683/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30683/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30683/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30683/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30683/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30683/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30683/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30683/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30683/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30683/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30683/\n"
    Dec 30 04:12:59.735: INFO: stdout: "\naffinity-nodeport-transition-h9ssx\naffinity-nodeport-transition-s8hm5\naffinity-nodeport-transition-g956s\naffinity-nodeport-transition-h9ssx\naffinity-nodeport-transition-s8hm5\naffinity-nodeport-transition-g956s\naffinity-nodeport-transition-h9ssx\naffinity-nodeport-transition-s8hm5\naffinity-nodeport-transition-g956s\naffinity-nodeport-transition-h9ssx\naffinity-nodeport-transition-s8hm5\naffinity-nodeport-transition-g956s\naffinity-nodeport-transition-h9ssx\naffinity-nodeport-transition-s8hm5\naffinity-nodeport-transition-g956s\naffinity-nodeport-transition-h9ssx"
    Dec 30 04:12:59.735: INFO: Received response from host: affinity-nodeport-transition-h9ssx
    Dec 30 04:12:59.735: INFO: Received response from host: affinity-nodeport-transition-s8hm5
    Dec 30 04:12:59.735: INFO: Received response from host: affinity-nodeport-transition-g956s
    Dec 30 04:12:59.735: INFO: Received response from host: affinity-nodeport-transition-h9ssx
    Dec 30 04:12:59.735: INFO: Received response from host: affinity-nodeport-transition-s8hm5
    Dec 30 04:12:59.735: INFO: Received response from host: affinity-nodeport-transition-g956s
    Dec 30 04:12:59.735: INFO: Received response from host: affinity-nodeport-transition-h9ssx
    Dec 30 04:12:59.735: INFO: Received response from host: affinity-nodeport-transition-s8hm5
    Dec 30 04:12:59.735: INFO: Received response from host: affinity-nodeport-transition-g956s
    Dec 30 04:12:59.735: INFO: Received response from host: affinity-nodeport-transition-h9ssx
    Dec 30 04:12:59.735: INFO: Received response from host: affinity-nodeport-transition-s8hm5
    Dec 30 04:12:59.735: INFO: Received response from host: affinity-nodeport-transition-g956s
    Dec 30 04:12:59.735: INFO: Received response from host: affinity-nodeport-transition-h9ssx
    Dec 30 04:12:59.735: INFO: Received response from host: affinity-nodeport-transition-s8hm5
    Dec 30 04:12:59.735: INFO: Received response from host: affinity-nodeport-transition-g956s
    Dec 30 04:12:59.735: INFO: Received response from host: affinity-nodeport-transition-h9ssx
    Dec 30 04:12:59.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-2756 exec execpod-affinityxh4g4 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.78.26.140:30683/ ; done'
    Dec 30 04:13:00.046: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30683/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30683/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30683/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30683/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30683/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30683/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30683/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30683/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30683/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30683/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30683/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30683/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30683/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30683/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30683/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30683/\n"
    Dec 30 04:13:00.046: INFO: stdout: "\naffinity-nodeport-transition-s8hm5\naffinity-nodeport-transition-s8hm5\naffinity-nodeport-transition-s8hm5\naffinity-nodeport-transition-s8hm5\naffinity-nodeport-transition-s8hm5\naffinity-nodeport-transition-s8hm5\naffinity-nodeport-transition-s8hm5\naffinity-nodeport-transition-s8hm5\naffinity-nodeport-transition-s8hm5\naffinity-nodeport-transition-s8hm5\naffinity-nodeport-transition-s8hm5\naffinity-nodeport-transition-s8hm5\naffinity-nodeport-transition-s8hm5\naffinity-nodeport-transition-s8hm5\naffinity-nodeport-transition-s8hm5\naffinity-nodeport-transition-s8hm5"
    Dec 30 04:13:00.046: INFO: Received response from host: affinity-nodeport-transition-s8hm5
    Dec 30 04:13:00.046: INFO: Received response from host: affinity-nodeport-transition-s8hm5
    Dec 30 04:13:00.046: INFO: Received response from host: affinity-nodeport-transition-s8hm5
    Dec 30 04:13:00.046: INFO: Received response from host: affinity-nodeport-transition-s8hm5
    Dec 30 04:13:00.046: INFO: Received response from host: affinity-nodeport-transition-s8hm5
    Dec 30 04:13:00.046: INFO: Received response from host: affinity-nodeport-transition-s8hm5
    Dec 30 04:13:00.046: INFO: Received response from host: affinity-nodeport-transition-s8hm5
    Dec 30 04:13:00.046: INFO: Received response from host: affinity-nodeport-transition-s8hm5
    Dec 30 04:13:00.046: INFO: Received response from host: affinity-nodeport-transition-s8hm5
    Dec 30 04:13:00.046: INFO: Received response from host: affinity-nodeport-transition-s8hm5
    Dec 30 04:13:00.046: INFO: Received response from host: affinity-nodeport-transition-s8hm5
    Dec 30 04:13:00.046: INFO: Received response from host: affinity-nodeport-transition-s8hm5
    Dec 30 04:13:00.046: INFO: Received response from host: affinity-nodeport-transition-s8hm5
    Dec 30 04:13:00.046: INFO: Received response from host: affinity-nodeport-transition-s8hm5
    Dec 30 04:13:00.046: INFO: Received response from host: affinity-nodeport-transition-s8hm5
    Dec 30 04:13:00.046: INFO: Received response from host: affinity-nodeport-transition-s8hm5
    Dec 30 04:13:00.046: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-2756, will wait for the garbage collector to delete the pods 12/30/22 04:13:00.06
    Dec 30 04:13:00.121: INFO: Deleting ReplicationController affinity-nodeport-transition took: 6.496761ms
    Dec 30 04:13:00.222: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.549998ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Dec 30 04:13:02.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-2756" for this suite. 12/30/22 04:13:02.648
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:13:02.657
Dec 30 04:13:02.657: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename webhook 12/30/22 04:13:02.659
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:13:02.676
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:13:02.679
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 12/30/22 04:13:02.696
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/30/22 04:13:03.119
STEP: Deploying the webhook pod 12/30/22 04:13:03.124
STEP: Wait for the deployment to be ready 12/30/22 04:13:03.135
Dec 30 04:13:03.143: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 12/30/22 04:13:05.156
STEP: Verifying the service has paired with the endpoint 12/30/22 04:13:05.168
Dec 30 04:13:06.169: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322
Dec 30 04:13:06.173: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7275-crds.webhook.example.com via the AdmissionRegistration API 12/30/22 04:13:11.685
STEP: Creating a custom resource while v1 is storage version 12/30/22 04:13:11.705
STEP: Patching Custom Resource Definition to set v2 as storage 12/30/22 04:13:13.792
STEP: Patching the custom resource while v2 is storage version 12/30/22 04:13:13.816
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Dec 30 04:13:14.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9392" for this suite. 12/30/22 04:13:14.419
STEP: Destroying namespace "webhook-9392-markers" for this suite. 12/30/22 04:13:14.426
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","completed":213,"skipped":3916,"failed":0}
------------------------------
• [SLOW TEST] [11.808 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:13:02.657
    Dec 30 04:13:02.657: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename webhook 12/30/22 04:13:02.659
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:13:02.676
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:13:02.679
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 12/30/22 04:13:02.696
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/30/22 04:13:03.119
    STEP: Deploying the webhook pod 12/30/22 04:13:03.124
    STEP: Wait for the deployment to be ready 12/30/22 04:13:03.135
    Dec 30 04:13:03.143: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 12/30/22 04:13:05.156
    STEP: Verifying the service has paired with the endpoint 12/30/22 04:13:05.168
    Dec 30 04:13:06.169: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:322
    Dec 30 04:13:06.173: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7275-crds.webhook.example.com via the AdmissionRegistration API 12/30/22 04:13:11.685
    STEP: Creating a custom resource while v1 is storage version 12/30/22 04:13:11.705
    STEP: Patching Custom Resource Definition to set v2 as storage 12/30/22 04:13:13.792
    STEP: Patching the custom resource while v2 is storage version 12/30/22 04:13:13.816
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Dec 30 04:13:14.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-9392" for this suite. 12/30/22 04:13:14.419
    STEP: Destroying namespace "webhook-9392-markers" for this suite. 12/30/22 04:13:14.426
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:13:14.47
Dec 30 04:13:14.470: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename secrets 12/30/22 04:13:14.471
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:13:14.487
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:13:14.49
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153
STEP: creating a secret 12/30/22 04:13:14.493
STEP: listing secrets in all namespaces to ensure that there are more than zero 12/30/22 04:13:14.498
STEP: patching the secret 12/30/22 04:13:14.501
STEP: deleting the secret using a LabelSelector 12/30/22 04:13:14.512
STEP: listing secrets in all namespaces, searching for label name and value in patch 12/30/22 04:13:14.52
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Dec 30 04:13:14.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8481" for this suite. 12/30/22 04:13:14.529
{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","completed":214,"skipped":3985,"failed":0}
------------------------------
• [0.065 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:13:14.47
    Dec 30 04:13:14.470: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename secrets 12/30/22 04:13:14.471
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:13:14.487
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:13:14.49
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:153
    STEP: creating a secret 12/30/22 04:13:14.493
    STEP: listing secrets in all namespaces to ensure that there are more than zero 12/30/22 04:13:14.498
    STEP: patching the secret 12/30/22 04:13:14.501
    STEP: deleting the secret using a LabelSelector 12/30/22 04:13:14.512
    STEP: listing secrets in all namespaces, searching for label name and value in patch 12/30/22 04:13:14.52
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Dec 30 04:13:14.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-8481" for this suite. 12/30/22 04:13:14.529
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:13:14.541
Dec 30 04:13:14.541: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename subpath 12/30/22 04:13:14.542
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:13:14.558
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:13:14.561
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 12/30/22 04:13:14.564
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-wjds 12/30/22 04:13:14.574
STEP: Creating a pod to test atomic-volume-subpath 12/30/22 04:13:14.574
Dec 30 04:13:14.583: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-wjds" in namespace "subpath-1730" to be "Succeeded or Failed"
Dec 30 04:13:14.587: INFO: Pod "pod-subpath-test-configmap-wjds": Phase="Pending", Reason="", readiness=false. Elapsed: 3.582534ms
Dec 30 04:13:16.591: INFO: Pod "pod-subpath-test-configmap-wjds": Phase="Running", Reason="", readiness=true. Elapsed: 2.007491669s
Dec 30 04:13:18.593: INFO: Pod "pod-subpath-test-configmap-wjds": Phase="Running", Reason="", readiness=true. Elapsed: 4.009791021s
Dec 30 04:13:20.594: INFO: Pod "pod-subpath-test-configmap-wjds": Phase="Running", Reason="", readiness=true. Elapsed: 6.010519194s
Dec 30 04:13:22.592: INFO: Pod "pod-subpath-test-configmap-wjds": Phase="Running", Reason="", readiness=true. Elapsed: 8.009171484s
Dec 30 04:13:24.592: INFO: Pod "pod-subpath-test-configmap-wjds": Phase="Running", Reason="", readiness=true. Elapsed: 10.00858074s
Dec 30 04:13:26.592: INFO: Pod "pod-subpath-test-configmap-wjds": Phase="Running", Reason="", readiness=true. Elapsed: 12.008783618s
Dec 30 04:13:28.594: INFO: Pod "pod-subpath-test-configmap-wjds": Phase="Running", Reason="", readiness=true. Elapsed: 14.01136557s
Dec 30 04:13:30.593: INFO: Pod "pod-subpath-test-configmap-wjds": Phase="Running", Reason="", readiness=true. Elapsed: 16.009889104s
Dec 30 04:13:32.594: INFO: Pod "pod-subpath-test-configmap-wjds": Phase="Running", Reason="", readiness=true. Elapsed: 18.010484423s
Dec 30 04:13:34.594: INFO: Pod "pod-subpath-test-configmap-wjds": Phase="Running", Reason="", readiness=true. Elapsed: 20.010565786s
Dec 30 04:13:36.593: INFO: Pod "pod-subpath-test-configmap-wjds": Phase="Running", Reason="", readiness=false. Elapsed: 22.009420325s
Dec 30 04:13:38.593: INFO: Pod "pod-subpath-test-configmap-wjds": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.010365486s
STEP: Saw pod success 12/30/22 04:13:38.593
Dec 30 04:13:38.594: INFO: Pod "pod-subpath-test-configmap-wjds" satisfied condition "Succeeded or Failed"
Dec 30 04:13:38.599: INFO: Trying to get logs from node k8s-mgmt01 pod pod-subpath-test-configmap-wjds container test-container-subpath-configmap-wjds: <nil>
STEP: delete the pod 12/30/22 04:13:38.62
Dec 30 04:13:38.633: INFO: Waiting for pod pod-subpath-test-configmap-wjds to disappear
Dec 30 04:13:38.637: INFO: Pod pod-subpath-test-configmap-wjds no longer exists
STEP: Deleting pod pod-subpath-test-configmap-wjds 12/30/22 04:13:38.637
Dec 30 04:13:38.638: INFO: Deleting pod "pod-subpath-test-configmap-wjds" in namespace "subpath-1730"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Dec 30 04:13:38.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1730" for this suite. 12/30/22 04:13:38.647
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]","completed":215,"skipped":4076,"failed":0}
------------------------------
• [SLOW TEST] [24.114 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:13:14.541
    Dec 30 04:13:14.541: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename subpath 12/30/22 04:13:14.542
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:13:14.558
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:13:14.561
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 12/30/22 04:13:14.564
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-wjds 12/30/22 04:13:14.574
    STEP: Creating a pod to test atomic-volume-subpath 12/30/22 04:13:14.574
    Dec 30 04:13:14.583: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-wjds" in namespace "subpath-1730" to be "Succeeded or Failed"
    Dec 30 04:13:14.587: INFO: Pod "pod-subpath-test-configmap-wjds": Phase="Pending", Reason="", readiness=false. Elapsed: 3.582534ms
    Dec 30 04:13:16.591: INFO: Pod "pod-subpath-test-configmap-wjds": Phase="Running", Reason="", readiness=true. Elapsed: 2.007491669s
    Dec 30 04:13:18.593: INFO: Pod "pod-subpath-test-configmap-wjds": Phase="Running", Reason="", readiness=true. Elapsed: 4.009791021s
    Dec 30 04:13:20.594: INFO: Pod "pod-subpath-test-configmap-wjds": Phase="Running", Reason="", readiness=true. Elapsed: 6.010519194s
    Dec 30 04:13:22.592: INFO: Pod "pod-subpath-test-configmap-wjds": Phase="Running", Reason="", readiness=true. Elapsed: 8.009171484s
    Dec 30 04:13:24.592: INFO: Pod "pod-subpath-test-configmap-wjds": Phase="Running", Reason="", readiness=true. Elapsed: 10.00858074s
    Dec 30 04:13:26.592: INFO: Pod "pod-subpath-test-configmap-wjds": Phase="Running", Reason="", readiness=true. Elapsed: 12.008783618s
    Dec 30 04:13:28.594: INFO: Pod "pod-subpath-test-configmap-wjds": Phase="Running", Reason="", readiness=true. Elapsed: 14.01136557s
    Dec 30 04:13:30.593: INFO: Pod "pod-subpath-test-configmap-wjds": Phase="Running", Reason="", readiness=true. Elapsed: 16.009889104s
    Dec 30 04:13:32.594: INFO: Pod "pod-subpath-test-configmap-wjds": Phase="Running", Reason="", readiness=true. Elapsed: 18.010484423s
    Dec 30 04:13:34.594: INFO: Pod "pod-subpath-test-configmap-wjds": Phase="Running", Reason="", readiness=true. Elapsed: 20.010565786s
    Dec 30 04:13:36.593: INFO: Pod "pod-subpath-test-configmap-wjds": Phase="Running", Reason="", readiness=false. Elapsed: 22.009420325s
    Dec 30 04:13:38.593: INFO: Pod "pod-subpath-test-configmap-wjds": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.010365486s
    STEP: Saw pod success 12/30/22 04:13:38.593
    Dec 30 04:13:38.594: INFO: Pod "pod-subpath-test-configmap-wjds" satisfied condition "Succeeded or Failed"
    Dec 30 04:13:38.599: INFO: Trying to get logs from node k8s-mgmt01 pod pod-subpath-test-configmap-wjds container test-container-subpath-configmap-wjds: <nil>
    STEP: delete the pod 12/30/22 04:13:38.62
    Dec 30 04:13:38.633: INFO: Waiting for pod pod-subpath-test-configmap-wjds to disappear
    Dec 30 04:13:38.637: INFO: Pod pod-subpath-test-configmap-wjds no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-wjds 12/30/22 04:13:38.637
    Dec 30 04:13:38.638: INFO: Deleting pod "pod-subpath-test-configmap-wjds" in namespace "subpath-1730"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Dec 30 04:13:38.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-1730" for this suite. 12/30/22 04:13:38.647
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1711
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:13:38.663
Dec 30 04:13:38.663: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename kubectl 12/30/22 04:13:38.664
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:13:38.682
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:13:38.685
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1698
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1711
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 12/30/22 04:13:38.688
Dec 30 04:13:38.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-3121 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2'
Dec 30 04:13:38.801: INFO: stderr: ""
Dec 30 04:13:38.801: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 12/30/22 04:13:38.801
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1702
Dec 30 04:13:38.806: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-3121 delete pods e2e-test-httpd-pod'
Dec 30 04:13:40.808: INFO: stderr: ""
Dec 30 04:13:40.808: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Dec 30 04:13:40.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3121" for this suite. 12/30/22 04:13:40.814
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","completed":216,"skipped":4125,"failed":0}
------------------------------
• [2.159 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1695
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1711

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:13:38.663
    Dec 30 04:13:38.663: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename kubectl 12/30/22 04:13:38.664
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:13:38.682
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:13:38.685
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1698
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1711
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 12/30/22 04:13:38.688
    Dec 30 04:13:38.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-3121 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2'
    Dec 30 04:13:38.801: INFO: stderr: ""
    Dec 30 04:13:38.801: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 12/30/22 04:13:38.801
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1702
    Dec 30 04:13:38.806: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-3121 delete pods e2e-test-httpd-pod'
    Dec 30 04:13:40.808: INFO: stderr: ""
    Dec 30 04:13:40.808: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Dec 30 04:13:40.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-3121" for this suite. 12/30/22 04:13:40.814
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:975
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:13:40.822
Dec 30 04:13:40.822: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename statefulset 12/30/22 04:13:40.824
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:13:40.839
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:13:40.843
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-9280 12/30/22 04:13:40.853
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:975
STEP: Creating statefulset ss in namespace statefulset-9280 12/30/22 04:13:40.862
Dec 30 04:13:40.872: INFO: Found 0 stateful pods, waiting for 1
Dec 30 04:13:50.878: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 12/30/22 04:13:50.887
STEP: Getting /status 12/30/22 04:13:50.901
Dec 30 04:13:50.907: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 12/30/22 04:13:50.907
Dec 30 04:13:50.918: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 12/30/22 04:13:50.918
Dec 30 04:13:50.920: INFO: Observed &StatefulSet event: ADDED
Dec 30 04:13:50.920: INFO: Found Statefulset ss in namespace statefulset-9280 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Dec 30 04:13:50.921: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 12/30/22 04:13:50.921
Dec 30 04:13:50.921: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Dec 30 04:13:50.928: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 12/30/22 04:13:50.928
Dec 30 04:13:50.931: INFO: Observed &StatefulSet event: ADDED
Dec 30 04:13:50.931: INFO: Observed Statefulset ss in namespace statefulset-9280 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Dec 30 04:13:50.931: INFO: Observed &StatefulSet event: MODIFIED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Dec 30 04:13:50.931: INFO: Deleting all statefulset in ns statefulset-9280
Dec 30 04:13:50.935: INFO: Scaling statefulset ss to 0
Dec 30 04:14:00.961: INFO: Waiting for statefulset status.replicas updated to 0
Dec 30 04:14:00.965: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Dec 30 04:14:00.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9280" for this suite. 12/30/22 04:14:00.985
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]","completed":217,"skipped":4128,"failed":0}
------------------------------
• [SLOW TEST] [20.171 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:975

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:13:40.822
    Dec 30 04:13:40.822: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename statefulset 12/30/22 04:13:40.824
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:13:40.839
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:13:40.843
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-9280 12/30/22 04:13:40.853
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:975
    STEP: Creating statefulset ss in namespace statefulset-9280 12/30/22 04:13:40.862
    Dec 30 04:13:40.872: INFO: Found 0 stateful pods, waiting for 1
    Dec 30 04:13:50.878: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 12/30/22 04:13:50.887
    STEP: Getting /status 12/30/22 04:13:50.901
    Dec 30 04:13:50.907: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 12/30/22 04:13:50.907
    Dec 30 04:13:50.918: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 12/30/22 04:13:50.918
    Dec 30 04:13:50.920: INFO: Observed &StatefulSet event: ADDED
    Dec 30 04:13:50.920: INFO: Found Statefulset ss in namespace statefulset-9280 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Dec 30 04:13:50.921: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 12/30/22 04:13:50.921
    Dec 30 04:13:50.921: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Dec 30 04:13:50.928: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 12/30/22 04:13:50.928
    Dec 30 04:13:50.931: INFO: Observed &StatefulSet event: ADDED
    Dec 30 04:13:50.931: INFO: Observed Statefulset ss in namespace statefulset-9280 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Dec 30 04:13:50.931: INFO: Observed &StatefulSet event: MODIFIED
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Dec 30 04:13:50.931: INFO: Deleting all statefulset in ns statefulset-9280
    Dec 30 04:13:50.935: INFO: Scaling statefulset ss to 0
    Dec 30 04:14:00.961: INFO: Waiting for statefulset status.replicas updated to 0
    Dec 30 04:14:00.965: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Dec 30 04:14:00.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-9280" for this suite. 12/30/22 04:14:00.985
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:14:00.996
Dec 30 04:14:00.996: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 12/30/22 04:14:00.997
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:14:01.013
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:14:01.017
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 12/30/22 04:14:01.02
STEP: Creating hostNetwork=false pod 12/30/22 04:14:01.02
Dec 30 04:14:01.031: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-2550" to be "running and ready"
Dec 30 04:14:01.034: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.521637ms
Dec 30 04:14:01.034: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Dec 30 04:14:03.040: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009127976s
Dec 30 04:14:03.040: INFO: The phase of Pod test-pod is Running (Ready = true)
Dec 30 04:14:03.040: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 12/30/22 04:14:03.045
Dec 30 04:14:03.052: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-2550" to be "running and ready"
Dec 30 04:14:03.055: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.408224ms
Dec 30 04:14:03.055: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Dec 30 04:14:05.060: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008414035s
Dec 30 04:14:05.060: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
Dec 30 04:14:05.060: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 12/30/22 04:14:05.064
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 12/30/22 04:14:05.065
Dec 30 04:14:05.065: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2550 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 30 04:14:05.065: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
Dec 30 04:14:05.066: INFO: ExecWithOptions: Clientset creation
Dec 30 04:14:05.066: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2550/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Dec 30 04:14:05.196: INFO: Exec stderr: ""
Dec 30 04:14:05.196: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2550 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 30 04:14:05.196: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
Dec 30 04:14:05.197: INFO: ExecWithOptions: Clientset creation
Dec 30 04:14:05.197: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2550/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Dec 30 04:14:05.292: INFO: Exec stderr: ""
Dec 30 04:14:05.292: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2550 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 30 04:14:05.292: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
Dec 30 04:14:05.293: INFO: ExecWithOptions: Clientset creation
Dec 30 04:14:05.293: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2550/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Dec 30 04:14:05.384: INFO: Exec stderr: ""
Dec 30 04:14:05.384: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2550 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 30 04:14:05.384: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
Dec 30 04:14:05.385: INFO: ExecWithOptions: Clientset creation
Dec 30 04:14:05.385: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2550/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Dec 30 04:14:05.475: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 12/30/22 04:14:05.475
Dec 30 04:14:05.476: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2550 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 30 04:14:05.476: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
Dec 30 04:14:05.477: INFO: ExecWithOptions: Clientset creation
Dec 30 04:14:05.477: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2550/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Dec 30 04:14:05.568: INFO: Exec stderr: ""
Dec 30 04:14:05.568: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2550 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 30 04:14:05.568: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
Dec 30 04:14:05.569: INFO: ExecWithOptions: Clientset creation
Dec 30 04:14:05.569: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2550/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Dec 30 04:14:05.663: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 12/30/22 04:14:05.663
Dec 30 04:14:05.664: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2550 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 30 04:14:05.664: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
Dec 30 04:14:05.665: INFO: ExecWithOptions: Clientset creation
Dec 30 04:14:05.665: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2550/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Dec 30 04:14:05.784: INFO: Exec stderr: ""
Dec 30 04:14:05.784: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2550 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 30 04:14:05.784: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
Dec 30 04:14:05.785: INFO: ExecWithOptions: Clientset creation
Dec 30 04:14:05.785: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2550/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Dec 30 04:14:05.846: INFO: Exec stderr: ""
Dec 30 04:14:05.846: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2550 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 30 04:14:05.846: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
Dec 30 04:14:05.846: INFO: ExecWithOptions: Clientset creation
Dec 30 04:14:05.846: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2550/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Dec 30 04:14:05.962: INFO: Exec stderr: ""
Dec 30 04:14:05.962: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2550 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 30 04:14:05.962: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
Dec 30 04:14:05.963: INFO: ExecWithOptions: Clientset creation
Dec 30 04:14:05.963: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2550/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Dec 30 04:14:06.073: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:187
Dec 30 04:14:06.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-2550" for this suite. 12/30/22 04:14:06.079
{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","completed":218,"skipped":4157,"failed":0}
------------------------------
• [SLOW TEST] [5.090 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:14:00.996
    Dec 30 04:14:00.996: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 12/30/22 04:14:00.997
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:14:01.013
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:14:01.017
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 12/30/22 04:14:01.02
    STEP: Creating hostNetwork=false pod 12/30/22 04:14:01.02
    Dec 30 04:14:01.031: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-2550" to be "running and ready"
    Dec 30 04:14:01.034: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.521637ms
    Dec 30 04:14:01.034: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Dec 30 04:14:03.040: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009127976s
    Dec 30 04:14:03.040: INFO: The phase of Pod test-pod is Running (Ready = true)
    Dec 30 04:14:03.040: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 12/30/22 04:14:03.045
    Dec 30 04:14:03.052: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-2550" to be "running and ready"
    Dec 30 04:14:03.055: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.408224ms
    Dec 30 04:14:03.055: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Dec 30 04:14:05.060: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008414035s
    Dec 30 04:14:05.060: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    Dec 30 04:14:05.060: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 12/30/22 04:14:05.064
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 12/30/22 04:14:05.065
    Dec 30 04:14:05.065: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2550 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 30 04:14:05.065: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    Dec 30 04:14:05.066: INFO: ExecWithOptions: Clientset creation
    Dec 30 04:14:05.066: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2550/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Dec 30 04:14:05.196: INFO: Exec stderr: ""
    Dec 30 04:14:05.196: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2550 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 30 04:14:05.196: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    Dec 30 04:14:05.197: INFO: ExecWithOptions: Clientset creation
    Dec 30 04:14:05.197: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2550/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Dec 30 04:14:05.292: INFO: Exec stderr: ""
    Dec 30 04:14:05.292: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2550 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 30 04:14:05.292: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    Dec 30 04:14:05.293: INFO: ExecWithOptions: Clientset creation
    Dec 30 04:14:05.293: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2550/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Dec 30 04:14:05.384: INFO: Exec stderr: ""
    Dec 30 04:14:05.384: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2550 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 30 04:14:05.384: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    Dec 30 04:14:05.385: INFO: ExecWithOptions: Clientset creation
    Dec 30 04:14:05.385: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2550/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Dec 30 04:14:05.475: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 12/30/22 04:14:05.475
    Dec 30 04:14:05.476: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2550 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 30 04:14:05.476: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    Dec 30 04:14:05.477: INFO: ExecWithOptions: Clientset creation
    Dec 30 04:14:05.477: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2550/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Dec 30 04:14:05.568: INFO: Exec stderr: ""
    Dec 30 04:14:05.568: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2550 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 30 04:14:05.568: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    Dec 30 04:14:05.569: INFO: ExecWithOptions: Clientset creation
    Dec 30 04:14:05.569: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2550/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Dec 30 04:14:05.663: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 12/30/22 04:14:05.663
    Dec 30 04:14:05.664: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2550 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 30 04:14:05.664: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    Dec 30 04:14:05.665: INFO: ExecWithOptions: Clientset creation
    Dec 30 04:14:05.665: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2550/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Dec 30 04:14:05.784: INFO: Exec stderr: ""
    Dec 30 04:14:05.784: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2550 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 30 04:14:05.784: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    Dec 30 04:14:05.785: INFO: ExecWithOptions: Clientset creation
    Dec 30 04:14:05.785: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2550/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Dec 30 04:14:05.846: INFO: Exec stderr: ""
    Dec 30 04:14:05.846: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2550 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 30 04:14:05.846: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    Dec 30 04:14:05.846: INFO: ExecWithOptions: Clientset creation
    Dec 30 04:14:05.846: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2550/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Dec 30 04:14:05.962: INFO: Exec stderr: ""
    Dec 30 04:14:05.962: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2550 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 30 04:14:05.962: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    Dec 30 04:14:05.963: INFO: ExecWithOptions: Clientset creation
    Dec 30 04:14:05.963: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2550/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Dec 30 04:14:06.073: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/framework.go:187
    Dec 30 04:14:06.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-2550" for this suite. 12/30/22 04:14:06.079
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:14:06.087
Dec 30 04:14:06.087: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename configmap 12/30/22 04:14:06.089
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:14:06.105
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:14:06.108
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174
STEP: Creating configMap with name configmap-test-upd-5fbaa18c-eaed-4c51-bf46-25a551788839 12/30/22 04:14:06.118
STEP: Creating the pod 12/30/22 04:14:06.123
Dec 30 04:14:06.133: INFO: Waiting up to 5m0s for pod "pod-configmaps-46475f01-b584-4253-966d-c3dffa73dd92" in namespace "configmap-2829" to be "running"
Dec 30 04:14:06.137: INFO: Pod "pod-configmaps-46475f01-b584-4253-966d-c3dffa73dd92": Phase="Pending", Reason="", readiness=false. Elapsed: 3.709402ms
Dec 30 04:14:08.143: INFO: Pod "pod-configmaps-46475f01-b584-4253-966d-c3dffa73dd92": Phase="Running", Reason="", readiness=false. Elapsed: 2.009498661s
Dec 30 04:14:08.143: INFO: Pod "pod-configmaps-46475f01-b584-4253-966d-c3dffa73dd92" satisfied condition "running"
STEP: Waiting for pod with text data 12/30/22 04:14:08.143
STEP: Waiting for pod with binary data 12/30/22 04:14:08.164
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Dec 30 04:14:08.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2829" for this suite. 12/30/22 04:14:08.178
{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","completed":219,"skipped":4159,"failed":0}
------------------------------
• [2.098 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:14:06.087
    Dec 30 04:14:06.087: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename configmap 12/30/22 04:14:06.089
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:14:06.105
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:14:06.108
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:174
    STEP: Creating configMap with name configmap-test-upd-5fbaa18c-eaed-4c51-bf46-25a551788839 12/30/22 04:14:06.118
    STEP: Creating the pod 12/30/22 04:14:06.123
    Dec 30 04:14:06.133: INFO: Waiting up to 5m0s for pod "pod-configmaps-46475f01-b584-4253-966d-c3dffa73dd92" in namespace "configmap-2829" to be "running"
    Dec 30 04:14:06.137: INFO: Pod "pod-configmaps-46475f01-b584-4253-966d-c3dffa73dd92": Phase="Pending", Reason="", readiness=false. Elapsed: 3.709402ms
    Dec 30 04:14:08.143: INFO: Pod "pod-configmaps-46475f01-b584-4253-966d-c3dffa73dd92": Phase="Running", Reason="", readiness=false. Elapsed: 2.009498661s
    Dec 30 04:14:08.143: INFO: Pod "pod-configmaps-46475f01-b584-4253-966d-c3dffa73dd92" satisfied condition "running"
    STEP: Waiting for pod with text data 12/30/22 04:14:08.143
    STEP: Waiting for pod with binary data 12/30/22 04:14:08.164
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Dec 30 04:14:08.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-2829" for this suite. 12/30/22 04:14:08.178
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:14:08.189
Dec 30 04:14:08.189: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename events 12/30/22 04:14:08.19
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:14:08.208
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:14:08.211
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 12/30/22 04:14:08.215
STEP: listing all events in all namespaces 12/30/22 04:14:08.219
STEP: patching the test event 12/30/22 04:14:08.228
STEP: fetching the test event 12/30/22 04:14:08.235
STEP: updating the test event 12/30/22 04:14:08.239
STEP: getting the test event 12/30/22 04:14:08.25
STEP: deleting the test event 12/30/22 04:14:08.254
STEP: listing all events in all namespaces 12/30/22 04:14:08.263
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
Dec 30 04:14:08.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9316" for this suite. 12/30/22 04:14:08.277
{"msg":"PASSED [sig-instrumentation] Events should manage the lifecycle of an event [Conformance]","completed":220,"skipped":4194,"failed":0}
------------------------------
• [0.095 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:14:08.189
    Dec 30 04:14:08.189: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename events 12/30/22 04:14:08.19
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:14:08.208
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:14:08.211
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 12/30/22 04:14:08.215
    STEP: listing all events in all namespaces 12/30/22 04:14:08.219
    STEP: patching the test event 12/30/22 04:14:08.228
    STEP: fetching the test event 12/30/22 04:14:08.235
    STEP: updating the test event 12/30/22 04:14:08.239
    STEP: getting the test event 12/30/22 04:14:08.25
    STEP: deleting the test event 12/30/22 04:14:08.254
    STEP: listing all events in all namespaces 12/30/22 04:14:08.263
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:187
    Dec 30 04:14:08.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-9316" for this suite. 12/30/22 04:14:08.277
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:14:08.286
Dec 30 04:14:08.286: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename services 12/30/22 04:14:08.288
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:14:08.304
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:14:08.308
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268
STEP: creating service nodeport-test with type=NodePort in namespace services-7270 12/30/22 04:14:08.311
STEP: creating replication controller nodeport-test in namespace services-7270 12/30/22 04:14:08.327
I1230 04:14:08.333731      25 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-7270, replica count: 2
I1230 04:14:11.385756      25 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 30 04:14:11.385: INFO: Creating new exec pod
Dec 30 04:14:11.395: INFO: Waiting up to 5m0s for pod "execpodcmz85" in namespace "services-7270" to be "running"
Dec 30 04:14:11.399: INFO: Pod "execpodcmz85": Phase="Pending", Reason="", readiness=false. Elapsed: 3.719102ms
Dec 30 04:14:13.404: INFO: Pod "execpodcmz85": Phase="Running", Reason="", readiness=true. Elapsed: 2.008492184s
Dec 30 04:14:13.404: INFO: Pod "execpodcmz85" satisfied condition "running"
Dec 30 04:14:14.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-7270 exec execpodcmz85 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Dec 30 04:14:14.639: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Dec 30 04:14:14.639: INFO: stdout: "nodeport-test-xr7r7"
Dec 30 04:14:14.639: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-7270 exec execpodcmz85 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.36.189 80'
Dec 30 04:14:14.844: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.36.189 80\nConnection to 10.233.36.189 80 port [tcp/http] succeeded!\n"
Dec 30 04:14:14.845: INFO: stdout: "nodeport-test-gtv4z"
Dec 30 04:14:14.845: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-7270 exec execpodcmz85 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.78.26.195 31931'
Dec 30 04:14:15.057: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.78.26.195 31931\nConnection to 10.78.26.195 31931 port [tcp/*] succeeded!\n"
Dec 30 04:14:15.057: INFO: stdout: "nodeport-test-xr7r7"
Dec 30 04:14:15.057: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-7270 exec execpodcmz85 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.78.26.142 31931'
Dec 30 04:14:15.256: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.78.26.142 31931\nConnection to 10.78.26.142 31931 port [tcp/*] succeeded!\n"
Dec 30 04:14:15.256: INFO: stdout: "nodeport-test-xr7r7"
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Dec 30 04:14:15.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7270" for this suite. 12/30/22 04:14:15.263
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","completed":221,"skipped":4220,"failed":0}
------------------------------
• [SLOW TEST] [6.985 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:14:08.286
    Dec 30 04:14:08.286: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename services 12/30/22 04:14:08.288
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:14:08.304
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:14:08.308
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1268
    STEP: creating service nodeport-test with type=NodePort in namespace services-7270 12/30/22 04:14:08.311
    STEP: creating replication controller nodeport-test in namespace services-7270 12/30/22 04:14:08.327
    I1230 04:14:08.333731      25 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-7270, replica count: 2
    I1230 04:14:11.385756      25 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Dec 30 04:14:11.385: INFO: Creating new exec pod
    Dec 30 04:14:11.395: INFO: Waiting up to 5m0s for pod "execpodcmz85" in namespace "services-7270" to be "running"
    Dec 30 04:14:11.399: INFO: Pod "execpodcmz85": Phase="Pending", Reason="", readiness=false. Elapsed: 3.719102ms
    Dec 30 04:14:13.404: INFO: Pod "execpodcmz85": Phase="Running", Reason="", readiness=true. Elapsed: 2.008492184s
    Dec 30 04:14:13.404: INFO: Pod "execpodcmz85" satisfied condition "running"
    Dec 30 04:14:14.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-7270 exec execpodcmz85 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Dec 30 04:14:14.639: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Dec 30 04:14:14.639: INFO: stdout: "nodeport-test-xr7r7"
    Dec 30 04:14:14.639: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-7270 exec execpodcmz85 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.36.189 80'
    Dec 30 04:14:14.844: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.36.189 80\nConnection to 10.233.36.189 80 port [tcp/http] succeeded!\n"
    Dec 30 04:14:14.845: INFO: stdout: "nodeport-test-gtv4z"
    Dec 30 04:14:14.845: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-7270 exec execpodcmz85 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.78.26.195 31931'
    Dec 30 04:14:15.057: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.78.26.195 31931\nConnection to 10.78.26.195 31931 port [tcp/*] succeeded!\n"
    Dec 30 04:14:15.057: INFO: stdout: "nodeport-test-xr7r7"
    Dec 30 04:14:15.057: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-7270 exec execpodcmz85 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.78.26.142 31931'
    Dec 30 04:14:15.256: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.78.26.142 31931\nConnection to 10.78.26.142 31931 port [tcp/*] succeeded!\n"
    Dec 30 04:14:15.256: INFO: stdout: "nodeport-test-xr7r7"
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Dec 30 04:14:15.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-7270" for this suite. 12/30/22 04:14:15.263
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:14:15.272
Dec 30 04:14:15.272: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename crd-publish-openapi 12/30/22 04:14:15.273
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:14:15.29
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:14:15.293
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 12/30/22 04:14:15.297
Dec 30 04:14:15.298: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
Dec 30 04:14:23.694: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Dec 30 04:14:43.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6634" for this suite. 12/30/22 04:14:43.578
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","completed":222,"skipped":4225,"failed":0}
------------------------------
• [SLOW TEST] [28.315 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:14:15.272
    Dec 30 04:14:15.272: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename crd-publish-openapi 12/30/22 04:14:15.273
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:14:15.29
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:14:15.293
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:275
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 12/30/22 04:14:15.297
    Dec 30 04:14:15.298: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    Dec 30 04:14:23.694: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Dec 30 04:14:43.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-6634" for this suite. 12/30/22 04:14:43.578
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:14:43.588
Dec 30 04:14:43.588: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename projected 12/30/22 04:14:43.59
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:14:43.608
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:14:43.612
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46
STEP: Creating configMap with name projected-configmap-test-volume-4224fe66-2fc5-42e8-a05f-0c6c9b266633 12/30/22 04:14:43.615
STEP: Creating a pod to test consume configMaps 12/30/22 04:14:43.621
Dec 30 04:14:43.630: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2d6176c6-547a-49d2-b904-eea58f0515e6" in namespace "projected-7767" to be "Succeeded or Failed"
Dec 30 04:14:43.634: INFO: Pod "pod-projected-configmaps-2d6176c6-547a-49d2-b904-eea58f0515e6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.477068ms
Dec 30 04:14:45.640: INFO: Pod "pod-projected-configmaps-2d6176c6-547a-49d2-b904-eea58f0515e6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009937039s
Dec 30 04:14:47.640: INFO: Pod "pod-projected-configmaps-2d6176c6-547a-49d2-b904-eea58f0515e6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009264559s
STEP: Saw pod success 12/30/22 04:14:47.64
Dec 30 04:14:47.640: INFO: Pod "pod-projected-configmaps-2d6176c6-547a-49d2-b904-eea58f0515e6" satisfied condition "Succeeded or Failed"
Dec 30 04:14:47.644: INFO: Trying to get logs from node k8s-mgmt01 pod pod-projected-configmaps-2d6176c6-547a-49d2-b904-eea58f0515e6 container agnhost-container: <nil>
STEP: delete the pod 12/30/22 04:14:47.665
Dec 30 04:14:47.676: INFO: Waiting for pod pod-projected-configmaps-2d6176c6-547a-49d2-b904-eea58f0515e6 to disappear
Dec 30 04:14:47.679: INFO: Pod pod-projected-configmaps-2d6176c6-547a-49d2-b904-eea58f0515e6 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Dec 30 04:14:47.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7767" for this suite. 12/30/22 04:14:47.686
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","completed":223,"skipped":4239,"failed":0}
------------------------------
• [4.105 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:14:43.588
    Dec 30 04:14:43.588: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename projected 12/30/22 04:14:43.59
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:14:43.608
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:14:43.612
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:46
    STEP: Creating configMap with name projected-configmap-test-volume-4224fe66-2fc5-42e8-a05f-0c6c9b266633 12/30/22 04:14:43.615
    STEP: Creating a pod to test consume configMaps 12/30/22 04:14:43.621
    Dec 30 04:14:43.630: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2d6176c6-547a-49d2-b904-eea58f0515e6" in namespace "projected-7767" to be "Succeeded or Failed"
    Dec 30 04:14:43.634: INFO: Pod "pod-projected-configmaps-2d6176c6-547a-49d2-b904-eea58f0515e6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.477068ms
    Dec 30 04:14:45.640: INFO: Pod "pod-projected-configmaps-2d6176c6-547a-49d2-b904-eea58f0515e6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009937039s
    Dec 30 04:14:47.640: INFO: Pod "pod-projected-configmaps-2d6176c6-547a-49d2-b904-eea58f0515e6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009264559s
    STEP: Saw pod success 12/30/22 04:14:47.64
    Dec 30 04:14:47.640: INFO: Pod "pod-projected-configmaps-2d6176c6-547a-49d2-b904-eea58f0515e6" satisfied condition "Succeeded or Failed"
    Dec 30 04:14:47.644: INFO: Trying to get logs from node k8s-mgmt01 pod pod-projected-configmaps-2d6176c6-547a-49d2-b904-eea58f0515e6 container agnhost-container: <nil>
    STEP: delete the pod 12/30/22 04:14:47.665
    Dec 30 04:14:47.676: INFO: Waiting for pod pod-projected-configmaps-2d6176c6-547a-49d2-b904-eea58f0515e6 to disappear
    Dec 30 04:14:47.679: INFO: Pod pod-projected-configmaps-2d6176c6-547a-49d2-b904-eea58f0515e6 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Dec 30 04:14:47.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7767" for this suite. 12/30/22 04:14:47.686
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:14:47.694
Dec 30 04:14:47.694: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename secrets 12/30/22 04:14:47.695
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:14:47.711
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:14:47.715
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98
STEP: Creating secret with name secret-test-84f261ce-2e2f-48bd-bed7-2f750accaed6 12/30/22 04:14:47.739
STEP: Creating a pod to test consume secrets 12/30/22 04:14:47.743
Dec 30 04:14:47.753: INFO: Waiting up to 5m0s for pod "pod-secrets-a9698b0d-82a9-4b0b-8b45-c503be4f9758" in namespace "secrets-9385" to be "Succeeded or Failed"
Dec 30 04:14:47.757: INFO: Pod "pod-secrets-a9698b0d-82a9-4b0b-8b45-c503be4f9758": Phase="Pending", Reason="", readiness=false. Elapsed: 4.18331ms
Dec 30 04:14:49.764: INFO: Pod "pod-secrets-a9698b0d-82a9-4b0b-8b45-c503be4f9758": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010798165s
Dec 30 04:14:51.762: INFO: Pod "pod-secrets-a9698b0d-82a9-4b0b-8b45-c503be4f9758": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009610397s
STEP: Saw pod success 12/30/22 04:14:51.762
Dec 30 04:14:51.763: INFO: Pod "pod-secrets-a9698b0d-82a9-4b0b-8b45-c503be4f9758" satisfied condition "Succeeded or Failed"
Dec 30 04:14:51.766: INFO: Trying to get logs from node k8s-mgmt01 pod pod-secrets-a9698b0d-82a9-4b0b-8b45-c503be4f9758 container secret-volume-test: <nil>
STEP: delete the pod 12/30/22 04:14:51.775
Dec 30 04:14:51.789: INFO: Waiting for pod pod-secrets-a9698b0d-82a9-4b0b-8b45-c503be4f9758 to disappear
Dec 30 04:14:51.792: INFO: Pod pod-secrets-a9698b0d-82a9-4b0b-8b45-c503be4f9758 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Dec 30 04:14:51.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9385" for this suite. 12/30/22 04:14:51.798
STEP: Destroying namespace "secret-namespace-1465" for this suite. 12/30/22 04:14:51.805
{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","completed":224,"skipped":4247,"failed":0}
------------------------------
• [4.118 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:14:47.694
    Dec 30 04:14:47.694: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename secrets 12/30/22 04:14:47.695
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:14:47.711
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:14:47.715
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:98
    STEP: Creating secret with name secret-test-84f261ce-2e2f-48bd-bed7-2f750accaed6 12/30/22 04:14:47.739
    STEP: Creating a pod to test consume secrets 12/30/22 04:14:47.743
    Dec 30 04:14:47.753: INFO: Waiting up to 5m0s for pod "pod-secrets-a9698b0d-82a9-4b0b-8b45-c503be4f9758" in namespace "secrets-9385" to be "Succeeded or Failed"
    Dec 30 04:14:47.757: INFO: Pod "pod-secrets-a9698b0d-82a9-4b0b-8b45-c503be4f9758": Phase="Pending", Reason="", readiness=false. Elapsed: 4.18331ms
    Dec 30 04:14:49.764: INFO: Pod "pod-secrets-a9698b0d-82a9-4b0b-8b45-c503be4f9758": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010798165s
    Dec 30 04:14:51.762: INFO: Pod "pod-secrets-a9698b0d-82a9-4b0b-8b45-c503be4f9758": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009610397s
    STEP: Saw pod success 12/30/22 04:14:51.762
    Dec 30 04:14:51.763: INFO: Pod "pod-secrets-a9698b0d-82a9-4b0b-8b45-c503be4f9758" satisfied condition "Succeeded or Failed"
    Dec 30 04:14:51.766: INFO: Trying to get logs from node k8s-mgmt01 pod pod-secrets-a9698b0d-82a9-4b0b-8b45-c503be4f9758 container secret-volume-test: <nil>
    STEP: delete the pod 12/30/22 04:14:51.775
    Dec 30 04:14:51.789: INFO: Waiting for pod pod-secrets-a9698b0d-82a9-4b0b-8b45-c503be4f9758 to disappear
    Dec 30 04:14:51.792: INFO: Pod pod-secrets-a9698b0d-82a9-4b0b-8b45-c503be4f9758 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Dec 30 04:14:51.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-9385" for this suite. 12/30/22 04:14:51.798
    STEP: Destroying namespace "secret-namespace-1465" for this suite. 12/30/22 04:14:51.805
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:14:51.813
Dec 30 04:14:51.813: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename discovery 12/30/22 04:14:51.814
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:14:51.832
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:14:51.836
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 12/30/22 04:14:51.841
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
Dec 30 04:14:52.334: INFO: Checking APIGroup: apiregistration.k8s.io
Dec 30 04:14:52.335: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Dec 30 04:14:52.335: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Dec 30 04:14:52.335: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Dec 30 04:14:52.335: INFO: Checking APIGroup: apps
Dec 30 04:14:52.336: INFO: PreferredVersion.GroupVersion: apps/v1
Dec 30 04:14:52.336: INFO: Versions found [{apps/v1 v1}]
Dec 30 04:14:52.336: INFO: apps/v1 matches apps/v1
Dec 30 04:14:52.336: INFO: Checking APIGroup: events.k8s.io
Dec 30 04:14:52.337: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Dec 30 04:14:52.337: INFO: Versions found [{events.k8s.io/v1 v1}]
Dec 30 04:14:52.337: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Dec 30 04:14:52.337: INFO: Checking APIGroup: authentication.k8s.io
Dec 30 04:14:52.339: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Dec 30 04:14:52.339: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Dec 30 04:14:52.339: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Dec 30 04:14:52.339: INFO: Checking APIGroup: authorization.k8s.io
Dec 30 04:14:52.340: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Dec 30 04:14:52.340: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Dec 30 04:14:52.340: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Dec 30 04:14:52.340: INFO: Checking APIGroup: autoscaling
Dec 30 04:14:52.341: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Dec 30 04:14:52.341: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta2 v2beta2}]
Dec 30 04:14:52.341: INFO: autoscaling/v2 matches autoscaling/v2
Dec 30 04:14:52.341: INFO: Checking APIGroup: batch
Dec 30 04:14:52.343: INFO: PreferredVersion.GroupVersion: batch/v1
Dec 30 04:14:52.343: INFO: Versions found [{batch/v1 v1}]
Dec 30 04:14:52.343: INFO: batch/v1 matches batch/v1
Dec 30 04:14:52.343: INFO: Checking APIGroup: certificates.k8s.io
Dec 30 04:14:52.344: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Dec 30 04:14:52.344: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Dec 30 04:14:52.344: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Dec 30 04:14:52.344: INFO: Checking APIGroup: networking.k8s.io
Dec 30 04:14:52.345: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Dec 30 04:14:52.345: INFO: Versions found [{networking.k8s.io/v1 v1}]
Dec 30 04:14:52.345: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Dec 30 04:14:52.345: INFO: Checking APIGroup: policy
Dec 30 04:14:52.346: INFO: PreferredVersion.GroupVersion: policy/v1
Dec 30 04:14:52.346: INFO: Versions found [{policy/v1 v1}]
Dec 30 04:14:52.346: INFO: policy/v1 matches policy/v1
Dec 30 04:14:52.346: INFO: Checking APIGroup: rbac.authorization.k8s.io
Dec 30 04:14:52.348: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Dec 30 04:14:52.348: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Dec 30 04:14:52.348: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Dec 30 04:14:52.348: INFO: Checking APIGroup: storage.k8s.io
Dec 30 04:14:52.349: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Dec 30 04:14:52.349: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Dec 30 04:14:52.349: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Dec 30 04:14:52.349: INFO: Checking APIGroup: admissionregistration.k8s.io
Dec 30 04:14:52.350: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Dec 30 04:14:52.350: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Dec 30 04:14:52.350: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Dec 30 04:14:52.350: INFO: Checking APIGroup: apiextensions.k8s.io
Dec 30 04:14:52.352: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Dec 30 04:14:52.352: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Dec 30 04:14:52.352: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Dec 30 04:14:52.352: INFO: Checking APIGroup: scheduling.k8s.io
Dec 30 04:14:52.353: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Dec 30 04:14:52.353: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Dec 30 04:14:52.353: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Dec 30 04:14:52.353: INFO: Checking APIGroup: coordination.k8s.io
Dec 30 04:14:52.354: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Dec 30 04:14:52.354: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Dec 30 04:14:52.354: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Dec 30 04:14:52.354: INFO: Checking APIGroup: node.k8s.io
Dec 30 04:14:52.355: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Dec 30 04:14:52.355: INFO: Versions found [{node.k8s.io/v1 v1}]
Dec 30 04:14:52.355: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Dec 30 04:14:52.355: INFO: Checking APIGroup: discovery.k8s.io
Dec 30 04:14:52.357: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Dec 30 04:14:52.357: INFO: Versions found [{discovery.k8s.io/v1 v1}]
Dec 30 04:14:52.357: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Dec 30 04:14:52.357: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Dec 30 04:14:52.358: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
Dec 30 04:14:52.358: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Dec 30 04:14:52.358: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
Dec 30 04:14:52.358: INFO: Checking APIGroup: crd.projectcalico.org
Dec 30 04:14:52.359: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Dec 30 04:14:52.359: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Dec 30 04:14:52.359: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Dec 30 04:14:52.359: INFO: Checking APIGroup: metrics.k8s.io
Dec 30 04:14:52.360: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Dec 30 04:14:52.360: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Dec 30 04:14:52.360: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:187
Dec 30 04:14:52.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-7269" for this suite. 12/30/22 04:14:52.366
{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","completed":225,"skipped":4263,"failed":0}
------------------------------
• [0.560 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:14:51.813
    Dec 30 04:14:51.813: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename discovery 12/30/22 04:14:51.814
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:14:51.832
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:14:51.836
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 12/30/22 04:14:51.841
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    Dec 30 04:14:52.334: INFO: Checking APIGroup: apiregistration.k8s.io
    Dec 30 04:14:52.335: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    Dec 30 04:14:52.335: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    Dec 30 04:14:52.335: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    Dec 30 04:14:52.335: INFO: Checking APIGroup: apps
    Dec 30 04:14:52.336: INFO: PreferredVersion.GroupVersion: apps/v1
    Dec 30 04:14:52.336: INFO: Versions found [{apps/v1 v1}]
    Dec 30 04:14:52.336: INFO: apps/v1 matches apps/v1
    Dec 30 04:14:52.336: INFO: Checking APIGroup: events.k8s.io
    Dec 30 04:14:52.337: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    Dec 30 04:14:52.337: INFO: Versions found [{events.k8s.io/v1 v1}]
    Dec 30 04:14:52.337: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    Dec 30 04:14:52.337: INFO: Checking APIGroup: authentication.k8s.io
    Dec 30 04:14:52.339: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    Dec 30 04:14:52.339: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    Dec 30 04:14:52.339: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    Dec 30 04:14:52.339: INFO: Checking APIGroup: authorization.k8s.io
    Dec 30 04:14:52.340: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    Dec 30 04:14:52.340: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    Dec 30 04:14:52.340: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    Dec 30 04:14:52.340: INFO: Checking APIGroup: autoscaling
    Dec 30 04:14:52.341: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    Dec 30 04:14:52.341: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta2 v2beta2}]
    Dec 30 04:14:52.341: INFO: autoscaling/v2 matches autoscaling/v2
    Dec 30 04:14:52.341: INFO: Checking APIGroup: batch
    Dec 30 04:14:52.343: INFO: PreferredVersion.GroupVersion: batch/v1
    Dec 30 04:14:52.343: INFO: Versions found [{batch/v1 v1}]
    Dec 30 04:14:52.343: INFO: batch/v1 matches batch/v1
    Dec 30 04:14:52.343: INFO: Checking APIGroup: certificates.k8s.io
    Dec 30 04:14:52.344: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    Dec 30 04:14:52.344: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    Dec 30 04:14:52.344: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    Dec 30 04:14:52.344: INFO: Checking APIGroup: networking.k8s.io
    Dec 30 04:14:52.345: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    Dec 30 04:14:52.345: INFO: Versions found [{networking.k8s.io/v1 v1}]
    Dec 30 04:14:52.345: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    Dec 30 04:14:52.345: INFO: Checking APIGroup: policy
    Dec 30 04:14:52.346: INFO: PreferredVersion.GroupVersion: policy/v1
    Dec 30 04:14:52.346: INFO: Versions found [{policy/v1 v1}]
    Dec 30 04:14:52.346: INFO: policy/v1 matches policy/v1
    Dec 30 04:14:52.346: INFO: Checking APIGroup: rbac.authorization.k8s.io
    Dec 30 04:14:52.348: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    Dec 30 04:14:52.348: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    Dec 30 04:14:52.348: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    Dec 30 04:14:52.348: INFO: Checking APIGroup: storage.k8s.io
    Dec 30 04:14:52.349: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    Dec 30 04:14:52.349: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    Dec 30 04:14:52.349: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    Dec 30 04:14:52.349: INFO: Checking APIGroup: admissionregistration.k8s.io
    Dec 30 04:14:52.350: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    Dec 30 04:14:52.350: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    Dec 30 04:14:52.350: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    Dec 30 04:14:52.350: INFO: Checking APIGroup: apiextensions.k8s.io
    Dec 30 04:14:52.352: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    Dec 30 04:14:52.352: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    Dec 30 04:14:52.352: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    Dec 30 04:14:52.352: INFO: Checking APIGroup: scheduling.k8s.io
    Dec 30 04:14:52.353: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    Dec 30 04:14:52.353: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    Dec 30 04:14:52.353: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    Dec 30 04:14:52.353: INFO: Checking APIGroup: coordination.k8s.io
    Dec 30 04:14:52.354: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    Dec 30 04:14:52.354: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    Dec 30 04:14:52.354: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    Dec 30 04:14:52.354: INFO: Checking APIGroup: node.k8s.io
    Dec 30 04:14:52.355: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    Dec 30 04:14:52.355: INFO: Versions found [{node.k8s.io/v1 v1}]
    Dec 30 04:14:52.355: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    Dec 30 04:14:52.355: INFO: Checking APIGroup: discovery.k8s.io
    Dec 30 04:14:52.357: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    Dec 30 04:14:52.357: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    Dec 30 04:14:52.357: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    Dec 30 04:14:52.357: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    Dec 30 04:14:52.358: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
    Dec 30 04:14:52.358: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
    Dec 30 04:14:52.358: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
    Dec 30 04:14:52.358: INFO: Checking APIGroup: crd.projectcalico.org
    Dec 30 04:14:52.359: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
    Dec 30 04:14:52.359: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
    Dec 30 04:14:52.359: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
    Dec 30 04:14:52.359: INFO: Checking APIGroup: metrics.k8s.io
    Dec 30 04:14:52.360: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
    Dec 30 04:14:52.360: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
    Dec 30 04:14:52.360: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/framework.go:187
    Dec 30 04:14:52.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "discovery-7269" for this suite. 12/30/22 04:14:52.366
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:14:52.375
Dec 30 04:14:52.375: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename secrets 12/30/22 04:14:52.376
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:14:52.401
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:14:52.405
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Dec 30 04:14:52.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4914" for this suite. 12/30/22 04:14:52.457
{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","completed":226,"skipped":4278,"failed":0}
------------------------------
• [0.089 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:14:52.375
    Dec 30 04:14:52.375: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename secrets 12/30/22 04:14:52.376
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:14:52.401
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:14:52.405
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:385
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Dec 30 04:14:52.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-4914" for this suite. 12/30/22 04:14:52.457
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:14:52.464
Dec 30 04:14:52.465: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename resourcequota 12/30/22 04:14:52.466
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:14:52.481
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:14:52.484
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316
STEP: Counting existing ResourceQuota 12/30/22 04:15:09.493
STEP: Creating a ResourceQuota 12/30/22 04:15:14.498
STEP: Ensuring resource quota status is calculated 12/30/22 04:15:14.505
STEP: Creating a ConfigMap 12/30/22 04:15:16.511
STEP: Ensuring resource quota status captures configMap creation 12/30/22 04:15:16.524
STEP: Deleting a ConfigMap 12/30/22 04:15:18.531
STEP: Ensuring resource quota status released usage 12/30/22 04:15:18.537
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Dec 30 04:15:20.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4014" for this suite. 12/30/22 04:15:20.55
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","completed":227,"skipped":4282,"failed":0}
------------------------------
• [SLOW TEST] [28.094 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:14:52.464
    Dec 30 04:14:52.465: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename resourcequota 12/30/22 04:14:52.466
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:14:52.481
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:14:52.484
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:316
    STEP: Counting existing ResourceQuota 12/30/22 04:15:09.493
    STEP: Creating a ResourceQuota 12/30/22 04:15:14.498
    STEP: Ensuring resource quota status is calculated 12/30/22 04:15:14.505
    STEP: Creating a ConfigMap 12/30/22 04:15:16.511
    STEP: Ensuring resource quota status captures configMap creation 12/30/22 04:15:16.524
    STEP: Deleting a ConfigMap 12/30/22 04:15:18.531
    STEP: Ensuring resource quota status released usage 12/30/22 04:15:18.537
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Dec 30 04:15:20.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-4014" for this suite. 12/30/22 04:15:20.55
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:15:20.558
Dec 30 04:15:20.558: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename downward-api 12/30/22 04:15:20.56
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:15:20.578
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:15:20.582
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260
STEP: Creating a pod to test downward API volume plugin 12/30/22 04:15:20.585
Dec 30 04:15:20.596: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6aebe5b1-53b3-46cb-897a-42167d929d1d" in namespace "downward-api-506" to be "Succeeded or Failed"
Dec 30 04:15:20.600: INFO: Pod "downwardapi-volume-6aebe5b1-53b3-46cb-897a-42167d929d1d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.045217ms
Dec 30 04:15:22.605: INFO: Pod "downwardapi-volume-6aebe5b1-53b3-46cb-897a-42167d929d1d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008877787s
Dec 30 04:15:24.606: INFO: Pod "downwardapi-volume-6aebe5b1-53b3-46cb-897a-42167d929d1d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009892813s
STEP: Saw pod success 12/30/22 04:15:24.606
Dec 30 04:15:24.606: INFO: Pod "downwardapi-volume-6aebe5b1-53b3-46cb-897a-42167d929d1d" satisfied condition "Succeeded or Failed"
Dec 30 04:15:24.611: INFO: Trying to get logs from node k8s-mgmt01 pod downwardapi-volume-6aebe5b1-53b3-46cb-897a-42167d929d1d container client-container: <nil>
STEP: delete the pod 12/30/22 04:15:24.619
Dec 30 04:15:24.630: INFO: Waiting for pod downwardapi-volume-6aebe5b1-53b3-46cb-897a-42167d929d1d to disappear
Dec 30 04:15:24.634: INFO: Pod downwardapi-volume-6aebe5b1-53b3-46cb-897a-42167d929d1d no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Dec 30 04:15:24.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-506" for this suite. 12/30/22 04:15:24.64
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","completed":228,"skipped":4282,"failed":0}
------------------------------
• [4.089 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:15:20.558
    Dec 30 04:15:20.558: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename downward-api 12/30/22 04:15:20.56
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:15:20.578
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:15:20.582
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:260
    STEP: Creating a pod to test downward API volume plugin 12/30/22 04:15:20.585
    Dec 30 04:15:20.596: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6aebe5b1-53b3-46cb-897a-42167d929d1d" in namespace "downward-api-506" to be "Succeeded or Failed"
    Dec 30 04:15:20.600: INFO: Pod "downwardapi-volume-6aebe5b1-53b3-46cb-897a-42167d929d1d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.045217ms
    Dec 30 04:15:22.605: INFO: Pod "downwardapi-volume-6aebe5b1-53b3-46cb-897a-42167d929d1d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008877787s
    Dec 30 04:15:24.606: INFO: Pod "downwardapi-volume-6aebe5b1-53b3-46cb-897a-42167d929d1d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009892813s
    STEP: Saw pod success 12/30/22 04:15:24.606
    Dec 30 04:15:24.606: INFO: Pod "downwardapi-volume-6aebe5b1-53b3-46cb-897a-42167d929d1d" satisfied condition "Succeeded or Failed"
    Dec 30 04:15:24.611: INFO: Trying to get logs from node k8s-mgmt01 pod downwardapi-volume-6aebe5b1-53b3-46cb-897a-42167d929d1d container client-container: <nil>
    STEP: delete the pod 12/30/22 04:15:24.619
    Dec 30 04:15:24.630: INFO: Waiting for pod downwardapi-volume-6aebe5b1-53b3-46cb-897a-42167d929d1d to disappear
    Dec 30 04:15:24.634: INFO: Pod downwardapi-volume-6aebe5b1-53b3-46cb-897a-42167d929d1d no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Dec 30 04:15:24.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-506" for this suite. 12/30/22 04:15:24.64
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:15:24.649
Dec 30 04:15:24.649: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename projected 12/30/22 04:15:24.651
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:15:24.668
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:15:24.672
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88
STEP: Creating configMap with name projected-configmap-test-volume-map-d4a37470-00d8-41de-bbcd-a2c34cc065f6 12/30/22 04:15:24.676
STEP: Creating a pod to test consume configMaps 12/30/22 04:15:24.681
Dec 30 04:15:24.691: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1dda412d-6530-4d83-a738-6dec39e7650c" in namespace "projected-8408" to be "Succeeded or Failed"
Dec 30 04:15:24.695: INFO: Pod "pod-projected-configmaps-1dda412d-6530-4d83-a738-6dec39e7650c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.098275ms
Dec 30 04:15:26.700: INFO: Pod "pod-projected-configmaps-1dda412d-6530-4d83-a738-6dec39e7650c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009284915s
Dec 30 04:15:28.702: INFO: Pod "pod-projected-configmaps-1dda412d-6530-4d83-a738-6dec39e7650c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011387365s
STEP: Saw pod success 12/30/22 04:15:28.702
Dec 30 04:15:28.703: INFO: Pod "pod-projected-configmaps-1dda412d-6530-4d83-a738-6dec39e7650c" satisfied condition "Succeeded or Failed"
Dec 30 04:15:28.706: INFO: Trying to get logs from node k8s-mgmt01 pod pod-projected-configmaps-1dda412d-6530-4d83-a738-6dec39e7650c container agnhost-container: <nil>
STEP: delete the pod 12/30/22 04:15:28.715
Dec 30 04:15:28.728: INFO: Waiting for pod pod-projected-configmaps-1dda412d-6530-4d83-a738-6dec39e7650c to disappear
Dec 30 04:15:28.732: INFO: Pod pod-projected-configmaps-1dda412d-6530-4d83-a738-6dec39e7650c no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Dec 30 04:15:28.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8408" for this suite. 12/30/22 04:15:28.738
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":229,"skipped":4307,"failed":0}
------------------------------
• [4.096 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:15:24.649
    Dec 30 04:15:24.649: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename projected 12/30/22 04:15:24.651
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:15:24.668
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:15:24.672
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:88
    STEP: Creating configMap with name projected-configmap-test-volume-map-d4a37470-00d8-41de-bbcd-a2c34cc065f6 12/30/22 04:15:24.676
    STEP: Creating a pod to test consume configMaps 12/30/22 04:15:24.681
    Dec 30 04:15:24.691: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1dda412d-6530-4d83-a738-6dec39e7650c" in namespace "projected-8408" to be "Succeeded or Failed"
    Dec 30 04:15:24.695: INFO: Pod "pod-projected-configmaps-1dda412d-6530-4d83-a738-6dec39e7650c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.098275ms
    Dec 30 04:15:26.700: INFO: Pod "pod-projected-configmaps-1dda412d-6530-4d83-a738-6dec39e7650c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009284915s
    Dec 30 04:15:28.702: INFO: Pod "pod-projected-configmaps-1dda412d-6530-4d83-a738-6dec39e7650c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011387365s
    STEP: Saw pod success 12/30/22 04:15:28.702
    Dec 30 04:15:28.703: INFO: Pod "pod-projected-configmaps-1dda412d-6530-4d83-a738-6dec39e7650c" satisfied condition "Succeeded or Failed"
    Dec 30 04:15:28.706: INFO: Trying to get logs from node k8s-mgmt01 pod pod-projected-configmaps-1dda412d-6530-4d83-a738-6dec39e7650c container agnhost-container: <nil>
    STEP: delete the pod 12/30/22 04:15:28.715
    Dec 30 04:15:28.728: INFO: Waiting for pod pod-projected-configmaps-1dda412d-6530-4d83-a738-6dec39e7650c to disappear
    Dec 30 04:15:28.732: INFO: Pod pod-projected-configmaps-1dda412d-6530-4d83-a738-6dec39e7650c no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Dec 30 04:15:28.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8408" for this suite. 12/30/22 04:15:28.738
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:15:28.75
Dec 30 04:15:28.750: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename replication-controller 12/30/22 04:15:28.752
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:15:28.769
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:15:28.772
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66
STEP: Creating replication controller my-hostname-basic-d0bbbf31-78ce-4062-ad20-fcdb2bd7c856 12/30/22 04:15:28.776
Dec 30 04:15:28.786: INFO: Pod name my-hostname-basic-d0bbbf31-78ce-4062-ad20-fcdb2bd7c856: Found 0 pods out of 1
Dec 30 04:15:33.793: INFO: Pod name my-hostname-basic-d0bbbf31-78ce-4062-ad20-fcdb2bd7c856: Found 1 pods out of 1
Dec 30 04:15:33.793: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-d0bbbf31-78ce-4062-ad20-fcdb2bd7c856" are running
Dec 30 04:15:33.793: INFO: Waiting up to 5m0s for pod "my-hostname-basic-d0bbbf31-78ce-4062-ad20-fcdb2bd7c856-vqrfm" in namespace "replication-controller-908" to be "running"
Dec 30 04:15:33.797: INFO: Pod "my-hostname-basic-d0bbbf31-78ce-4062-ad20-fcdb2bd7c856-vqrfm": Phase="Running", Reason="", readiness=true. Elapsed: 4.182994ms
Dec 30 04:15:33.797: INFO: Pod "my-hostname-basic-d0bbbf31-78ce-4062-ad20-fcdb2bd7c856-vqrfm" satisfied condition "running"
Dec 30 04:15:33.797: INFO: Pod "my-hostname-basic-d0bbbf31-78ce-4062-ad20-fcdb2bd7c856-vqrfm" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-30 04:15:28 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-30 04:15:30 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-30 04:15:30 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-30 04:15:28 +0000 UTC Reason: Message:}])
Dec 30 04:15:33.798: INFO: Trying to dial the pod
Dec 30 04:15:38.813: INFO: Controller my-hostname-basic-d0bbbf31-78ce-4062-ad20-fcdb2bd7c856: Got expected result from replica 1 [my-hostname-basic-d0bbbf31-78ce-4062-ad20-fcdb2bd7c856-vqrfm]: "my-hostname-basic-d0bbbf31-78ce-4062-ad20-fcdb2bd7c856-vqrfm", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Dec 30 04:15:38.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-908" for this suite. 12/30/22 04:15:38.819
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","completed":230,"skipped":4344,"failed":0}
------------------------------
• [SLOW TEST] [10.076 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:15:28.75
    Dec 30 04:15:28.750: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename replication-controller 12/30/22 04:15:28.752
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:15:28.769
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:15:28.772
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:66
    STEP: Creating replication controller my-hostname-basic-d0bbbf31-78ce-4062-ad20-fcdb2bd7c856 12/30/22 04:15:28.776
    Dec 30 04:15:28.786: INFO: Pod name my-hostname-basic-d0bbbf31-78ce-4062-ad20-fcdb2bd7c856: Found 0 pods out of 1
    Dec 30 04:15:33.793: INFO: Pod name my-hostname-basic-d0bbbf31-78ce-4062-ad20-fcdb2bd7c856: Found 1 pods out of 1
    Dec 30 04:15:33.793: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-d0bbbf31-78ce-4062-ad20-fcdb2bd7c856" are running
    Dec 30 04:15:33.793: INFO: Waiting up to 5m0s for pod "my-hostname-basic-d0bbbf31-78ce-4062-ad20-fcdb2bd7c856-vqrfm" in namespace "replication-controller-908" to be "running"
    Dec 30 04:15:33.797: INFO: Pod "my-hostname-basic-d0bbbf31-78ce-4062-ad20-fcdb2bd7c856-vqrfm": Phase="Running", Reason="", readiness=true. Elapsed: 4.182994ms
    Dec 30 04:15:33.797: INFO: Pod "my-hostname-basic-d0bbbf31-78ce-4062-ad20-fcdb2bd7c856-vqrfm" satisfied condition "running"
    Dec 30 04:15:33.797: INFO: Pod "my-hostname-basic-d0bbbf31-78ce-4062-ad20-fcdb2bd7c856-vqrfm" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-30 04:15:28 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-30 04:15:30 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-30 04:15:30 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-30 04:15:28 +0000 UTC Reason: Message:}])
    Dec 30 04:15:33.798: INFO: Trying to dial the pod
    Dec 30 04:15:38.813: INFO: Controller my-hostname-basic-d0bbbf31-78ce-4062-ad20-fcdb2bd7c856: Got expected result from replica 1 [my-hostname-basic-d0bbbf31-78ce-4062-ad20-fcdb2bd7c856-vqrfm]: "my-hostname-basic-d0bbbf31-78ce-4062-ad20-fcdb2bd7c856-vqrfm", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Dec 30 04:15:38.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-908" for this suite. 12/30/22 04:15:38.819
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:15:38.828
Dec 30 04:15:38.828: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename dns 12/30/22 04:15:38.83
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:15:38.847
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:15:38.851
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 12/30/22 04:15:38.855
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6183 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6183;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6183 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6183;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6183.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6183.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6183.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6183.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6183.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6183.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6183.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6183.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6183.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6183.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6183.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6183.svc;check="$$(dig +notcp +noall +answer +search 199.26.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.26.199_udp@PTR;check="$$(dig +tcp +noall +answer +search 199.26.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.26.199_tcp@PTR;sleep 1; done
 12/30/22 04:15:38.871
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6183 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6183;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6183 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6183;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6183.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6183.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6183.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6183.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6183.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6183.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6183.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6183.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6183.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6183.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6183.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6183.svc;check="$$(dig +notcp +noall +answer +search 199.26.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.26.199_udp@PTR;check="$$(dig +tcp +noall +answer +search 199.26.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.26.199_tcp@PTR;sleep 1; done
 12/30/22 04:15:38.871
STEP: creating a pod to probe DNS 12/30/22 04:15:38.871
STEP: submitting the pod to kubernetes 12/30/22 04:15:38.871
Dec 30 04:15:38.883: INFO: Waiting up to 15m0s for pod "dns-test-fd671a9d-9cd1-403d-9dd5-6aa51bbcb291" in namespace "dns-6183" to be "running"
Dec 30 04:15:38.887: INFO: Pod "dns-test-fd671a9d-9cd1-403d-9dd5-6aa51bbcb291": Phase="Pending", Reason="", readiness=false. Elapsed: 4.430311ms
Dec 30 04:15:40.893: INFO: Pod "dns-test-fd671a9d-9cd1-403d-9dd5-6aa51bbcb291": Phase="Running", Reason="", readiness=true. Elapsed: 2.010214306s
Dec 30 04:15:40.893: INFO: Pod "dns-test-fd671a9d-9cd1-403d-9dd5-6aa51bbcb291" satisfied condition "running"
STEP: retrieving the pod 12/30/22 04:15:40.893
STEP: looking for the results for each expected name from probers 12/30/22 04:15:40.898
Dec 30 04:15:40.904: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6183/dns-test-fd671a9d-9cd1-403d-9dd5-6aa51bbcb291: the server could not find the requested resource (get pods dns-test-fd671a9d-9cd1-403d-9dd5-6aa51bbcb291)
Dec 30 04:15:40.909: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6183/dns-test-fd671a9d-9cd1-403d-9dd5-6aa51bbcb291: the server could not find the requested resource (get pods dns-test-fd671a9d-9cd1-403d-9dd5-6aa51bbcb291)
Dec 30 04:15:40.914: INFO: Unable to read wheezy_udp@dns-test-service.dns-6183 from pod dns-6183/dns-test-fd671a9d-9cd1-403d-9dd5-6aa51bbcb291: the server could not find the requested resource (get pods dns-test-fd671a9d-9cd1-403d-9dd5-6aa51bbcb291)
Dec 30 04:15:40.918: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6183 from pod dns-6183/dns-test-fd671a9d-9cd1-403d-9dd5-6aa51bbcb291: the server could not find the requested resource (get pods dns-test-fd671a9d-9cd1-403d-9dd5-6aa51bbcb291)
Dec 30 04:15:40.923: INFO: Unable to read wheezy_udp@dns-test-service.dns-6183.svc from pod dns-6183/dns-test-fd671a9d-9cd1-403d-9dd5-6aa51bbcb291: the server could not find the requested resource (get pods dns-test-fd671a9d-9cd1-403d-9dd5-6aa51bbcb291)
Dec 30 04:15:40.928: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6183.svc from pod dns-6183/dns-test-fd671a9d-9cd1-403d-9dd5-6aa51bbcb291: the server could not find the requested resource (get pods dns-test-fd671a9d-9cd1-403d-9dd5-6aa51bbcb291)
Dec 30 04:15:40.937: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6183.svc from pod dns-6183/dns-test-fd671a9d-9cd1-403d-9dd5-6aa51bbcb291: the server could not find the requested resource (get pods dns-test-fd671a9d-9cd1-403d-9dd5-6aa51bbcb291)
Dec 30 04:15:40.942: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6183.svc from pod dns-6183/dns-test-fd671a9d-9cd1-403d-9dd5-6aa51bbcb291: the server could not find the requested resource (get pods dns-test-fd671a9d-9cd1-403d-9dd5-6aa51bbcb291)
Dec 30 04:15:40.966: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6183/dns-test-fd671a9d-9cd1-403d-9dd5-6aa51bbcb291: the server could not find the requested resource (get pods dns-test-fd671a9d-9cd1-403d-9dd5-6aa51bbcb291)
Dec 30 04:15:40.970: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6183/dns-test-fd671a9d-9cd1-403d-9dd5-6aa51bbcb291: the server could not find the requested resource (get pods dns-test-fd671a9d-9cd1-403d-9dd5-6aa51bbcb291)
Dec 30 04:15:40.975: INFO: Unable to read jessie_udp@dns-test-service.dns-6183 from pod dns-6183/dns-test-fd671a9d-9cd1-403d-9dd5-6aa51bbcb291: the server could not find the requested resource (get pods dns-test-fd671a9d-9cd1-403d-9dd5-6aa51bbcb291)
Dec 30 04:15:40.980: INFO: Unable to read jessie_tcp@dns-test-service.dns-6183 from pod dns-6183/dns-test-fd671a9d-9cd1-403d-9dd5-6aa51bbcb291: the server could not find the requested resource (get pods dns-test-fd671a9d-9cd1-403d-9dd5-6aa51bbcb291)
Dec 30 04:15:40.985: INFO: Unable to read jessie_udp@dns-test-service.dns-6183.svc from pod dns-6183/dns-test-fd671a9d-9cd1-403d-9dd5-6aa51bbcb291: the server could not find the requested resource (get pods dns-test-fd671a9d-9cd1-403d-9dd5-6aa51bbcb291)
Dec 30 04:15:40.991: INFO: Unable to read jessie_tcp@dns-test-service.dns-6183.svc from pod dns-6183/dns-test-fd671a9d-9cd1-403d-9dd5-6aa51bbcb291: the server could not find the requested resource (get pods dns-test-fd671a9d-9cd1-403d-9dd5-6aa51bbcb291)
Dec 30 04:15:40.998: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6183.svc from pod dns-6183/dns-test-fd671a9d-9cd1-403d-9dd5-6aa51bbcb291: the server could not find the requested resource (get pods dns-test-fd671a9d-9cd1-403d-9dd5-6aa51bbcb291)
Dec 30 04:15:41.004: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6183.svc from pod dns-6183/dns-test-fd671a9d-9cd1-403d-9dd5-6aa51bbcb291: the server could not find the requested resource (get pods dns-test-fd671a9d-9cd1-403d-9dd5-6aa51bbcb291)
Dec 30 04:15:41.029: INFO: Lookups using dns-6183/dns-test-fd671a9d-9cd1-403d-9dd5-6aa51bbcb291 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6183 wheezy_tcp@dns-test-service.dns-6183 wheezy_udp@dns-test-service.dns-6183.svc wheezy_tcp@dns-test-service.dns-6183.svc wheezy_udp@_http._tcp.dns-test-service.dns-6183.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6183.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6183 jessie_tcp@dns-test-service.dns-6183 jessie_udp@dns-test-service.dns-6183.svc jessie_tcp@dns-test-service.dns-6183.svc jessie_udp@_http._tcp.dns-test-service.dns-6183.svc jessie_tcp@_http._tcp.dns-test-service.dns-6183.svc]

Dec 30 04:15:46.149: INFO: DNS probes using dns-6183/dns-test-fd671a9d-9cd1-403d-9dd5-6aa51bbcb291 succeeded

STEP: deleting the pod 12/30/22 04:15:46.149
STEP: deleting the test service 12/30/22 04:15:46.162
STEP: deleting the test headless service 12/30/22 04:15:46.179
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Dec 30 04:15:46.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6183" for this suite. 12/30/22 04:15:46.196
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","completed":231,"skipped":4364,"failed":0}
------------------------------
• [SLOW TEST] [7.375 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:15:38.828
    Dec 30 04:15:38.828: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename dns 12/30/22 04:15:38.83
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:15:38.847
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:15:38.851
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 12/30/22 04:15:38.855
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6183 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6183;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6183 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6183;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6183.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6183.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6183.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6183.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6183.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6183.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6183.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6183.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6183.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6183.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6183.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6183.svc;check="$$(dig +notcp +noall +answer +search 199.26.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.26.199_udp@PTR;check="$$(dig +tcp +noall +answer +search 199.26.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.26.199_tcp@PTR;sleep 1; done
     12/30/22 04:15:38.871
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6183 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6183;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6183 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6183;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6183.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6183.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6183.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6183.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6183.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6183.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6183.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6183.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6183.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6183.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6183.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6183.svc;check="$$(dig +notcp +noall +answer +search 199.26.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.26.199_udp@PTR;check="$$(dig +tcp +noall +answer +search 199.26.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.26.199_tcp@PTR;sleep 1; done
     12/30/22 04:15:38.871
    STEP: creating a pod to probe DNS 12/30/22 04:15:38.871
    STEP: submitting the pod to kubernetes 12/30/22 04:15:38.871
    Dec 30 04:15:38.883: INFO: Waiting up to 15m0s for pod "dns-test-fd671a9d-9cd1-403d-9dd5-6aa51bbcb291" in namespace "dns-6183" to be "running"
    Dec 30 04:15:38.887: INFO: Pod "dns-test-fd671a9d-9cd1-403d-9dd5-6aa51bbcb291": Phase="Pending", Reason="", readiness=false. Elapsed: 4.430311ms
    Dec 30 04:15:40.893: INFO: Pod "dns-test-fd671a9d-9cd1-403d-9dd5-6aa51bbcb291": Phase="Running", Reason="", readiness=true. Elapsed: 2.010214306s
    Dec 30 04:15:40.893: INFO: Pod "dns-test-fd671a9d-9cd1-403d-9dd5-6aa51bbcb291" satisfied condition "running"
    STEP: retrieving the pod 12/30/22 04:15:40.893
    STEP: looking for the results for each expected name from probers 12/30/22 04:15:40.898
    Dec 30 04:15:40.904: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6183/dns-test-fd671a9d-9cd1-403d-9dd5-6aa51bbcb291: the server could not find the requested resource (get pods dns-test-fd671a9d-9cd1-403d-9dd5-6aa51bbcb291)
    Dec 30 04:15:40.909: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6183/dns-test-fd671a9d-9cd1-403d-9dd5-6aa51bbcb291: the server could not find the requested resource (get pods dns-test-fd671a9d-9cd1-403d-9dd5-6aa51bbcb291)
    Dec 30 04:15:40.914: INFO: Unable to read wheezy_udp@dns-test-service.dns-6183 from pod dns-6183/dns-test-fd671a9d-9cd1-403d-9dd5-6aa51bbcb291: the server could not find the requested resource (get pods dns-test-fd671a9d-9cd1-403d-9dd5-6aa51bbcb291)
    Dec 30 04:15:40.918: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6183 from pod dns-6183/dns-test-fd671a9d-9cd1-403d-9dd5-6aa51bbcb291: the server could not find the requested resource (get pods dns-test-fd671a9d-9cd1-403d-9dd5-6aa51bbcb291)
    Dec 30 04:15:40.923: INFO: Unable to read wheezy_udp@dns-test-service.dns-6183.svc from pod dns-6183/dns-test-fd671a9d-9cd1-403d-9dd5-6aa51bbcb291: the server could not find the requested resource (get pods dns-test-fd671a9d-9cd1-403d-9dd5-6aa51bbcb291)
    Dec 30 04:15:40.928: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6183.svc from pod dns-6183/dns-test-fd671a9d-9cd1-403d-9dd5-6aa51bbcb291: the server could not find the requested resource (get pods dns-test-fd671a9d-9cd1-403d-9dd5-6aa51bbcb291)
    Dec 30 04:15:40.937: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6183.svc from pod dns-6183/dns-test-fd671a9d-9cd1-403d-9dd5-6aa51bbcb291: the server could not find the requested resource (get pods dns-test-fd671a9d-9cd1-403d-9dd5-6aa51bbcb291)
    Dec 30 04:15:40.942: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6183.svc from pod dns-6183/dns-test-fd671a9d-9cd1-403d-9dd5-6aa51bbcb291: the server could not find the requested resource (get pods dns-test-fd671a9d-9cd1-403d-9dd5-6aa51bbcb291)
    Dec 30 04:15:40.966: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6183/dns-test-fd671a9d-9cd1-403d-9dd5-6aa51bbcb291: the server could not find the requested resource (get pods dns-test-fd671a9d-9cd1-403d-9dd5-6aa51bbcb291)
    Dec 30 04:15:40.970: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6183/dns-test-fd671a9d-9cd1-403d-9dd5-6aa51bbcb291: the server could not find the requested resource (get pods dns-test-fd671a9d-9cd1-403d-9dd5-6aa51bbcb291)
    Dec 30 04:15:40.975: INFO: Unable to read jessie_udp@dns-test-service.dns-6183 from pod dns-6183/dns-test-fd671a9d-9cd1-403d-9dd5-6aa51bbcb291: the server could not find the requested resource (get pods dns-test-fd671a9d-9cd1-403d-9dd5-6aa51bbcb291)
    Dec 30 04:15:40.980: INFO: Unable to read jessie_tcp@dns-test-service.dns-6183 from pod dns-6183/dns-test-fd671a9d-9cd1-403d-9dd5-6aa51bbcb291: the server could not find the requested resource (get pods dns-test-fd671a9d-9cd1-403d-9dd5-6aa51bbcb291)
    Dec 30 04:15:40.985: INFO: Unable to read jessie_udp@dns-test-service.dns-6183.svc from pod dns-6183/dns-test-fd671a9d-9cd1-403d-9dd5-6aa51bbcb291: the server could not find the requested resource (get pods dns-test-fd671a9d-9cd1-403d-9dd5-6aa51bbcb291)
    Dec 30 04:15:40.991: INFO: Unable to read jessie_tcp@dns-test-service.dns-6183.svc from pod dns-6183/dns-test-fd671a9d-9cd1-403d-9dd5-6aa51bbcb291: the server could not find the requested resource (get pods dns-test-fd671a9d-9cd1-403d-9dd5-6aa51bbcb291)
    Dec 30 04:15:40.998: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6183.svc from pod dns-6183/dns-test-fd671a9d-9cd1-403d-9dd5-6aa51bbcb291: the server could not find the requested resource (get pods dns-test-fd671a9d-9cd1-403d-9dd5-6aa51bbcb291)
    Dec 30 04:15:41.004: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6183.svc from pod dns-6183/dns-test-fd671a9d-9cd1-403d-9dd5-6aa51bbcb291: the server could not find the requested resource (get pods dns-test-fd671a9d-9cd1-403d-9dd5-6aa51bbcb291)
    Dec 30 04:15:41.029: INFO: Lookups using dns-6183/dns-test-fd671a9d-9cd1-403d-9dd5-6aa51bbcb291 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6183 wheezy_tcp@dns-test-service.dns-6183 wheezy_udp@dns-test-service.dns-6183.svc wheezy_tcp@dns-test-service.dns-6183.svc wheezy_udp@_http._tcp.dns-test-service.dns-6183.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6183.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6183 jessie_tcp@dns-test-service.dns-6183 jessie_udp@dns-test-service.dns-6183.svc jessie_tcp@dns-test-service.dns-6183.svc jessie_udp@_http._tcp.dns-test-service.dns-6183.svc jessie_tcp@_http._tcp.dns-test-service.dns-6183.svc]

    Dec 30 04:15:46.149: INFO: DNS probes using dns-6183/dns-test-fd671a9d-9cd1-403d-9dd5-6aa51bbcb291 succeeded

    STEP: deleting the pod 12/30/22 04:15:46.149
    STEP: deleting the test service 12/30/22 04:15:46.162
    STEP: deleting the test headless service 12/30/22 04:15:46.179
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Dec 30 04:15:46.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-6183" for this suite. 12/30/22 04:15:46.196
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:15:46.204
Dec 30 04:15:46.204: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename downward-api 12/30/22 04:15:46.206
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:15:46.222
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:15:46.226
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206
STEP: Creating a pod to test downward API volume plugin 12/30/22 04:15:46.23
Dec 30 04:15:46.240: INFO: Waiting up to 5m0s for pod "downwardapi-volume-66f4acb4-740a-4e4c-b96f-165c693a692f" in namespace "downward-api-7756" to be "Succeeded or Failed"
Dec 30 04:15:46.244: INFO: Pod "downwardapi-volume-66f4acb4-740a-4e4c-b96f-165c693a692f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.341929ms
Dec 30 04:15:48.249: INFO: Pod "downwardapi-volume-66f4acb4-740a-4e4c-b96f-165c693a692f": Phase="Running", Reason="", readiness=false. Elapsed: 2.009492969s
Dec 30 04:15:50.250: INFO: Pod "downwardapi-volume-66f4acb4-740a-4e4c-b96f-165c693a692f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010045187s
STEP: Saw pod success 12/30/22 04:15:50.25
Dec 30 04:15:50.250: INFO: Pod "downwardapi-volume-66f4acb4-740a-4e4c-b96f-165c693a692f" satisfied condition "Succeeded or Failed"
Dec 30 04:15:50.255: INFO: Trying to get logs from node k8s-mgmt01 pod downwardapi-volume-66f4acb4-740a-4e4c-b96f-165c693a692f container client-container: <nil>
STEP: delete the pod 12/30/22 04:15:50.263
Dec 30 04:15:50.273: INFO: Waiting for pod downwardapi-volume-66f4acb4-740a-4e4c-b96f-165c693a692f to disappear
Dec 30 04:15:50.277: INFO: Pod downwardapi-volume-66f4acb4-740a-4e4c-b96f-165c693a692f no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Dec 30 04:15:50.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7756" for this suite. 12/30/22 04:15:50.283
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","completed":232,"skipped":4370,"failed":0}
------------------------------
• [4.086 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:15:46.204
    Dec 30 04:15:46.204: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename downward-api 12/30/22 04:15:46.206
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:15:46.222
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:15:46.226
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:206
    STEP: Creating a pod to test downward API volume plugin 12/30/22 04:15:46.23
    Dec 30 04:15:46.240: INFO: Waiting up to 5m0s for pod "downwardapi-volume-66f4acb4-740a-4e4c-b96f-165c693a692f" in namespace "downward-api-7756" to be "Succeeded or Failed"
    Dec 30 04:15:46.244: INFO: Pod "downwardapi-volume-66f4acb4-740a-4e4c-b96f-165c693a692f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.341929ms
    Dec 30 04:15:48.249: INFO: Pod "downwardapi-volume-66f4acb4-740a-4e4c-b96f-165c693a692f": Phase="Running", Reason="", readiness=false. Elapsed: 2.009492969s
    Dec 30 04:15:50.250: INFO: Pod "downwardapi-volume-66f4acb4-740a-4e4c-b96f-165c693a692f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010045187s
    STEP: Saw pod success 12/30/22 04:15:50.25
    Dec 30 04:15:50.250: INFO: Pod "downwardapi-volume-66f4acb4-740a-4e4c-b96f-165c693a692f" satisfied condition "Succeeded or Failed"
    Dec 30 04:15:50.255: INFO: Trying to get logs from node k8s-mgmt01 pod downwardapi-volume-66f4acb4-740a-4e4c-b96f-165c693a692f container client-container: <nil>
    STEP: delete the pod 12/30/22 04:15:50.263
    Dec 30 04:15:50.273: INFO: Waiting for pod downwardapi-volume-66f4acb4-740a-4e4c-b96f-165c693a692f to disappear
    Dec 30 04:15:50.277: INFO: Pod downwardapi-volume-66f4acb4-740a-4e4c-b96f-165c693a692f no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Dec 30 04:15:50.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-7756" for this suite. 12/30/22 04:15:50.283
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:15:50.293
Dec 30 04:15:50.293: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename runtimeclass 12/30/22 04:15:50.294
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:15:50.317
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:15:50.321
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
Dec 30 04:15:50.339: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-1119 to be scheduled
Dec 30 04:15:50.343: INFO: 1 pods are not scheduled: [runtimeclass-1119/test-runtimeclass-runtimeclass-1119-preconfigured-handler-tfbd7(d135ecaf-582b-4c0a-8413-696ef73307b5)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Dec 30 04:15:52.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-1119" for this suite. 12/30/22 04:15:52.363
{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]","completed":233,"skipped":4393,"failed":0}
------------------------------
• [2.077 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:15:50.293
    Dec 30 04:15:50.293: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename runtimeclass 12/30/22 04:15:50.294
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:15:50.317
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:15:50.321
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    Dec 30 04:15:50.339: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-1119 to be scheduled
    Dec 30 04:15:50.343: INFO: 1 pods are not scheduled: [runtimeclass-1119/test-runtimeclass-runtimeclass-1119-preconfigured-handler-tfbd7(d135ecaf-582b-4c0a-8413-696ef73307b5)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Dec 30 04:15:52.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-1119" for this suite. 12/30/22 04:15:52.363
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:15:52.371
Dec 30 04:15:52.371: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename services 12/30/22 04:15:52.373
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:15:52.39
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:15:52.393
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206
STEP: fetching services 12/30/22 04:15:52.397
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Dec 30 04:15:52.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7976" for this suite. 12/30/22 04:15:52.408
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","completed":234,"skipped":4411,"failed":0}
------------------------------
• [0.043 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:15:52.371
    Dec 30 04:15:52.371: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename services 12/30/22 04:15:52.373
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:15:52.39
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:15:52.393
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3206
    STEP: fetching services 12/30/22 04:15:52.397
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Dec 30 04:15:52.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-7976" for this suite. 12/30/22 04:15:52.408
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:781
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:15:52.417
Dec 30 04:15:52.418: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename services 12/30/22 04:15:52.419
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:15:52.435
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:15:52.439
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:781
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Dec 30 04:15:52.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-409" for this suite. 12/30/22 04:15:52.452
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","completed":235,"skipped":4456,"failed":0}
------------------------------
• [0.041 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:781

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:15:52.417
    Dec 30 04:15:52.418: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename services 12/30/22 04:15:52.419
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:15:52.435
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:15:52.439
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:781
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Dec 30 04:15:52.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-409" for this suite. 12/30/22 04:15:52.452
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:15:52.459
Dec 30 04:15:52.459: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename projected 12/30/22 04:15:52.461
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:15:52.477
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:15:52.481
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108
STEP: Creating configMap with name projected-configmap-test-volume-map-da91ff11-5276-43a1-839a-377df17f75ed 12/30/22 04:15:52.485
STEP: Creating a pod to test consume configMaps 12/30/22 04:15:52.489
Dec 30 04:15:52.499: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1adddcab-d28d-4a42-baf1-0f9906eeb248" in namespace "projected-5434" to be "Succeeded or Failed"
Dec 30 04:15:52.503: INFO: Pod "pod-projected-configmaps-1adddcab-d28d-4a42-baf1-0f9906eeb248": Phase="Pending", Reason="", readiness=false. Elapsed: 3.490356ms
Dec 30 04:15:54.509: INFO: Pod "pod-projected-configmaps-1adddcab-d28d-4a42-baf1-0f9906eeb248": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009878425s
Dec 30 04:15:56.508: INFO: Pod "pod-projected-configmaps-1adddcab-d28d-4a42-baf1-0f9906eeb248": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008964792s
STEP: Saw pod success 12/30/22 04:15:56.508
Dec 30 04:15:56.509: INFO: Pod "pod-projected-configmaps-1adddcab-d28d-4a42-baf1-0f9906eeb248" satisfied condition "Succeeded or Failed"
Dec 30 04:15:56.513: INFO: Trying to get logs from node k8s-mgmt02 pod pod-projected-configmaps-1adddcab-d28d-4a42-baf1-0f9906eeb248 container agnhost-container: <nil>
STEP: delete the pod 12/30/22 04:15:56.534
Dec 30 04:15:56.549: INFO: Waiting for pod pod-projected-configmaps-1adddcab-d28d-4a42-baf1-0f9906eeb248 to disappear
Dec 30 04:15:56.553: INFO: Pod pod-projected-configmaps-1adddcab-d28d-4a42-baf1-0f9906eeb248 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Dec 30 04:15:56.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5434" for this suite. 12/30/22 04:15:56.559
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","completed":236,"skipped":4465,"failed":0}
------------------------------
• [4.106 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:15:52.459
    Dec 30 04:15:52.459: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename projected 12/30/22 04:15:52.461
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:15:52.477
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:15:52.481
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:108
    STEP: Creating configMap with name projected-configmap-test-volume-map-da91ff11-5276-43a1-839a-377df17f75ed 12/30/22 04:15:52.485
    STEP: Creating a pod to test consume configMaps 12/30/22 04:15:52.489
    Dec 30 04:15:52.499: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1adddcab-d28d-4a42-baf1-0f9906eeb248" in namespace "projected-5434" to be "Succeeded or Failed"
    Dec 30 04:15:52.503: INFO: Pod "pod-projected-configmaps-1adddcab-d28d-4a42-baf1-0f9906eeb248": Phase="Pending", Reason="", readiness=false. Elapsed: 3.490356ms
    Dec 30 04:15:54.509: INFO: Pod "pod-projected-configmaps-1adddcab-d28d-4a42-baf1-0f9906eeb248": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009878425s
    Dec 30 04:15:56.508: INFO: Pod "pod-projected-configmaps-1adddcab-d28d-4a42-baf1-0f9906eeb248": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008964792s
    STEP: Saw pod success 12/30/22 04:15:56.508
    Dec 30 04:15:56.509: INFO: Pod "pod-projected-configmaps-1adddcab-d28d-4a42-baf1-0f9906eeb248" satisfied condition "Succeeded or Failed"
    Dec 30 04:15:56.513: INFO: Trying to get logs from node k8s-mgmt02 pod pod-projected-configmaps-1adddcab-d28d-4a42-baf1-0f9906eeb248 container agnhost-container: <nil>
    STEP: delete the pod 12/30/22 04:15:56.534
    Dec 30 04:15:56.549: INFO: Waiting for pod pod-projected-configmaps-1adddcab-d28d-4a42-baf1-0f9906eeb248 to disappear
    Dec 30 04:15:56.553: INFO: Pod pod-projected-configmaps-1adddcab-d28d-4a42-baf1-0f9906eeb248 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Dec 30 04:15:56.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5434" for this suite. 12/30/22 04:15:56.559
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:15:56.565
Dec 30 04:15:56.566: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename services 12/30/22 04:15:56.567
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:15:56.584
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:15:56.588
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404
STEP: creating a service externalname-service with the type=ExternalName in namespace services-8110 12/30/22 04:15:56.592
STEP: changing the ExternalName service to type=ClusterIP 12/30/22 04:15:56.598
STEP: creating replication controller externalname-service in namespace services-8110 12/30/22 04:15:56.611
I1230 04:15:56.617747      25 runners.go:193] Created replication controller with name: externalname-service, namespace: services-8110, replica count: 2
I1230 04:15:59.669117      25 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 30 04:15:59.669: INFO: Creating new exec pod
Dec 30 04:15:59.678: INFO: Waiting up to 5m0s for pod "execpodjhqcj" in namespace "services-8110" to be "running"
Dec 30 04:15:59.682: INFO: Pod "execpodjhqcj": Phase="Pending", Reason="", readiness=false. Elapsed: 3.994191ms
Dec 30 04:16:01.688: INFO: Pod "execpodjhqcj": Phase="Running", Reason="", readiness=true. Elapsed: 2.009964408s
Dec 30 04:16:01.688: INFO: Pod "execpodjhqcj" satisfied condition "running"
Dec 30 04:16:02.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-8110 exec execpodjhqcj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Dec 30 04:16:02.933: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Dec 30 04:16:02.933: INFO: stdout: "externalname-service-88jlp"
Dec 30 04:16:02.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-8110 exec execpodjhqcj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.3.125 80'
Dec 30 04:16:03.134: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.3.125 80\nConnection to 10.233.3.125 80 port [tcp/http] succeeded!\n"
Dec 30 04:16:03.134: INFO: stdout: ""
Dec 30 04:16:04.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-8110 exec execpodjhqcj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.3.125 80'
Dec 30 04:16:04.345: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.3.125 80\nConnection to 10.233.3.125 80 port [tcp/http] succeeded!\n"
Dec 30 04:16:04.345: INFO: stdout: ""
Dec 30 04:16:05.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-8110 exec execpodjhqcj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.3.125 80'
Dec 30 04:16:05.342: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.3.125 80\nConnection to 10.233.3.125 80 port [tcp/http] succeeded!\n"
Dec 30 04:16:05.342: INFO: stdout: "externalname-service-tsq2n"
Dec 30 04:16:05.342: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Dec 30 04:16:05.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8110" for this suite. 12/30/22 04:16:05.364
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","completed":237,"skipped":4465,"failed":0}
------------------------------
• [SLOW TEST] [8.806 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:15:56.565
    Dec 30 04:15:56.566: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename services 12/30/22 04:15:56.567
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:15:56.584
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:15:56.588
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1404
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-8110 12/30/22 04:15:56.592
    STEP: changing the ExternalName service to type=ClusterIP 12/30/22 04:15:56.598
    STEP: creating replication controller externalname-service in namespace services-8110 12/30/22 04:15:56.611
    I1230 04:15:56.617747      25 runners.go:193] Created replication controller with name: externalname-service, namespace: services-8110, replica count: 2
    I1230 04:15:59.669117      25 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Dec 30 04:15:59.669: INFO: Creating new exec pod
    Dec 30 04:15:59.678: INFO: Waiting up to 5m0s for pod "execpodjhqcj" in namespace "services-8110" to be "running"
    Dec 30 04:15:59.682: INFO: Pod "execpodjhqcj": Phase="Pending", Reason="", readiness=false. Elapsed: 3.994191ms
    Dec 30 04:16:01.688: INFO: Pod "execpodjhqcj": Phase="Running", Reason="", readiness=true. Elapsed: 2.009964408s
    Dec 30 04:16:01.688: INFO: Pod "execpodjhqcj" satisfied condition "running"
    Dec 30 04:16:02.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-8110 exec execpodjhqcj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Dec 30 04:16:02.933: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Dec 30 04:16:02.933: INFO: stdout: "externalname-service-88jlp"
    Dec 30 04:16:02.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-8110 exec execpodjhqcj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.3.125 80'
    Dec 30 04:16:03.134: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.3.125 80\nConnection to 10.233.3.125 80 port [tcp/http] succeeded!\n"
    Dec 30 04:16:03.134: INFO: stdout: ""
    Dec 30 04:16:04.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-8110 exec execpodjhqcj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.3.125 80'
    Dec 30 04:16:04.345: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.3.125 80\nConnection to 10.233.3.125 80 port [tcp/http] succeeded!\n"
    Dec 30 04:16:04.345: INFO: stdout: ""
    Dec 30 04:16:05.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-8110 exec execpodjhqcj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.3.125 80'
    Dec 30 04:16:05.342: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.3.125 80\nConnection to 10.233.3.125 80 port [tcp/http] succeeded!\n"
    Dec 30 04:16:05.342: INFO: stdout: "externalname-service-tsq2n"
    Dec 30 04:16:05.342: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Dec 30 04:16:05.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-8110" for this suite. 12/30/22 04:16:05.364
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:16:05.374
Dec 30 04:16:05.374: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename secrets 12/30/22 04:16:05.375
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:16:05.392
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:16:05.396
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46
STEP: Creating secret with name secret-test-8fa95097-6cc9-4f97-a767-2ab3c82d1c80 12/30/22 04:16:05.399
STEP: Creating a pod to test consume secrets 12/30/22 04:16:05.405
Dec 30 04:16:05.415: INFO: Waiting up to 5m0s for pod "pod-secrets-6de3d328-1fde-4065-97a6-978ed019b3b6" in namespace "secrets-3696" to be "Succeeded or Failed"
Dec 30 04:16:05.419: INFO: Pod "pod-secrets-6de3d328-1fde-4065-97a6-978ed019b3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03352ms
Dec 30 04:16:07.423: INFO: Pod "pod-secrets-6de3d328-1fde-4065-97a6-978ed019b3b6": Phase="Running", Reason="", readiness=false. Elapsed: 2.008811043s
Dec 30 04:16:09.425: INFO: Pod "pod-secrets-6de3d328-1fde-4065-97a6-978ed019b3b6": Phase="Running", Reason="", readiness=false. Elapsed: 4.010562699s
Dec 30 04:16:11.424: INFO: Pod "pod-secrets-6de3d328-1fde-4065-97a6-978ed019b3b6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008903057s
STEP: Saw pod success 12/30/22 04:16:11.424
Dec 30 04:16:11.424: INFO: Pod "pod-secrets-6de3d328-1fde-4065-97a6-978ed019b3b6" satisfied condition "Succeeded or Failed"
Dec 30 04:16:11.428: INFO: Trying to get logs from node k8s-mgmt02 pod pod-secrets-6de3d328-1fde-4065-97a6-978ed019b3b6 container secret-volume-test: <nil>
STEP: delete the pod 12/30/22 04:16:11.436
Dec 30 04:16:11.449: INFO: Waiting for pod pod-secrets-6de3d328-1fde-4065-97a6-978ed019b3b6 to disappear
Dec 30 04:16:11.453: INFO: Pod pod-secrets-6de3d328-1fde-4065-97a6-978ed019b3b6 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Dec 30 04:16:11.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3696" for this suite. 12/30/22 04:16:11.46
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","completed":238,"skipped":4493,"failed":0}
------------------------------
• [SLOW TEST] [6.093 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:16:05.374
    Dec 30 04:16:05.374: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename secrets 12/30/22 04:16:05.375
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:16:05.392
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:16:05.396
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:46
    STEP: Creating secret with name secret-test-8fa95097-6cc9-4f97-a767-2ab3c82d1c80 12/30/22 04:16:05.399
    STEP: Creating a pod to test consume secrets 12/30/22 04:16:05.405
    Dec 30 04:16:05.415: INFO: Waiting up to 5m0s for pod "pod-secrets-6de3d328-1fde-4065-97a6-978ed019b3b6" in namespace "secrets-3696" to be "Succeeded or Failed"
    Dec 30 04:16:05.419: INFO: Pod "pod-secrets-6de3d328-1fde-4065-97a6-978ed019b3b6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03352ms
    Dec 30 04:16:07.423: INFO: Pod "pod-secrets-6de3d328-1fde-4065-97a6-978ed019b3b6": Phase="Running", Reason="", readiness=false. Elapsed: 2.008811043s
    Dec 30 04:16:09.425: INFO: Pod "pod-secrets-6de3d328-1fde-4065-97a6-978ed019b3b6": Phase="Running", Reason="", readiness=false. Elapsed: 4.010562699s
    Dec 30 04:16:11.424: INFO: Pod "pod-secrets-6de3d328-1fde-4065-97a6-978ed019b3b6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008903057s
    STEP: Saw pod success 12/30/22 04:16:11.424
    Dec 30 04:16:11.424: INFO: Pod "pod-secrets-6de3d328-1fde-4065-97a6-978ed019b3b6" satisfied condition "Succeeded or Failed"
    Dec 30 04:16:11.428: INFO: Trying to get logs from node k8s-mgmt02 pod pod-secrets-6de3d328-1fde-4065-97a6-978ed019b3b6 container secret-volume-test: <nil>
    STEP: delete the pod 12/30/22 04:16:11.436
    Dec 30 04:16:11.449: INFO: Waiting for pod pod-secrets-6de3d328-1fde-4065-97a6-978ed019b3b6 to disappear
    Dec 30 04:16:11.453: INFO: Pod pod-secrets-6de3d328-1fde-4065-97a6-978ed019b3b6 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Dec 30 04:16:11.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-3696" for this suite. 12/30/22 04:16:11.46
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:16:11.467
Dec 30 04:16:11.467: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename emptydir 12/30/22 04:16:11.469
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:16:11.484
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:16:11.488
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176
STEP: Creating a pod to test emptydir 0666 on node default medium 12/30/22 04:16:11.491
Dec 30 04:16:11.501: INFO: Waiting up to 5m0s for pod "pod-e7cbfe53-f1d6-4936-b235-780a49ebc866" in namespace "emptydir-5467" to be "Succeeded or Failed"
Dec 30 04:16:11.504: INFO: Pod "pod-e7cbfe53-f1d6-4936-b235-780a49ebc866": Phase="Pending", Reason="", readiness=false. Elapsed: 3.606636ms
Dec 30 04:16:13.510: INFO: Pod "pod-e7cbfe53-f1d6-4936-b235-780a49ebc866": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009682664s
Dec 30 04:16:15.509: INFO: Pod "pod-e7cbfe53-f1d6-4936-b235-780a49ebc866": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008390257s
STEP: Saw pod success 12/30/22 04:16:15.509
Dec 30 04:16:15.509: INFO: Pod "pod-e7cbfe53-f1d6-4936-b235-780a49ebc866" satisfied condition "Succeeded or Failed"
Dec 30 04:16:15.514: INFO: Trying to get logs from node k8s-mgmt01 pod pod-e7cbfe53-f1d6-4936-b235-780a49ebc866 container test-container: <nil>
STEP: delete the pod 12/30/22 04:16:15.523
Dec 30 04:16:15.537: INFO: Waiting for pod pod-e7cbfe53-f1d6-4936-b235-780a49ebc866 to disappear
Dec 30 04:16:15.540: INFO: Pod pod-e7cbfe53-f1d6-4936-b235-780a49ebc866 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Dec 30 04:16:15.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5467" for this suite. 12/30/22 04:16:15.546
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":239,"skipped":4493,"failed":0}
------------------------------
• [4.086 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:16:11.467
    Dec 30 04:16:11.467: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename emptydir 12/30/22 04:16:11.469
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:16:11.484
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:16:11.488
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:176
    STEP: Creating a pod to test emptydir 0666 on node default medium 12/30/22 04:16:11.491
    Dec 30 04:16:11.501: INFO: Waiting up to 5m0s for pod "pod-e7cbfe53-f1d6-4936-b235-780a49ebc866" in namespace "emptydir-5467" to be "Succeeded or Failed"
    Dec 30 04:16:11.504: INFO: Pod "pod-e7cbfe53-f1d6-4936-b235-780a49ebc866": Phase="Pending", Reason="", readiness=false. Elapsed: 3.606636ms
    Dec 30 04:16:13.510: INFO: Pod "pod-e7cbfe53-f1d6-4936-b235-780a49ebc866": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009682664s
    Dec 30 04:16:15.509: INFO: Pod "pod-e7cbfe53-f1d6-4936-b235-780a49ebc866": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008390257s
    STEP: Saw pod success 12/30/22 04:16:15.509
    Dec 30 04:16:15.509: INFO: Pod "pod-e7cbfe53-f1d6-4936-b235-780a49ebc866" satisfied condition "Succeeded or Failed"
    Dec 30 04:16:15.514: INFO: Trying to get logs from node k8s-mgmt01 pod pod-e7cbfe53-f1d6-4936-b235-780a49ebc866 container test-container: <nil>
    STEP: delete the pod 12/30/22 04:16:15.523
    Dec 30 04:16:15.537: INFO: Waiting for pod pod-e7cbfe53-f1d6-4936-b235-780a49ebc866 to disappear
    Dec 30 04:16:15.540: INFO: Pod pod-e7cbfe53-f1d6-4936-b235-780a49ebc866 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Dec 30 04:16:15.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-5467" for this suite. 12/30/22 04:16:15.546
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:16:15.554
Dec 30 04:16:15.555: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename resourcequota 12/30/22 04:16:15.556
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:16:15.572
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:16:15.575
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680
STEP: Creating a ResourceQuota with terminating scope 12/30/22 04:16:15.579
STEP: Ensuring ResourceQuota status is calculated 12/30/22 04:16:15.584
STEP: Creating a ResourceQuota with not terminating scope 12/30/22 04:16:17.589
STEP: Ensuring ResourceQuota status is calculated 12/30/22 04:16:17.594
STEP: Creating a long running pod 12/30/22 04:16:19.6
STEP: Ensuring resource quota with not terminating scope captures the pod usage 12/30/22 04:16:19.616
STEP: Ensuring resource quota with terminating scope ignored the pod usage 12/30/22 04:16:21.621
STEP: Deleting the pod 12/30/22 04:16:23.626
STEP: Ensuring resource quota status released the pod usage 12/30/22 04:16:23.637
STEP: Creating a terminating pod 12/30/22 04:16:25.641
STEP: Ensuring resource quota with terminating scope captures the pod usage 12/30/22 04:16:25.653
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 12/30/22 04:16:27.66
STEP: Deleting the pod 12/30/22 04:16:29.665
STEP: Ensuring resource quota status released the pod usage 12/30/22 04:16:29.677
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Dec 30 04:16:31.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2866" for this suite. 12/30/22 04:16:31.69
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","completed":240,"skipped":4501,"failed":0}
------------------------------
• [SLOW TEST] [16.144 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:16:15.554
    Dec 30 04:16:15.555: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename resourcequota 12/30/22 04:16:15.556
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:16:15.572
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:16:15.575
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:680
    STEP: Creating a ResourceQuota with terminating scope 12/30/22 04:16:15.579
    STEP: Ensuring ResourceQuota status is calculated 12/30/22 04:16:15.584
    STEP: Creating a ResourceQuota with not terminating scope 12/30/22 04:16:17.589
    STEP: Ensuring ResourceQuota status is calculated 12/30/22 04:16:17.594
    STEP: Creating a long running pod 12/30/22 04:16:19.6
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 12/30/22 04:16:19.616
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 12/30/22 04:16:21.621
    STEP: Deleting the pod 12/30/22 04:16:23.626
    STEP: Ensuring resource quota status released the pod usage 12/30/22 04:16:23.637
    STEP: Creating a terminating pod 12/30/22 04:16:25.641
    STEP: Ensuring resource quota with terminating scope captures the pod usage 12/30/22 04:16:25.653
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 12/30/22 04:16:27.66
    STEP: Deleting the pod 12/30/22 04:16:29.665
    STEP: Ensuring resource quota status released the pod usage 12/30/22 04:16:29.677
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Dec 30 04:16:31.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-2866" for this suite. 12/30/22 04:16:31.69
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:16:31.699
Dec 30 04:16:31.699: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename webhook 12/30/22 04:16:31.7
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:16:31.718
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:16:31.722
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 12/30/22 04:16:31.742
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/30/22 04:16:32.126
STEP: Deploying the webhook pod 12/30/22 04:16:32.135
STEP: Wait for the deployment to be ready 12/30/22 04:16:32.148
Dec 30 04:16:32.156: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 12/30/22 04:16:34.172
STEP: Verifying the service has paired with the endpoint 12/30/22 04:16:34.183
Dec 30 04:16:35.184: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581
STEP: Listing all of the created validation webhooks 12/30/22 04:16:35.249
STEP: Creating a configMap that does not comply to the validation webhook rules 12/30/22 04:16:35.284
STEP: Deleting the collection of validation webhooks 12/30/22 04:16:35.317
STEP: Creating a configMap that does not comply to the validation webhook rules 12/30/22 04:16:35.371
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Dec 30 04:16:35.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5636" for this suite. 12/30/22 04:16:35.388
STEP: Destroying namespace "webhook-5636-markers" for this suite. 12/30/22 04:16:35.395
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","completed":241,"skipped":4503,"failed":0}
------------------------------
• [3.741 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:16:31.699
    Dec 30 04:16:31.699: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename webhook 12/30/22 04:16:31.7
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:16:31.718
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:16:31.722
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 12/30/22 04:16:31.742
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/30/22 04:16:32.126
    STEP: Deploying the webhook pod 12/30/22 04:16:32.135
    STEP: Wait for the deployment to be ready 12/30/22 04:16:32.148
    Dec 30 04:16:32.156: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 12/30/22 04:16:34.172
    STEP: Verifying the service has paired with the endpoint 12/30/22 04:16:34.183
    Dec 30 04:16:35.184: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:581
    STEP: Listing all of the created validation webhooks 12/30/22 04:16:35.249
    STEP: Creating a configMap that does not comply to the validation webhook rules 12/30/22 04:16:35.284
    STEP: Deleting the collection of validation webhooks 12/30/22 04:16:35.317
    STEP: Creating a configMap that does not comply to the validation webhook rules 12/30/22 04:16:35.371
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Dec 30 04:16:35.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-5636" for this suite. 12/30/22 04:16:35.388
    STEP: Destroying namespace "webhook-5636-markers" for this suite. 12/30/22 04:16:35.395
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:16:35.44
Dec 30 04:16:35.440: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename emptydir 12/30/22 04:16:35.442
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:16:35.46
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:16:35.464
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186
STEP: Creating a pod to test emptydir 0777 on node default medium 12/30/22 04:16:35.467
Dec 30 04:16:35.477: INFO: Waiting up to 5m0s for pod "pod-14582e76-1519-4cfd-a9d1-23113f1f4d5f" in namespace "emptydir-8" to be "Succeeded or Failed"
Dec 30 04:16:35.480: INFO: Pod "pod-14582e76-1519-4cfd-a9d1-23113f1f4d5f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.420478ms
Dec 30 04:16:37.486: INFO: Pod "pod-14582e76-1519-4cfd-a9d1-23113f1f4d5f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009314844s
Dec 30 04:16:39.486: INFO: Pod "pod-14582e76-1519-4cfd-a9d1-23113f1f4d5f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009450302s
STEP: Saw pod success 12/30/22 04:16:39.486
Dec 30 04:16:39.486: INFO: Pod "pod-14582e76-1519-4cfd-a9d1-23113f1f4d5f" satisfied condition "Succeeded or Failed"
Dec 30 04:16:39.490: INFO: Trying to get logs from node k8s-mgmt01 pod pod-14582e76-1519-4cfd-a9d1-23113f1f4d5f container test-container: <nil>
STEP: delete the pod 12/30/22 04:16:39.498
Dec 30 04:16:39.510: INFO: Waiting for pod pod-14582e76-1519-4cfd-a9d1-23113f1f4d5f to disappear
Dec 30 04:16:39.514: INFO: Pod pod-14582e76-1519-4cfd-a9d1-23113f1f4d5f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Dec 30 04:16:39.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8" for this suite. 12/30/22 04:16:39.52
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":242,"skipped":4506,"failed":0}
------------------------------
• [4.086 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:16:35.44
    Dec 30 04:16:35.440: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename emptydir 12/30/22 04:16:35.442
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:16:35.46
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:16:35.464
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:186
    STEP: Creating a pod to test emptydir 0777 on node default medium 12/30/22 04:16:35.467
    Dec 30 04:16:35.477: INFO: Waiting up to 5m0s for pod "pod-14582e76-1519-4cfd-a9d1-23113f1f4d5f" in namespace "emptydir-8" to be "Succeeded or Failed"
    Dec 30 04:16:35.480: INFO: Pod "pod-14582e76-1519-4cfd-a9d1-23113f1f4d5f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.420478ms
    Dec 30 04:16:37.486: INFO: Pod "pod-14582e76-1519-4cfd-a9d1-23113f1f4d5f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009314844s
    Dec 30 04:16:39.486: INFO: Pod "pod-14582e76-1519-4cfd-a9d1-23113f1f4d5f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009450302s
    STEP: Saw pod success 12/30/22 04:16:39.486
    Dec 30 04:16:39.486: INFO: Pod "pod-14582e76-1519-4cfd-a9d1-23113f1f4d5f" satisfied condition "Succeeded or Failed"
    Dec 30 04:16:39.490: INFO: Trying to get logs from node k8s-mgmt01 pod pod-14582e76-1519-4cfd-a9d1-23113f1f4d5f container test-container: <nil>
    STEP: delete the pod 12/30/22 04:16:39.498
    Dec 30 04:16:39.510: INFO: Waiting for pod pod-14582e76-1519-4cfd-a9d1-23113f1f4d5f to disappear
    Dec 30 04:16:39.514: INFO: Pod pod-14582e76-1519-4cfd-a9d1-23113f1f4d5f no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Dec 30 04:16:39.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-8" for this suite. 12/30/22 04:16:39.52
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:16:39.527
Dec 30 04:16:39.527: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename downward-api 12/30/22 04:16:39.528
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:16:39.544
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:16:39.548
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220
STEP: Creating a pod to test downward API volume plugin 12/30/22 04:16:39.552
Dec 30 04:16:39.561: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5740f54f-63e8-40ba-88d5-6579e82aa14d" in namespace "downward-api-2883" to be "Succeeded or Failed"
Dec 30 04:16:39.565: INFO: Pod "downwardapi-volume-5740f54f-63e8-40ba-88d5-6579e82aa14d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.967253ms
Dec 30 04:16:41.570: INFO: Pod "downwardapi-volume-5740f54f-63e8-40ba-88d5-6579e82aa14d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008528322s
Dec 30 04:16:43.571: INFO: Pod "downwardapi-volume-5740f54f-63e8-40ba-88d5-6579e82aa14d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010380546s
STEP: Saw pod success 12/30/22 04:16:43.572
Dec 30 04:16:43.572: INFO: Pod "downwardapi-volume-5740f54f-63e8-40ba-88d5-6579e82aa14d" satisfied condition "Succeeded or Failed"
Dec 30 04:16:43.576: INFO: Trying to get logs from node k8s-mgmt01 pod downwardapi-volume-5740f54f-63e8-40ba-88d5-6579e82aa14d container client-container: <nil>
STEP: delete the pod 12/30/22 04:16:43.585
Dec 30 04:16:43.596: INFO: Waiting for pod downwardapi-volume-5740f54f-63e8-40ba-88d5-6579e82aa14d to disappear
Dec 30 04:16:43.600: INFO: Pod downwardapi-volume-5740f54f-63e8-40ba-88d5-6579e82aa14d no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Dec 30 04:16:43.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2883" for this suite. 12/30/22 04:16:43.605
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","completed":243,"skipped":4517,"failed":0}
------------------------------
• [4.086 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:16:39.527
    Dec 30 04:16:39.527: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename downward-api 12/30/22 04:16:39.528
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:16:39.544
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:16:39.548
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:220
    STEP: Creating a pod to test downward API volume plugin 12/30/22 04:16:39.552
    Dec 30 04:16:39.561: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5740f54f-63e8-40ba-88d5-6579e82aa14d" in namespace "downward-api-2883" to be "Succeeded or Failed"
    Dec 30 04:16:39.565: INFO: Pod "downwardapi-volume-5740f54f-63e8-40ba-88d5-6579e82aa14d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.967253ms
    Dec 30 04:16:41.570: INFO: Pod "downwardapi-volume-5740f54f-63e8-40ba-88d5-6579e82aa14d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008528322s
    Dec 30 04:16:43.571: INFO: Pod "downwardapi-volume-5740f54f-63e8-40ba-88d5-6579e82aa14d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010380546s
    STEP: Saw pod success 12/30/22 04:16:43.572
    Dec 30 04:16:43.572: INFO: Pod "downwardapi-volume-5740f54f-63e8-40ba-88d5-6579e82aa14d" satisfied condition "Succeeded or Failed"
    Dec 30 04:16:43.576: INFO: Trying to get logs from node k8s-mgmt01 pod downwardapi-volume-5740f54f-63e8-40ba-88d5-6579e82aa14d container client-container: <nil>
    STEP: delete the pod 12/30/22 04:16:43.585
    Dec 30 04:16:43.596: INFO: Waiting for pod downwardapi-volume-5740f54f-63e8-40ba-88d5-6579e82aa14d to disappear
    Dec 30 04:16:43.600: INFO: Pod downwardapi-volume-5740f54f-63e8-40ba-88d5-6579e82aa14d no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Dec 30 04:16:43.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-2883" for this suite. 12/30/22 04:16:43.605
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:16:43.614
Dec 30 04:16:43.614: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename security-context 12/30/22 04:16:43.615
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:16:43.634
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:16:43.637
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 12/30/22 04:16:43.641
Dec 30 04:16:43.650: INFO: Waiting up to 5m0s for pod "security-context-8b1b15cd-b441-4d31-939d-cad0aa86ba5e" in namespace "security-context-6412" to be "Succeeded or Failed"
Dec 30 04:16:43.654: INFO: Pod "security-context-8b1b15cd-b441-4d31-939d-cad0aa86ba5e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.768294ms
Dec 30 04:16:45.659: INFO: Pod "security-context-8b1b15cd-b441-4d31-939d-cad0aa86ba5e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009166071s
Dec 30 04:16:47.661: INFO: Pod "security-context-8b1b15cd-b441-4d31-939d-cad0aa86ba5e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01113082s
STEP: Saw pod success 12/30/22 04:16:47.661
Dec 30 04:16:47.661: INFO: Pod "security-context-8b1b15cd-b441-4d31-939d-cad0aa86ba5e" satisfied condition "Succeeded or Failed"
Dec 30 04:16:47.665: INFO: Trying to get logs from node k8s-mgmt01 pod security-context-8b1b15cd-b441-4d31-939d-cad0aa86ba5e container test-container: <nil>
STEP: delete the pod 12/30/22 04:16:47.673
Dec 30 04:16:47.684: INFO: Waiting for pod security-context-8b1b15cd-b441-4d31-939d-cad0aa86ba5e to disappear
Dec 30 04:16:47.687: INFO: Pod security-context-8b1b15cd-b441-4d31-939d-cad0aa86ba5e no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Dec 30 04:16:47.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-6412" for this suite. 12/30/22 04:16:47.693
{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","completed":244,"skipped":4525,"failed":0}
------------------------------
• [4.087 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:16:43.614
    Dec 30 04:16:43.614: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename security-context 12/30/22 04:16:43.615
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:16:43.634
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:16:43.637
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:132
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 12/30/22 04:16:43.641
    Dec 30 04:16:43.650: INFO: Waiting up to 5m0s for pod "security-context-8b1b15cd-b441-4d31-939d-cad0aa86ba5e" in namespace "security-context-6412" to be "Succeeded or Failed"
    Dec 30 04:16:43.654: INFO: Pod "security-context-8b1b15cd-b441-4d31-939d-cad0aa86ba5e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.768294ms
    Dec 30 04:16:45.659: INFO: Pod "security-context-8b1b15cd-b441-4d31-939d-cad0aa86ba5e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009166071s
    Dec 30 04:16:47.661: INFO: Pod "security-context-8b1b15cd-b441-4d31-939d-cad0aa86ba5e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01113082s
    STEP: Saw pod success 12/30/22 04:16:47.661
    Dec 30 04:16:47.661: INFO: Pod "security-context-8b1b15cd-b441-4d31-939d-cad0aa86ba5e" satisfied condition "Succeeded or Failed"
    Dec 30 04:16:47.665: INFO: Trying to get logs from node k8s-mgmt01 pod security-context-8b1b15cd-b441-4d31-939d-cad0aa86ba5e container test-container: <nil>
    STEP: delete the pod 12/30/22 04:16:47.673
    Dec 30 04:16:47.684: INFO: Waiting for pod security-context-8b1b15cd-b441-4d31-939d-cad0aa86ba5e to disappear
    Dec 30 04:16:47.687: INFO: Pod security-context-8b1b15cd-b441-4d31-939d-cad0aa86ba5e no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Dec 30 04:16:47.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-6412" for this suite. 12/30/22 04:16:47.693
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:16:47.702
Dec 30 04:16:47.702: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename taint-single-pod 12/30/22 04:16:47.703
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:16:47.721
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:16:47.725
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:166
Dec 30 04:16:47.728: INFO: Waiting up to 1m0s for all nodes to be ready
Dec 30 04:17:47.781: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289
Dec 30 04:17:47.785: INFO: Starting informer...
STEP: Starting pod... 12/30/22 04:17:47.785
Dec 30 04:17:48.003: INFO: Pod is running on k8s-mgmt01. Tainting Node
STEP: Trying to apply a taint on the Node 12/30/22 04:17:48.003
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 12/30/22 04:17:48.02
STEP: Waiting short time to make sure Pod is queued for deletion 12/30/22 04:17:48.025
Dec 30 04:17:48.025: INFO: Pod wasn't evicted. Proceeding
Dec 30 04:17:48.025: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 12/30/22 04:17:48.041
STEP: Waiting some time to make sure that toleration time passed. 12/30/22 04:17:48.046
Dec 30 04:19:03.047: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:187
Dec 30 04:19:03.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-8047" for this suite. 12/30/22 04:19:03.054
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","completed":245,"skipped":4534,"failed":0}
------------------------------
• [SLOW TEST] [135.361 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:16:47.702
    Dec 30 04:16:47.702: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename taint-single-pod 12/30/22 04:16:47.703
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:16:47.721
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:16:47.725
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:166
    Dec 30 04:16:47.728: INFO: Waiting up to 1m0s for all nodes to be ready
    Dec 30 04:17:47.781: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:289
    Dec 30 04:17:47.785: INFO: Starting informer...
    STEP: Starting pod... 12/30/22 04:17:47.785
    Dec 30 04:17:48.003: INFO: Pod is running on k8s-mgmt01. Tainting Node
    STEP: Trying to apply a taint on the Node 12/30/22 04:17:48.003
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 12/30/22 04:17:48.02
    STEP: Waiting short time to make sure Pod is queued for deletion 12/30/22 04:17:48.025
    Dec 30 04:17:48.025: INFO: Pod wasn't evicted. Proceeding
    Dec 30 04:17:48.025: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 12/30/22 04:17:48.041
    STEP: Waiting some time to make sure that toleration time passed. 12/30/22 04:17:48.046
    Dec 30 04:19:03.047: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/framework.go:187
    Dec 30 04:19:03.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "taint-single-pod-8047" for this suite. 12/30/22 04:19:03.054
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:19:03.064
Dec 30 04:19:03.064: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename runtimeclass 12/30/22 04:19:03.066
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:19:03.084
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:19:03.088
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Dec 30 04:19:03.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-8937" for this suite. 12/30/22 04:19:03.107
{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]","completed":246,"skipped":4550,"failed":0}
------------------------------
• [0.051 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:19:03.064
    Dec 30 04:19:03.064: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename runtimeclass 12/30/22 04:19:03.066
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:19:03.084
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:19:03.088
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Dec 30 04:19:03.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-8937" for this suite. 12/30/22 04:19:03.107
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:19:03.117
Dec 30 04:19:03.117: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename replication-controller 12/30/22 04:19:03.118
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:19:03.133
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:19:03.136
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109
STEP: creating a ReplicationController 12/30/22 04:19:03.144
STEP: waiting for RC to be added 12/30/22 04:19:03.15
STEP: waiting for available Replicas 12/30/22 04:19:03.151
STEP: patching ReplicationController 12/30/22 04:19:04.751
STEP: waiting for RC to be modified 12/30/22 04:19:04.763
STEP: patching ReplicationController status 12/30/22 04:19:04.764
STEP: waiting for RC to be modified 12/30/22 04:19:04.772
STEP: waiting for available Replicas 12/30/22 04:19:04.772
STEP: fetching ReplicationController status 12/30/22 04:19:04.778
STEP: patching ReplicationController scale 12/30/22 04:19:04.783
STEP: waiting for RC to be modified 12/30/22 04:19:04.792
STEP: waiting for ReplicationController's scale to be the max amount 12/30/22 04:19:04.792
STEP: fetching ReplicationController; ensuring that it's patched 12/30/22 04:19:05.866
STEP: updating ReplicationController status 12/30/22 04:19:05.871
STEP: waiting for RC to be modified 12/30/22 04:19:05.878
STEP: listing all ReplicationControllers 12/30/22 04:19:05.878
STEP: checking that ReplicationController has expected values 12/30/22 04:19:05.882
STEP: deleting ReplicationControllers by collection 12/30/22 04:19:05.882
STEP: waiting for ReplicationController to have a DELETED watchEvent 12/30/22 04:19:05.89
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Dec 30 04:19:05.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7213" for this suite. 12/30/22 04:19:05.963
{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","completed":247,"skipped":4569,"failed":0}
------------------------------
• [2.853 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:19:03.117
    Dec 30 04:19:03.117: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename replication-controller 12/30/22 04:19:03.118
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:19:03.133
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:19:03.136
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:109
    STEP: creating a ReplicationController 12/30/22 04:19:03.144
    STEP: waiting for RC to be added 12/30/22 04:19:03.15
    STEP: waiting for available Replicas 12/30/22 04:19:03.151
    STEP: patching ReplicationController 12/30/22 04:19:04.751
    STEP: waiting for RC to be modified 12/30/22 04:19:04.763
    STEP: patching ReplicationController status 12/30/22 04:19:04.764
    STEP: waiting for RC to be modified 12/30/22 04:19:04.772
    STEP: waiting for available Replicas 12/30/22 04:19:04.772
    STEP: fetching ReplicationController status 12/30/22 04:19:04.778
    STEP: patching ReplicationController scale 12/30/22 04:19:04.783
    STEP: waiting for RC to be modified 12/30/22 04:19:04.792
    STEP: waiting for ReplicationController's scale to be the max amount 12/30/22 04:19:04.792
    STEP: fetching ReplicationController; ensuring that it's patched 12/30/22 04:19:05.866
    STEP: updating ReplicationController status 12/30/22 04:19:05.871
    STEP: waiting for RC to be modified 12/30/22 04:19:05.878
    STEP: listing all ReplicationControllers 12/30/22 04:19:05.878
    STEP: checking that ReplicationController has expected values 12/30/22 04:19:05.882
    STEP: deleting ReplicationControllers by collection 12/30/22 04:19:05.882
    STEP: waiting for ReplicationController to have a DELETED watchEvent 12/30/22 04:19:05.89
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Dec 30 04:19:05.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-7213" for this suite. 12/30/22 04:19:05.963
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:19:05.971
Dec 30 04:19:05.971: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename crd-publish-openapi 12/30/22 04:19:05.972
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:19:05.989
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:19:05.993
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 12/30/22 04:19:05.996
Dec 30 04:19:05.997: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 12/30/22 04:19:26.408
Dec 30 04:19:26.409: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
Dec 30 04:19:34.745: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Dec 30 04:19:56.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7598" for this suite. 12/30/22 04:19:56.024
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","completed":248,"skipped":4570,"failed":0}
------------------------------
• [SLOW TEST] [50.062 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:19:05.971
    Dec 30 04:19:05.971: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename crd-publish-openapi 12/30/22 04:19:05.972
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:19:05.989
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:19:05.993
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:308
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 12/30/22 04:19:05.996
    Dec 30 04:19:05.997: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 12/30/22 04:19:26.408
    Dec 30 04:19:26.409: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    Dec 30 04:19:34.745: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Dec 30 04:19:56.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-7598" for this suite. 12/30/22 04:19:56.024
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:733
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:19:56.035
Dec 30 04:19:56.035: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename sched-preemption 12/30/22 04:19:56.037
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:19:56.051
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:19:56.054
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Dec 30 04:19:56.070: INFO: Waiting up to 1m0s for all nodes to be ready
Dec 30 04:20:56.133: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:20:56.137
Dec 30 04:20:56.137: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename sched-preemption-path 12/30/22 04:20:56.138
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:20:56.15
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:20:56.152
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:690
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:733
Dec 30 04:20:56.169: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Dec 30 04:20:56.173: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/framework.go:187
Dec 30 04:20:56.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-7167" for this suite. 12/30/22 04:20:56.198
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:706
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Dec 30 04:20:56.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-4360" for this suite. 12/30/22 04:20:56.223
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","completed":249,"skipped":4604,"failed":0}
------------------------------
• [SLOW TEST] [60.269 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:683
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:733

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:19:56.035
    Dec 30 04:19:56.035: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename sched-preemption 12/30/22 04:19:56.037
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:19:56.051
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:19:56.054
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Dec 30 04:19:56.070: INFO: Waiting up to 1m0s for all nodes to be ready
    Dec 30 04:20:56.133: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:20:56.137
    Dec 30 04:20:56.137: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename sched-preemption-path 12/30/22 04:20:56.138
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:20:56.15
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:20:56.152
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:690
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:733
    Dec 30 04:20:56.169: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Dec 30 04:20:56.173: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/framework.go:187
    Dec 30 04:20:56.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-path-7167" for this suite. 12/30/22 04:20:56.198
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:706
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Dec 30 04:20:56.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-4360" for this suite. 12/30/22 04:20:56.223
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:20:56.308
Dec 30 04:20:56.308: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename webhook 12/30/22 04:20:56.31
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:20:56.323
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:20:56.326
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 12/30/22 04:20:56.341
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/30/22 04:20:56.944
STEP: Deploying the webhook pod 12/30/22 04:20:56.952
STEP: Wait for the deployment to be ready 12/30/22 04:20:56.961
Dec 30 04:20:56.969: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 12/30/22 04:20:58.984
STEP: Verifying the service has paired with the endpoint 12/30/22 04:20:58.995
Dec 30 04:20:59.995: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290
Dec 30 04:20:59.999: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5937-crds.webhook.example.com via the AdmissionRegistration API 12/30/22 04:21:05.51
STEP: Creating a custom resource that should be mutated by the webhook 12/30/22 04:21:05.53
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Dec 30 04:21:08.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3815" for this suite. 12/30/22 04:21:08.123
STEP: Destroying namespace "webhook-3815-markers" for this suite. 12/30/22 04:21:08.13
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","completed":250,"skipped":4650,"failed":0}
------------------------------
• [SLOW TEST] [11.870 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:20:56.308
    Dec 30 04:20:56.308: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename webhook 12/30/22 04:20:56.31
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:20:56.323
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:20:56.326
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 12/30/22 04:20:56.341
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/30/22 04:20:56.944
    STEP: Deploying the webhook pod 12/30/22 04:20:56.952
    STEP: Wait for the deployment to be ready 12/30/22 04:20:56.961
    Dec 30 04:20:56.969: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 12/30/22 04:20:58.984
    STEP: Verifying the service has paired with the endpoint 12/30/22 04:20:58.995
    Dec 30 04:20:59.995: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:290
    Dec 30 04:20:59.999: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5937-crds.webhook.example.com via the AdmissionRegistration API 12/30/22 04:21:05.51
    STEP: Creating a custom resource that should be mutated by the webhook 12/30/22 04:21:05.53
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Dec 30 04:21:08.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-3815" for this suite. 12/30/22 04:21:08.123
    STEP: Destroying namespace "webhook-3815-markers" for this suite. 12/30/22 04:21:08.13
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:21:08.179
Dec 30 04:21:08.179: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename cronjob 12/30/22 04:21:08.18
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:21:08.193
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:21:08.196
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 12/30/22 04:21:08.199
STEP: creating 12/30/22 04:21:08.2
STEP: getting 12/30/22 04:21:08.206
STEP: listing 12/30/22 04:21:08.209
STEP: watching 12/30/22 04:21:08.213
Dec 30 04:21:08.213: INFO: starting watch
STEP: cluster-wide listing 12/30/22 04:21:08.214
STEP: cluster-wide watching 12/30/22 04:21:08.218
Dec 30 04:21:08.219: INFO: starting watch
STEP: patching 12/30/22 04:21:08.22
STEP: updating 12/30/22 04:21:08.227
Dec 30 04:21:08.237: INFO: waiting for watch events with expected annotations
Dec 30 04:21:08.237: INFO: saw patched and updated annotations
STEP: patching /status 12/30/22 04:21:08.237
STEP: updating /status 12/30/22 04:21:08.244
STEP: get /status 12/30/22 04:21:08.254
STEP: deleting 12/30/22 04:21:08.258
STEP: deleting a collection 12/30/22 04:21:08.273
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Dec 30 04:21:08.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-230" for this suite. 12/30/22 04:21:08.291
{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","completed":251,"skipped":4656,"failed":0}
------------------------------
• [0.119 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:21:08.179
    Dec 30 04:21:08.179: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename cronjob 12/30/22 04:21:08.18
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:21:08.193
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:21:08.196
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 12/30/22 04:21:08.199
    STEP: creating 12/30/22 04:21:08.2
    STEP: getting 12/30/22 04:21:08.206
    STEP: listing 12/30/22 04:21:08.209
    STEP: watching 12/30/22 04:21:08.213
    Dec 30 04:21:08.213: INFO: starting watch
    STEP: cluster-wide listing 12/30/22 04:21:08.214
    STEP: cluster-wide watching 12/30/22 04:21:08.218
    Dec 30 04:21:08.219: INFO: starting watch
    STEP: patching 12/30/22 04:21:08.22
    STEP: updating 12/30/22 04:21:08.227
    Dec 30 04:21:08.237: INFO: waiting for watch events with expected annotations
    Dec 30 04:21:08.237: INFO: saw patched and updated annotations
    STEP: patching /status 12/30/22 04:21:08.237
    STEP: updating /status 12/30/22 04:21:08.244
    STEP: get /status 12/30/22 04:21:08.254
    STEP: deleting 12/30/22 04:21:08.258
    STEP: deleting a collection 12/30/22 04:21:08.273
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Dec 30 04:21:08.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-230" for this suite. 12/30/22 04:21:08.291
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:21:08.299
Dec 30 04:21:08.299: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename downward-api 12/30/22 04:21:08.301
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:21:08.314
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:21:08.317
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43
STEP: Creating a pod to test downward api env vars 12/30/22 04:21:08.32
Dec 30 04:21:08.330: INFO: Waiting up to 5m0s for pod "downward-api-e6bc1737-831e-4278-bfb9-3f5b53d5e6b4" in namespace "downward-api-97" to be "Succeeded or Failed"
Dec 30 04:21:08.334: INFO: Pod "downward-api-e6bc1737-831e-4278-bfb9-3f5b53d5e6b4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.978436ms
Dec 30 04:21:10.338: INFO: Pod "downward-api-e6bc1737-831e-4278-bfb9-3f5b53d5e6b4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008088583s
Dec 30 04:21:12.340: INFO: Pod "downward-api-e6bc1737-831e-4278-bfb9-3f5b53d5e6b4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010280856s
STEP: Saw pod success 12/30/22 04:21:12.34
Dec 30 04:21:12.341: INFO: Pod "downward-api-e6bc1737-831e-4278-bfb9-3f5b53d5e6b4" satisfied condition "Succeeded or Failed"
Dec 30 04:21:12.345: INFO: Trying to get logs from node k8s-mgmt01 pod downward-api-e6bc1737-831e-4278-bfb9-3f5b53d5e6b4 container dapi-container: <nil>
STEP: delete the pod 12/30/22 04:21:12.367
Dec 30 04:21:12.384: INFO: Waiting for pod downward-api-e6bc1737-831e-4278-bfb9-3f5b53d5e6b4 to disappear
Dec 30 04:21:12.387: INFO: Pod downward-api-e6bc1737-831e-4278-bfb9-3f5b53d5e6b4 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Dec 30 04:21:12.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-97" for this suite. 12/30/22 04:21:12.393
{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","completed":252,"skipped":4676,"failed":0}
------------------------------
• [4.102 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:21:08.299
    Dec 30 04:21:08.299: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename downward-api 12/30/22 04:21:08.301
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:21:08.314
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:21:08.317
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:43
    STEP: Creating a pod to test downward api env vars 12/30/22 04:21:08.32
    Dec 30 04:21:08.330: INFO: Waiting up to 5m0s for pod "downward-api-e6bc1737-831e-4278-bfb9-3f5b53d5e6b4" in namespace "downward-api-97" to be "Succeeded or Failed"
    Dec 30 04:21:08.334: INFO: Pod "downward-api-e6bc1737-831e-4278-bfb9-3f5b53d5e6b4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.978436ms
    Dec 30 04:21:10.338: INFO: Pod "downward-api-e6bc1737-831e-4278-bfb9-3f5b53d5e6b4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008088583s
    Dec 30 04:21:12.340: INFO: Pod "downward-api-e6bc1737-831e-4278-bfb9-3f5b53d5e6b4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010280856s
    STEP: Saw pod success 12/30/22 04:21:12.34
    Dec 30 04:21:12.341: INFO: Pod "downward-api-e6bc1737-831e-4278-bfb9-3f5b53d5e6b4" satisfied condition "Succeeded or Failed"
    Dec 30 04:21:12.345: INFO: Trying to get logs from node k8s-mgmt01 pod downward-api-e6bc1737-831e-4278-bfb9-3f5b53d5e6b4 container dapi-container: <nil>
    STEP: delete the pod 12/30/22 04:21:12.367
    Dec 30 04:21:12.384: INFO: Waiting for pod downward-api-e6bc1737-831e-4278-bfb9-3f5b53d5e6b4 to disappear
    Dec 30 04:21:12.387: INFO: Pod downward-api-e6bc1737-831e-4278-bfb9-3f5b53d5e6b4 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Dec 30 04:21:12.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-97" for this suite. 12/30/22 04:21:12.393
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:21:12.401
Dec 30 04:21:12.402: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename pods 12/30/22 04:21:12.403
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:21:12.417
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:21:12.42
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082
STEP: Create a pod 12/30/22 04:21:12.423
Dec 30 04:21:12.431: INFO: Waiting up to 5m0s for pod "pod-w6tbc" in namespace "pods-5231" to be "running"
Dec 30 04:21:12.435: INFO: Pod "pod-w6tbc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.984049ms
Dec 30 04:21:14.442: INFO: Pod "pod-w6tbc": Phase="Running", Reason="", readiness=true. Elapsed: 2.010311222s
Dec 30 04:21:14.442: INFO: Pod "pod-w6tbc" satisfied condition "running"
STEP: patching /status 12/30/22 04:21:14.442
Dec 30 04:21:14.452: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Dec 30 04:21:14.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5231" for this suite. 12/30/22 04:21:14.457
{"msg":"PASSED [sig-node] Pods should patch a pod status [Conformance]","completed":253,"skipped":4679,"failed":0}
------------------------------
• [2.062 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:21:12.401
    Dec 30 04:21:12.402: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename pods 12/30/22 04:21:12.403
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:21:12.417
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:21:12.42
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1082
    STEP: Create a pod 12/30/22 04:21:12.423
    Dec 30 04:21:12.431: INFO: Waiting up to 5m0s for pod "pod-w6tbc" in namespace "pods-5231" to be "running"
    Dec 30 04:21:12.435: INFO: Pod "pod-w6tbc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.984049ms
    Dec 30 04:21:14.442: INFO: Pod "pod-w6tbc": Phase="Running", Reason="", readiness=true. Elapsed: 2.010311222s
    Dec 30 04:21:14.442: INFO: Pod "pod-w6tbc" satisfied condition "running"
    STEP: patching /status 12/30/22 04:21:14.442
    Dec 30 04:21:14.452: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Dec 30 04:21:14.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-5231" for this suite. 12/30/22 04:21:14.457
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:21:14.464
Dec 30 04:21:14.465: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename resourcequota 12/30/22 04:21:14.466
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:21:14.479
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:21:14.482
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382
STEP: Counting existing ResourceQuota 12/30/22 04:21:14.485
STEP: Creating a ResourceQuota 12/30/22 04:21:19.49
STEP: Ensuring resource quota status is calculated 12/30/22 04:21:19.497
STEP: Creating a ReplicationController 12/30/22 04:21:21.502
STEP: Ensuring resource quota status captures replication controller creation 12/30/22 04:21:21.516
STEP: Deleting a ReplicationController 12/30/22 04:21:23.521
STEP: Ensuring resource quota status released usage 12/30/22 04:21:23.528
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Dec 30 04:21:25.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9497" for this suite. 12/30/22 04:21:25.541
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","completed":254,"skipped":4681,"failed":0}
------------------------------
• [SLOW TEST] [11.084 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:21:14.464
    Dec 30 04:21:14.465: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename resourcequota 12/30/22 04:21:14.466
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:21:14.479
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:21:14.482
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:382
    STEP: Counting existing ResourceQuota 12/30/22 04:21:14.485
    STEP: Creating a ResourceQuota 12/30/22 04:21:19.49
    STEP: Ensuring resource quota status is calculated 12/30/22 04:21:19.497
    STEP: Creating a ReplicationController 12/30/22 04:21:21.502
    STEP: Ensuring resource quota status captures replication controller creation 12/30/22 04:21:21.516
    STEP: Deleting a ReplicationController 12/30/22 04:21:23.521
    STEP: Ensuring resource quota status released usage 12/30/22 04:21:23.528
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Dec 30 04:21:25.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-9497" for this suite. 12/30/22 04:21:25.541
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:21:25.55
Dec 30 04:21:25.550: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename container-probe 12/30/22 04:21:25.551
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:21:25.566
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:21:25.569
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211
STEP: Creating pod test-webserver-57ed59d8-462f-4778-86e3-3427a1eef2fc in namespace container-probe-815 12/30/22 04:21:25.573
Dec 30 04:21:25.582: INFO: Waiting up to 5m0s for pod "test-webserver-57ed59d8-462f-4778-86e3-3427a1eef2fc" in namespace "container-probe-815" to be "not pending"
Dec 30 04:21:25.585: INFO: Pod "test-webserver-57ed59d8-462f-4778-86e3-3427a1eef2fc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.471417ms
Dec 30 04:21:27.591: INFO: Pod "test-webserver-57ed59d8-462f-4778-86e3-3427a1eef2fc": Phase="Running", Reason="", readiness=true. Elapsed: 2.009410096s
Dec 30 04:21:27.591: INFO: Pod "test-webserver-57ed59d8-462f-4778-86e3-3427a1eef2fc" satisfied condition "not pending"
Dec 30 04:21:27.591: INFO: Started pod test-webserver-57ed59d8-462f-4778-86e3-3427a1eef2fc in namespace container-probe-815
STEP: checking the pod's current state and verifying that restartCount is present 12/30/22 04:21:27.591
Dec 30 04:21:27.596: INFO: Initial restart count of pod test-webserver-57ed59d8-462f-4778-86e3-3427a1eef2fc is 0
STEP: deleting the pod 12/30/22 04:25:28.238
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Dec 30 04:25:28.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-815" for this suite. 12/30/22 04:25:28.258
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","completed":255,"skipped":4690,"failed":0}
------------------------------
• [SLOW TEST] [242.715 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:21:25.55
    Dec 30 04:21:25.550: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename container-probe 12/30/22 04:21:25.551
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:21:25.566
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:21:25.569
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:211
    STEP: Creating pod test-webserver-57ed59d8-462f-4778-86e3-3427a1eef2fc in namespace container-probe-815 12/30/22 04:21:25.573
    Dec 30 04:21:25.582: INFO: Waiting up to 5m0s for pod "test-webserver-57ed59d8-462f-4778-86e3-3427a1eef2fc" in namespace "container-probe-815" to be "not pending"
    Dec 30 04:21:25.585: INFO: Pod "test-webserver-57ed59d8-462f-4778-86e3-3427a1eef2fc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.471417ms
    Dec 30 04:21:27.591: INFO: Pod "test-webserver-57ed59d8-462f-4778-86e3-3427a1eef2fc": Phase="Running", Reason="", readiness=true. Elapsed: 2.009410096s
    Dec 30 04:21:27.591: INFO: Pod "test-webserver-57ed59d8-462f-4778-86e3-3427a1eef2fc" satisfied condition "not pending"
    Dec 30 04:21:27.591: INFO: Started pod test-webserver-57ed59d8-462f-4778-86e3-3427a1eef2fc in namespace container-probe-815
    STEP: checking the pod's current state and verifying that restartCount is present 12/30/22 04:21:27.591
    Dec 30 04:21:27.596: INFO: Initial restart count of pod test-webserver-57ed59d8-462f-4778-86e3-3427a1eef2fc is 0
    STEP: deleting the pod 12/30/22 04:25:28.238
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Dec 30 04:25:28.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-815" for this suite. 12/30/22 04:25:28.258
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:25:28.265
Dec 30 04:25:28.265: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename disruption 12/30/22 04:25:28.266
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:25:28.281
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:25:28.284
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163
STEP: Waiting for the pdb to be processed 12/30/22 04:25:28.292
STEP: Updating PodDisruptionBudget status 12/30/22 04:25:30.3
STEP: Waiting for all pods to be running 12/30/22 04:25:30.31
Dec 30 04:25:30.314: INFO: running pods: 0 < 1
STEP: locating a running pod 12/30/22 04:25:32.32
STEP: Waiting for the pdb to be processed 12/30/22 04:25:32.334
STEP: Patching PodDisruptionBudget status 12/30/22 04:25:32.342
STEP: Waiting for the pdb to be processed 12/30/22 04:25:32.352
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Dec 30 04:25:32.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-8140" for this suite. 12/30/22 04:25:32.361
{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","completed":256,"skipped":4690,"failed":0}
------------------------------
• [4.103 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:25:28.265
    Dec 30 04:25:28.265: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename disruption 12/30/22 04:25:28.266
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:25:28.281
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:25:28.284
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:163
    STEP: Waiting for the pdb to be processed 12/30/22 04:25:28.292
    STEP: Updating PodDisruptionBudget status 12/30/22 04:25:30.3
    STEP: Waiting for all pods to be running 12/30/22 04:25:30.31
    Dec 30 04:25:30.314: INFO: running pods: 0 < 1
    STEP: locating a running pod 12/30/22 04:25:32.32
    STEP: Waiting for the pdb to be processed 12/30/22 04:25:32.334
    STEP: Patching PodDisruptionBudget status 12/30/22 04:25:32.342
    STEP: Waiting for the pdb to be processed 12/30/22 04:25:32.352
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Dec 30 04:25:32.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-8140" for this suite. 12/30/22 04:25:32.361
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:25:32.369
Dec 30 04:25:32.369: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename aggregator 12/30/22 04:25:32.37
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:25:32.383
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:25:32.386
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
Dec 30 04:25:32.389: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 12/30/22 04:25:32.39
Dec 30 04:25:32.899: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Dec 30 04:25:34.944: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 30 04:25:36.949: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 30 04:25:38.949: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 30 04:25:40.949: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 30 04:25:42.949: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 30 04:25:44.949: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 30 04:25:46.949: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 30 04:25:48.949: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 30 04:25:50.950: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 30 04:25:52.950: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 30 04:25:54.950: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 30 04:25:57.084: INFO: Waited 128.625224ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com 12/30/22 04:25:57.135
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 12/30/22 04:25:57.14
STEP: List APIServices 12/30/22 04:25:57.148
Dec 30 04:25:57.158: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:187
Dec 30 04:25:57.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-1623" for this suite. 12/30/22 04:25:57.366
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","completed":257,"skipped":4698,"failed":0}
------------------------------
• [SLOW TEST] [25.167 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:25:32.369
    Dec 30 04:25:32.369: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename aggregator 12/30/22 04:25:32.37
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:25:32.383
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:25:32.386
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    Dec 30 04:25:32.389: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 12/30/22 04:25:32.39
    Dec 30 04:25:32.899: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
    Dec 30 04:25:34.944: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Dec 30 04:25:36.949: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Dec 30 04:25:38.949: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Dec 30 04:25:40.949: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Dec 30 04:25:42.949: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Dec 30 04:25:44.949: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Dec 30 04:25:46.949: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Dec 30 04:25:48.949: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Dec 30 04:25:50.950: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Dec 30 04:25:52.950: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Dec 30 04:25:54.950: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 30, 4, 25, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-78794cb777\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Dec 30 04:25:57.084: INFO: Waited 128.625224ms for the sample-apiserver to be ready to handle requests.
    STEP: Read Status for v1alpha1.wardle.example.com 12/30/22 04:25:57.135
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 12/30/22 04:25:57.14
    STEP: List APIServices 12/30/22 04:25:57.148
    Dec 30 04:25:57.158: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/framework.go:187
    Dec 30 04:25:57.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "aggregator-1623" for this suite. 12/30/22 04:25:57.366
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:25:57.537
Dec 30 04:25:57.537: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename emptydir 12/30/22 04:25:57.539
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:25:57.687
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:25:57.69
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226
STEP: Creating Pod 12/30/22 04:25:57.756
Dec 30 04:25:57.767: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-96de9e12-399c-4dd1-ade6-a479729f2c10" in namespace "emptydir-7154" to be "running"
Dec 30 04:25:57.892: INFO: Pod "pod-sharedvolume-96de9e12-399c-4dd1-ade6-a479729f2c10": Phase="Pending", Reason="", readiness=false. Elapsed: 125.198716ms
Dec 30 04:25:59.898: INFO: Pod "pod-sharedvolume-96de9e12-399c-4dd1-ade6-a479729f2c10": Phase="Running", Reason="", readiness=false. Elapsed: 2.131546746s
Dec 30 04:25:59.898: INFO: Pod "pod-sharedvolume-96de9e12-399c-4dd1-ade6-a479729f2c10" satisfied condition "running"
STEP: Reading file content from the nginx-container 12/30/22 04:25:59.898
Dec 30 04:25:59.899: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-7154 PodName:pod-sharedvolume-96de9e12-399c-4dd1-ade6-a479729f2c10 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 30 04:25:59.899: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
Dec 30 04:25:59.900: INFO: ExecWithOptions: Clientset creation
Dec 30 04:25:59.900: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/emptydir-7154/pods/pod-sharedvolume-96de9e12-399c-4dd1-ade6-a479729f2c10/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Dec 30 04:25:59.987: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Dec 30 04:25:59.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7154" for this suite. 12/30/22 04:25:59.993
{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","completed":258,"skipped":4702,"failed":0}
------------------------------
• [2.462 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:25:57.537
    Dec 30 04:25:57.537: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename emptydir 12/30/22 04:25:57.539
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:25:57.687
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:25:57.69
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:226
    STEP: Creating Pod 12/30/22 04:25:57.756
    Dec 30 04:25:57.767: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-96de9e12-399c-4dd1-ade6-a479729f2c10" in namespace "emptydir-7154" to be "running"
    Dec 30 04:25:57.892: INFO: Pod "pod-sharedvolume-96de9e12-399c-4dd1-ade6-a479729f2c10": Phase="Pending", Reason="", readiness=false. Elapsed: 125.198716ms
    Dec 30 04:25:59.898: INFO: Pod "pod-sharedvolume-96de9e12-399c-4dd1-ade6-a479729f2c10": Phase="Running", Reason="", readiness=false. Elapsed: 2.131546746s
    Dec 30 04:25:59.898: INFO: Pod "pod-sharedvolume-96de9e12-399c-4dd1-ade6-a479729f2c10" satisfied condition "running"
    STEP: Reading file content from the nginx-container 12/30/22 04:25:59.898
    Dec 30 04:25:59.899: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-7154 PodName:pod-sharedvolume-96de9e12-399c-4dd1-ade6-a479729f2c10 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 30 04:25:59.899: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    Dec 30 04:25:59.900: INFO: ExecWithOptions: Clientset creation
    Dec 30 04:25:59.900: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/emptydir-7154/pods/pod-sharedvolume-96de9e12-399c-4dd1-ade6-a479729f2c10/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    Dec 30 04:25:59.987: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Dec 30 04:25:59.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-7154" for this suite. 12/30/22 04:25:59.993
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:26:00
Dec 30 04:26:00.000: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename daemonsets 12/30/22 04:26:00.002
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:26:00.015
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:26:00.018
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373
Dec 30 04:26:00.053: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 12/30/22 04:26:00.059
Dec 30 04:26:00.067: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 30 04:26:00.067: INFO: Node k8s-mgmt01 is running 0 daemon pod, expected 1
Dec 30 04:26:01.079: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 30 04:26:01.079: INFO: Node k8s-mgmt01 is running 0 daemon pod, expected 1
Dec 30 04:26:02.079: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Dec 30 04:26:02.079: INFO: Node k8s-worker02 is running 0 daemon pod, expected 1
Dec 30 04:26:03.080: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
Dec 30 04:26:03.080: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
STEP: Update daemon pods image. 12/30/22 04:26:03.097
STEP: Check that daemon pods images are updated. 12/30/22 04:26:03.109
Dec 30 04:26:03.114: INFO: Wrong image for pod: daemon-set-8sn55. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Dec 30 04:26:03.114: INFO: Wrong image for pod: daemon-set-brtz6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Dec 30 04:26:03.114: INFO: Wrong image for pod: daemon-set-v6xqv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Dec 30 04:26:03.114: INFO: Wrong image for pod: daemon-set-ztpwm. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Dec 30 04:26:03.114: INFO: Wrong image for pod: daemon-set-zzz5w. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Dec 30 04:26:04.125: INFO: Wrong image for pod: daemon-set-8sn55. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Dec 30 04:26:04.125: INFO: Wrong image for pod: daemon-set-brtz6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Dec 30 04:26:04.125: INFO: Wrong image for pod: daemon-set-v6xqv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Dec 30 04:26:04.125: INFO: Wrong image for pod: daemon-set-zzz5w. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Dec 30 04:26:05.126: INFO: Wrong image for pod: daemon-set-8sn55. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Dec 30 04:26:05.126: INFO: Wrong image for pod: daemon-set-brtz6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Dec 30 04:26:05.126: INFO: Wrong image for pod: daemon-set-v6xqv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Dec 30 04:26:05.126: INFO: Pod daemon-set-x46v5 is not available
Dec 30 04:26:05.126: INFO: Wrong image for pod: daemon-set-zzz5w. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Dec 30 04:26:06.126: INFO: Wrong image for pod: daemon-set-8sn55. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Dec 30 04:26:06.126: INFO: Wrong image for pod: daemon-set-v6xqv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Dec 30 04:26:06.126: INFO: Wrong image for pod: daemon-set-zzz5w. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Dec 30 04:26:07.126: INFO: Wrong image for pod: daemon-set-8sn55. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Dec 30 04:26:07.126: INFO: Pod daemon-set-jcn2r is not available
Dec 30 04:26:07.126: INFO: Wrong image for pod: daemon-set-v6xqv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Dec 30 04:26:07.126: INFO: Wrong image for pod: daemon-set-zzz5w. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Dec 30 04:26:08.127: INFO: Wrong image for pod: daemon-set-v6xqv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Dec 30 04:26:08.127: INFO: Wrong image for pod: daemon-set-zzz5w. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Dec 30 04:26:09.128: INFO: Pod daemon-set-qmn2p is not available
Dec 30 04:26:09.128: INFO: Wrong image for pod: daemon-set-v6xqv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Dec 30 04:26:09.128: INFO: Wrong image for pod: daemon-set-zzz5w. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Dec 30 04:26:10.127: INFO: Wrong image for pod: daemon-set-zzz5w. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Dec 30 04:26:11.127: INFO: Wrong image for pod: daemon-set-zzz5w. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Dec 30 04:26:12.126: INFO: Pod daemon-set-vr7pb is not available
Dec 30 04:26:12.126: INFO: Wrong image for pod: daemon-set-zzz5w. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Dec 30 04:26:13.126: INFO: Pod daemon-set-vr7pb is not available
Dec 30 04:26:13.126: INFO: Wrong image for pod: daemon-set-zzz5w. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Dec 30 04:26:15.125: INFO: Pod daemon-set-cs7dl is not available
STEP: Check that daemon pods are still running on every node of the cluster. 12/30/22 04:26:15.131
Dec 30 04:26:15.140: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Dec 30 04:26:15.140: INFO: Node k8s-worker01 is running 0 daemon pod, expected 1
Dec 30 04:26:16.152: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Dec 30 04:26:16.152: INFO: Node k8s-worker01 is running 0 daemon pod, expected 1
Dec 30 04:26:17.153: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
Dec 30 04:26:17.153: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 12/30/22 04:26:17.181
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3503, will wait for the garbage collector to delete the pods 12/30/22 04:26:17.181
Dec 30 04:26:17.243: INFO: Deleting DaemonSet.extensions daemon-set took: 6.780586ms
Dec 30 04:26:17.344: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.90055ms
Dec 30 04:26:21.049: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 30 04:26:21.049: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Dec 30 04:26:21.053: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"447566"},"items":null}

Dec 30 04:26:21.056: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"447566"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Dec 30 04:26:21.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3503" for this suite. 12/30/22 04:26:21.087
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","completed":259,"skipped":4704,"failed":0}
------------------------------
• [SLOW TEST] [21.094 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:26:00
    Dec 30 04:26:00.000: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename daemonsets 12/30/22 04:26:00.002
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:26:00.015
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:26:00.018
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:373
    Dec 30 04:26:00.053: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 12/30/22 04:26:00.059
    Dec 30 04:26:00.067: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 30 04:26:00.067: INFO: Node k8s-mgmt01 is running 0 daemon pod, expected 1
    Dec 30 04:26:01.079: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 30 04:26:01.079: INFO: Node k8s-mgmt01 is running 0 daemon pod, expected 1
    Dec 30 04:26:02.079: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Dec 30 04:26:02.079: INFO: Node k8s-worker02 is running 0 daemon pod, expected 1
    Dec 30 04:26:03.080: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
    Dec 30 04:26:03.080: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
    STEP: Update daemon pods image. 12/30/22 04:26:03.097
    STEP: Check that daemon pods images are updated. 12/30/22 04:26:03.109
    Dec 30 04:26:03.114: INFO: Wrong image for pod: daemon-set-8sn55. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Dec 30 04:26:03.114: INFO: Wrong image for pod: daemon-set-brtz6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Dec 30 04:26:03.114: INFO: Wrong image for pod: daemon-set-v6xqv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Dec 30 04:26:03.114: INFO: Wrong image for pod: daemon-set-ztpwm. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Dec 30 04:26:03.114: INFO: Wrong image for pod: daemon-set-zzz5w. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Dec 30 04:26:04.125: INFO: Wrong image for pod: daemon-set-8sn55. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Dec 30 04:26:04.125: INFO: Wrong image for pod: daemon-set-brtz6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Dec 30 04:26:04.125: INFO: Wrong image for pod: daemon-set-v6xqv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Dec 30 04:26:04.125: INFO: Wrong image for pod: daemon-set-zzz5w. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Dec 30 04:26:05.126: INFO: Wrong image for pod: daemon-set-8sn55. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Dec 30 04:26:05.126: INFO: Wrong image for pod: daemon-set-brtz6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Dec 30 04:26:05.126: INFO: Wrong image for pod: daemon-set-v6xqv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Dec 30 04:26:05.126: INFO: Pod daemon-set-x46v5 is not available
    Dec 30 04:26:05.126: INFO: Wrong image for pod: daemon-set-zzz5w. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Dec 30 04:26:06.126: INFO: Wrong image for pod: daemon-set-8sn55. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Dec 30 04:26:06.126: INFO: Wrong image for pod: daemon-set-v6xqv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Dec 30 04:26:06.126: INFO: Wrong image for pod: daemon-set-zzz5w. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Dec 30 04:26:07.126: INFO: Wrong image for pod: daemon-set-8sn55. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Dec 30 04:26:07.126: INFO: Pod daemon-set-jcn2r is not available
    Dec 30 04:26:07.126: INFO: Wrong image for pod: daemon-set-v6xqv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Dec 30 04:26:07.126: INFO: Wrong image for pod: daemon-set-zzz5w. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Dec 30 04:26:08.127: INFO: Wrong image for pod: daemon-set-v6xqv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Dec 30 04:26:08.127: INFO: Wrong image for pod: daemon-set-zzz5w. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Dec 30 04:26:09.128: INFO: Pod daemon-set-qmn2p is not available
    Dec 30 04:26:09.128: INFO: Wrong image for pod: daemon-set-v6xqv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Dec 30 04:26:09.128: INFO: Wrong image for pod: daemon-set-zzz5w. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Dec 30 04:26:10.127: INFO: Wrong image for pod: daemon-set-zzz5w. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Dec 30 04:26:11.127: INFO: Wrong image for pod: daemon-set-zzz5w. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Dec 30 04:26:12.126: INFO: Pod daemon-set-vr7pb is not available
    Dec 30 04:26:12.126: INFO: Wrong image for pod: daemon-set-zzz5w. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Dec 30 04:26:13.126: INFO: Pod daemon-set-vr7pb is not available
    Dec 30 04:26:13.126: INFO: Wrong image for pod: daemon-set-zzz5w. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Dec 30 04:26:15.125: INFO: Pod daemon-set-cs7dl is not available
    STEP: Check that daemon pods are still running on every node of the cluster. 12/30/22 04:26:15.131
    Dec 30 04:26:15.140: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Dec 30 04:26:15.140: INFO: Node k8s-worker01 is running 0 daemon pod, expected 1
    Dec 30 04:26:16.152: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Dec 30 04:26:16.152: INFO: Node k8s-worker01 is running 0 daemon pod, expected 1
    Dec 30 04:26:17.153: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
    Dec 30 04:26:17.153: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 12/30/22 04:26:17.181
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3503, will wait for the garbage collector to delete the pods 12/30/22 04:26:17.181
    Dec 30 04:26:17.243: INFO: Deleting DaemonSet.extensions daemon-set took: 6.780586ms
    Dec 30 04:26:17.344: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.90055ms
    Dec 30 04:26:21.049: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 30 04:26:21.049: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Dec 30 04:26:21.053: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"447566"},"items":null}

    Dec 30 04:26:21.056: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"447566"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Dec 30 04:26:21.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-3503" for this suite. 12/30/22 04:26:21.087
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:26:21.096
Dec 30 04:26:21.096: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename gc 12/30/22 04:26:21.097
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:26:21.111
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:26:21.114
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
Dec 30 04:26:21.143: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"af76ca85-394a-4faa-824f-5ee8a28003e1", Controller:(*bool)(0xc005c974da), BlockOwnerDeletion:(*bool)(0xc005c974db)}}
Dec 30 04:26:21.150: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"2c112214-aefa-49db-9c67-d19e38915ca7", Controller:(*bool)(0xc00332730a), BlockOwnerDeletion:(*bool)(0xc00332730b)}}
Dec 30 04:26:21.156: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"68b0f191-cde6-4d2e-9917-4fc1bfc5b444", Controller:(*bool)(0xc003327592), BlockOwnerDeletion:(*bool)(0xc003327593)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Dec 30 04:26:26.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1754" for this suite. 12/30/22 04:26:26.176
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","completed":260,"skipped":4723,"failed":0}
------------------------------
• [SLOW TEST] [5.088 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:26:21.096
    Dec 30 04:26:21.096: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename gc 12/30/22 04:26:21.097
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:26:21.111
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:26:21.114
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    Dec 30 04:26:21.143: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"af76ca85-394a-4faa-824f-5ee8a28003e1", Controller:(*bool)(0xc005c974da), BlockOwnerDeletion:(*bool)(0xc005c974db)}}
    Dec 30 04:26:21.150: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"2c112214-aefa-49db-9c67-d19e38915ca7", Controller:(*bool)(0xc00332730a), BlockOwnerDeletion:(*bool)(0xc00332730b)}}
    Dec 30 04:26:21.156: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"68b0f191-cde6-4d2e-9917-4fc1bfc5b444", Controller:(*bool)(0xc003327592), BlockOwnerDeletion:(*bool)(0xc003327593)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Dec 30 04:26:26.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-1754" for this suite. 12/30/22 04:26:26.176
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:26:26.184
Dec 30 04:26:26.185: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename deployment 12/30/22 04:26:26.186
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:26:26.2
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:26:26.203
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 12/30/22 04:26:26.211
Dec 30 04:26:26.211: INFO: Creating simple deployment test-deployment-vvngb
Dec 30 04:26:26.225: INFO: deployment "test-deployment-vvngb" doesn't have the required revision set
STEP: Getting /status 12/30/22 04:26:28.242
Dec 30 04:26:28.247: INFO: Deployment test-deployment-vvngb has Conditions: [{Available True 2022-12-30 04:26:27 +0000 UTC 2022-12-30 04:26:27 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2022-12-30 04:26:27 +0000 UTC 2022-12-30 04:26:26 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-vvngb-777898ffcc" has successfully progressed.}]
STEP: updating Deployment Status 12/30/22 04:26:28.247
Dec 30 04:26:28.258: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.December, 30, 4, 26, 27, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 30, 4, 26, 27, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 30, 4, 26, 27, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 30, 4, 26, 26, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-vvngb-777898ffcc\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 12/30/22 04:26:28.258
Dec 30 04:26:28.261: INFO: Observed &Deployment event: ADDED
Dec 30 04:26:28.261: INFO: Observed Deployment test-deployment-vvngb in namespace deployment-7489 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-30 04:26:26 +0000 UTC 2022-12-30 04:26:26 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-vvngb-777898ffcc"}
Dec 30 04:26:28.261: INFO: Observed &Deployment event: MODIFIED
Dec 30 04:26:28.261: INFO: Observed Deployment test-deployment-vvngb in namespace deployment-7489 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-30 04:26:26 +0000 UTC 2022-12-30 04:26:26 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-vvngb-777898ffcc"}
Dec 30 04:26:28.261: INFO: Observed Deployment test-deployment-vvngb in namespace deployment-7489 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-12-30 04:26:26 +0000 UTC 2022-12-30 04:26:26 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Dec 30 04:26:28.261: INFO: Observed &Deployment event: MODIFIED
Dec 30 04:26:28.261: INFO: Observed Deployment test-deployment-vvngb in namespace deployment-7489 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-12-30 04:26:26 +0000 UTC 2022-12-30 04:26:26 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Dec 30 04:26:28.261: INFO: Observed Deployment test-deployment-vvngb in namespace deployment-7489 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-30 04:26:26 +0000 UTC 2022-12-30 04:26:26 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-vvngb-777898ffcc" is progressing.}
Dec 30 04:26:28.261: INFO: Observed &Deployment event: MODIFIED
Dec 30 04:26:28.261: INFO: Observed Deployment test-deployment-vvngb in namespace deployment-7489 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-12-30 04:26:27 +0000 UTC 2022-12-30 04:26:27 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Dec 30 04:26:28.261: INFO: Observed Deployment test-deployment-vvngb in namespace deployment-7489 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-30 04:26:27 +0000 UTC 2022-12-30 04:26:26 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-vvngb-777898ffcc" has successfully progressed.}
Dec 30 04:26:28.262: INFO: Observed &Deployment event: MODIFIED
Dec 30 04:26:28.262: INFO: Observed Deployment test-deployment-vvngb in namespace deployment-7489 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-12-30 04:26:27 +0000 UTC 2022-12-30 04:26:27 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Dec 30 04:26:28.262: INFO: Observed Deployment test-deployment-vvngb in namespace deployment-7489 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-30 04:26:27 +0000 UTC 2022-12-30 04:26:26 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-vvngb-777898ffcc" has successfully progressed.}
Dec 30 04:26:28.262: INFO: Found Deployment test-deployment-vvngb in namespace deployment-7489 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Dec 30 04:26:28.262: INFO: Deployment test-deployment-vvngb has an updated status
STEP: patching the Statefulset Status 12/30/22 04:26:28.262
Dec 30 04:26:28.262: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Dec 30 04:26:28.269: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 12/30/22 04:26:28.269
Dec 30 04:26:28.271: INFO: Observed &Deployment event: ADDED
Dec 30 04:26:28.271: INFO: Observed deployment test-deployment-vvngb in namespace deployment-7489 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-30 04:26:26 +0000 UTC 2022-12-30 04:26:26 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-vvngb-777898ffcc"}
Dec 30 04:26:28.271: INFO: Observed &Deployment event: MODIFIED
Dec 30 04:26:28.271: INFO: Observed deployment test-deployment-vvngb in namespace deployment-7489 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-30 04:26:26 +0000 UTC 2022-12-30 04:26:26 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-vvngb-777898ffcc"}
Dec 30 04:26:28.271: INFO: Observed deployment test-deployment-vvngb in namespace deployment-7489 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-12-30 04:26:26 +0000 UTC 2022-12-30 04:26:26 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Dec 30 04:26:28.272: INFO: Observed &Deployment event: MODIFIED
Dec 30 04:26:28.272: INFO: Observed deployment test-deployment-vvngb in namespace deployment-7489 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-12-30 04:26:26 +0000 UTC 2022-12-30 04:26:26 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Dec 30 04:26:28.272: INFO: Observed deployment test-deployment-vvngb in namespace deployment-7489 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-30 04:26:26 +0000 UTC 2022-12-30 04:26:26 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-vvngb-777898ffcc" is progressing.}
Dec 30 04:26:28.272: INFO: Observed &Deployment event: MODIFIED
Dec 30 04:26:28.272: INFO: Observed deployment test-deployment-vvngb in namespace deployment-7489 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-12-30 04:26:27 +0000 UTC 2022-12-30 04:26:27 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Dec 30 04:26:28.272: INFO: Observed deployment test-deployment-vvngb in namespace deployment-7489 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-30 04:26:27 +0000 UTC 2022-12-30 04:26:26 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-vvngb-777898ffcc" has successfully progressed.}
Dec 30 04:26:28.272: INFO: Observed &Deployment event: MODIFIED
Dec 30 04:26:28.272: INFO: Observed deployment test-deployment-vvngb in namespace deployment-7489 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-12-30 04:26:27 +0000 UTC 2022-12-30 04:26:27 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Dec 30 04:26:28.272: INFO: Observed deployment test-deployment-vvngb in namespace deployment-7489 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-30 04:26:27 +0000 UTC 2022-12-30 04:26:26 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-vvngb-777898ffcc" has successfully progressed.}
Dec 30 04:26:28.272: INFO: Observed deployment test-deployment-vvngb in namespace deployment-7489 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Dec 30 04:26:28.273: INFO: Observed &Deployment event: MODIFIED
Dec 30 04:26:28.273: INFO: Found deployment test-deployment-vvngb in namespace deployment-7489 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Dec 30 04:26:28.273: INFO: Deployment test-deployment-vvngb has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Dec 30 04:26:28.277: INFO: Deployment "test-deployment-vvngb":
&Deployment{ObjectMeta:{test-deployment-vvngb  deployment-7489  cf39a4ab-649a-43e1-be31-3a5d24cb3395 447711 1 2022-12-30 04:26:26 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2022-12-30 04:26:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-30 04:26:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2022-12-30 04:26:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0044ff4e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Dec 30 04:26:28.281: INFO: New ReplicaSet "test-deployment-vvngb-777898ffcc" of Deployment "test-deployment-vvngb":
&ReplicaSet{ObjectMeta:{test-deployment-vvngb-777898ffcc  deployment-7489  a1c13655-c4ac-47fa-b8b2-7c249f3d12bd 447706 1 2022-12-30 04:26:26 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-vvngb cf39a4ab-649a-43e1-be31-3a5d24cb3395 0xc0044ff8d0 0xc0044ff8d1}] [] [{kube-controller-manager Update apps/v1 2022-12-30 04:26:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cf39a4ab-649a-43e1-be31-3a5d24cb3395\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-30 04:26:27 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 777898ffcc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0044ff978 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Dec 30 04:26:28.286: INFO: Pod "test-deployment-vvngb-777898ffcc-bmjvw" is available:
&Pod{ObjectMeta:{test-deployment-vvngb-777898ffcc-bmjvw test-deployment-vvngb-777898ffcc- deployment-7489  8b3c0c0a-46c5-436a-80c4-3f10bcbebb6f 447705 0 2022-12-30 04:26:26 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[cni.projectcalico.org/containerID:ace7b4bf893a81d19d75205419c0ce41efb28d2d8f492825f714c767a81876cd cni.projectcalico.org/podIP:10.233.112.153/32 cni.projectcalico.org/podIPs:10.233.112.153/32] [{apps/v1 ReplicaSet test-deployment-vvngb-777898ffcc a1c13655-c4ac-47fa-b8b2-7c249f3d12bd 0xc0044ffd40 0xc0044ffd41}] [] [{Go-http-client Update v1 2022-12-30 04:26:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-12-30 04:26:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a1c13655-c4ac-47fa-b8b2-7c249f3d12bd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-30 04:26:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.112.153\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l564b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l564b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-mgmt01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 04:26:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 04:26:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 04:26:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 04:26:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.140,PodIP:10.233.112.153,StartTime:2022-12-30 04:26:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-30 04:26:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://29d72555e8ea164579f680d6adc035196e4230b0aa4c6b8bdab3c72d68688b1b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.112.153,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Dec 30 04:26:28.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7489" for this suite. 12/30/22 04:26:28.291
{"msg":"PASSED [sig-apps] Deployment should validate Deployment Status endpoints [Conformance]","completed":261,"skipped":4725,"failed":0}
------------------------------
• [2.113 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:26:26.184
    Dec 30 04:26:26.185: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename deployment 12/30/22 04:26:26.186
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:26:26.2
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:26:26.203
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 12/30/22 04:26:26.211
    Dec 30 04:26:26.211: INFO: Creating simple deployment test-deployment-vvngb
    Dec 30 04:26:26.225: INFO: deployment "test-deployment-vvngb" doesn't have the required revision set
    STEP: Getting /status 12/30/22 04:26:28.242
    Dec 30 04:26:28.247: INFO: Deployment test-deployment-vvngb has Conditions: [{Available True 2022-12-30 04:26:27 +0000 UTC 2022-12-30 04:26:27 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2022-12-30 04:26:27 +0000 UTC 2022-12-30 04:26:26 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-vvngb-777898ffcc" has successfully progressed.}]
    STEP: updating Deployment Status 12/30/22 04:26:28.247
    Dec 30 04:26:28.258: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.December, 30, 4, 26, 27, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 30, 4, 26, 27, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 30, 4, 26, 27, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 30, 4, 26, 26, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-vvngb-777898ffcc\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 12/30/22 04:26:28.258
    Dec 30 04:26:28.261: INFO: Observed &Deployment event: ADDED
    Dec 30 04:26:28.261: INFO: Observed Deployment test-deployment-vvngb in namespace deployment-7489 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-30 04:26:26 +0000 UTC 2022-12-30 04:26:26 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-vvngb-777898ffcc"}
    Dec 30 04:26:28.261: INFO: Observed &Deployment event: MODIFIED
    Dec 30 04:26:28.261: INFO: Observed Deployment test-deployment-vvngb in namespace deployment-7489 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-30 04:26:26 +0000 UTC 2022-12-30 04:26:26 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-vvngb-777898ffcc"}
    Dec 30 04:26:28.261: INFO: Observed Deployment test-deployment-vvngb in namespace deployment-7489 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-12-30 04:26:26 +0000 UTC 2022-12-30 04:26:26 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Dec 30 04:26:28.261: INFO: Observed &Deployment event: MODIFIED
    Dec 30 04:26:28.261: INFO: Observed Deployment test-deployment-vvngb in namespace deployment-7489 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-12-30 04:26:26 +0000 UTC 2022-12-30 04:26:26 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Dec 30 04:26:28.261: INFO: Observed Deployment test-deployment-vvngb in namespace deployment-7489 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-30 04:26:26 +0000 UTC 2022-12-30 04:26:26 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-vvngb-777898ffcc" is progressing.}
    Dec 30 04:26:28.261: INFO: Observed &Deployment event: MODIFIED
    Dec 30 04:26:28.261: INFO: Observed Deployment test-deployment-vvngb in namespace deployment-7489 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-12-30 04:26:27 +0000 UTC 2022-12-30 04:26:27 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Dec 30 04:26:28.261: INFO: Observed Deployment test-deployment-vvngb in namespace deployment-7489 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-30 04:26:27 +0000 UTC 2022-12-30 04:26:26 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-vvngb-777898ffcc" has successfully progressed.}
    Dec 30 04:26:28.262: INFO: Observed &Deployment event: MODIFIED
    Dec 30 04:26:28.262: INFO: Observed Deployment test-deployment-vvngb in namespace deployment-7489 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-12-30 04:26:27 +0000 UTC 2022-12-30 04:26:27 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Dec 30 04:26:28.262: INFO: Observed Deployment test-deployment-vvngb in namespace deployment-7489 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-30 04:26:27 +0000 UTC 2022-12-30 04:26:26 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-vvngb-777898ffcc" has successfully progressed.}
    Dec 30 04:26:28.262: INFO: Found Deployment test-deployment-vvngb in namespace deployment-7489 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Dec 30 04:26:28.262: INFO: Deployment test-deployment-vvngb has an updated status
    STEP: patching the Statefulset Status 12/30/22 04:26:28.262
    Dec 30 04:26:28.262: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Dec 30 04:26:28.269: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 12/30/22 04:26:28.269
    Dec 30 04:26:28.271: INFO: Observed &Deployment event: ADDED
    Dec 30 04:26:28.271: INFO: Observed deployment test-deployment-vvngb in namespace deployment-7489 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-30 04:26:26 +0000 UTC 2022-12-30 04:26:26 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-vvngb-777898ffcc"}
    Dec 30 04:26:28.271: INFO: Observed &Deployment event: MODIFIED
    Dec 30 04:26:28.271: INFO: Observed deployment test-deployment-vvngb in namespace deployment-7489 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-30 04:26:26 +0000 UTC 2022-12-30 04:26:26 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-vvngb-777898ffcc"}
    Dec 30 04:26:28.271: INFO: Observed deployment test-deployment-vvngb in namespace deployment-7489 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-12-30 04:26:26 +0000 UTC 2022-12-30 04:26:26 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Dec 30 04:26:28.272: INFO: Observed &Deployment event: MODIFIED
    Dec 30 04:26:28.272: INFO: Observed deployment test-deployment-vvngb in namespace deployment-7489 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-12-30 04:26:26 +0000 UTC 2022-12-30 04:26:26 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Dec 30 04:26:28.272: INFO: Observed deployment test-deployment-vvngb in namespace deployment-7489 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-30 04:26:26 +0000 UTC 2022-12-30 04:26:26 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-vvngb-777898ffcc" is progressing.}
    Dec 30 04:26:28.272: INFO: Observed &Deployment event: MODIFIED
    Dec 30 04:26:28.272: INFO: Observed deployment test-deployment-vvngb in namespace deployment-7489 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-12-30 04:26:27 +0000 UTC 2022-12-30 04:26:27 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Dec 30 04:26:28.272: INFO: Observed deployment test-deployment-vvngb in namespace deployment-7489 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-30 04:26:27 +0000 UTC 2022-12-30 04:26:26 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-vvngb-777898ffcc" has successfully progressed.}
    Dec 30 04:26:28.272: INFO: Observed &Deployment event: MODIFIED
    Dec 30 04:26:28.272: INFO: Observed deployment test-deployment-vvngb in namespace deployment-7489 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-12-30 04:26:27 +0000 UTC 2022-12-30 04:26:27 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Dec 30 04:26:28.272: INFO: Observed deployment test-deployment-vvngb in namespace deployment-7489 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-30 04:26:27 +0000 UTC 2022-12-30 04:26:26 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-vvngb-777898ffcc" has successfully progressed.}
    Dec 30 04:26:28.272: INFO: Observed deployment test-deployment-vvngb in namespace deployment-7489 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Dec 30 04:26:28.273: INFO: Observed &Deployment event: MODIFIED
    Dec 30 04:26:28.273: INFO: Found deployment test-deployment-vvngb in namespace deployment-7489 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    Dec 30 04:26:28.273: INFO: Deployment test-deployment-vvngb has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Dec 30 04:26:28.277: INFO: Deployment "test-deployment-vvngb":
    &Deployment{ObjectMeta:{test-deployment-vvngb  deployment-7489  cf39a4ab-649a-43e1-be31-3a5d24cb3395 447711 1 2022-12-30 04:26:26 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2022-12-30 04:26:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-30 04:26:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2022-12-30 04:26:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0044ff4e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Dec 30 04:26:28.281: INFO: New ReplicaSet "test-deployment-vvngb-777898ffcc" of Deployment "test-deployment-vvngb":
    &ReplicaSet{ObjectMeta:{test-deployment-vvngb-777898ffcc  deployment-7489  a1c13655-c4ac-47fa-b8b2-7c249f3d12bd 447706 1 2022-12-30 04:26:26 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-vvngb cf39a4ab-649a-43e1-be31-3a5d24cb3395 0xc0044ff8d0 0xc0044ff8d1}] [] [{kube-controller-manager Update apps/v1 2022-12-30 04:26:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cf39a4ab-649a-43e1-be31-3a5d24cb3395\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-30 04:26:27 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 777898ffcc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0044ff978 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Dec 30 04:26:28.286: INFO: Pod "test-deployment-vvngb-777898ffcc-bmjvw" is available:
    &Pod{ObjectMeta:{test-deployment-vvngb-777898ffcc-bmjvw test-deployment-vvngb-777898ffcc- deployment-7489  8b3c0c0a-46c5-436a-80c4-3f10bcbebb6f 447705 0 2022-12-30 04:26:26 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[cni.projectcalico.org/containerID:ace7b4bf893a81d19d75205419c0ce41efb28d2d8f492825f714c767a81876cd cni.projectcalico.org/podIP:10.233.112.153/32 cni.projectcalico.org/podIPs:10.233.112.153/32] [{apps/v1 ReplicaSet test-deployment-vvngb-777898ffcc a1c13655-c4ac-47fa-b8b2-7c249f3d12bd 0xc0044ffd40 0xc0044ffd41}] [] [{Go-http-client Update v1 2022-12-30 04:26:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-12-30 04:26:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a1c13655-c4ac-47fa-b8b2-7c249f3d12bd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-30 04:26:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.112.153\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l564b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l564b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-mgmt01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 04:26:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 04:26:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 04:26:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 04:26:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.140,PodIP:10.233.112.153,StartTime:2022-12-30 04:26:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-30 04:26:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://29d72555e8ea164579f680d6adc035196e4230b0aa4c6b8bdab3c72d68688b1b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.112.153,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Dec 30 04:26:28.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-7489" for this suite. 12/30/22 04:26:28.291
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:26:28.299
Dec 30 04:26:28.299: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename resourcequota 12/30/22 04:26:28.301
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:26:28.314
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:26:28.317
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874
STEP: Creating a ResourceQuota 12/30/22 04:26:28.32
STEP: Getting a ResourceQuota 12/30/22 04:26:28.325
STEP: Updating a ResourceQuota 12/30/22 04:26:28.329
STEP: Verifying a ResourceQuota was modified 12/30/22 04:26:28.335
STEP: Deleting a ResourceQuota 12/30/22 04:26:28.339
STEP: Verifying the deleted ResourceQuota 12/30/22 04:26:28.346
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Dec 30 04:26:28.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9808" for this suite. 12/30/22 04:26:28.355
{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","completed":262,"skipped":4740,"failed":0}
------------------------------
• [0.062 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:26:28.299
    Dec 30 04:26:28.299: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename resourcequota 12/30/22 04:26:28.301
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:26:28.314
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:26:28.317
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:874
    STEP: Creating a ResourceQuota 12/30/22 04:26:28.32
    STEP: Getting a ResourceQuota 12/30/22 04:26:28.325
    STEP: Updating a ResourceQuota 12/30/22 04:26:28.329
    STEP: Verifying a ResourceQuota was modified 12/30/22 04:26:28.335
    STEP: Deleting a ResourceQuota 12/30/22 04:26:28.339
    STEP: Verifying the deleted ResourceQuota 12/30/22 04:26:28.346
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Dec 30 04:26:28.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-9808" for this suite. 12/30/22 04:26:28.355
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:26:28.362
Dec 30 04:26:28.362: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename projected 12/30/22 04:26:28.364
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:26:28.377
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:26:28.38
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45
STEP: Creating projection with secret that has name projected-secret-test-7f2935bc-af06-4440-a9b7-ef4ae6ef930c 12/30/22 04:26:28.383
STEP: Creating a pod to test consume secrets 12/30/22 04:26:28.388
Dec 30 04:26:28.397: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5a6e2fd3-9d4e-40b6-8acc-8ddc1e6694d6" in namespace "projected-5354" to be "Succeeded or Failed"
Dec 30 04:26:28.400: INFO: Pod "pod-projected-secrets-5a6e2fd3-9d4e-40b6-8acc-8ddc1e6694d6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.073204ms
Dec 30 04:26:30.404: INFO: Pod "pod-projected-secrets-5a6e2fd3-9d4e-40b6-8acc-8ddc1e6694d6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007311971s
Dec 30 04:26:32.406: INFO: Pod "pod-projected-secrets-5a6e2fd3-9d4e-40b6-8acc-8ddc1e6694d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00926171s
STEP: Saw pod success 12/30/22 04:26:32.406
Dec 30 04:26:32.407: INFO: Pod "pod-projected-secrets-5a6e2fd3-9d4e-40b6-8acc-8ddc1e6694d6" satisfied condition "Succeeded or Failed"
Dec 30 04:26:32.411: INFO: Trying to get logs from node k8s-mgmt01 pod pod-projected-secrets-5a6e2fd3-9d4e-40b6-8acc-8ddc1e6694d6 container projected-secret-volume-test: <nil>
STEP: delete the pod 12/30/22 04:26:32.431
Dec 30 04:26:32.444: INFO: Waiting for pod pod-projected-secrets-5a6e2fd3-9d4e-40b6-8acc-8ddc1e6694d6 to disappear
Dec 30 04:26:32.447: INFO: Pod pod-projected-secrets-5a6e2fd3-9d4e-40b6-8acc-8ddc1e6694d6 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Dec 30 04:26:32.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5354" for this suite. 12/30/22 04:26:32.453
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","completed":263,"skipped":4746,"failed":0}
------------------------------
• [4.098 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:26:28.362
    Dec 30 04:26:28.362: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename projected 12/30/22 04:26:28.364
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:26:28.377
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:26:28.38
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:45
    STEP: Creating projection with secret that has name projected-secret-test-7f2935bc-af06-4440-a9b7-ef4ae6ef930c 12/30/22 04:26:28.383
    STEP: Creating a pod to test consume secrets 12/30/22 04:26:28.388
    Dec 30 04:26:28.397: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5a6e2fd3-9d4e-40b6-8acc-8ddc1e6694d6" in namespace "projected-5354" to be "Succeeded or Failed"
    Dec 30 04:26:28.400: INFO: Pod "pod-projected-secrets-5a6e2fd3-9d4e-40b6-8acc-8ddc1e6694d6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.073204ms
    Dec 30 04:26:30.404: INFO: Pod "pod-projected-secrets-5a6e2fd3-9d4e-40b6-8acc-8ddc1e6694d6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007311971s
    Dec 30 04:26:32.406: INFO: Pod "pod-projected-secrets-5a6e2fd3-9d4e-40b6-8acc-8ddc1e6694d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00926171s
    STEP: Saw pod success 12/30/22 04:26:32.406
    Dec 30 04:26:32.407: INFO: Pod "pod-projected-secrets-5a6e2fd3-9d4e-40b6-8acc-8ddc1e6694d6" satisfied condition "Succeeded or Failed"
    Dec 30 04:26:32.411: INFO: Trying to get logs from node k8s-mgmt01 pod pod-projected-secrets-5a6e2fd3-9d4e-40b6-8acc-8ddc1e6694d6 container projected-secret-volume-test: <nil>
    STEP: delete the pod 12/30/22 04:26:32.431
    Dec 30 04:26:32.444: INFO: Waiting for pod pod-projected-secrets-5a6e2fd3-9d4e-40b6-8acc-8ddc1e6694d6 to disappear
    Dec 30 04:26:32.447: INFO: Pod pod-projected-secrets-5a6e2fd3-9d4e-40b6-8acc-8ddc1e6694d6 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Dec 30 04:26:32.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5354" for this suite. 12/30/22 04:26:32.453
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:26:32.462
Dec 30 04:26:32.462: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename cronjob 12/30/22 04:26:32.463
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:26:32.477
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:26:32.48
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 12/30/22 04:26:32.483
STEP: Ensuring a job is scheduled 12/30/22 04:26:32.49
STEP: Ensuring exactly one is scheduled 12/30/22 04:27:00.495
STEP: Ensuring exactly one running job exists by listing jobs explicitly 12/30/22 04:27:00.498
STEP: Ensuring the job is replaced with a new one 12/30/22 04:27:00.502
STEP: Removing cronjob 12/30/22 04:28:00.508
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Dec 30 04:28:00.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-7525" for this suite. 12/30/22 04:28:00.52
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","completed":264,"skipped":4763,"failed":0}
------------------------------
• [SLOW TEST] [88.065 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:26:32.462
    Dec 30 04:26:32.462: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename cronjob 12/30/22 04:26:32.463
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:26:32.477
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:26:32.48
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 12/30/22 04:26:32.483
    STEP: Ensuring a job is scheduled 12/30/22 04:26:32.49
    STEP: Ensuring exactly one is scheduled 12/30/22 04:27:00.495
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 12/30/22 04:27:00.498
    STEP: Ensuring the job is replaced with a new one 12/30/22 04:27:00.502
    STEP: Removing cronjob 12/30/22 04:28:00.508
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Dec 30 04:28:00.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-7525" for this suite. 12/30/22 04:28:00.52
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:28:00.527
Dec 30 04:28:00.528: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename container-probe 12/30/22 04:28:00.529
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:28:00.543
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:28:00.546
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180
STEP: Creating pod liveness-6a6319f2-9f15-44f5-9660-f37243eea0ad in namespace container-probe-4041 12/30/22 04:28:00.548
Dec 30 04:28:00.557: INFO: Waiting up to 5m0s for pod "liveness-6a6319f2-9f15-44f5-9660-f37243eea0ad" in namespace "container-probe-4041" to be "not pending"
Dec 30 04:28:00.560: INFO: Pod "liveness-6a6319f2-9f15-44f5-9660-f37243eea0ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.872902ms
Dec 30 04:28:02.566: INFO: Pod "liveness-6a6319f2-9f15-44f5-9660-f37243eea0ad": Phase="Running", Reason="", readiness=true. Elapsed: 2.008628398s
Dec 30 04:28:02.566: INFO: Pod "liveness-6a6319f2-9f15-44f5-9660-f37243eea0ad" satisfied condition "not pending"
Dec 30 04:28:02.566: INFO: Started pod liveness-6a6319f2-9f15-44f5-9660-f37243eea0ad in namespace container-probe-4041
STEP: checking the pod's current state and verifying that restartCount is present 12/30/22 04:28:02.566
Dec 30 04:28:02.572: INFO: Initial restart count of pod liveness-6a6319f2-9f15-44f5-9660-f37243eea0ad is 0
STEP: deleting the pod 12/30/22 04:32:03.241
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Dec 30 04:32:03.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4041" for this suite. 12/30/22 04:32:03.262
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","completed":265,"skipped":4764,"failed":0}
------------------------------
• [SLOW TEST] [242.742 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:28:00.527
    Dec 30 04:28:00.528: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename container-probe 12/30/22 04:28:00.529
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:28:00.543
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:28:00.546
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:180
    STEP: Creating pod liveness-6a6319f2-9f15-44f5-9660-f37243eea0ad in namespace container-probe-4041 12/30/22 04:28:00.548
    Dec 30 04:28:00.557: INFO: Waiting up to 5m0s for pod "liveness-6a6319f2-9f15-44f5-9660-f37243eea0ad" in namespace "container-probe-4041" to be "not pending"
    Dec 30 04:28:00.560: INFO: Pod "liveness-6a6319f2-9f15-44f5-9660-f37243eea0ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.872902ms
    Dec 30 04:28:02.566: INFO: Pod "liveness-6a6319f2-9f15-44f5-9660-f37243eea0ad": Phase="Running", Reason="", readiness=true. Elapsed: 2.008628398s
    Dec 30 04:28:02.566: INFO: Pod "liveness-6a6319f2-9f15-44f5-9660-f37243eea0ad" satisfied condition "not pending"
    Dec 30 04:28:02.566: INFO: Started pod liveness-6a6319f2-9f15-44f5-9660-f37243eea0ad in namespace container-probe-4041
    STEP: checking the pod's current state and verifying that restartCount is present 12/30/22 04:28:02.566
    Dec 30 04:28:02.572: INFO: Initial restart count of pod liveness-6a6319f2-9f15-44f5-9660-f37243eea0ad is 0
    STEP: deleting the pod 12/30/22 04:32:03.241
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Dec 30 04:32:03.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-4041" for this suite. 12/30/22 04:32:03.262
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1650
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:32:03.271
Dec 30 04:32:03.271: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename kubectl 12/30/22 04:32:03.273
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:32:03.286
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:32:03.289
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1650
STEP: creating Agnhost RC 12/30/22 04:32:03.293
Dec 30 04:32:03.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-8577 create -f -'
Dec 30 04:32:04.281: INFO: stderr: ""
Dec 30 04:32:04.281: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 12/30/22 04:32:04.281
Dec 30 04:32:05.287: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 30 04:32:05.287: INFO: Found 0 / 1
Dec 30 04:32:06.286: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 30 04:32:06.286: INFO: Found 1 / 1
Dec 30 04:32:06.286: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 12/30/22 04:32:06.286
Dec 30 04:32:06.291: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 30 04:32:06.291: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec 30 04:32:06.291: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-8577 patch pod agnhost-primary-thdt2 -p {"metadata":{"annotations":{"x":"y"}}}'
Dec 30 04:32:06.404: INFO: stderr: ""
Dec 30 04:32:06.404: INFO: stdout: "pod/agnhost-primary-thdt2 patched\n"
STEP: checking annotations 12/30/22 04:32:06.404
Dec 30 04:32:06.408: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 30 04:32:06.409: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Dec 30 04:32:06.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8577" for this suite. 12/30/22 04:32:06.414
{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","completed":266,"skipped":4785,"failed":0}
------------------------------
• [3.150 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1644
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:32:03.271
    Dec 30 04:32:03.271: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename kubectl 12/30/22 04:32:03.273
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:32:03.286
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:32:03.289
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1650
    STEP: creating Agnhost RC 12/30/22 04:32:03.293
    Dec 30 04:32:03.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-8577 create -f -'
    Dec 30 04:32:04.281: INFO: stderr: ""
    Dec 30 04:32:04.281: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 12/30/22 04:32:04.281
    Dec 30 04:32:05.287: INFO: Selector matched 1 pods for map[app:agnhost]
    Dec 30 04:32:05.287: INFO: Found 0 / 1
    Dec 30 04:32:06.286: INFO: Selector matched 1 pods for map[app:agnhost]
    Dec 30 04:32:06.286: INFO: Found 1 / 1
    Dec 30 04:32:06.286: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 12/30/22 04:32:06.286
    Dec 30 04:32:06.291: INFO: Selector matched 1 pods for map[app:agnhost]
    Dec 30 04:32:06.291: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Dec 30 04:32:06.291: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-8577 patch pod agnhost-primary-thdt2 -p {"metadata":{"annotations":{"x":"y"}}}'
    Dec 30 04:32:06.404: INFO: stderr: ""
    Dec 30 04:32:06.404: INFO: stdout: "pod/agnhost-primary-thdt2 patched\n"
    STEP: checking annotations 12/30/22 04:32:06.404
    Dec 30 04:32:06.408: INFO: Selector matched 1 pods for map[app:agnhost]
    Dec 30 04:32:06.409: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Dec 30 04:32:06.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-8577" for this suite. 12/30/22 04:32:06.414
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:32:06.425
Dec 30 04:32:06.425: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename daemonsets 12/30/22 04:32:06.427
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:32:06.439
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:32:06.442
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431
Dec 30 04:32:06.482: INFO: Create a RollingUpdate DaemonSet
Dec 30 04:32:06.489: INFO: Check that daemon pods launch on every node of the cluster
Dec 30 04:32:06.498: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 30 04:32:06.498: INFO: Node k8s-mgmt01 is running 0 daemon pod, expected 1
Dec 30 04:32:07.510: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 30 04:32:07.510: INFO: Node k8s-mgmt01 is running 0 daemon pod, expected 1
Dec 30 04:32:08.511: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Dec 30 04:32:08.511: INFO: Node k8s-worker01 is running 0 daemon pod, expected 1
Dec 30 04:32:09.510: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
Dec 30 04:32:09.510: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
Dec 30 04:32:09.510: INFO: Update the DaemonSet to trigger a rollout
Dec 30 04:32:09.522: INFO: Updating DaemonSet daemon-set
Dec 30 04:32:11.544: INFO: Roll back the DaemonSet before rollout is complete
Dec 30 04:32:11.554: INFO: Updating DaemonSet daemon-set
Dec 30 04:32:11.554: INFO: Make sure DaemonSet rollback is complete
Dec 30 04:32:11.558: INFO: Wrong image for pod: daemon-set-lzcqq. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
Dec 30 04:32:11.558: INFO: Pod daemon-set-lzcqq is not available
Dec 30 04:32:16.570: INFO: Pod daemon-set-98vql is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 12/30/22 04:32:16.584
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3252, will wait for the garbage collector to delete the pods 12/30/22 04:32:16.584
Dec 30 04:32:16.646: INFO: Deleting DaemonSet.extensions daemon-set took: 7.218393ms
Dec 30 04:32:16.747: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.896668ms
Dec 30 04:32:47.952: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 30 04:32:47.952: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Dec 30 04:32:47.956: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"449013"},"items":null}

Dec 30 04:32:47.960: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"449013"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Dec 30 04:32:47.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3252" for this suite. 12/30/22 04:32:47.991
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","completed":267,"skipped":4832,"failed":0}
------------------------------
• [SLOW TEST] [41.573 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:32:06.425
    Dec 30 04:32:06.425: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename daemonsets 12/30/22 04:32:06.427
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:32:06.439
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:32:06.442
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:431
    Dec 30 04:32:06.482: INFO: Create a RollingUpdate DaemonSet
    Dec 30 04:32:06.489: INFO: Check that daemon pods launch on every node of the cluster
    Dec 30 04:32:06.498: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 30 04:32:06.498: INFO: Node k8s-mgmt01 is running 0 daemon pod, expected 1
    Dec 30 04:32:07.510: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 30 04:32:07.510: INFO: Node k8s-mgmt01 is running 0 daemon pod, expected 1
    Dec 30 04:32:08.511: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Dec 30 04:32:08.511: INFO: Node k8s-worker01 is running 0 daemon pod, expected 1
    Dec 30 04:32:09.510: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
    Dec 30 04:32:09.510: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
    Dec 30 04:32:09.510: INFO: Update the DaemonSet to trigger a rollout
    Dec 30 04:32:09.522: INFO: Updating DaemonSet daemon-set
    Dec 30 04:32:11.544: INFO: Roll back the DaemonSet before rollout is complete
    Dec 30 04:32:11.554: INFO: Updating DaemonSet daemon-set
    Dec 30 04:32:11.554: INFO: Make sure DaemonSet rollback is complete
    Dec 30 04:32:11.558: INFO: Wrong image for pod: daemon-set-lzcqq. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
    Dec 30 04:32:11.558: INFO: Pod daemon-set-lzcqq is not available
    Dec 30 04:32:16.570: INFO: Pod daemon-set-98vql is not available
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 12/30/22 04:32:16.584
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3252, will wait for the garbage collector to delete the pods 12/30/22 04:32:16.584
    Dec 30 04:32:16.646: INFO: Deleting DaemonSet.extensions daemon-set took: 7.218393ms
    Dec 30 04:32:16.747: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.896668ms
    Dec 30 04:32:47.952: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 30 04:32:47.952: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Dec 30 04:32:47.956: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"449013"},"items":null}

    Dec 30 04:32:47.960: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"449013"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Dec 30 04:32:47.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-3252" for this suite. 12/30/22 04:32:47.991
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:32:47.999
Dec 30 04:32:48.000: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename gc 12/30/22 04:32:48.001
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:32:48.016
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:32:48.019
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 12/30/22 04:32:48.028
STEP: delete the rc 12/30/22 04:32:53.04
STEP: wait for the rc to be deleted 12/30/22 04:32:53.047
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 12/30/22 04:32:58.052
STEP: Gathering metrics 12/30/22 04:33:28.078
Dec 30 04:33:28.115: INFO: Waiting up to 5m0s for pod "kube-controller-manager-k8s-mgmt03" in namespace "kube-system" to be "running and ready"
Dec 30 04:33:28.119: INFO: Pod "kube-controller-manager-k8s-mgmt03": Phase="Running", Reason="", readiness=true. Elapsed: 4.3577ms
Dec 30 04:33:28.119: INFO: The phase of Pod kube-controller-manager-k8s-mgmt03 is Running (Ready = true)
Dec 30 04:33:28.119: INFO: Pod "kube-controller-manager-k8s-mgmt03" satisfied condition "running and ready"
Dec 30 04:33:28.222: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Dec 30 04:33:28.222: INFO: Deleting pod "simpletest.rc-27fwd" in namespace "gc-878"
Dec 30 04:33:28.234: INFO: Deleting pod "simpletest.rc-479lq" in namespace "gc-878"
Dec 30 04:33:28.248: INFO: Deleting pod "simpletest.rc-4sltc" in namespace "gc-878"
Dec 30 04:33:28.259: INFO: Deleting pod "simpletest.rc-4vttf" in namespace "gc-878"
Dec 30 04:33:28.269: INFO: Deleting pod "simpletest.rc-595ll" in namespace "gc-878"
Dec 30 04:33:28.279: INFO: Deleting pod "simpletest.rc-62gz5" in namespace "gc-878"
Dec 30 04:33:28.293: INFO: Deleting pod "simpletest.rc-65tvz" in namespace "gc-878"
Dec 30 04:33:28.306: INFO: Deleting pod "simpletest.rc-68wtp" in namespace "gc-878"
Dec 30 04:33:28.316: INFO: Deleting pod "simpletest.rc-6sskv" in namespace "gc-878"
Dec 30 04:33:28.331: INFO: Deleting pod "simpletest.rc-6vmvd" in namespace "gc-878"
Dec 30 04:33:28.343: INFO: Deleting pod "simpletest.rc-6xzzt" in namespace "gc-878"
Dec 30 04:33:28.353: INFO: Deleting pod "simpletest.rc-7b9fp" in namespace "gc-878"
Dec 30 04:33:28.362: INFO: Deleting pod "simpletest.rc-7mq8j" in namespace "gc-878"
Dec 30 04:33:28.373: INFO: Deleting pod "simpletest.rc-845k4" in namespace "gc-878"
Dec 30 04:33:28.386: INFO: Deleting pod "simpletest.rc-8zbh9" in namespace "gc-878"
Dec 30 04:33:28.396: INFO: Deleting pod "simpletest.rc-8zjj5" in namespace "gc-878"
Dec 30 04:33:28.408: INFO: Deleting pod "simpletest.rc-9pjzb" in namespace "gc-878"
Dec 30 04:33:28.420: INFO: Deleting pod "simpletest.rc-b84w9" in namespace "gc-878"
Dec 30 04:33:28.432: INFO: Deleting pod "simpletest.rc-bgglr" in namespace "gc-878"
Dec 30 04:33:28.442: INFO: Deleting pod "simpletest.rc-bj2tn" in namespace "gc-878"
Dec 30 04:33:28.453: INFO: Deleting pod "simpletest.rc-bq4c5" in namespace "gc-878"
Dec 30 04:33:28.465: INFO: Deleting pod "simpletest.rc-c5dtf" in namespace "gc-878"
Dec 30 04:33:28.477: INFO: Deleting pod "simpletest.rc-c8grl" in namespace "gc-878"
Dec 30 04:33:28.486: INFO: Deleting pod "simpletest.rc-cm59d" in namespace "gc-878"
Dec 30 04:33:28.495: INFO: Deleting pod "simpletest.rc-cpngn" in namespace "gc-878"
Dec 30 04:33:28.506: INFO: Deleting pod "simpletest.rc-cstxd" in namespace "gc-878"
Dec 30 04:33:28.518: INFO: Deleting pod "simpletest.rc-cvs8b" in namespace "gc-878"
Dec 30 04:33:28.530: INFO: Deleting pod "simpletest.rc-czf9w" in namespace "gc-878"
Dec 30 04:33:28.543: INFO: Deleting pod "simpletest.rc-d2g6g" in namespace "gc-878"
Dec 30 04:33:28.553: INFO: Deleting pod "simpletest.rc-f78bk" in namespace "gc-878"
Dec 30 04:33:28.564: INFO: Deleting pod "simpletest.rc-f8qll" in namespace "gc-878"
Dec 30 04:33:28.575: INFO: Deleting pod "simpletest.rc-f9ks6" in namespace "gc-878"
Dec 30 04:33:28.585: INFO: Deleting pod "simpletest.rc-fb4w9" in namespace "gc-878"
Dec 30 04:33:28.595: INFO: Deleting pod "simpletest.rc-fqqcs" in namespace "gc-878"
Dec 30 04:33:28.612: INFO: Deleting pod "simpletest.rc-g22cd" in namespace "gc-878"
Dec 30 04:33:28.624: INFO: Deleting pod "simpletest.rc-g62xx" in namespace "gc-878"
Dec 30 04:33:28.632: INFO: Deleting pod "simpletest.rc-g6859" in namespace "gc-878"
Dec 30 04:33:28.643: INFO: Deleting pod "simpletest.rc-gftgv" in namespace "gc-878"
Dec 30 04:33:28.653: INFO: Deleting pod "simpletest.rc-gqv5q" in namespace "gc-878"
Dec 30 04:33:28.664: INFO: Deleting pod "simpletest.rc-gs2fj" in namespace "gc-878"
Dec 30 04:33:28.673: INFO: Deleting pod "simpletest.rc-h4n6r" in namespace "gc-878"
Dec 30 04:33:28.685: INFO: Deleting pod "simpletest.rc-h8njs" in namespace "gc-878"
Dec 30 04:33:28.696: INFO: Deleting pod "simpletest.rc-hb47b" in namespace "gc-878"
Dec 30 04:33:28.707: INFO: Deleting pod "simpletest.rc-hkdkx" in namespace "gc-878"
Dec 30 04:33:28.718: INFO: Deleting pod "simpletest.rc-hrtq8" in namespace "gc-878"
Dec 30 04:33:28.728: INFO: Deleting pod "simpletest.rc-jcnph" in namespace "gc-878"
Dec 30 04:33:28.736: INFO: Deleting pod "simpletest.rc-jtdvh" in namespace "gc-878"
Dec 30 04:33:28.764: INFO: Deleting pod "simpletest.rc-jxjcm" in namespace "gc-878"
Dec 30 04:33:28.771: INFO: Deleting pod "simpletest.rc-jxpzb" in namespace "gc-878"
Dec 30 04:33:28.781: INFO: Deleting pod "simpletest.rc-jzgg8" in namespace "gc-878"
Dec 30 04:33:28.792: INFO: Deleting pod "simpletest.rc-k5q7g" in namespace "gc-878"
Dec 30 04:33:28.801: INFO: Deleting pod "simpletest.rc-klrdr" in namespace "gc-878"
Dec 30 04:33:28.814: INFO: Deleting pod "simpletest.rc-km5w7" in namespace "gc-878"
Dec 30 04:33:28.825: INFO: Deleting pod "simpletest.rc-kx6d4" in namespace "gc-878"
Dec 30 04:33:28.835: INFO: Deleting pod "simpletest.rc-l2cvt" in namespace "gc-878"
Dec 30 04:33:28.846: INFO: Deleting pod "simpletest.rc-l7cxx" in namespace "gc-878"
Dec 30 04:33:28.858: INFO: Deleting pod "simpletest.rc-lhqlm" in namespace "gc-878"
Dec 30 04:33:28.867: INFO: Deleting pod "simpletest.rc-lmhk6" in namespace "gc-878"
Dec 30 04:33:28.878: INFO: Deleting pod "simpletest.rc-lmjgc" in namespace "gc-878"
Dec 30 04:33:28.888: INFO: Deleting pod "simpletest.rc-lnb2l" in namespace "gc-878"
Dec 30 04:33:28.898: INFO: Deleting pod "simpletest.rc-m89lv" in namespace "gc-878"
Dec 30 04:33:28.915: INFO: Deleting pod "simpletest.rc-mnfcn" in namespace "gc-878"
Dec 30 04:33:28.928: INFO: Deleting pod "simpletest.rc-n6xq2" in namespace "gc-878"
Dec 30 04:33:28.940: INFO: Deleting pod "simpletest.rc-n95k8" in namespace "gc-878"
Dec 30 04:33:28.972: INFO: Deleting pod "simpletest.rc-ndsjd" in namespace "gc-878"
Dec 30 04:33:29.016: INFO: Deleting pod "simpletest.rc-nlpkx" in namespace "gc-878"
Dec 30 04:33:29.068: INFO: Deleting pod "simpletest.rc-nnfsh" in namespace "gc-878"
Dec 30 04:33:29.120: INFO: Deleting pod "simpletest.rc-np9qw" in namespace "gc-878"
Dec 30 04:33:29.171: INFO: Deleting pod "simpletest.rc-p2mj7" in namespace "gc-878"
Dec 30 04:33:29.219: INFO: Deleting pod "simpletest.rc-p8gj8" in namespace "gc-878"
Dec 30 04:33:29.272: INFO: Deleting pod "simpletest.rc-p99z6" in namespace "gc-878"
Dec 30 04:33:29.315: INFO: Deleting pod "simpletest.rc-pmf8c" in namespace "gc-878"
Dec 30 04:33:29.366: INFO: Deleting pod "simpletest.rc-q6qtg" in namespace "gc-878"
Dec 30 04:33:29.417: INFO: Deleting pod "simpletest.rc-qb7f5" in namespace "gc-878"
Dec 30 04:33:29.471: INFO: Deleting pod "simpletest.rc-qplhj" in namespace "gc-878"
Dec 30 04:33:29.518: INFO: Deleting pod "simpletest.rc-rm787" in namespace "gc-878"
Dec 30 04:33:29.565: INFO: Deleting pod "simpletest.rc-rmvjj" in namespace "gc-878"
Dec 30 04:33:29.682: INFO: Deleting pod "simpletest.rc-skpqr" in namespace "gc-878"
Dec 30 04:33:29.715: INFO: Deleting pod "simpletest.rc-slw9q" in namespace "gc-878"
Dec 30 04:33:29.726: INFO: Deleting pod "simpletest.rc-srzvb" in namespace "gc-878"
Dec 30 04:33:29.765: INFO: Deleting pod "simpletest.rc-t6kj8" in namespace "gc-878"
Dec 30 04:33:29.820: INFO: Deleting pod "simpletest.rc-tmf94" in namespace "gc-878"
Dec 30 04:33:29.868: INFO: Deleting pod "simpletest.rc-tnvw6" in namespace "gc-878"
Dec 30 04:33:29.924: INFO: Deleting pod "simpletest.rc-tp8x7" in namespace "gc-878"
Dec 30 04:33:29.967: INFO: Deleting pod "simpletest.rc-ttpf9" in namespace "gc-878"
Dec 30 04:33:30.015: INFO: Deleting pod "simpletest.rc-ttxp6" in namespace "gc-878"
Dec 30 04:33:30.067: INFO: Deleting pod "simpletest.rc-tx2xc" in namespace "gc-878"
Dec 30 04:33:30.116: INFO: Deleting pod "simpletest.rc-v2rjg" in namespace "gc-878"
Dec 30 04:33:30.168: INFO: Deleting pod "simpletest.rc-v2tlw" in namespace "gc-878"
Dec 30 04:33:30.215: INFO: Deleting pod "simpletest.rc-v56r9" in namespace "gc-878"
Dec 30 04:33:30.268: INFO: Deleting pod "simpletest.rc-vmzrm" in namespace "gc-878"
Dec 30 04:33:30.316: INFO: Deleting pod "simpletest.rc-vphng" in namespace "gc-878"
Dec 30 04:33:30.368: INFO: Deleting pod "simpletest.rc-wn6sp" in namespace "gc-878"
Dec 30 04:33:30.417: INFO: Deleting pod "simpletest.rc-x4n75" in namespace "gc-878"
Dec 30 04:33:30.468: INFO: Deleting pod "simpletest.rc-x86xm" in namespace "gc-878"
Dec 30 04:33:30.514: INFO: Deleting pod "simpletest.rc-xlq8k" in namespace "gc-878"
Dec 30 04:33:30.569: INFO: Deleting pod "simpletest.rc-xqkpp" in namespace "gc-878"
Dec 30 04:33:30.620: INFO: Deleting pod "simpletest.rc-zbkfx" in namespace "gc-878"
Dec 30 04:33:30.664: INFO: Deleting pod "simpletest.rc-zwr6d" in namespace "gc-878"
Dec 30 04:33:30.717: INFO: Deleting pod "simpletest.rc-zzhm4" in namespace "gc-878"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Dec 30 04:33:30.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-878" for this suite. 12/30/22 04:33:30.813
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","completed":268,"skipped":4846,"failed":0}
------------------------------
• [SLOW TEST] [42.863 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:32:47.999
    Dec 30 04:32:48.000: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename gc 12/30/22 04:32:48.001
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:32:48.016
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:32:48.019
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 12/30/22 04:32:48.028
    STEP: delete the rc 12/30/22 04:32:53.04
    STEP: wait for the rc to be deleted 12/30/22 04:32:53.047
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 12/30/22 04:32:58.052
    STEP: Gathering metrics 12/30/22 04:33:28.078
    Dec 30 04:33:28.115: INFO: Waiting up to 5m0s for pod "kube-controller-manager-k8s-mgmt03" in namespace "kube-system" to be "running and ready"
    Dec 30 04:33:28.119: INFO: Pod "kube-controller-manager-k8s-mgmt03": Phase="Running", Reason="", readiness=true. Elapsed: 4.3577ms
    Dec 30 04:33:28.119: INFO: The phase of Pod kube-controller-manager-k8s-mgmt03 is Running (Ready = true)
    Dec 30 04:33:28.119: INFO: Pod "kube-controller-manager-k8s-mgmt03" satisfied condition "running and ready"
    Dec 30 04:33:28.222: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Dec 30 04:33:28.222: INFO: Deleting pod "simpletest.rc-27fwd" in namespace "gc-878"
    Dec 30 04:33:28.234: INFO: Deleting pod "simpletest.rc-479lq" in namespace "gc-878"
    Dec 30 04:33:28.248: INFO: Deleting pod "simpletest.rc-4sltc" in namespace "gc-878"
    Dec 30 04:33:28.259: INFO: Deleting pod "simpletest.rc-4vttf" in namespace "gc-878"
    Dec 30 04:33:28.269: INFO: Deleting pod "simpletest.rc-595ll" in namespace "gc-878"
    Dec 30 04:33:28.279: INFO: Deleting pod "simpletest.rc-62gz5" in namespace "gc-878"
    Dec 30 04:33:28.293: INFO: Deleting pod "simpletest.rc-65tvz" in namespace "gc-878"
    Dec 30 04:33:28.306: INFO: Deleting pod "simpletest.rc-68wtp" in namespace "gc-878"
    Dec 30 04:33:28.316: INFO: Deleting pod "simpletest.rc-6sskv" in namespace "gc-878"
    Dec 30 04:33:28.331: INFO: Deleting pod "simpletest.rc-6vmvd" in namespace "gc-878"
    Dec 30 04:33:28.343: INFO: Deleting pod "simpletest.rc-6xzzt" in namespace "gc-878"
    Dec 30 04:33:28.353: INFO: Deleting pod "simpletest.rc-7b9fp" in namespace "gc-878"
    Dec 30 04:33:28.362: INFO: Deleting pod "simpletest.rc-7mq8j" in namespace "gc-878"
    Dec 30 04:33:28.373: INFO: Deleting pod "simpletest.rc-845k4" in namespace "gc-878"
    Dec 30 04:33:28.386: INFO: Deleting pod "simpletest.rc-8zbh9" in namespace "gc-878"
    Dec 30 04:33:28.396: INFO: Deleting pod "simpletest.rc-8zjj5" in namespace "gc-878"
    Dec 30 04:33:28.408: INFO: Deleting pod "simpletest.rc-9pjzb" in namespace "gc-878"
    Dec 30 04:33:28.420: INFO: Deleting pod "simpletest.rc-b84w9" in namespace "gc-878"
    Dec 30 04:33:28.432: INFO: Deleting pod "simpletest.rc-bgglr" in namespace "gc-878"
    Dec 30 04:33:28.442: INFO: Deleting pod "simpletest.rc-bj2tn" in namespace "gc-878"
    Dec 30 04:33:28.453: INFO: Deleting pod "simpletest.rc-bq4c5" in namespace "gc-878"
    Dec 30 04:33:28.465: INFO: Deleting pod "simpletest.rc-c5dtf" in namespace "gc-878"
    Dec 30 04:33:28.477: INFO: Deleting pod "simpletest.rc-c8grl" in namespace "gc-878"
    Dec 30 04:33:28.486: INFO: Deleting pod "simpletest.rc-cm59d" in namespace "gc-878"
    Dec 30 04:33:28.495: INFO: Deleting pod "simpletest.rc-cpngn" in namespace "gc-878"
    Dec 30 04:33:28.506: INFO: Deleting pod "simpletest.rc-cstxd" in namespace "gc-878"
    Dec 30 04:33:28.518: INFO: Deleting pod "simpletest.rc-cvs8b" in namespace "gc-878"
    Dec 30 04:33:28.530: INFO: Deleting pod "simpletest.rc-czf9w" in namespace "gc-878"
    Dec 30 04:33:28.543: INFO: Deleting pod "simpletest.rc-d2g6g" in namespace "gc-878"
    Dec 30 04:33:28.553: INFO: Deleting pod "simpletest.rc-f78bk" in namespace "gc-878"
    Dec 30 04:33:28.564: INFO: Deleting pod "simpletest.rc-f8qll" in namespace "gc-878"
    Dec 30 04:33:28.575: INFO: Deleting pod "simpletest.rc-f9ks6" in namespace "gc-878"
    Dec 30 04:33:28.585: INFO: Deleting pod "simpletest.rc-fb4w9" in namespace "gc-878"
    Dec 30 04:33:28.595: INFO: Deleting pod "simpletest.rc-fqqcs" in namespace "gc-878"
    Dec 30 04:33:28.612: INFO: Deleting pod "simpletest.rc-g22cd" in namespace "gc-878"
    Dec 30 04:33:28.624: INFO: Deleting pod "simpletest.rc-g62xx" in namespace "gc-878"
    Dec 30 04:33:28.632: INFO: Deleting pod "simpletest.rc-g6859" in namespace "gc-878"
    Dec 30 04:33:28.643: INFO: Deleting pod "simpletest.rc-gftgv" in namespace "gc-878"
    Dec 30 04:33:28.653: INFO: Deleting pod "simpletest.rc-gqv5q" in namespace "gc-878"
    Dec 30 04:33:28.664: INFO: Deleting pod "simpletest.rc-gs2fj" in namespace "gc-878"
    Dec 30 04:33:28.673: INFO: Deleting pod "simpletest.rc-h4n6r" in namespace "gc-878"
    Dec 30 04:33:28.685: INFO: Deleting pod "simpletest.rc-h8njs" in namespace "gc-878"
    Dec 30 04:33:28.696: INFO: Deleting pod "simpletest.rc-hb47b" in namespace "gc-878"
    Dec 30 04:33:28.707: INFO: Deleting pod "simpletest.rc-hkdkx" in namespace "gc-878"
    Dec 30 04:33:28.718: INFO: Deleting pod "simpletest.rc-hrtq8" in namespace "gc-878"
    Dec 30 04:33:28.728: INFO: Deleting pod "simpletest.rc-jcnph" in namespace "gc-878"
    Dec 30 04:33:28.736: INFO: Deleting pod "simpletest.rc-jtdvh" in namespace "gc-878"
    Dec 30 04:33:28.764: INFO: Deleting pod "simpletest.rc-jxjcm" in namespace "gc-878"
    Dec 30 04:33:28.771: INFO: Deleting pod "simpletest.rc-jxpzb" in namespace "gc-878"
    Dec 30 04:33:28.781: INFO: Deleting pod "simpletest.rc-jzgg8" in namespace "gc-878"
    Dec 30 04:33:28.792: INFO: Deleting pod "simpletest.rc-k5q7g" in namespace "gc-878"
    Dec 30 04:33:28.801: INFO: Deleting pod "simpletest.rc-klrdr" in namespace "gc-878"
    Dec 30 04:33:28.814: INFO: Deleting pod "simpletest.rc-km5w7" in namespace "gc-878"
    Dec 30 04:33:28.825: INFO: Deleting pod "simpletest.rc-kx6d4" in namespace "gc-878"
    Dec 30 04:33:28.835: INFO: Deleting pod "simpletest.rc-l2cvt" in namespace "gc-878"
    Dec 30 04:33:28.846: INFO: Deleting pod "simpletest.rc-l7cxx" in namespace "gc-878"
    Dec 30 04:33:28.858: INFO: Deleting pod "simpletest.rc-lhqlm" in namespace "gc-878"
    Dec 30 04:33:28.867: INFO: Deleting pod "simpletest.rc-lmhk6" in namespace "gc-878"
    Dec 30 04:33:28.878: INFO: Deleting pod "simpletest.rc-lmjgc" in namespace "gc-878"
    Dec 30 04:33:28.888: INFO: Deleting pod "simpletest.rc-lnb2l" in namespace "gc-878"
    Dec 30 04:33:28.898: INFO: Deleting pod "simpletest.rc-m89lv" in namespace "gc-878"
    Dec 30 04:33:28.915: INFO: Deleting pod "simpletest.rc-mnfcn" in namespace "gc-878"
    Dec 30 04:33:28.928: INFO: Deleting pod "simpletest.rc-n6xq2" in namespace "gc-878"
    Dec 30 04:33:28.940: INFO: Deleting pod "simpletest.rc-n95k8" in namespace "gc-878"
    Dec 30 04:33:28.972: INFO: Deleting pod "simpletest.rc-ndsjd" in namespace "gc-878"
    Dec 30 04:33:29.016: INFO: Deleting pod "simpletest.rc-nlpkx" in namespace "gc-878"
    Dec 30 04:33:29.068: INFO: Deleting pod "simpletest.rc-nnfsh" in namespace "gc-878"
    Dec 30 04:33:29.120: INFO: Deleting pod "simpletest.rc-np9qw" in namespace "gc-878"
    Dec 30 04:33:29.171: INFO: Deleting pod "simpletest.rc-p2mj7" in namespace "gc-878"
    Dec 30 04:33:29.219: INFO: Deleting pod "simpletest.rc-p8gj8" in namespace "gc-878"
    Dec 30 04:33:29.272: INFO: Deleting pod "simpletest.rc-p99z6" in namespace "gc-878"
    Dec 30 04:33:29.315: INFO: Deleting pod "simpletest.rc-pmf8c" in namespace "gc-878"
    Dec 30 04:33:29.366: INFO: Deleting pod "simpletest.rc-q6qtg" in namespace "gc-878"
    Dec 30 04:33:29.417: INFO: Deleting pod "simpletest.rc-qb7f5" in namespace "gc-878"
    Dec 30 04:33:29.471: INFO: Deleting pod "simpletest.rc-qplhj" in namespace "gc-878"
    Dec 30 04:33:29.518: INFO: Deleting pod "simpletest.rc-rm787" in namespace "gc-878"
    Dec 30 04:33:29.565: INFO: Deleting pod "simpletest.rc-rmvjj" in namespace "gc-878"
    Dec 30 04:33:29.682: INFO: Deleting pod "simpletest.rc-skpqr" in namespace "gc-878"
    Dec 30 04:33:29.715: INFO: Deleting pod "simpletest.rc-slw9q" in namespace "gc-878"
    Dec 30 04:33:29.726: INFO: Deleting pod "simpletest.rc-srzvb" in namespace "gc-878"
    Dec 30 04:33:29.765: INFO: Deleting pod "simpletest.rc-t6kj8" in namespace "gc-878"
    Dec 30 04:33:29.820: INFO: Deleting pod "simpletest.rc-tmf94" in namespace "gc-878"
    Dec 30 04:33:29.868: INFO: Deleting pod "simpletest.rc-tnvw6" in namespace "gc-878"
    Dec 30 04:33:29.924: INFO: Deleting pod "simpletest.rc-tp8x7" in namespace "gc-878"
    Dec 30 04:33:29.967: INFO: Deleting pod "simpletest.rc-ttpf9" in namespace "gc-878"
    Dec 30 04:33:30.015: INFO: Deleting pod "simpletest.rc-ttxp6" in namespace "gc-878"
    Dec 30 04:33:30.067: INFO: Deleting pod "simpletest.rc-tx2xc" in namespace "gc-878"
    Dec 30 04:33:30.116: INFO: Deleting pod "simpletest.rc-v2rjg" in namespace "gc-878"
    Dec 30 04:33:30.168: INFO: Deleting pod "simpletest.rc-v2tlw" in namespace "gc-878"
    Dec 30 04:33:30.215: INFO: Deleting pod "simpletest.rc-v56r9" in namespace "gc-878"
    Dec 30 04:33:30.268: INFO: Deleting pod "simpletest.rc-vmzrm" in namespace "gc-878"
    Dec 30 04:33:30.316: INFO: Deleting pod "simpletest.rc-vphng" in namespace "gc-878"
    Dec 30 04:33:30.368: INFO: Deleting pod "simpletest.rc-wn6sp" in namespace "gc-878"
    Dec 30 04:33:30.417: INFO: Deleting pod "simpletest.rc-x4n75" in namespace "gc-878"
    Dec 30 04:33:30.468: INFO: Deleting pod "simpletest.rc-x86xm" in namespace "gc-878"
    Dec 30 04:33:30.514: INFO: Deleting pod "simpletest.rc-xlq8k" in namespace "gc-878"
    Dec 30 04:33:30.569: INFO: Deleting pod "simpletest.rc-xqkpp" in namespace "gc-878"
    Dec 30 04:33:30.620: INFO: Deleting pod "simpletest.rc-zbkfx" in namespace "gc-878"
    Dec 30 04:33:30.664: INFO: Deleting pod "simpletest.rc-zwr6d" in namespace "gc-878"
    Dec 30 04:33:30.717: INFO: Deleting pod "simpletest.rc-zzhm4" in namespace "gc-878"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Dec 30 04:33:30.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-878" for this suite. 12/30/22 04:33:30.813
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:33:30.865
Dec 30 04:33:30.865: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename projected 12/30/22 04:33:30.866
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:33:30.878
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:33:30.881
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67
STEP: Creating a pod to test downward API volume plugin 12/30/22 04:33:30.884
Dec 30 04:33:30.892: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9777754c-76fa-470a-8b07-7deab5b91c65" in namespace "projected-69" to be "Succeeded or Failed"
Dec 30 04:33:30.896: INFO: Pod "downwardapi-volume-9777754c-76fa-470a-8b07-7deab5b91c65": Phase="Pending", Reason="", readiness=false. Elapsed: 3.278686ms
Dec 30 04:33:32.901: INFO: Pod "downwardapi-volume-9777754c-76fa-470a-8b07-7deab5b91c65": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008307393s
Dec 30 04:33:34.901: INFO: Pod "downwardapi-volume-9777754c-76fa-470a-8b07-7deab5b91c65": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009047476s
Dec 30 04:33:37.031: INFO: Pod "downwardapi-volume-9777754c-76fa-470a-8b07-7deab5b91c65": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.138438376s
STEP: Saw pod success 12/30/22 04:33:37.031
Dec 30 04:33:37.031: INFO: Pod "downwardapi-volume-9777754c-76fa-470a-8b07-7deab5b91c65" satisfied condition "Succeeded or Failed"
Dec 30 04:33:37.035: INFO: Trying to get logs from node k8s-mgmt01 pod downwardapi-volume-9777754c-76fa-470a-8b07-7deab5b91c65 container client-container: <nil>
STEP: delete the pod 12/30/22 04:33:37.052
Dec 30 04:33:37.066: INFO: Waiting for pod downwardapi-volume-9777754c-76fa-470a-8b07-7deab5b91c65 to disappear
Dec 30 04:33:37.070: INFO: Pod downwardapi-volume-9777754c-76fa-470a-8b07-7deab5b91c65 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Dec 30 04:33:37.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-69" for this suite. 12/30/22 04:33:37.075
{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","completed":269,"skipped":4879,"failed":0}
------------------------------
• [SLOW TEST] [6.217 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:33:30.865
    Dec 30 04:33:30.865: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename projected 12/30/22 04:33:30.866
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:33:30.878
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:33:30.881
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:67
    STEP: Creating a pod to test downward API volume plugin 12/30/22 04:33:30.884
    Dec 30 04:33:30.892: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9777754c-76fa-470a-8b07-7deab5b91c65" in namespace "projected-69" to be "Succeeded or Failed"
    Dec 30 04:33:30.896: INFO: Pod "downwardapi-volume-9777754c-76fa-470a-8b07-7deab5b91c65": Phase="Pending", Reason="", readiness=false. Elapsed: 3.278686ms
    Dec 30 04:33:32.901: INFO: Pod "downwardapi-volume-9777754c-76fa-470a-8b07-7deab5b91c65": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008307393s
    Dec 30 04:33:34.901: INFO: Pod "downwardapi-volume-9777754c-76fa-470a-8b07-7deab5b91c65": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009047476s
    Dec 30 04:33:37.031: INFO: Pod "downwardapi-volume-9777754c-76fa-470a-8b07-7deab5b91c65": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.138438376s
    STEP: Saw pod success 12/30/22 04:33:37.031
    Dec 30 04:33:37.031: INFO: Pod "downwardapi-volume-9777754c-76fa-470a-8b07-7deab5b91c65" satisfied condition "Succeeded or Failed"
    Dec 30 04:33:37.035: INFO: Trying to get logs from node k8s-mgmt01 pod downwardapi-volume-9777754c-76fa-470a-8b07-7deab5b91c65 container client-container: <nil>
    STEP: delete the pod 12/30/22 04:33:37.052
    Dec 30 04:33:37.066: INFO: Waiting for pod downwardapi-volume-9777754c-76fa-470a-8b07-7deab5b91c65 to disappear
    Dec 30 04:33:37.070: INFO: Pod downwardapi-volume-9777754c-76fa-470a-8b07-7deab5b91c65 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Dec 30 04:33:37.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-69" for this suite. 12/30/22 04:33:37.075
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:33:37.082
Dec 30 04:33:37.083: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename gc 12/30/22 04:33:37.084
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:33:37.097
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:33:37.1
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 12/30/22 04:33:37.109
STEP: create the rc2 12/30/22 04:33:37.115
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 12/30/22 04:33:42.125
STEP: delete the rc simpletest-rc-to-be-deleted 12/30/22 04:33:42.487
STEP: wait for the rc to be deleted 12/30/22 04:33:42.493
Dec 30 04:33:47.515: INFO: 66 pods remaining
Dec 30 04:33:47.515: INFO: 66 pods has nil DeletionTimestamp
Dec 30 04:33:47.515: INFO: 
STEP: Gathering metrics 12/30/22 04:33:52.514
Dec 30 04:33:52.543: INFO: Waiting up to 5m0s for pod "kube-controller-manager-k8s-mgmt03" in namespace "kube-system" to be "running and ready"
Dec 30 04:33:52.546: INFO: Pod "kube-controller-manager-k8s-mgmt03": Phase="Running", Reason="", readiness=true. Elapsed: 3.501157ms
Dec 30 04:33:52.546: INFO: The phase of Pod kube-controller-manager-k8s-mgmt03 is Running (Ready = true)
Dec 30 04:33:52.546: INFO: Pod "kube-controller-manager-k8s-mgmt03" satisfied condition "running and ready"
Dec 30 04:33:52.648: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Dec 30 04:33:52.648: INFO: Deleting pod "simpletest-rc-to-be-deleted-276nz" in namespace "gc-3718"
Dec 30 04:33:52.658: INFO: Deleting pod "simpletest-rc-to-be-deleted-27gmz" in namespace "gc-3718"
Dec 30 04:33:52.672: INFO: Deleting pod "simpletest-rc-to-be-deleted-27nlf" in namespace "gc-3718"
Dec 30 04:33:52.681: INFO: Deleting pod "simpletest-rc-to-be-deleted-2dbl9" in namespace "gc-3718"
Dec 30 04:33:52.692: INFO: Deleting pod "simpletest-rc-to-be-deleted-55xc9" in namespace "gc-3718"
Dec 30 04:33:52.704: INFO: Deleting pod "simpletest-rc-to-be-deleted-5k2nz" in namespace "gc-3718"
Dec 30 04:33:52.715: INFO: Deleting pod "simpletest-rc-to-be-deleted-5tf8q" in namespace "gc-3718"
Dec 30 04:33:52.727: INFO: Deleting pod "simpletest-rc-to-be-deleted-5zvln" in namespace "gc-3718"
Dec 30 04:33:52.738: INFO: Deleting pod "simpletest-rc-to-be-deleted-69rnc" in namespace "gc-3718"
Dec 30 04:33:52.750: INFO: Deleting pod "simpletest-rc-to-be-deleted-75n48" in namespace "gc-3718"
Dec 30 04:33:52.760: INFO: Deleting pod "simpletest-rc-to-be-deleted-77784" in namespace "gc-3718"
Dec 30 04:33:52.773: INFO: Deleting pod "simpletest-rc-to-be-deleted-7fgl4" in namespace "gc-3718"
Dec 30 04:33:52.786: INFO: Deleting pod "simpletest-rc-to-be-deleted-7hq9m" in namespace "gc-3718"
Dec 30 04:33:52.799: INFO: Deleting pod "simpletest-rc-to-be-deleted-7jhfb" in namespace "gc-3718"
Dec 30 04:33:52.814: INFO: Deleting pod "simpletest-rc-to-be-deleted-7nc7q" in namespace "gc-3718"
Dec 30 04:33:52.825: INFO: Deleting pod "simpletest-rc-to-be-deleted-7pp5w" in namespace "gc-3718"
Dec 30 04:33:52.836: INFO: Deleting pod "simpletest-rc-to-be-deleted-7rrbx" in namespace "gc-3718"
Dec 30 04:33:52.846: INFO: Deleting pod "simpletest-rc-to-be-deleted-7tl58" in namespace "gc-3718"
Dec 30 04:33:52.858: INFO: Deleting pod "simpletest-rc-to-be-deleted-8qjsx" in namespace "gc-3718"
Dec 30 04:33:52.871: INFO: Deleting pod "simpletest-rc-to-be-deleted-8sggz" in namespace "gc-3718"
Dec 30 04:33:52.884: INFO: Deleting pod "simpletest-rc-to-be-deleted-8vjmh" in namespace "gc-3718"
Dec 30 04:33:52.895: INFO: Deleting pod "simpletest-rc-to-be-deleted-94k8d" in namespace "gc-3718"
Dec 30 04:33:52.906: INFO: Deleting pod "simpletest-rc-to-be-deleted-9bqrr" in namespace "gc-3718"
Dec 30 04:33:52.922: INFO: Deleting pod "simpletest-rc-to-be-deleted-9bvp5" in namespace "gc-3718"
Dec 30 04:33:52.943: INFO: Deleting pod "simpletest-rc-to-be-deleted-9dvv7" in namespace "gc-3718"
Dec 30 04:33:52.958: INFO: Deleting pod "simpletest-rc-to-be-deleted-bcv78" in namespace "gc-3718"
Dec 30 04:33:52.970: INFO: Deleting pod "simpletest-rc-to-be-deleted-c7bmm" in namespace "gc-3718"
Dec 30 04:33:52.980: INFO: Deleting pod "simpletest-rc-to-be-deleted-c7mvd" in namespace "gc-3718"
Dec 30 04:33:52.989: INFO: Deleting pod "simpletest-rc-to-be-deleted-c9r44" in namespace "gc-3718"
Dec 30 04:33:52.998: INFO: Deleting pod "simpletest-rc-to-be-deleted-csc8d" in namespace "gc-3718"
Dec 30 04:33:53.016: INFO: Deleting pod "simpletest-rc-to-be-deleted-d8xtk" in namespace "gc-3718"
Dec 30 04:33:53.032: INFO: Deleting pod "simpletest-rc-to-be-deleted-ddtsf" in namespace "gc-3718"
Dec 30 04:33:53.047: INFO: Deleting pod "simpletest-rc-to-be-deleted-dnbl5" in namespace "gc-3718"
Dec 30 04:33:53.059: INFO: Deleting pod "simpletest-rc-to-be-deleted-dw6qn" in namespace "gc-3718"
Dec 30 04:33:53.071: INFO: Deleting pod "simpletest-rc-to-be-deleted-frplt" in namespace "gc-3718"
Dec 30 04:33:53.081: INFO: Deleting pod "simpletest-rc-to-be-deleted-fvqw9" in namespace "gc-3718"
Dec 30 04:33:53.093: INFO: Deleting pod "simpletest-rc-to-be-deleted-g26vq" in namespace "gc-3718"
Dec 30 04:33:53.104: INFO: Deleting pod "simpletest-rc-to-be-deleted-g26w4" in namespace "gc-3718"
Dec 30 04:33:53.115: INFO: Deleting pod "simpletest-rc-to-be-deleted-g28c2" in namespace "gc-3718"
Dec 30 04:33:53.122: INFO: Deleting pod "simpletest-rc-to-be-deleted-g5txr" in namespace "gc-3718"
Dec 30 04:33:53.132: INFO: Deleting pod "simpletest-rc-to-be-deleted-gghnt" in namespace "gc-3718"
Dec 30 04:33:53.150: INFO: Deleting pod "simpletest-rc-to-be-deleted-gh9xz" in namespace "gc-3718"
Dec 30 04:33:53.159: INFO: Deleting pod "simpletest-rc-to-be-deleted-ghd49" in namespace "gc-3718"
Dec 30 04:33:53.168: INFO: Deleting pod "simpletest-rc-to-be-deleted-gjf55" in namespace "gc-3718"
Dec 30 04:33:53.179: INFO: Deleting pod "simpletest-rc-to-be-deleted-hgjwm" in namespace "gc-3718"
Dec 30 04:33:53.188: INFO: Deleting pod "simpletest-rc-to-be-deleted-hlzkz" in namespace "gc-3718"
Dec 30 04:33:53.200: INFO: Deleting pod "simpletest-rc-to-be-deleted-hm4md" in namespace "gc-3718"
Dec 30 04:33:53.209: INFO: Deleting pod "simpletest-rc-to-be-deleted-hs4pl" in namespace "gc-3718"
Dec 30 04:33:53.219: INFO: Deleting pod "simpletest-rc-to-be-deleted-j5q7c" in namespace "gc-3718"
Dec 30 04:33:53.231: INFO: Deleting pod "simpletest-rc-to-be-deleted-j5rw2" in namespace "gc-3718"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Dec 30 04:33:53.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3718" for this suite. 12/30/22 04:33:53.247
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","completed":270,"skipped":4881,"failed":0}
------------------------------
• [SLOW TEST] [16.171 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:33:37.082
    Dec 30 04:33:37.083: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename gc 12/30/22 04:33:37.084
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:33:37.097
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:33:37.1
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 12/30/22 04:33:37.109
    STEP: create the rc2 12/30/22 04:33:37.115
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 12/30/22 04:33:42.125
    STEP: delete the rc simpletest-rc-to-be-deleted 12/30/22 04:33:42.487
    STEP: wait for the rc to be deleted 12/30/22 04:33:42.493
    Dec 30 04:33:47.515: INFO: 66 pods remaining
    Dec 30 04:33:47.515: INFO: 66 pods has nil DeletionTimestamp
    Dec 30 04:33:47.515: INFO: 
    STEP: Gathering metrics 12/30/22 04:33:52.514
    Dec 30 04:33:52.543: INFO: Waiting up to 5m0s for pod "kube-controller-manager-k8s-mgmt03" in namespace "kube-system" to be "running and ready"
    Dec 30 04:33:52.546: INFO: Pod "kube-controller-manager-k8s-mgmt03": Phase="Running", Reason="", readiness=true. Elapsed: 3.501157ms
    Dec 30 04:33:52.546: INFO: The phase of Pod kube-controller-manager-k8s-mgmt03 is Running (Ready = true)
    Dec 30 04:33:52.546: INFO: Pod "kube-controller-manager-k8s-mgmt03" satisfied condition "running and ready"
    Dec 30 04:33:52.648: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Dec 30 04:33:52.648: INFO: Deleting pod "simpletest-rc-to-be-deleted-276nz" in namespace "gc-3718"
    Dec 30 04:33:52.658: INFO: Deleting pod "simpletest-rc-to-be-deleted-27gmz" in namespace "gc-3718"
    Dec 30 04:33:52.672: INFO: Deleting pod "simpletest-rc-to-be-deleted-27nlf" in namespace "gc-3718"
    Dec 30 04:33:52.681: INFO: Deleting pod "simpletest-rc-to-be-deleted-2dbl9" in namespace "gc-3718"
    Dec 30 04:33:52.692: INFO: Deleting pod "simpletest-rc-to-be-deleted-55xc9" in namespace "gc-3718"
    Dec 30 04:33:52.704: INFO: Deleting pod "simpletest-rc-to-be-deleted-5k2nz" in namespace "gc-3718"
    Dec 30 04:33:52.715: INFO: Deleting pod "simpletest-rc-to-be-deleted-5tf8q" in namespace "gc-3718"
    Dec 30 04:33:52.727: INFO: Deleting pod "simpletest-rc-to-be-deleted-5zvln" in namespace "gc-3718"
    Dec 30 04:33:52.738: INFO: Deleting pod "simpletest-rc-to-be-deleted-69rnc" in namespace "gc-3718"
    Dec 30 04:33:52.750: INFO: Deleting pod "simpletest-rc-to-be-deleted-75n48" in namespace "gc-3718"
    Dec 30 04:33:52.760: INFO: Deleting pod "simpletest-rc-to-be-deleted-77784" in namespace "gc-3718"
    Dec 30 04:33:52.773: INFO: Deleting pod "simpletest-rc-to-be-deleted-7fgl4" in namespace "gc-3718"
    Dec 30 04:33:52.786: INFO: Deleting pod "simpletest-rc-to-be-deleted-7hq9m" in namespace "gc-3718"
    Dec 30 04:33:52.799: INFO: Deleting pod "simpletest-rc-to-be-deleted-7jhfb" in namespace "gc-3718"
    Dec 30 04:33:52.814: INFO: Deleting pod "simpletest-rc-to-be-deleted-7nc7q" in namespace "gc-3718"
    Dec 30 04:33:52.825: INFO: Deleting pod "simpletest-rc-to-be-deleted-7pp5w" in namespace "gc-3718"
    Dec 30 04:33:52.836: INFO: Deleting pod "simpletest-rc-to-be-deleted-7rrbx" in namespace "gc-3718"
    Dec 30 04:33:52.846: INFO: Deleting pod "simpletest-rc-to-be-deleted-7tl58" in namespace "gc-3718"
    Dec 30 04:33:52.858: INFO: Deleting pod "simpletest-rc-to-be-deleted-8qjsx" in namespace "gc-3718"
    Dec 30 04:33:52.871: INFO: Deleting pod "simpletest-rc-to-be-deleted-8sggz" in namespace "gc-3718"
    Dec 30 04:33:52.884: INFO: Deleting pod "simpletest-rc-to-be-deleted-8vjmh" in namespace "gc-3718"
    Dec 30 04:33:52.895: INFO: Deleting pod "simpletest-rc-to-be-deleted-94k8d" in namespace "gc-3718"
    Dec 30 04:33:52.906: INFO: Deleting pod "simpletest-rc-to-be-deleted-9bqrr" in namespace "gc-3718"
    Dec 30 04:33:52.922: INFO: Deleting pod "simpletest-rc-to-be-deleted-9bvp5" in namespace "gc-3718"
    Dec 30 04:33:52.943: INFO: Deleting pod "simpletest-rc-to-be-deleted-9dvv7" in namespace "gc-3718"
    Dec 30 04:33:52.958: INFO: Deleting pod "simpletest-rc-to-be-deleted-bcv78" in namespace "gc-3718"
    Dec 30 04:33:52.970: INFO: Deleting pod "simpletest-rc-to-be-deleted-c7bmm" in namespace "gc-3718"
    Dec 30 04:33:52.980: INFO: Deleting pod "simpletest-rc-to-be-deleted-c7mvd" in namespace "gc-3718"
    Dec 30 04:33:52.989: INFO: Deleting pod "simpletest-rc-to-be-deleted-c9r44" in namespace "gc-3718"
    Dec 30 04:33:52.998: INFO: Deleting pod "simpletest-rc-to-be-deleted-csc8d" in namespace "gc-3718"
    Dec 30 04:33:53.016: INFO: Deleting pod "simpletest-rc-to-be-deleted-d8xtk" in namespace "gc-3718"
    Dec 30 04:33:53.032: INFO: Deleting pod "simpletest-rc-to-be-deleted-ddtsf" in namespace "gc-3718"
    Dec 30 04:33:53.047: INFO: Deleting pod "simpletest-rc-to-be-deleted-dnbl5" in namespace "gc-3718"
    Dec 30 04:33:53.059: INFO: Deleting pod "simpletest-rc-to-be-deleted-dw6qn" in namespace "gc-3718"
    Dec 30 04:33:53.071: INFO: Deleting pod "simpletest-rc-to-be-deleted-frplt" in namespace "gc-3718"
    Dec 30 04:33:53.081: INFO: Deleting pod "simpletest-rc-to-be-deleted-fvqw9" in namespace "gc-3718"
    Dec 30 04:33:53.093: INFO: Deleting pod "simpletest-rc-to-be-deleted-g26vq" in namespace "gc-3718"
    Dec 30 04:33:53.104: INFO: Deleting pod "simpletest-rc-to-be-deleted-g26w4" in namespace "gc-3718"
    Dec 30 04:33:53.115: INFO: Deleting pod "simpletest-rc-to-be-deleted-g28c2" in namespace "gc-3718"
    Dec 30 04:33:53.122: INFO: Deleting pod "simpletest-rc-to-be-deleted-g5txr" in namespace "gc-3718"
    Dec 30 04:33:53.132: INFO: Deleting pod "simpletest-rc-to-be-deleted-gghnt" in namespace "gc-3718"
    Dec 30 04:33:53.150: INFO: Deleting pod "simpletest-rc-to-be-deleted-gh9xz" in namespace "gc-3718"
    Dec 30 04:33:53.159: INFO: Deleting pod "simpletest-rc-to-be-deleted-ghd49" in namespace "gc-3718"
    Dec 30 04:33:53.168: INFO: Deleting pod "simpletest-rc-to-be-deleted-gjf55" in namespace "gc-3718"
    Dec 30 04:33:53.179: INFO: Deleting pod "simpletest-rc-to-be-deleted-hgjwm" in namespace "gc-3718"
    Dec 30 04:33:53.188: INFO: Deleting pod "simpletest-rc-to-be-deleted-hlzkz" in namespace "gc-3718"
    Dec 30 04:33:53.200: INFO: Deleting pod "simpletest-rc-to-be-deleted-hm4md" in namespace "gc-3718"
    Dec 30 04:33:53.209: INFO: Deleting pod "simpletest-rc-to-be-deleted-hs4pl" in namespace "gc-3718"
    Dec 30 04:33:53.219: INFO: Deleting pod "simpletest-rc-to-be-deleted-j5q7c" in namespace "gc-3718"
    Dec 30 04:33:53.231: INFO: Deleting pod "simpletest-rc-to-be-deleted-j5rw2" in namespace "gc-3718"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Dec 30 04:33:53.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-3718" for this suite. 12/30/22 04:33:53.247
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:33:53.254
Dec 30 04:33:53.254: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename projected 12/30/22 04:33:53.255
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:33:53.268
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:33:53.271
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123
STEP: Creating projection with configMap that has name projected-configmap-test-upd-cf0c719b-a258-4cfb-a267-6f6043df975b 12/30/22 04:33:53.278
STEP: Creating the pod 12/30/22 04:33:53.282
Dec 30 04:33:53.297: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-30725232-f229-409c-b598-3b92298b0368" in namespace "projected-6717" to be "running and ready"
Dec 30 04:33:53.300: INFO: Pod "pod-projected-configmaps-30725232-f229-409c-b598-3b92298b0368": Phase="Pending", Reason="", readiness=false. Elapsed: 2.925481ms
Dec 30 04:33:53.300: INFO: The phase of Pod pod-projected-configmaps-30725232-f229-409c-b598-3b92298b0368 is Pending, waiting for it to be Running (with Ready = true)
Dec 30 04:33:55.305: INFO: Pod "pod-projected-configmaps-30725232-f229-409c-b598-3b92298b0368": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008025138s
Dec 30 04:33:55.305: INFO: The phase of Pod pod-projected-configmaps-30725232-f229-409c-b598-3b92298b0368 is Pending, waiting for it to be Running (with Ready = true)
Dec 30 04:33:57.305: INFO: Pod "pod-projected-configmaps-30725232-f229-409c-b598-3b92298b0368": Phase="Running", Reason="", readiness=true. Elapsed: 4.007478673s
Dec 30 04:33:57.305: INFO: The phase of Pod pod-projected-configmaps-30725232-f229-409c-b598-3b92298b0368 is Running (Ready = true)
Dec 30 04:33:57.305: INFO: Pod "pod-projected-configmaps-30725232-f229-409c-b598-3b92298b0368" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-cf0c719b-a258-4cfb-a267-6f6043df975b 12/30/22 04:33:57.316
STEP: waiting to observe update in volume 12/30/22 04:33:57.321
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Dec 30 04:34:59.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6717" for this suite. 12/30/22 04:34:59.64
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","completed":271,"skipped":4886,"failed":0}
------------------------------
• [SLOW TEST] [66.392 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:33:53.254
    Dec 30 04:33:53.254: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename projected 12/30/22 04:33:53.255
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:33:53.268
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:33:53.271
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:123
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-cf0c719b-a258-4cfb-a267-6f6043df975b 12/30/22 04:33:53.278
    STEP: Creating the pod 12/30/22 04:33:53.282
    Dec 30 04:33:53.297: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-30725232-f229-409c-b598-3b92298b0368" in namespace "projected-6717" to be "running and ready"
    Dec 30 04:33:53.300: INFO: Pod "pod-projected-configmaps-30725232-f229-409c-b598-3b92298b0368": Phase="Pending", Reason="", readiness=false. Elapsed: 2.925481ms
    Dec 30 04:33:53.300: INFO: The phase of Pod pod-projected-configmaps-30725232-f229-409c-b598-3b92298b0368 is Pending, waiting for it to be Running (with Ready = true)
    Dec 30 04:33:55.305: INFO: Pod "pod-projected-configmaps-30725232-f229-409c-b598-3b92298b0368": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008025138s
    Dec 30 04:33:55.305: INFO: The phase of Pod pod-projected-configmaps-30725232-f229-409c-b598-3b92298b0368 is Pending, waiting for it to be Running (with Ready = true)
    Dec 30 04:33:57.305: INFO: Pod "pod-projected-configmaps-30725232-f229-409c-b598-3b92298b0368": Phase="Running", Reason="", readiness=true. Elapsed: 4.007478673s
    Dec 30 04:33:57.305: INFO: The phase of Pod pod-projected-configmaps-30725232-f229-409c-b598-3b92298b0368 is Running (Ready = true)
    Dec 30 04:33:57.305: INFO: Pod "pod-projected-configmaps-30725232-f229-409c-b598-3b92298b0368" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-cf0c719b-a258-4cfb-a267-6f6043df975b 12/30/22 04:33:57.316
    STEP: waiting to observe update in volume 12/30/22 04:33:57.321
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Dec 30 04:34:59.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6717" for this suite. 12/30/22 04:34:59.64
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:34:59.647
Dec 30 04:34:59.647: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename pods 12/30/22 04:34:59.649
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:34:59.663
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:34:59.666
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397
STEP: creating the pod 12/30/22 04:34:59.669
STEP: submitting the pod to kubernetes 12/30/22 04:34:59.669
Dec 30 04:34:59.678: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-825faac3-5c86-4183-a4bf-769531ee3865" in namespace "pods-5531" to be "running and ready"
Dec 30 04:34:59.682: INFO: Pod "pod-update-activedeadlineseconds-825faac3-5c86-4183-a4bf-769531ee3865": Phase="Pending", Reason="", readiness=false. Elapsed: 4.339101ms
Dec 30 04:34:59.682: INFO: The phase of Pod pod-update-activedeadlineseconds-825faac3-5c86-4183-a4bf-769531ee3865 is Pending, waiting for it to be Running (with Ready = true)
Dec 30 04:35:01.687: INFO: Pod "pod-update-activedeadlineseconds-825faac3-5c86-4183-a4bf-769531ee3865": Phase="Running", Reason="", readiness=true. Elapsed: 2.009691326s
Dec 30 04:35:01.687: INFO: The phase of Pod pod-update-activedeadlineseconds-825faac3-5c86-4183-a4bf-769531ee3865 is Running (Ready = true)
Dec 30 04:35:01.687: INFO: Pod "pod-update-activedeadlineseconds-825faac3-5c86-4183-a4bf-769531ee3865" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 12/30/22 04:35:01.691
STEP: updating the pod 12/30/22 04:35:01.695
Dec 30 04:35:02.209: INFO: Successfully updated pod "pod-update-activedeadlineseconds-825faac3-5c86-4183-a4bf-769531ee3865"
Dec 30 04:35:02.209: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-825faac3-5c86-4183-a4bf-769531ee3865" in namespace "pods-5531" to be "terminated with reason DeadlineExceeded"
Dec 30 04:35:02.214: INFO: Pod "pod-update-activedeadlineseconds-825faac3-5c86-4183-a4bf-769531ee3865": Phase="Running", Reason="", readiness=true. Elapsed: 4.179673ms
Dec 30 04:35:04.220: INFO: Pod "pod-update-activedeadlineseconds-825faac3-5c86-4183-a4bf-769531ee3865": Phase="Running", Reason="", readiness=true. Elapsed: 2.010316128s
Dec 30 04:35:06.219: INFO: Pod "pod-update-activedeadlineseconds-825faac3-5c86-4183-a4bf-769531ee3865": Phase="Running", Reason="", readiness=false. Elapsed: 4.00934665s
Dec 30 04:35:08.219: INFO: Pod "pod-update-activedeadlineseconds-825faac3-5c86-4183-a4bf-769531ee3865": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.009375501s
Dec 30 04:35:08.219: INFO: Pod "pod-update-activedeadlineseconds-825faac3-5c86-4183-a4bf-769531ee3865" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Dec 30 04:35:08.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5531" for this suite. 12/30/22 04:35:08.225
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","completed":272,"skipped":4890,"failed":0}
------------------------------
• [SLOW TEST] [8.586 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:34:59.647
    Dec 30 04:34:59.647: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename pods 12/30/22 04:34:59.649
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:34:59.663
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:34:59.666
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:397
    STEP: creating the pod 12/30/22 04:34:59.669
    STEP: submitting the pod to kubernetes 12/30/22 04:34:59.669
    Dec 30 04:34:59.678: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-825faac3-5c86-4183-a4bf-769531ee3865" in namespace "pods-5531" to be "running and ready"
    Dec 30 04:34:59.682: INFO: Pod "pod-update-activedeadlineseconds-825faac3-5c86-4183-a4bf-769531ee3865": Phase="Pending", Reason="", readiness=false. Elapsed: 4.339101ms
    Dec 30 04:34:59.682: INFO: The phase of Pod pod-update-activedeadlineseconds-825faac3-5c86-4183-a4bf-769531ee3865 is Pending, waiting for it to be Running (with Ready = true)
    Dec 30 04:35:01.687: INFO: Pod "pod-update-activedeadlineseconds-825faac3-5c86-4183-a4bf-769531ee3865": Phase="Running", Reason="", readiness=true. Elapsed: 2.009691326s
    Dec 30 04:35:01.687: INFO: The phase of Pod pod-update-activedeadlineseconds-825faac3-5c86-4183-a4bf-769531ee3865 is Running (Ready = true)
    Dec 30 04:35:01.687: INFO: Pod "pod-update-activedeadlineseconds-825faac3-5c86-4183-a4bf-769531ee3865" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 12/30/22 04:35:01.691
    STEP: updating the pod 12/30/22 04:35:01.695
    Dec 30 04:35:02.209: INFO: Successfully updated pod "pod-update-activedeadlineseconds-825faac3-5c86-4183-a4bf-769531ee3865"
    Dec 30 04:35:02.209: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-825faac3-5c86-4183-a4bf-769531ee3865" in namespace "pods-5531" to be "terminated with reason DeadlineExceeded"
    Dec 30 04:35:02.214: INFO: Pod "pod-update-activedeadlineseconds-825faac3-5c86-4183-a4bf-769531ee3865": Phase="Running", Reason="", readiness=true. Elapsed: 4.179673ms
    Dec 30 04:35:04.220: INFO: Pod "pod-update-activedeadlineseconds-825faac3-5c86-4183-a4bf-769531ee3865": Phase="Running", Reason="", readiness=true. Elapsed: 2.010316128s
    Dec 30 04:35:06.219: INFO: Pod "pod-update-activedeadlineseconds-825faac3-5c86-4183-a4bf-769531ee3865": Phase="Running", Reason="", readiness=false. Elapsed: 4.00934665s
    Dec 30 04:35:08.219: INFO: Pod "pod-update-activedeadlineseconds-825faac3-5c86-4183-a4bf-769531ee3865": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.009375501s
    Dec 30 04:35:08.219: INFO: Pod "pod-update-activedeadlineseconds-825faac3-5c86-4183-a4bf-769531ee3865" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Dec 30 04:35:08.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-5531" for this suite. 12/30/22 04:35:08.225
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:35:08.234
Dec 30 04:35:08.234: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename endpointslicemirroring 12/30/22 04:35:08.235
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:35:08.249
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:35:08.252
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 12/30/22 04:35:08.266
Dec 30 04:35:08.275: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 12/30/22 04:35:10.28
Dec 30 04:35:10.289: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint 12/30/22 04:35:12.295
Dec 30 04:35:12.305: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:187
Dec 30 04:35:14.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-4483" for this suite. 12/30/22 04:35:14.316
{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","completed":273,"skipped":4894,"failed":0}
------------------------------
• [SLOW TEST] [6.089 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:35:08.234
    Dec 30 04:35:08.234: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename endpointslicemirroring 12/30/22 04:35:08.235
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:35:08.249
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:35:08.252
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 12/30/22 04:35:08.266
    Dec 30 04:35:08.275: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 12/30/22 04:35:10.28
    Dec 30 04:35:10.289: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
    STEP: mirroring deletion of a custom Endpoint 12/30/22 04:35:12.295
    Dec 30 04:35:12.305: INFO: Waiting for 0 EndpointSlices to exist, got 1
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/framework.go:187
    Dec 30 04:35:14.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslicemirroring-4483" for this suite. 12/30/22 04:35:14.316
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:350
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:35:14.324
Dec 30 04:35:14.324: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename kubectl 12/30/22 04:35:14.325
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:35:14.34
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:35:14.343
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:324
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:350
STEP: creating a replication controller 12/30/22 04:35:14.347
Dec 30 04:35:14.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-1067 create -f -'
Dec 30 04:35:15.420: INFO: stderr: ""
Dec 30 04:35:15.420: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 12/30/22 04:35:15.42
Dec 30 04:35:15.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-1067 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec 30 04:35:15.521: INFO: stderr: ""
Dec 30 04:35:15.521: INFO: stdout: "update-demo-nautilus-bjbch update-demo-nautilus-srphb "
Dec 30 04:35:15.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-1067 get pods update-demo-nautilus-bjbch -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 30 04:35:15.610: INFO: stderr: ""
Dec 30 04:35:15.610: INFO: stdout: ""
Dec 30 04:35:15.610: INFO: update-demo-nautilus-bjbch is created but not running
Dec 30 04:35:20.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-1067 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec 30 04:35:20.718: INFO: stderr: ""
Dec 30 04:35:20.718: INFO: stdout: "update-demo-nautilus-bjbch update-demo-nautilus-srphb "
Dec 30 04:35:20.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-1067 get pods update-demo-nautilus-bjbch -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 30 04:35:20.815: INFO: stderr: ""
Dec 30 04:35:20.815: INFO: stdout: "true"
Dec 30 04:35:20.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-1067 get pods update-demo-nautilus-bjbch -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec 30 04:35:20.912: INFO: stderr: ""
Dec 30 04:35:20.912: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Dec 30 04:35:20.912: INFO: validating pod update-demo-nautilus-bjbch
Dec 30 04:35:20.918: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 30 04:35:20.919: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 30 04:35:20.919: INFO: update-demo-nautilus-bjbch is verified up and running
Dec 30 04:35:20.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-1067 get pods update-demo-nautilus-srphb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 30 04:35:21.020: INFO: stderr: ""
Dec 30 04:35:21.021: INFO: stdout: "true"
Dec 30 04:35:21.021: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-1067 get pods update-demo-nautilus-srphb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec 30 04:35:21.121: INFO: stderr: ""
Dec 30 04:35:21.121: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Dec 30 04:35:21.121: INFO: validating pod update-demo-nautilus-srphb
Dec 30 04:35:21.127: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 30 04:35:21.128: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 30 04:35:21.128: INFO: update-demo-nautilus-srphb is verified up and running
STEP: scaling down the replication controller 12/30/22 04:35:21.128
Dec 30 04:35:21.130: INFO: scanned /root for discovery docs: <nil>
Dec 30 04:35:21.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-1067 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Dec 30 04:35:22.258: INFO: stderr: ""
Dec 30 04:35:22.258: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 12/30/22 04:35:22.258
Dec 30 04:35:22.258: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-1067 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec 30 04:35:22.355: INFO: stderr: ""
Dec 30 04:35:22.355: INFO: stdout: "update-demo-nautilus-bjbch update-demo-nautilus-srphb "
STEP: Replicas for name=update-demo: expected=1 actual=2 12/30/22 04:35:22.355
Dec 30 04:35:27.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-1067 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec 30 04:35:27.459: INFO: stderr: ""
Dec 30 04:35:27.459: INFO: stdout: "update-demo-nautilus-bjbch "
Dec 30 04:35:27.459: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-1067 get pods update-demo-nautilus-bjbch -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 30 04:35:27.558: INFO: stderr: ""
Dec 30 04:35:27.558: INFO: stdout: "true"
Dec 30 04:35:27.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-1067 get pods update-demo-nautilus-bjbch -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec 30 04:35:27.656: INFO: stderr: ""
Dec 30 04:35:27.656: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Dec 30 04:35:27.656: INFO: validating pod update-demo-nautilus-bjbch
Dec 30 04:35:27.661: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 30 04:35:27.661: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 30 04:35:27.661: INFO: update-demo-nautilus-bjbch is verified up and running
STEP: scaling up the replication controller 12/30/22 04:35:27.661
Dec 30 04:35:27.663: INFO: scanned /root for discovery docs: <nil>
Dec 30 04:35:27.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-1067 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Dec 30 04:35:28.781: INFO: stderr: ""
Dec 30 04:35:28.781: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 12/30/22 04:35:28.781
Dec 30 04:35:28.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-1067 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec 30 04:35:28.890: INFO: stderr: ""
Dec 30 04:35:28.890: INFO: stdout: "update-demo-nautilus-675s2 update-demo-nautilus-bjbch "
Dec 30 04:35:28.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-1067 get pods update-demo-nautilus-675s2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 30 04:35:28.991: INFO: stderr: ""
Dec 30 04:35:28.991: INFO: stdout: ""
Dec 30 04:35:28.991: INFO: update-demo-nautilus-675s2 is created but not running
Dec 30 04:35:33.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-1067 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec 30 04:35:34.101: INFO: stderr: ""
Dec 30 04:35:34.101: INFO: stdout: "update-demo-nautilus-675s2 update-demo-nautilus-bjbch "
Dec 30 04:35:34.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-1067 get pods update-demo-nautilus-675s2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 30 04:35:34.203: INFO: stderr: ""
Dec 30 04:35:34.203: INFO: stdout: "true"
Dec 30 04:35:34.203: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-1067 get pods update-demo-nautilus-675s2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec 30 04:35:34.299: INFO: stderr: ""
Dec 30 04:35:34.299: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Dec 30 04:35:34.299: INFO: validating pod update-demo-nautilus-675s2
Dec 30 04:35:34.305: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 30 04:35:34.305: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 30 04:35:34.305: INFO: update-demo-nautilus-675s2 is verified up and running
Dec 30 04:35:34.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-1067 get pods update-demo-nautilus-bjbch -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 30 04:35:34.403: INFO: stderr: ""
Dec 30 04:35:34.403: INFO: stdout: "true"
Dec 30 04:35:34.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-1067 get pods update-demo-nautilus-bjbch -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec 30 04:35:34.500: INFO: stderr: ""
Dec 30 04:35:34.500: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Dec 30 04:35:34.500: INFO: validating pod update-demo-nautilus-bjbch
Dec 30 04:35:34.505: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 30 04:35:34.505: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 30 04:35:34.505: INFO: update-demo-nautilus-bjbch is verified up and running
STEP: using delete to clean up resources 12/30/22 04:35:34.505
Dec 30 04:35:34.505: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-1067 delete --grace-period=0 --force -f -'
Dec 30 04:35:34.602: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 30 04:35:34.602: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Dec 30 04:35:34.602: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-1067 get rc,svc -l name=update-demo --no-headers'
Dec 30 04:35:34.703: INFO: stderr: "No resources found in kubectl-1067 namespace.\n"
Dec 30 04:35:34.703: INFO: stdout: ""
Dec 30 04:35:34.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-1067 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 30 04:35:34.804: INFO: stderr: ""
Dec 30 04:35:34.804: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Dec 30 04:35:34.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1067" for this suite. 12/30/22 04:35:34.814
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","completed":274,"skipped":4903,"failed":0}
------------------------------
• [SLOW TEST] [20.496 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:322
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:350

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:35:14.324
    Dec 30 04:35:14.324: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename kubectl 12/30/22 04:35:14.325
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:35:14.34
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:35:14.343
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:324
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:350
    STEP: creating a replication controller 12/30/22 04:35:14.347
    Dec 30 04:35:14.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-1067 create -f -'
    Dec 30 04:35:15.420: INFO: stderr: ""
    Dec 30 04:35:15.420: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 12/30/22 04:35:15.42
    Dec 30 04:35:15.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-1067 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Dec 30 04:35:15.521: INFO: stderr: ""
    Dec 30 04:35:15.521: INFO: stdout: "update-demo-nautilus-bjbch update-demo-nautilus-srphb "
    Dec 30 04:35:15.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-1067 get pods update-demo-nautilus-bjbch -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Dec 30 04:35:15.610: INFO: stderr: ""
    Dec 30 04:35:15.610: INFO: stdout: ""
    Dec 30 04:35:15.610: INFO: update-demo-nautilus-bjbch is created but not running
    Dec 30 04:35:20.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-1067 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Dec 30 04:35:20.718: INFO: stderr: ""
    Dec 30 04:35:20.718: INFO: stdout: "update-demo-nautilus-bjbch update-demo-nautilus-srphb "
    Dec 30 04:35:20.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-1067 get pods update-demo-nautilus-bjbch -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Dec 30 04:35:20.815: INFO: stderr: ""
    Dec 30 04:35:20.815: INFO: stdout: "true"
    Dec 30 04:35:20.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-1067 get pods update-demo-nautilus-bjbch -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Dec 30 04:35:20.912: INFO: stderr: ""
    Dec 30 04:35:20.912: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Dec 30 04:35:20.912: INFO: validating pod update-demo-nautilus-bjbch
    Dec 30 04:35:20.918: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Dec 30 04:35:20.919: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Dec 30 04:35:20.919: INFO: update-demo-nautilus-bjbch is verified up and running
    Dec 30 04:35:20.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-1067 get pods update-demo-nautilus-srphb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Dec 30 04:35:21.020: INFO: stderr: ""
    Dec 30 04:35:21.021: INFO: stdout: "true"
    Dec 30 04:35:21.021: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-1067 get pods update-demo-nautilus-srphb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Dec 30 04:35:21.121: INFO: stderr: ""
    Dec 30 04:35:21.121: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Dec 30 04:35:21.121: INFO: validating pod update-demo-nautilus-srphb
    Dec 30 04:35:21.127: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Dec 30 04:35:21.128: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Dec 30 04:35:21.128: INFO: update-demo-nautilus-srphb is verified up and running
    STEP: scaling down the replication controller 12/30/22 04:35:21.128
    Dec 30 04:35:21.130: INFO: scanned /root for discovery docs: <nil>
    Dec 30 04:35:21.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-1067 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    Dec 30 04:35:22.258: INFO: stderr: ""
    Dec 30 04:35:22.258: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 12/30/22 04:35:22.258
    Dec 30 04:35:22.258: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-1067 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Dec 30 04:35:22.355: INFO: stderr: ""
    Dec 30 04:35:22.355: INFO: stdout: "update-demo-nautilus-bjbch update-demo-nautilus-srphb "
    STEP: Replicas for name=update-demo: expected=1 actual=2 12/30/22 04:35:22.355
    Dec 30 04:35:27.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-1067 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Dec 30 04:35:27.459: INFO: stderr: ""
    Dec 30 04:35:27.459: INFO: stdout: "update-demo-nautilus-bjbch "
    Dec 30 04:35:27.459: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-1067 get pods update-demo-nautilus-bjbch -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Dec 30 04:35:27.558: INFO: stderr: ""
    Dec 30 04:35:27.558: INFO: stdout: "true"
    Dec 30 04:35:27.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-1067 get pods update-demo-nautilus-bjbch -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Dec 30 04:35:27.656: INFO: stderr: ""
    Dec 30 04:35:27.656: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Dec 30 04:35:27.656: INFO: validating pod update-demo-nautilus-bjbch
    Dec 30 04:35:27.661: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Dec 30 04:35:27.661: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Dec 30 04:35:27.661: INFO: update-demo-nautilus-bjbch is verified up and running
    STEP: scaling up the replication controller 12/30/22 04:35:27.661
    Dec 30 04:35:27.663: INFO: scanned /root for discovery docs: <nil>
    Dec 30 04:35:27.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-1067 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    Dec 30 04:35:28.781: INFO: stderr: ""
    Dec 30 04:35:28.781: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 12/30/22 04:35:28.781
    Dec 30 04:35:28.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-1067 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Dec 30 04:35:28.890: INFO: stderr: ""
    Dec 30 04:35:28.890: INFO: stdout: "update-demo-nautilus-675s2 update-demo-nautilus-bjbch "
    Dec 30 04:35:28.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-1067 get pods update-demo-nautilus-675s2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Dec 30 04:35:28.991: INFO: stderr: ""
    Dec 30 04:35:28.991: INFO: stdout: ""
    Dec 30 04:35:28.991: INFO: update-demo-nautilus-675s2 is created but not running
    Dec 30 04:35:33.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-1067 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Dec 30 04:35:34.101: INFO: stderr: ""
    Dec 30 04:35:34.101: INFO: stdout: "update-demo-nautilus-675s2 update-demo-nautilus-bjbch "
    Dec 30 04:35:34.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-1067 get pods update-demo-nautilus-675s2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Dec 30 04:35:34.203: INFO: stderr: ""
    Dec 30 04:35:34.203: INFO: stdout: "true"
    Dec 30 04:35:34.203: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-1067 get pods update-demo-nautilus-675s2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Dec 30 04:35:34.299: INFO: stderr: ""
    Dec 30 04:35:34.299: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Dec 30 04:35:34.299: INFO: validating pod update-demo-nautilus-675s2
    Dec 30 04:35:34.305: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Dec 30 04:35:34.305: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Dec 30 04:35:34.305: INFO: update-demo-nautilus-675s2 is verified up and running
    Dec 30 04:35:34.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-1067 get pods update-demo-nautilus-bjbch -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Dec 30 04:35:34.403: INFO: stderr: ""
    Dec 30 04:35:34.403: INFO: stdout: "true"
    Dec 30 04:35:34.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-1067 get pods update-demo-nautilus-bjbch -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Dec 30 04:35:34.500: INFO: stderr: ""
    Dec 30 04:35:34.500: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Dec 30 04:35:34.500: INFO: validating pod update-demo-nautilus-bjbch
    Dec 30 04:35:34.505: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Dec 30 04:35:34.505: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Dec 30 04:35:34.505: INFO: update-demo-nautilus-bjbch is verified up and running
    STEP: using delete to clean up resources 12/30/22 04:35:34.505
    Dec 30 04:35:34.505: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-1067 delete --grace-period=0 --force -f -'
    Dec 30 04:35:34.602: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Dec 30 04:35:34.602: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Dec 30 04:35:34.602: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-1067 get rc,svc -l name=update-demo --no-headers'
    Dec 30 04:35:34.703: INFO: stderr: "No resources found in kubectl-1067 namespace.\n"
    Dec 30 04:35:34.703: INFO: stdout: ""
    Dec 30 04:35:34.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=kubectl-1067 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Dec 30 04:35:34.804: INFO: stderr: ""
    Dec 30 04:35:34.804: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Dec 30 04:35:34.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-1067" for this suite. 12/30/22 04:35:34.814
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:215
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:35:34.821
Dec 30 04:35:34.821: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename container-runtime 12/30/22 04:35:34.823
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:35:34.842
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:35:34.846
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:215
STEP: create the container 12/30/22 04:35:34.849
STEP: wait for the container to reach Failed 12/30/22 04:35:34.857
STEP: get the container status 12/30/22 04:35:38.881
STEP: the container should be terminated 12/30/22 04:35:38.884
STEP: the termination message should be set 12/30/22 04:35:38.884
Dec 30 04:35:38.884: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 12/30/22 04:35:38.884
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Dec 30 04:35:38.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8115" for this suite. 12/30/22 04:35:38.905
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":275,"skipped":4909,"failed":0}
------------------------------
• [4.090 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:35:34.821
    Dec 30 04:35:34.821: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename container-runtime 12/30/22 04:35:34.823
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:35:34.842
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:35:34.846
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:215
    STEP: create the container 12/30/22 04:35:34.849
    STEP: wait for the container to reach Failed 12/30/22 04:35:34.857
    STEP: get the container status 12/30/22 04:35:38.881
    STEP: the container should be terminated 12/30/22 04:35:38.884
    STEP: the termination message should be set 12/30/22 04:35:38.884
    Dec 30 04:35:38.884: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 12/30/22 04:35:38.884
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Dec 30 04:35:38.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-8115" for this suite. 12/30/22 04:35:38.905
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:35:38.912
Dec 30 04:35:38.913: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename projected 12/30/22 04:35:38.914
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:35:38.929
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:35:38.932
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118
STEP: Creating secret with name projected-secret-test-ca896b2f-0989-4331-9578-d56beb5eae98 12/30/22 04:35:38.935
STEP: Creating a pod to test consume secrets 12/30/22 04:35:38.94
Dec 30 04:35:38.950: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f43f7b90-14dc-4f9f-9228-0b1de4dcbff3" in namespace "projected-6646" to be "Succeeded or Failed"
Dec 30 04:35:38.953: INFO: Pod "pod-projected-secrets-f43f7b90-14dc-4f9f-9228-0b1de4dcbff3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.183972ms
Dec 30 04:35:40.958: INFO: Pod "pod-projected-secrets-f43f7b90-14dc-4f9f-9228-0b1de4dcbff3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008313286s
Dec 30 04:35:42.958: INFO: Pod "pod-projected-secrets-f43f7b90-14dc-4f9f-9228-0b1de4dcbff3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008248959s
STEP: Saw pod success 12/30/22 04:35:42.958
Dec 30 04:35:42.958: INFO: Pod "pod-projected-secrets-f43f7b90-14dc-4f9f-9228-0b1de4dcbff3" satisfied condition "Succeeded or Failed"
Dec 30 04:35:42.962: INFO: Trying to get logs from node k8s-mgmt01 pod pod-projected-secrets-f43f7b90-14dc-4f9f-9228-0b1de4dcbff3 container secret-volume-test: <nil>
STEP: delete the pod 12/30/22 04:35:42.97
Dec 30 04:35:42.985: INFO: Waiting for pod pod-projected-secrets-f43f7b90-14dc-4f9f-9228-0b1de4dcbff3 to disappear
Dec 30 04:35:42.988: INFO: Pod pod-projected-secrets-f43f7b90-14dc-4f9f-9228-0b1de4dcbff3 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Dec 30 04:35:42.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6646" for this suite. 12/30/22 04:35:42.994
{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","completed":276,"skipped":4914,"failed":0}
------------------------------
• [4.089 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:35:38.912
    Dec 30 04:35:38.913: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename projected 12/30/22 04:35:38.914
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:35:38.929
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:35:38.932
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:118
    STEP: Creating secret with name projected-secret-test-ca896b2f-0989-4331-9578-d56beb5eae98 12/30/22 04:35:38.935
    STEP: Creating a pod to test consume secrets 12/30/22 04:35:38.94
    Dec 30 04:35:38.950: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f43f7b90-14dc-4f9f-9228-0b1de4dcbff3" in namespace "projected-6646" to be "Succeeded or Failed"
    Dec 30 04:35:38.953: INFO: Pod "pod-projected-secrets-f43f7b90-14dc-4f9f-9228-0b1de4dcbff3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.183972ms
    Dec 30 04:35:40.958: INFO: Pod "pod-projected-secrets-f43f7b90-14dc-4f9f-9228-0b1de4dcbff3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008313286s
    Dec 30 04:35:42.958: INFO: Pod "pod-projected-secrets-f43f7b90-14dc-4f9f-9228-0b1de4dcbff3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008248959s
    STEP: Saw pod success 12/30/22 04:35:42.958
    Dec 30 04:35:42.958: INFO: Pod "pod-projected-secrets-f43f7b90-14dc-4f9f-9228-0b1de4dcbff3" satisfied condition "Succeeded or Failed"
    Dec 30 04:35:42.962: INFO: Trying to get logs from node k8s-mgmt01 pod pod-projected-secrets-f43f7b90-14dc-4f9f-9228-0b1de4dcbff3 container secret-volume-test: <nil>
    STEP: delete the pod 12/30/22 04:35:42.97
    Dec 30 04:35:42.985: INFO: Waiting for pod pod-projected-secrets-f43f7b90-14dc-4f9f-9228-0b1de4dcbff3 to disappear
    Dec 30 04:35:42.988: INFO: Pod pod-projected-secrets-f43f7b90-14dc-4f9f-9228-0b1de4dcbff3 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Dec 30 04:35:42.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6646" for this suite. 12/30/22 04:35:42.994
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:35:43.002
Dec 30 04:35:43.002: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename services 12/30/22 04:35:43.004
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:35:43.018
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:35:43.022
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189
STEP: creating service in namespace services-16 12/30/22 04:35:43.025
STEP: creating service affinity-clusterip-transition in namespace services-16 12/30/22 04:35:43.025
STEP: creating replication controller affinity-clusterip-transition in namespace services-16 12/30/22 04:35:43.036
I1230 04:35:43.042653      25 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-16, replica count: 3
I1230 04:35:46.094240      25 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 30 04:35:46.101: INFO: Creating new exec pod
Dec 30 04:35:46.107: INFO: Waiting up to 5m0s for pod "execpod-affinityrxghw" in namespace "services-16" to be "running"
Dec 30 04:35:46.111: INFO: Pod "execpod-affinityrxghw": Phase="Pending", Reason="", readiness=false. Elapsed: 4.201021ms
Dec 30 04:35:48.117: INFO: Pod "execpod-affinityrxghw": Phase="Running", Reason="", readiness=true. Elapsed: 2.010173243s
Dec 30 04:35:48.117: INFO: Pod "execpod-affinityrxghw" satisfied condition "running"
Dec 30 04:35:49.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-16 exec execpod-affinityrxghw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
Dec 30 04:35:49.311: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Dec 30 04:35:49.311: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 30 04:35:49.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-16 exec execpod-affinityrxghw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.40.112 80'
Dec 30 04:35:49.508: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.40.112 80\nConnection to 10.233.40.112 80 port [tcp/http] succeeded!\n"
Dec 30 04:35:49.508: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 30 04:35:49.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-16 exec execpod-affinityrxghw -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.40.112:80/ ; done'
Dec 30 04:35:49.861: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.112:80/\n"
Dec 30 04:35:49.861: INFO: stdout: "\naffinity-clusterip-transition-nmlvj\naffinity-clusterip-transition-srkcx\naffinity-clusterip-transition-bfpjs\naffinity-clusterip-transition-nmlvj\naffinity-clusterip-transition-srkcx\naffinity-clusterip-transition-bfpjs\naffinity-clusterip-transition-nmlvj\naffinity-clusterip-transition-srkcx\naffinity-clusterip-transition-bfpjs\naffinity-clusterip-transition-nmlvj\naffinity-clusterip-transition-srkcx\naffinity-clusterip-transition-bfpjs\naffinity-clusterip-transition-nmlvj\naffinity-clusterip-transition-srkcx\naffinity-clusterip-transition-bfpjs\naffinity-clusterip-transition-nmlvj"
Dec 30 04:35:49.861: INFO: Received response from host: affinity-clusterip-transition-nmlvj
Dec 30 04:35:49.861: INFO: Received response from host: affinity-clusterip-transition-srkcx
Dec 30 04:35:49.861: INFO: Received response from host: affinity-clusterip-transition-bfpjs
Dec 30 04:35:49.861: INFO: Received response from host: affinity-clusterip-transition-nmlvj
Dec 30 04:35:49.861: INFO: Received response from host: affinity-clusterip-transition-srkcx
Dec 30 04:35:49.861: INFO: Received response from host: affinity-clusterip-transition-bfpjs
Dec 30 04:35:49.861: INFO: Received response from host: affinity-clusterip-transition-nmlvj
Dec 30 04:35:49.861: INFO: Received response from host: affinity-clusterip-transition-srkcx
Dec 30 04:35:49.861: INFO: Received response from host: affinity-clusterip-transition-bfpjs
Dec 30 04:35:49.861: INFO: Received response from host: affinity-clusterip-transition-nmlvj
Dec 30 04:35:49.861: INFO: Received response from host: affinity-clusterip-transition-srkcx
Dec 30 04:35:49.861: INFO: Received response from host: affinity-clusterip-transition-bfpjs
Dec 30 04:35:49.861: INFO: Received response from host: affinity-clusterip-transition-nmlvj
Dec 30 04:35:49.861: INFO: Received response from host: affinity-clusterip-transition-srkcx
Dec 30 04:35:49.861: INFO: Received response from host: affinity-clusterip-transition-bfpjs
Dec 30 04:35:49.861: INFO: Received response from host: affinity-clusterip-transition-nmlvj
Dec 30 04:35:49.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-16 exec execpod-affinityrxghw -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.40.112:80/ ; done'
Dec 30 04:35:50.186: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.112:80/\n"
Dec 30 04:35:50.186: INFO: stdout: "\naffinity-clusterip-transition-bfpjs\naffinity-clusterip-transition-bfpjs\naffinity-clusterip-transition-bfpjs\naffinity-clusterip-transition-bfpjs\naffinity-clusterip-transition-bfpjs\naffinity-clusterip-transition-bfpjs\naffinity-clusterip-transition-bfpjs\naffinity-clusterip-transition-bfpjs\naffinity-clusterip-transition-bfpjs\naffinity-clusterip-transition-bfpjs\naffinity-clusterip-transition-bfpjs\naffinity-clusterip-transition-bfpjs\naffinity-clusterip-transition-bfpjs\naffinity-clusterip-transition-bfpjs\naffinity-clusterip-transition-bfpjs\naffinity-clusterip-transition-bfpjs"
Dec 30 04:35:50.186: INFO: Received response from host: affinity-clusterip-transition-bfpjs
Dec 30 04:35:50.186: INFO: Received response from host: affinity-clusterip-transition-bfpjs
Dec 30 04:35:50.186: INFO: Received response from host: affinity-clusterip-transition-bfpjs
Dec 30 04:35:50.186: INFO: Received response from host: affinity-clusterip-transition-bfpjs
Dec 30 04:35:50.186: INFO: Received response from host: affinity-clusterip-transition-bfpjs
Dec 30 04:35:50.186: INFO: Received response from host: affinity-clusterip-transition-bfpjs
Dec 30 04:35:50.186: INFO: Received response from host: affinity-clusterip-transition-bfpjs
Dec 30 04:35:50.186: INFO: Received response from host: affinity-clusterip-transition-bfpjs
Dec 30 04:35:50.186: INFO: Received response from host: affinity-clusterip-transition-bfpjs
Dec 30 04:35:50.186: INFO: Received response from host: affinity-clusterip-transition-bfpjs
Dec 30 04:35:50.186: INFO: Received response from host: affinity-clusterip-transition-bfpjs
Dec 30 04:35:50.186: INFO: Received response from host: affinity-clusterip-transition-bfpjs
Dec 30 04:35:50.186: INFO: Received response from host: affinity-clusterip-transition-bfpjs
Dec 30 04:35:50.186: INFO: Received response from host: affinity-clusterip-transition-bfpjs
Dec 30 04:35:50.186: INFO: Received response from host: affinity-clusterip-transition-bfpjs
Dec 30 04:35:50.186: INFO: Received response from host: affinity-clusterip-transition-bfpjs
Dec 30 04:35:50.186: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-16, will wait for the garbage collector to delete the pods 12/30/22 04:35:50.199
Dec 30 04:35:50.261: INFO: Deleting ReplicationController affinity-clusterip-transition took: 7.546543ms
Dec 30 04:35:50.462: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 200.680573ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Dec 30 04:35:52.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-16" for this suite. 12/30/22 04:35:52.789
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","completed":277,"skipped":4916,"failed":0}
------------------------------
• [SLOW TEST] [9.794 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:35:43.002
    Dec 30 04:35:43.002: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename services 12/30/22 04:35:43.004
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:35:43.018
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:35:43.022
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2189
    STEP: creating service in namespace services-16 12/30/22 04:35:43.025
    STEP: creating service affinity-clusterip-transition in namespace services-16 12/30/22 04:35:43.025
    STEP: creating replication controller affinity-clusterip-transition in namespace services-16 12/30/22 04:35:43.036
    I1230 04:35:43.042653      25 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-16, replica count: 3
    I1230 04:35:46.094240      25 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Dec 30 04:35:46.101: INFO: Creating new exec pod
    Dec 30 04:35:46.107: INFO: Waiting up to 5m0s for pod "execpod-affinityrxghw" in namespace "services-16" to be "running"
    Dec 30 04:35:46.111: INFO: Pod "execpod-affinityrxghw": Phase="Pending", Reason="", readiness=false. Elapsed: 4.201021ms
    Dec 30 04:35:48.117: INFO: Pod "execpod-affinityrxghw": Phase="Running", Reason="", readiness=true. Elapsed: 2.010173243s
    Dec 30 04:35:48.117: INFO: Pod "execpod-affinityrxghw" satisfied condition "running"
    Dec 30 04:35:49.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-16 exec execpod-affinityrxghw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
    Dec 30 04:35:49.311: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    Dec 30 04:35:49.311: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Dec 30 04:35:49.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-16 exec execpod-affinityrxghw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.40.112 80'
    Dec 30 04:35:49.508: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.40.112 80\nConnection to 10.233.40.112 80 port [tcp/http] succeeded!\n"
    Dec 30 04:35:49.508: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Dec 30 04:35:49.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-16 exec execpod-affinityrxghw -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.40.112:80/ ; done'
    Dec 30 04:35:49.861: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.112:80/\n"
    Dec 30 04:35:49.861: INFO: stdout: "\naffinity-clusterip-transition-nmlvj\naffinity-clusterip-transition-srkcx\naffinity-clusterip-transition-bfpjs\naffinity-clusterip-transition-nmlvj\naffinity-clusterip-transition-srkcx\naffinity-clusterip-transition-bfpjs\naffinity-clusterip-transition-nmlvj\naffinity-clusterip-transition-srkcx\naffinity-clusterip-transition-bfpjs\naffinity-clusterip-transition-nmlvj\naffinity-clusterip-transition-srkcx\naffinity-clusterip-transition-bfpjs\naffinity-clusterip-transition-nmlvj\naffinity-clusterip-transition-srkcx\naffinity-clusterip-transition-bfpjs\naffinity-clusterip-transition-nmlvj"
    Dec 30 04:35:49.861: INFO: Received response from host: affinity-clusterip-transition-nmlvj
    Dec 30 04:35:49.861: INFO: Received response from host: affinity-clusterip-transition-srkcx
    Dec 30 04:35:49.861: INFO: Received response from host: affinity-clusterip-transition-bfpjs
    Dec 30 04:35:49.861: INFO: Received response from host: affinity-clusterip-transition-nmlvj
    Dec 30 04:35:49.861: INFO: Received response from host: affinity-clusterip-transition-srkcx
    Dec 30 04:35:49.861: INFO: Received response from host: affinity-clusterip-transition-bfpjs
    Dec 30 04:35:49.861: INFO: Received response from host: affinity-clusterip-transition-nmlvj
    Dec 30 04:35:49.861: INFO: Received response from host: affinity-clusterip-transition-srkcx
    Dec 30 04:35:49.861: INFO: Received response from host: affinity-clusterip-transition-bfpjs
    Dec 30 04:35:49.861: INFO: Received response from host: affinity-clusterip-transition-nmlvj
    Dec 30 04:35:49.861: INFO: Received response from host: affinity-clusterip-transition-srkcx
    Dec 30 04:35:49.861: INFO: Received response from host: affinity-clusterip-transition-bfpjs
    Dec 30 04:35:49.861: INFO: Received response from host: affinity-clusterip-transition-nmlvj
    Dec 30 04:35:49.861: INFO: Received response from host: affinity-clusterip-transition-srkcx
    Dec 30 04:35:49.861: INFO: Received response from host: affinity-clusterip-transition-bfpjs
    Dec 30 04:35:49.861: INFO: Received response from host: affinity-clusterip-transition-nmlvj
    Dec 30 04:35:49.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-16 exec execpod-affinityrxghw -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.40.112:80/ ; done'
    Dec 30 04:35:50.186: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.112:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.112:80/\n"
    Dec 30 04:35:50.186: INFO: stdout: "\naffinity-clusterip-transition-bfpjs\naffinity-clusterip-transition-bfpjs\naffinity-clusterip-transition-bfpjs\naffinity-clusterip-transition-bfpjs\naffinity-clusterip-transition-bfpjs\naffinity-clusterip-transition-bfpjs\naffinity-clusterip-transition-bfpjs\naffinity-clusterip-transition-bfpjs\naffinity-clusterip-transition-bfpjs\naffinity-clusterip-transition-bfpjs\naffinity-clusterip-transition-bfpjs\naffinity-clusterip-transition-bfpjs\naffinity-clusterip-transition-bfpjs\naffinity-clusterip-transition-bfpjs\naffinity-clusterip-transition-bfpjs\naffinity-clusterip-transition-bfpjs"
    Dec 30 04:35:50.186: INFO: Received response from host: affinity-clusterip-transition-bfpjs
    Dec 30 04:35:50.186: INFO: Received response from host: affinity-clusterip-transition-bfpjs
    Dec 30 04:35:50.186: INFO: Received response from host: affinity-clusterip-transition-bfpjs
    Dec 30 04:35:50.186: INFO: Received response from host: affinity-clusterip-transition-bfpjs
    Dec 30 04:35:50.186: INFO: Received response from host: affinity-clusterip-transition-bfpjs
    Dec 30 04:35:50.186: INFO: Received response from host: affinity-clusterip-transition-bfpjs
    Dec 30 04:35:50.186: INFO: Received response from host: affinity-clusterip-transition-bfpjs
    Dec 30 04:35:50.186: INFO: Received response from host: affinity-clusterip-transition-bfpjs
    Dec 30 04:35:50.186: INFO: Received response from host: affinity-clusterip-transition-bfpjs
    Dec 30 04:35:50.186: INFO: Received response from host: affinity-clusterip-transition-bfpjs
    Dec 30 04:35:50.186: INFO: Received response from host: affinity-clusterip-transition-bfpjs
    Dec 30 04:35:50.186: INFO: Received response from host: affinity-clusterip-transition-bfpjs
    Dec 30 04:35:50.186: INFO: Received response from host: affinity-clusterip-transition-bfpjs
    Dec 30 04:35:50.186: INFO: Received response from host: affinity-clusterip-transition-bfpjs
    Dec 30 04:35:50.186: INFO: Received response from host: affinity-clusterip-transition-bfpjs
    Dec 30 04:35:50.186: INFO: Received response from host: affinity-clusterip-transition-bfpjs
    Dec 30 04:35:50.186: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-16, will wait for the garbage collector to delete the pods 12/30/22 04:35:50.199
    Dec 30 04:35:50.261: INFO: Deleting ReplicationController affinity-clusterip-transition took: 7.546543ms
    Dec 30 04:35:50.462: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 200.680573ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Dec 30 04:35:52.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-16" for this suite. 12/30/22 04:35:52.789
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:35:52.802
Dec 30 04:35:52.803: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename podtemplate 12/30/22 04:35:52.804
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:35:52.822
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:35:52.825
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 12/30/22 04:35:52.829
STEP: Replace a pod template 12/30/22 04:35:52.834
Dec 30 04:35:52.843: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Dec 30 04:35:52.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-2415" for this suite. 12/30/22 04:35:52.848
{"msg":"PASSED [sig-node] PodTemplates should replace a pod template [Conformance]","completed":278,"skipped":4987,"failed":0}
------------------------------
• [0.053 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:35:52.802
    Dec 30 04:35:52.803: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename podtemplate 12/30/22 04:35:52.804
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:35:52.822
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:35:52.825
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 12/30/22 04:35:52.829
    STEP: Replace a pod template 12/30/22 04:35:52.834
    Dec 30 04:35:52.843: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Dec 30 04:35:52.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-2415" for this suite. 12/30/22 04:35:52.848
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:35:52.858
Dec 30 04:35:52.858: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename container-probe 12/30/22 04:35:52.859
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:35:52.872
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:35:52.876
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165
STEP: Creating pod liveness-c9938100-6e51-4d58-9aab-ebe32fafbbdc in namespace container-probe-8597 12/30/22 04:35:52.879
Dec 30 04:35:52.890: INFO: Waiting up to 5m0s for pod "liveness-c9938100-6e51-4d58-9aab-ebe32fafbbdc" in namespace "container-probe-8597" to be "not pending"
Dec 30 04:35:52.893: INFO: Pod "liveness-c9938100-6e51-4d58-9aab-ebe32fafbbdc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.487996ms
Dec 30 04:35:54.899: INFO: Pod "liveness-c9938100-6e51-4d58-9aab-ebe32fafbbdc": Phase="Running", Reason="", readiness=true. Elapsed: 2.009170844s
Dec 30 04:35:54.899: INFO: Pod "liveness-c9938100-6e51-4d58-9aab-ebe32fafbbdc" satisfied condition "not pending"
Dec 30 04:35:54.899: INFO: Started pod liveness-c9938100-6e51-4d58-9aab-ebe32fafbbdc in namespace container-probe-8597
STEP: checking the pod's current state and verifying that restartCount is present 12/30/22 04:35:54.899
Dec 30 04:35:54.903: INFO: Initial restart count of pod liveness-c9938100-6e51-4d58-9aab-ebe32fafbbdc is 0
Dec 30 04:36:14.961: INFO: Restart count of pod container-probe-8597/liveness-c9938100-6e51-4d58-9aab-ebe32fafbbdc is now 1 (20.058913519s elapsed)
STEP: deleting the pod 12/30/22 04:36:14.962
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Dec 30 04:36:14.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8597" for this suite. 12/30/22 04:36:14.978
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","completed":279,"skipped":5016,"failed":0}
------------------------------
• [SLOW TEST] [22.127 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:35:52.858
    Dec 30 04:35:52.858: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename container-probe 12/30/22 04:35:52.859
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:35:52.872
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:35:52.876
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:165
    STEP: Creating pod liveness-c9938100-6e51-4d58-9aab-ebe32fafbbdc in namespace container-probe-8597 12/30/22 04:35:52.879
    Dec 30 04:35:52.890: INFO: Waiting up to 5m0s for pod "liveness-c9938100-6e51-4d58-9aab-ebe32fafbbdc" in namespace "container-probe-8597" to be "not pending"
    Dec 30 04:35:52.893: INFO: Pod "liveness-c9938100-6e51-4d58-9aab-ebe32fafbbdc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.487996ms
    Dec 30 04:35:54.899: INFO: Pod "liveness-c9938100-6e51-4d58-9aab-ebe32fafbbdc": Phase="Running", Reason="", readiness=true. Elapsed: 2.009170844s
    Dec 30 04:35:54.899: INFO: Pod "liveness-c9938100-6e51-4d58-9aab-ebe32fafbbdc" satisfied condition "not pending"
    Dec 30 04:35:54.899: INFO: Started pod liveness-c9938100-6e51-4d58-9aab-ebe32fafbbdc in namespace container-probe-8597
    STEP: checking the pod's current state and verifying that restartCount is present 12/30/22 04:35:54.899
    Dec 30 04:35:54.903: INFO: Initial restart count of pod liveness-c9938100-6e51-4d58-9aab-ebe32fafbbdc is 0
    Dec 30 04:36:14.961: INFO: Restart count of pod container-probe-8597/liveness-c9938100-6e51-4d58-9aab-ebe32fafbbdc is now 1 (20.058913519s elapsed)
    STEP: deleting the pod 12/30/22 04:36:14.962
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Dec 30 04:36:14.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-8597" for this suite. 12/30/22 04:36:14.978
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:36:14.993
Dec 30 04:36:14.993: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename resourcequota 12/30/22 04:36:14.994
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:36:15.006
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:36:15.01
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933
STEP: Creating a ResourceQuota 12/30/22 04:36:15.014
STEP: Getting a ResourceQuota 12/30/22 04:36:15.019
STEP: Listing all ResourceQuotas with LabelSelector 12/30/22 04:36:15.023
STEP: Patching the ResourceQuota 12/30/22 04:36:15.027
STEP: Deleting a Collection of ResourceQuotas 12/30/22 04:36:15.037
STEP: Verifying the deleted ResourceQuota 12/30/22 04:36:15.045
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Dec 30 04:36:15.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3671" for this suite. 12/30/22 04:36:15.054
{"msg":"PASSED [sig-api-machinery] ResourceQuota should manage the lifecycle of a ResourceQuota [Conformance]","completed":280,"skipped":5102,"failed":0}
------------------------------
• [0.069 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:36:14.993
    Dec 30 04:36:14.993: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename resourcequota 12/30/22 04:36:14.994
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:36:15.006
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:36:15.01
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:933
    STEP: Creating a ResourceQuota 12/30/22 04:36:15.014
    STEP: Getting a ResourceQuota 12/30/22 04:36:15.019
    STEP: Listing all ResourceQuotas with LabelSelector 12/30/22 04:36:15.023
    STEP: Patching the ResourceQuota 12/30/22 04:36:15.027
    STEP: Deleting a Collection of ResourceQuotas 12/30/22 04:36:15.037
    STEP: Verifying the deleted ResourceQuota 12/30/22 04:36:15.045
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Dec 30 04:36:15.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-3671" for this suite. 12/30/22 04:36:15.054
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:36:15.066
Dec 30 04:36:15.066: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename services 12/30/22 04:36:15.068
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:36:15.08
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:36:15.083
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221
STEP: creating service in namespace services-7926 12/30/22 04:36:15.086
Dec 30 04:36:15.095: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-7926" to be "running and ready"
Dec 30 04:36:15.099: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 3.852658ms
Dec 30 04:36:15.099: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Dec 30 04:36:17.104: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.008537984s
Dec 30 04:36:17.104: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
Dec 30 04:36:17.104: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
Dec 30 04:36:17.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-7926 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Dec 30 04:36:17.332: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Dec 30 04:36:17.332: INFO: stdout: "ipvs"
Dec 30 04:36:17.332: INFO: proxyMode: ipvs
Dec 30 04:36:17.343: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Dec 30 04:36:17.346: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-7926 12/30/22 04:36:17.346
STEP: creating replication controller affinity-nodeport-timeout in namespace services-7926 12/30/22 04:36:17.362
I1230 04:36:17.368879      25 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-7926, replica count: 3
I1230 04:36:20.421383      25 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 30 04:36:20.434: INFO: Creating new exec pod
Dec 30 04:36:20.440: INFO: Waiting up to 5m0s for pod "execpod-affinityklk9p" in namespace "services-7926" to be "running"
Dec 30 04:36:20.444: INFO: Pod "execpod-affinityklk9p": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031247ms
Dec 30 04:36:22.450: INFO: Pod "execpod-affinityklk9p": Phase="Running", Reason="", readiness=true. Elapsed: 2.009501709s
Dec 30 04:36:22.450: INFO: Pod "execpod-affinityklk9p" satisfied condition "running"
Dec 30 04:36:23.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-7926 exec execpod-affinityklk9p -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
Dec 30 04:36:23.681: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Dec 30 04:36:23.681: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 30 04:36:23.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-7926 exec execpod-affinityklk9p -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.7.134 80'
Dec 30 04:36:23.888: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.7.134 80\nConnection to 10.233.7.134 80 port [tcp/http] succeeded!\n"
Dec 30 04:36:23.888: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 30 04:36:23.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-7926 exec execpod-affinityklk9p -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.78.26.142 30048'
Dec 30 04:36:24.080: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.78.26.142 30048\nConnection to 10.78.26.142 30048 port [tcp/*] succeeded!\n"
Dec 30 04:36:24.080: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 30 04:36:24.080: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-7926 exec execpod-affinityklk9p -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.78.26.140 30048'
Dec 30 04:36:24.265: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.78.26.140 30048\nConnection to 10.78.26.140 30048 port [tcp/*] succeeded!\n"
Dec 30 04:36:24.265: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 30 04:36:24.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-7926 exec execpod-affinityklk9p -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.78.26.140:30048/ ; done'
Dec 30 04:36:24.564: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30048/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30048/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30048/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30048/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30048/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30048/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30048/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30048/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30048/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30048/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30048/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30048/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30048/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30048/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30048/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30048/\n"
Dec 30 04:36:24.564: INFO: stdout: "\naffinity-nodeport-timeout-8fgfz\naffinity-nodeport-timeout-8fgfz\naffinity-nodeport-timeout-8fgfz\naffinity-nodeport-timeout-8fgfz\naffinity-nodeport-timeout-8fgfz\naffinity-nodeport-timeout-8fgfz\naffinity-nodeport-timeout-8fgfz\naffinity-nodeport-timeout-8fgfz\naffinity-nodeport-timeout-8fgfz\naffinity-nodeport-timeout-8fgfz\naffinity-nodeport-timeout-8fgfz\naffinity-nodeport-timeout-8fgfz\naffinity-nodeport-timeout-8fgfz\naffinity-nodeport-timeout-8fgfz\naffinity-nodeport-timeout-8fgfz\naffinity-nodeport-timeout-8fgfz"
Dec 30 04:36:24.564: INFO: Received response from host: affinity-nodeport-timeout-8fgfz
Dec 30 04:36:24.564: INFO: Received response from host: affinity-nodeport-timeout-8fgfz
Dec 30 04:36:24.564: INFO: Received response from host: affinity-nodeport-timeout-8fgfz
Dec 30 04:36:24.564: INFO: Received response from host: affinity-nodeport-timeout-8fgfz
Dec 30 04:36:24.564: INFO: Received response from host: affinity-nodeport-timeout-8fgfz
Dec 30 04:36:24.564: INFO: Received response from host: affinity-nodeport-timeout-8fgfz
Dec 30 04:36:24.564: INFO: Received response from host: affinity-nodeport-timeout-8fgfz
Dec 30 04:36:24.564: INFO: Received response from host: affinity-nodeport-timeout-8fgfz
Dec 30 04:36:24.564: INFO: Received response from host: affinity-nodeport-timeout-8fgfz
Dec 30 04:36:24.564: INFO: Received response from host: affinity-nodeport-timeout-8fgfz
Dec 30 04:36:24.564: INFO: Received response from host: affinity-nodeport-timeout-8fgfz
Dec 30 04:36:24.564: INFO: Received response from host: affinity-nodeport-timeout-8fgfz
Dec 30 04:36:24.564: INFO: Received response from host: affinity-nodeport-timeout-8fgfz
Dec 30 04:36:24.564: INFO: Received response from host: affinity-nodeport-timeout-8fgfz
Dec 30 04:36:24.564: INFO: Received response from host: affinity-nodeport-timeout-8fgfz
Dec 30 04:36:24.564: INFO: Received response from host: affinity-nodeport-timeout-8fgfz
Dec 30 04:36:24.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-7926 exec execpod-affinityklk9p -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.78.26.140:30048/'
Dec 30 04:36:24.763: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.78.26.140:30048/\n"
Dec 30 04:36:24.763: INFO: stdout: "affinity-nodeport-timeout-8fgfz"
Dec 30 04:38:34.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-7926 exec execpod-affinityklk9p -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.78.26.140:30048/'
Dec 30 04:38:34.979: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.78.26.140:30048/\n"
Dec 30 04:38:34.979: INFO: stdout: "affinity-nodeport-timeout-chh8b"
Dec 30 04:38:34.979: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-7926, will wait for the garbage collector to delete the pods 12/30/22 04:38:34.992
Dec 30 04:38:35.054: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 6.731713ms
Dec 30 04:38:35.154: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 100.216296ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Dec 30 04:38:37.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7926" for this suite. 12/30/22 04:38:37.18
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","completed":281,"skipped":5156,"failed":0}
------------------------------
• [SLOW TEST] [142.121 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:36:15.066
    Dec 30 04:36:15.066: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename services 12/30/22 04:36:15.068
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:36:15.08
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:36:15.083
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2221
    STEP: creating service in namespace services-7926 12/30/22 04:36:15.086
    Dec 30 04:36:15.095: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-7926" to be "running and ready"
    Dec 30 04:36:15.099: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 3.852658ms
    Dec 30 04:36:15.099: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
    Dec 30 04:36:17.104: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.008537984s
    Dec 30 04:36:17.104: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
    Dec 30 04:36:17.104: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
    Dec 30 04:36:17.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-7926 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
    Dec 30 04:36:17.332: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
    Dec 30 04:36:17.332: INFO: stdout: "ipvs"
    Dec 30 04:36:17.332: INFO: proxyMode: ipvs
    Dec 30 04:36:17.343: INFO: Waiting for pod kube-proxy-mode-detector to disappear
    Dec 30 04:36:17.346: INFO: Pod kube-proxy-mode-detector no longer exists
    STEP: creating service affinity-nodeport-timeout in namespace services-7926 12/30/22 04:36:17.346
    STEP: creating replication controller affinity-nodeport-timeout in namespace services-7926 12/30/22 04:36:17.362
    I1230 04:36:17.368879      25 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-7926, replica count: 3
    I1230 04:36:20.421383      25 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Dec 30 04:36:20.434: INFO: Creating new exec pod
    Dec 30 04:36:20.440: INFO: Waiting up to 5m0s for pod "execpod-affinityklk9p" in namespace "services-7926" to be "running"
    Dec 30 04:36:20.444: INFO: Pod "execpod-affinityklk9p": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031247ms
    Dec 30 04:36:22.450: INFO: Pod "execpod-affinityklk9p": Phase="Running", Reason="", readiness=true. Elapsed: 2.009501709s
    Dec 30 04:36:22.450: INFO: Pod "execpod-affinityklk9p" satisfied condition "running"
    Dec 30 04:36:23.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-7926 exec execpod-affinityklk9p -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
    Dec 30 04:36:23.681: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
    Dec 30 04:36:23.681: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Dec 30 04:36:23.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-7926 exec execpod-affinityklk9p -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.7.134 80'
    Dec 30 04:36:23.888: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.7.134 80\nConnection to 10.233.7.134 80 port [tcp/http] succeeded!\n"
    Dec 30 04:36:23.888: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Dec 30 04:36:23.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-7926 exec execpod-affinityklk9p -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.78.26.142 30048'
    Dec 30 04:36:24.080: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.78.26.142 30048\nConnection to 10.78.26.142 30048 port [tcp/*] succeeded!\n"
    Dec 30 04:36:24.080: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Dec 30 04:36:24.080: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-7926 exec execpod-affinityklk9p -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.78.26.140 30048'
    Dec 30 04:36:24.265: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.78.26.140 30048\nConnection to 10.78.26.140 30048 port [tcp/*] succeeded!\n"
    Dec 30 04:36:24.265: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Dec 30 04:36:24.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-7926 exec execpod-affinityklk9p -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.78.26.140:30048/ ; done'
    Dec 30 04:36:24.564: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30048/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30048/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30048/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30048/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30048/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30048/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30048/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30048/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30048/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30048/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30048/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30048/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30048/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30048/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30048/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:30048/\n"
    Dec 30 04:36:24.564: INFO: stdout: "\naffinity-nodeport-timeout-8fgfz\naffinity-nodeport-timeout-8fgfz\naffinity-nodeport-timeout-8fgfz\naffinity-nodeport-timeout-8fgfz\naffinity-nodeport-timeout-8fgfz\naffinity-nodeport-timeout-8fgfz\naffinity-nodeport-timeout-8fgfz\naffinity-nodeport-timeout-8fgfz\naffinity-nodeport-timeout-8fgfz\naffinity-nodeport-timeout-8fgfz\naffinity-nodeport-timeout-8fgfz\naffinity-nodeport-timeout-8fgfz\naffinity-nodeport-timeout-8fgfz\naffinity-nodeport-timeout-8fgfz\naffinity-nodeport-timeout-8fgfz\naffinity-nodeport-timeout-8fgfz"
    Dec 30 04:36:24.564: INFO: Received response from host: affinity-nodeport-timeout-8fgfz
    Dec 30 04:36:24.564: INFO: Received response from host: affinity-nodeport-timeout-8fgfz
    Dec 30 04:36:24.564: INFO: Received response from host: affinity-nodeport-timeout-8fgfz
    Dec 30 04:36:24.564: INFO: Received response from host: affinity-nodeport-timeout-8fgfz
    Dec 30 04:36:24.564: INFO: Received response from host: affinity-nodeport-timeout-8fgfz
    Dec 30 04:36:24.564: INFO: Received response from host: affinity-nodeport-timeout-8fgfz
    Dec 30 04:36:24.564: INFO: Received response from host: affinity-nodeport-timeout-8fgfz
    Dec 30 04:36:24.564: INFO: Received response from host: affinity-nodeport-timeout-8fgfz
    Dec 30 04:36:24.564: INFO: Received response from host: affinity-nodeport-timeout-8fgfz
    Dec 30 04:36:24.564: INFO: Received response from host: affinity-nodeport-timeout-8fgfz
    Dec 30 04:36:24.564: INFO: Received response from host: affinity-nodeport-timeout-8fgfz
    Dec 30 04:36:24.564: INFO: Received response from host: affinity-nodeport-timeout-8fgfz
    Dec 30 04:36:24.564: INFO: Received response from host: affinity-nodeport-timeout-8fgfz
    Dec 30 04:36:24.564: INFO: Received response from host: affinity-nodeport-timeout-8fgfz
    Dec 30 04:36:24.564: INFO: Received response from host: affinity-nodeport-timeout-8fgfz
    Dec 30 04:36:24.564: INFO: Received response from host: affinity-nodeport-timeout-8fgfz
    Dec 30 04:36:24.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-7926 exec execpod-affinityklk9p -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.78.26.140:30048/'
    Dec 30 04:36:24.763: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.78.26.140:30048/\n"
    Dec 30 04:36:24.763: INFO: stdout: "affinity-nodeport-timeout-8fgfz"
    Dec 30 04:38:34.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-7926 exec execpod-affinityklk9p -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.78.26.140:30048/'
    Dec 30 04:38:34.979: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.78.26.140:30048/\n"
    Dec 30 04:38:34.979: INFO: stdout: "affinity-nodeport-timeout-chh8b"
    Dec 30 04:38:34.979: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-7926, will wait for the garbage collector to delete the pods 12/30/22 04:38:34.992
    Dec 30 04:38:35.054: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 6.731713ms
    Dec 30 04:38:35.154: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 100.216296ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Dec 30 04:38:37.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-7926" for this suite. 12/30/22 04:38:37.18
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:38:37.188
Dec 30 04:38:37.188: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename downward-api 12/30/22 04:38:37.189
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:38:37.203
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:38:37.206
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266
STEP: Creating a pod to test downward api env vars 12/30/22 04:38:37.209
Dec 30 04:38:37.218: INFO: Waiting up to 5m0s for pod "downward-api-9c7f9b68-2709-487a-acbf-480f80b11143" in namespace "downward-api-41" to be "Succeeded or Failed"
Dec 30 04:38:37.222: INFO: Pod "downward-api-9c7f9b68-2709-487a-acbf-480f80b11143": Phase="Pending", Reason="", readiness=false. Elapsed: 3.743197ms
Dec 30 04:38:39.228: INFO: Pod "downward-api-9c7f9b68-2709-487a-acbf-480f80b11143": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009522595s
Dec 30 04:38:41.228: INFO: Pod "downward-api-9c7f9b68-2709-487a-acbf-480f80b11143": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009246424s
STEP: Saw pod success 12/30/22 04:38:41.228
Dec 30 04:38:41.228: INFO: Pod "downward-api-9c7f9b68-2709-487a-acbf-480f80b11143" satisfied condition "Succeeded or Failed"
Dec 30 04:38:41.232: INFO: Trying to get logs from node k8s-mgmt01 pod downward-api-9c7f9b68-2709-487a-acbf-480f80b11143 container dapi-container: <nil>
STEP: delete the pod 12/30/22 04:38:41.252
Dec 30 04:38:41.266: INFO: Waiting for pod downward-api-9c7f9b68-2709-487a-acbf-480f80b11143 to disappear
Dec 30 04:38:41.270: INFO: Pod downward-api-9c7f9b68-2709-487a-acbf-480f80b11143 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Dec 30 04:38:41.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-41" for this suite. 12/30/22 04:38:41.275
{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","completed":282,"skipped":5158,"failed":0}
------------------------------
• [4.094 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:38:37.188
    Dec 30 04:38:37.188: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename downward-api 12/30/22 04:38:37.189
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:38:37.203
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:38:37.206
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:266
    STEP: Creating a pod to test downward api env vars 12/30/22 04:38:37.209
    Dec 30 04:38:37.218: INFO: Waiting up to 5m0s for pod "downward-api-9c7f9b68-2709-487a-acbf-480f80b11143" in namespace "downward-api-41" to be "Succeeded or Failed"
    Dec 30 04:38:37.222: INFO: Pod "downward-api-9c7f9b68-2709-487a-acbf-480f80b11143": Phase="Pending", Reason="", readiness=false. Elapsed: 3.743197ms
    Dec 30 04:38:39.228: INFO: Pod "downward-api-9c7f9b68-2709-487a-acbf-480f80b11143": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009522595s
    Dec 30 04:38:41.228: INFO: Pod "downward-api-9c7f9b68-2709-487a-acbf-480f80b11143": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009246424s
    STEP: Saw pod success 12/30/22 04:38:41.228
    Dec 30 04:38:41.228: INFO: Pod "downward-api-9c7f9b68-2709-487a-acbf-480f80b11143" satisfied condition "Succeeded or Failed"
    Dec 30 04:38:41.232: INFO: Trying to get logs from node k8s-mgmt01 pod downward-api-9c7f9b68-2709-487a-acbf-480f80b11143 container dapi-container: <nil>
    STEP: delete the pod 12/30/22 04:38:41.252
    Dec 30 04:38:41.266: INFO: Waiting for pod downward-api-9c7f9b68-2709-487a-acbf-480f80b11143 to disappear
    Dec 30 04:38:41.270: INFO: Pod downward-api-9c7f9b68-2709-487a-acbf-480f80b11143 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Dec 30 04:38:41.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-41" for this suite. 12/30/22 04:38:41.275
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:38:41.283
Dec 30 04:38:41.283: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename gc 12/30/22 04:38:41.284
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:38:41.298
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:38:41.302
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 12/30/22 04:38:41.305
STEP: delete the rc 12/30/22 04:38:46.316
STEP: wait for all pods to be garbage collected 12/30/22 04:38:46.322
STEP: Gathering metrics 12/30/22 04:38:51.331
Dec 30 04:38:51.370: INFO: Waiting up to 5m0s for pod "kube-controller-manager-k8s-mgmt03" in namespace "kube-system" to be "running and ready"
Dec 30 04:38:51.374: INFO: Pod "kube-controller-manager-k8s-mgmt03": Phase="Running", Reason="", readiness=true. Elapsed: 4.154679ms
Dec 30 04:38:51.374: INFO: The phase of Pod kube-controller-manager-k8s-mgmt03 is Running (Ready = true)
Dec 30 04:38:51.374: INFO: Pod "kube-controller-manager-k8s-mgmt03" satisfied condition "running and ready"
Dec 30 04:38:51.464: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Dec 30 04:38:51.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1879" for this suite. 12/30/22 04:38:51.47
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","completed":283,"skipped":5165,"failed":0}
------------------------------
• [SLOW TEST] [10.193 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:38:41.283
    Dec 30 04:38:41.283: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename gc 12/30/22 04:38:41.284
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:38:41.298
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:38:41.302
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 12/30/22 04:38:41.305
    STEP: delete the rc 12/30/22 04:38:46.316
    STEP: wait for all pods to be garbage collected 12/30/22 04:38:46.322
    STEP: Gathering metrics 12/30/22 04:38:51.331
    Dec 30 04:38:51.370: INFO: Waiting up to 5m0s for pod "kube-controller-manager-k8s-mgmt03" in namespace "kube-system" to be "running and ready"
    Dec 30 04:38:51.374: INFO: Pod "kube-controller-manager-k8s-mgmt03": Phase="Running", Reason="", readiness=true. Elapsed: 4.154679ms
    Dec 30 04:38:51.374: INFO: The phase of Pod kube-controller-manager-k8s-mgmt03 is Running (Ready = true)
    Dec 30 04:38:51.374: INFO: Pod "kube-controller-manager-k8s-mgmt03" satisfied condition "running and ready"
    Dec 30 04:38:51.464: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Dec 30 04:38:51.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-1879" for this suite. 12/30/22 04:38:51.47
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:38:51.478
Dec 30 04:38:51.478: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename resourcequota 12/30/22 04:38:51.48
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:38:51.493
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:38:51.496
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220
STEP: Counting existing ResourceQuota 12/30/22 04:38:51.499
STEP: Creating a ResourceQuota 12/30/22 04:38:56.504
STEP: Ensuring resource quota status is calculated 12/30/22 04:38:56.509
STEP: Creating a Pod that fits quota 12/30/22 04:38:58.515
STEP: Ensuring ResourceQuota status captures the pod usage 12/30/22 04:38:58.533
STEP: Not allowing a pod to be created that exceeds remaining quota 12/30/22 04:39:00.539
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 12/30/22 04:39:00.542
STEP: Ensuring a pod cannot update its resource requirements 12/30/22 04:39:00.546
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 12/30/22 04:39:00.552
STEP: Deleting the pod 12/30/22 04:39:02.558
STEP: Ensuring resource quota status released the pod usage 12/30/22 04:39:02.572
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Dec 30 04:39:04.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9381" for this suite. 12/30/22 04:39:04.584
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","completed":284,"skipped":5173,"failed":0}
------------------------------
• [SLOW TEST] [13.113 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:38:51.478
    Dec 30 04:38:51.478: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename resourcequota 12/30/22 04:38:51.48
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:38:51.493
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:38:51.496
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:220
    STEP: Counting existing ResourceQuota 12/30/22 04:38:51.499
    STEP: Creating a ResourceQuota 12/30/22 04:38:56.504
    STEP: Ensuring resource quota status is calculated 12/30/22 04:38:56.509
    STEP: Creating a Pod that fits quota 12/30/22 04:38:58.515
    STEP: Ensuring ResourceQuota status captures the pod usage 12/30/22 04:38:58.533
    STEP: Not allowing a pod to be created that exceeds remaining quota 12/30/22 04:39:00.539
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 12/30/22 04:39:00.542
    STEP: Ensuring a pod cannot update its resource requirements 12/30/22 04:39:00.546
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 12/30/22 04:39:00.552
    STEP: Deleting the pod 12/30/22 04:39:02.558
    STEP: Ensuring resource quota status released the pod usage 12/30/22 04:39:02.572
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Dec 30 04:39:04.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-9381" for this suite. 12/30/22 04:39:04.584
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:39:04.595
Dec 30 04:39:04.595: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename crd-publish-openapi 12/30/22 04:39:04.596
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:39:04.609
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:39:04.612
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441
STEP: set up a multi version CRD 12/30/22 04:39:04.615
Dec 30 04:39:04.616: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: mark a version not serverd 12/30/22 04:39:19.711
STEP: check the unserved version gets removed 12/30/22 04:39:19.737
STEP: check the other version is not changed 12/30/22 04:39:24.15
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Dec 30 04:39:30.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3443" for this suite. 12/30/22 04:39:30.846
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","completed":285,"skipped":5210,"failed":0}
------------------------------
• [SLOW TEST] [26.258 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:39:04.595
    Dec 30 04:39:04.595: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename crd-publish-openapi 12/30/22 04:39:04.596
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:39:04.609
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:39:04.612
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:441
    STEP: set up a multi version CRD 12/30/22 04:39:04.615
    Dec 30 04:39:04.616: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: mark a version not serverd 12/30/22 04:39:19.711
    STEP: check the unserved version gets removed 12/30/22 04:39:19.737
    STEP: check the other version is not changed 12/30/22 04:39:24.15
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Dec 30 04:39:30.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-3443" for this suite. 12/30/22 04:39:30.846
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:39:30.853
Dec 30 04:39:30.853: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename secrets 12/30/22 04:39:30.854
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:39:30.867
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:39:30.87
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56
STEP: Creating secret with name secret-test-4c46098d-33d4-422d-8853-dd330ae78b3a 12/30/22 04:39:30.873
STEP: Creating a pod to test consume secrets 12/30/22 04:39:30.878
Dec 30 04:39:30.887: INFO: Waiting up to 5m0s for pod "pod-secrets-d8b13b7c-9781-4ac7-9c1f-abd9c86f7933" in namespace "secrets-9788" to be "Succeeded or Failed"
Dec 30 04:39:30.891: INFO: Pod "pod-secrets-d8b13b7c-9781-4ac7-9c1f-abd9c86f7933": Phase="Pending", Reason="", readiness=false. Elapsed: 3.488068ms
Dec 30 04:39:32.897: INFO: Pod "pod-secrets-d8b13b7c-9781-4ac7-9c1f-abd9c86f7933": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009316992s
Dec 30 04:39:34.897: INFO: Pod "pod-secrets-d8b13b7c-9781-4ac7-9c1f-abd9c86f7933": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00960242s
STEP: Saw pod success 12/30/22 04:39:34.897
Dec 30 04:39:34.897: INFO: Pod "pod-secrets-d8b13b7c-9781-4ac7-9c1f-abd9c86f7933" satisfied condition "Succeeded or Failed"
Dec 30 04:39:34.901: INFO: Trying to get logs from node k8s-mgmt01 pod pod-secrets-d8b13b7c-9781-4ac7-9c1f-abd9c86f7933 container secret-volume-test: <nil>
STEP: delete the pod 12/30/22 04:39:34.909
Dec 30 04:39:34.922: INFO: Waiting for pod pod-secrets-d8b13b7c-9781-4ac7-9c1f-abd9c86f7933 to disappear
Dec 30 04:39:34.925: INFO: Pod pod-secrets-d8b13b7c-9781-4ac7-9c1f-abd9c86f7933 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Dec 30 04:39:34.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9788" for this suite. 12/30/22 04:39:34.931
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":286,"skipped":5215,"failed":0}
------------------------------
• [4.084 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:39:30.853
    Dec 30 04:39:30.853: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename secrets 12/30/22 04:39:30.854
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:39:30.867
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:39:30.87
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:56
    STEP: Creating secret with name secret-test-4c46098d-33d4-422d-8853-dd330ae78b3a 12/30/22 04:39:30.873
    STEP: Creating a pod to test consume secrets 12/30/22 04:39:30.878
    Dec 30 04:39:30.887: INFO: Waiting up to 5m0s for pod "pod-secrets-d8b13b7c-9781-4ac7-9c1f-abd9c86f7933" in namespace "secrets-9788" to be "Succeeded or Failed"
    Dec 30 04:39:30.891: INFO: Pod "pod-secrets-d8b13b7c-9781-4ac7-9c1f-abd9c86f7933": Phase="Pending", Reason="", readiness=false. Elapsed: 3.488068ms
    Dec 30 04:39:32.897: INFO: Pod "pod-secrets-d8b13b7c-9781-4ac7-9c1f-abd9c86f7933": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009316992s
    Dec 30 04:39:34.897: INFO: Pod "pod-secrets-d8b13b7c-9781-4ac7-9c1f-abd9c86f7933": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00960242s
    STEP: Saw pod success 12/30/22 04:39:34.897
    Dec 30 04:39:34.897: INFO: Pod "pod-secrets-d8b13b7c-9781-4ac7-9c1f-abd9c86f7933" satisfied condition "Succeeded or Failed"
    Dec 30 04:39:34.901: INFO: Trying to get logs from node k8s-mgmt01 pod pod-secrets-d8b13b7c-9781-4ac7-9c1f-abd9c86f7933 container secret-volume-test: <nil>
    STEP: delete the pod 12/30/22 04:39:34.909
    Dec 30 04:39:34.922: INFO: Waiting for pod pod-secrets-d8b13b7c-9781-4ac7-9c1f-abd9c86f7933 to disappear
    Dec 30 04:39:34.925: INFO: Pod pod-secrets-d8b13b7c-9781-4ac7-9c1f-abd9c86f7933 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Dec 30 04:39:34.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-9788" for this suite. 12/30/22 04:39:34.931
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:39:34.939
Dec 30 04:39:34.939: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename configmap 12/30/22 04:39:34.94
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:39:34.953
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:39:34.957
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422
STEP: Creating configMap with name configmap-test-volume-d776a391-37fd-41f0-b857-2af771e5a3c9 12/30/22 04:39:34.961
STEP: Creating a pod to test consume configMaps 12/30/22 04:39:34.965
Dec 30 04:39:34.974: INFO: Waiting up to 5m0s for pod "pod-configmaps-ecd070c3-064e-4c6f-8b1c-2fa29ea99dd1" in namespace "configmap-3129" to be "Succeeded or Failed"
Dec 30 04:39:34.978: INFO: Pod "pod-configmaps-ecd070c3-064e-4c6f-8b1c-2fa29ea99dd1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.837853ms
Dec 30 04:39:36.982: INFO: Pod "pod-configmaps-ecd070c3-064e-4c6f-8b1c-2fa29ea99dd1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008490303s
Dec 30 04:39:38.983: INFO: Pod "pod-configmaps-ecd070c3-064e-4c6f-8b1c-2fa29ea99dd1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009736263s
STEP: Saw pod success 12/30/22 04:39:38.984
Dec 30 04:39:38.984: INFO: Pod "pod-configmaps-ecd070c3-064e-4c6f-8b1c-2fa29ea99dd1" satisfied condition "Succeeded or Failed"
Dec 30 04:39:38.987: INFO: Trying to get logs from node k8s-mgmt01 pod pod-configmaps-ecd070c3-064e-4c6f-8b1c-2fa29ea99dd1 container configmap-volume-test: <nil>
STEP: delete the pod 12/30/22 04:39:39.002
Dec 30 04:39:39.015: INFO: Waiting for pod pod-configmaps-ecd070c3-064e-4c6f-8b1c-2fa29ea99dd1 to disappear
Dec 30 04:39:39.018: INFO: Pod pod-configmaps-ecd070c3-064e-4c6f-8b1c-2fa29ea99dd1 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Dec 30 04:39:39.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3129" for this suite. 12/30/22 04:39:39.023
{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","completed":287,"skipped":5227,"failed":0}
------------------------------
• [4.091 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:39:34.939
    Dec 30 04:39:34.939: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename configmap 12/30/22 04:39:34.94
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:39:34.953
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:39:34.957
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:422
    STEP: Creating configMap with name configmap-test-volume-d776a391-37fd-41f0-b857-2af771e5a3c9 12/30/22 04:39:34.961
    STEP: Creating a pod to test consume configMaps 12/30/22 04:39:34.965
    Dec 30 04:39:34.974: INFO: Waiting up to 5m0s for pod "pod-configmaps-ecd070c3-064e-4c6f-8b1c-2fa29ea99dd1" in namespace "configmap-3129" to be "Succeeded or Failed"
    Dec 30 04:39:34.978: INFO: Pod "pod-configmaps-ecd070c3-064e-4c6f-8b1c-2fa29ea99dd1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.837853ms
    Dec 30 04:39:36.982: INFO: Pod "pod-configmaps-ecd070c3-064e-4c6f-8b1c-2fa29ea99dd1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008490303s
    Dec 30 04:39:38.983: INFO: Pod "pod-configmaps-ecd070c3-064e-4c6f-8b1c-2fa29ea99dd1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009736263s
    STEP: Saw pod success 12/30/22 04:39:38.984
    Dec 30 04:39:38.984: INFO: Pod "pod-configmaps-ecd070c3-064e-4c6f-8b1c-2fa29ea99dd1" satisfied condition "Succeeded or Failed"
    Dec 30 04:39:38.987: INFO: Trying to get logs from node k8s-mgmt01 pod pod-configmaps-ecd070c3-064e-4c6f-8b1c-2fa29ea99dd1 container configmap-volume-test: <nil>
    STEP: delete the pod 12/30/22 04:39:39.002
    Dec 30 04:39:39.015: INFO: Waiting for pod pod-configmaps-ecd070c3-064e-4c6f-8b1c-2fa29ea99dd1 to disappear
    Dec 30 04:39:39.018: INFO: Pod pod-configmaps-ecd070c3-064e-4c6f-8b1c-2fa29ea99dd1 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Dec 30 04:39:39.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-3129" for this suite. 12/30/22 04:39:39.023
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:39:39.03
Dec 30 04:39:39.030: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename watch 12/30/22 04:39:39.032
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:39:39.044
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:39:39.047
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 12/30/22 04:39:39.051
STEP: modifying the configmap once 12/30/22 04:39:39.056
STEP: modifying the configmap a second time 12/30/22 04:39:39.064
STEP: deleting the configmap 12/30/22 04:39:39.072
STEP: creating a watch on configmaps from the resource version returned by the first update 12/30/22 04:39:39.079
STEP: Expecting to observe notifications for all changes to the configmap after the first update 12/30/22 04:39:39.08
Dec 30 04:39:39.080: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-5226  0884e13c-3b13-45fa-88b0-4f7970265c01 455497 0 2022-12-30 04:39:39 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2022-12-30 04:39:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 30 04:39:39.081: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-5226  0884e13c-3b13-45fa-88b0-4f7970265c01 455498 0 2022-12-30 04:39:39 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2022-12-30 04:39:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Dec 30 04:39:39.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5226" for this suite. 12/30/22 04:39:39.086
{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","completed":288,"skipped":5227,"failed":0}
------------------------------
• [0.063 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:39:39.03
    Dec 30 04:39:39.030: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename watch 12/30/22 04:39:39.032
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:39:39.044
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:39:39.047
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 12/30/22 04:39:39.051
    STEP: modifying the configmap once 12/30/22 04:39:39.056
    STEP: modifying the configmap a second time 12/30/22 04:39:39.064
    STEP: deleting the configmap 12/30/22 04:39:39.072
    STEP: creating a watch on configmaps from the resource version returned by the first update 12/30/22 04:39:39.079
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 12/30/22 04:39:39.08
    Dec 30 04:39:39.080: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-5226  0884e13c-3b13-45fa-88b0-4f7970265c01 455497 0 2022-12-30 04:39:39 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2022-12-30 04:39:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Dec 30 04:39:39.081: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-5226  0884e13c-3b13-45fa-88b0-4f7970265c01 455498 0 2022-12-30 04:39:39 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2022-12-30 04:39:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Dec 30 04:39:39.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-5226" for this suite. 12/30/22 04:39:39.086
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:39:39.094
Dec 30 04:39:39.094: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename crd-publish-openapi 12/30/22 04:39:39.095
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:39:39.109
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:39:39.111
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68
Dec 30 04:39:39.115: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 12/30/22 04:39:47.481
Dec 30 04:39:47.481: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=crd-publish-openapi-7357 --namespace=crd-publish-openapi-7357 create -f -'
Dec 30 04:39:48.315: INFO: stderr: ""
Dec 30 04:39:48.315: INFO: stdout: "e2e-test-crd-publish-openapi-149-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Dec 30 04:39:48.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=crd-publish-openapi-7357 --namespace=crd-publish-openapi-7357 delete e2e-test-crd-publish-openapi-149-crds test-foo'
Dec 30 04:39:48.464: INFO: stderr: ""
Dec 30 04:39:48.464: INFO: stdout: "e2e-test-crd-publish-openapi-149-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Dec 30 04:39:48.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=crd-publish-openapi-7357 --namespace=crd-publish-openapi-7357 apply -f -'
Dec 30 04:39:49.234: INFO: stderr: ""
Dec 30 04:39:49.234: INFO: stdout: "e2e-test-crd-publish-openapi-149-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Dec 30 04:39:49.235: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=crd-publish-openapi-7357 --namespace=crd-publish-openapi-7357 delete e2e-test-crd-publish-openapi-149-crds test-foo'
Dec 30 04:39:49.337: INFO: stderr: ""
Dec 30 04:39:49.337: INFO: stdout: "e2e-test-crd-publish-openapi-149-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 12/30/22 04:39:49.337
Dec 30 04:39:49.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=crd-publish-openapi-7357 --namespace=crd-publish-openapi-7357 create -f -'
Dec 30 04:39:50.035: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 12/30/22 04:39:50.035
Dec 30 04:39:50.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=crd-publish-openapi-7357 --namespace=crd-publish-openapi-7357 create -f -'
Dec 30 04:39:50.293: INFO: rc: 1
Dec 30 04:39:50.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=crd-publish-openapi-7357 --namespace=crd-publish-openapi-7357 apply -f -'
Dec 30 04:39:50.586: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 12/30/22 04:39:50.587
Dec 30 04:39:50.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=crd-publish-openapi-7357 --namespace=crd-publish-openapi-7357 create -f -'
Dec 30 04:39:50.861: INFO: rc: 1
Dec 30 04:39:50.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=crd-publish-openapi-7357 --namespace=crd-publish-openapi-7357 apply -f -'
Dec 30 04:39:51.130: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 12/30/22 04:39:51.13
Dec 30 04:39:51.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=crd-publish-openapi-7357 explain e2e-test-crd-publish-openapi-149-crds'
Dec 30 04:39:51.417: INFO: stderr: ""
Dec 30 04:39:51.417: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-149-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 12/30/22 04:39:51.418
Dec 30 04:39:51.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=crd-publish-openapi-7357 explain e2e-test-crd-publish-openapi-149-crds.metadata'
Dec 30 04:39:51.688: INFO: stderr: ""
Dec 30 04:39:51.688: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-149-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Dec 30 04:39:51.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=crd-publish-openapi-7357 explain e2e-test-crd-publish-openapi-149-crds.spec'
Dec 30 04:39:51.947: INFO: stderr: ""
Dec 30 04:39:51.947: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-149-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Dec 30 04:39:51.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=crd-publish-openapi-7357 explain e2e-test-crd-publish-openapi-149-crds.spec.bars'
Dec 30 04:39:52.218: INFO: stderr: ""
Dec 30 04:39:52.218: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-149-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 12/30/22 04:39:52.218
Dec 30 04:39:52.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=crd-publish-openapi-7357 explain e2e-test-crd-publish-openapi-149-crds.spec.bars2'
Dec 30 04:39:52.479: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Dec 30 04:39:55.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7357" for this suite. 12/30/22 04:39:55.694
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","completed":289,"skipped":5234,"failed":0}
------------------------------
• [SLOW TEST] [16.607 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:39:39.094
    Dec 30 04:39:39.094: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename crd-publish-openapi 12/30/22 04:39:39.095
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:39:39.109
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:39:39.111
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:68
    Dec 30 04:39:39.115: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 12/30/22 04:39:47.481
    Dec 30 04:39:47.481: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=crd-publish-openapi-7357 --namespace=crd-publish-openapi-7357 create -f -'
    Dec 30 04:39:48.315: INFO: stderr: ""
    Dec 30 04:39:48.315: INFO: stdout: "e2e-test-crd-publish-openapi-149-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Dec 30 04:39:48.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=crd-publish-openapi-7357 --namespace=crd-publish-openapi-7357 delete e2e-test-crd-publish-openapi-149-crds test-foo'
    Dec 30 04:39:48.464: INFO: stderr: ""
    Dec 30 04:39:48.464: INFO: stdout: "e2e-test-crd-publish-openapi-149-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    Dec 30 04:39:48.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=crd-publish-openapi-7357 --namespace=crd-publish-openapi-7357 apply -f -'
    Dec 30 04:39:49.234: INFO: stderr: ""
    Dec 30 04:39:49.234: INFO: stdout: "e2e-test-crd-publish-openapi-149-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Dec 30 04:39:49.235: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=crd-publish-openapi-7357 --namespace=crd-publish-openapi-7357 delete e2e-test-crd-publish-openapi-149-crds test-foo'
    Dec 30 04:39:49.337: INFO: stderr: ""
    Dec 30 04:39:49.337: INFO: stdout: "e2e-test-crd-publish-openapi-149-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 12/30/22 04:39:49.337
    Dec 30 04:39:49.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=crd-publish-openapi-7357 --namespace=crd-publish-openapi-7357 create -f -'
    Dec 30 04:39:50.035: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 12/30/22 04:39:50.035
    Dec 30 04:39:50.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=crd-publish-openapi-7357 --namespace=crd-publish-openapi-7357 create -f -'
    Dec 30 04:39:50.293: INFO: rc: 1
    Dec 30 04:39:50.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=crd-publish-openapi-7357 --namespace=crd-publish-openapi-7357 apply -f -'
    Dec 30 04:39:50.586: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 12/30/22 04:39:50.587
    Dec 30 04:39:50.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=crd-publish-openapi-7357 --namespace=crd-publish-openapi-7357 create -f -'
    Dec 30 04:39:50.861: INFO: rc: 1
    Dec 30 04:39:50.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=crd-publish-openapi-7357 --namespace=crd-publish-openapi-7357 apply -f -'
    Dec 30 04:39:51.130: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 12/30/22 04:39:51.13
    Dec 30 04:39:51.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=crd-publish-openapi-7357 explain e2e-test-crd-publish-openapi-149-crds'
    Dec 30 04:39:51.417: INFO: stderr: ""
    Dec 30 04:39:51.417: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-149-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 12/30/22 04:39:51.418
    Dec 30 04:39:51.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=crd-publish-openapi-7357 explain e2e-test-crd-publish-openapi-149-crds.metadata'
    Dec 30 04:39:51.688: INFO: stderr: ""
    Dec 30 04:39:51.688: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-149-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    Dec 30 04:39:51.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=crd-publish-openapi-7357 explain e2e-test-crd-publish-openapi-149-crds.spec'
    Dec 30 04:39:51.947: INFO: stderr: ""
    Dec 30 04:39:51.947: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-149-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    Dec 30 04:39:51.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=crd-publish-openapi-7357 explain e2e-test-crd-publish-openapi-149-crds.spec.bars'
    Dec 30 04:39:52.218: INFO: stderr: ""
    Dec 30 04:39:52.218: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-149-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 12/30/22 04:39:52.218
    Dec 30 04:39:52.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=crd-publish-openapi-7357 explain e2e-test-crd-publish-openapi-149-crds.spec.bars2'
    Dec 30 04:39:52.479: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Dec 30 04:39:55.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-7357" for this suite. 12/30/22 04:39:55.694
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:39:55.704
Dec 30 04:39:55.704: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename emptydir 12/30/22 04:39:55.706
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:39:55.724
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:39:55.727
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106
STEP: Creating a pod to test emptydir 0666 on tmpfs 12/30/22 04:39:55.731
Dec 30 04:39:55.741: INFO: Waiting up to 5m0s for pod "pod-131d7c00-f3c6-4960-a692-c44a920f52e9" in namespace "emptydir-7456" to be "Succeeded or Failed"
Dec 30 04:39:55.745: INFO: Pod "pod-131d7c00-f3c6-4960-a692-c44a920f52e9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.645535ms
Dec 30 04:39:57.750: INFO: Pod "pod-131d7c00-f3c6-4960-a692-c44a920f52e9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008606828s
Dec 30 04:39:59.750: INFO: Pod "pod-131d7c00-f3c6-4960-a692-c44a920f52e9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009495239s
STEP: Saw pod success 12/30/22 04:39:59.751
Dec 30 04:39:59.751: INFO: Pod "pod-131d7c00-f3c6-4960-a692-c44a920f52e9" satisfied condition "Succeeded or Failed"
Dec 30 04:39:59.755: INFO: Trying to get logs from node k8s-mgmt01 pod pod-131d7c00-f3c6-4960-a692-c44a920f52e9 container test-container: <nil>
STEP: delete the pod 12/30/22 04:39:59.776
Dec 30 04:39:59.787: INFO: Waiting for pod pod-131d7c00-f3c6-4960-a692-c44a920f52e9 to disappear
Dec 30 04:39:59.791: INFO: Pod pod-131d7c00-f3c6-4960-a692-c44a920f52e9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Dec 30 04:39:59.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7456" for this suite. 12/30/22 04:39:59.797
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":290,"skipped":5261,"failed":0}
------------------------------
• [4.099 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:39:55.704
    Dec 30 04:39:55.704: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename emptydir 12/30/22 04:39:55.706
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:39:55.724
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:39:55.727
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:106
    STEP: Creating a pod to test emptydir 0666 on tmpfs 12/30/22 04:39:55.731
    Dec 30 04:39:55.741: INFO: Waiting up to 5m0s for pod "pod-131d7c00-f3c6-4960-a692-c44a920f52e9" in namespace "emptydir-7456" to be "Succeeded or Failed"
    Dec 30 04:39:55.745: INFO: Pod "pod-131d7c00-f3c6-4960-a692-c44a920f52e9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.645535ms
    Dec 30 04:39:57.750: INFO: Pod "pod-131d7c00-f3c6-4960-a692-c44a920f52e9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008606828s
    Dec 30 04:39:59.750: INFO: Pod "pod-131d7c00-f3c6-4960-a692-c44a920f52e9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009495239s
    STEP: Saw pod success 12/30/22 04:39:59.751
    Dec 30 04:39:59.751: INFO: Pod "pod-131d7c00-f3c6-4960-a692-c44a920f52e9" satisfied condition "Succeeded or Failed"
    Dec 30 04:39:59.755: INFO: Trying to get logs from node k8s-mgmt01 pod pod-131d7c00-f3c6-4960-a692-c44a920f52e9 container test-container: <nil>
    STEP: delete the pod 12/30/22 04:39:59.776
    Dec 30 04:39:59.787: INFO: Waiting for pod pod-131d7c00-f3c6-4960-a692-c44a920f52e9 to disappear
    Dec 30 04:39:59.791: INFO: Pod pod-131d7c00-f3c6-4960-a692-c44a920f52e9 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Dec 30 04:39:59.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-7456" for this suite. 12/30/22 04:39:59.797
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:39:59.815
Dec 30 04:39:59.815: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename watch 12/30/22 04:39:59.816
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:39:59.831
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:39:59.834
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 12/30/22 04:39:59.838
STEP: starting a background goroutine to produce watch events 12/30/22 04:39:59.842
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 12/30/22 04:39:59.842
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Dec 30 04:40:02.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2219" for this suite. 12/30/22 04:40:02.672
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","completed":291,"skipped":5337,"failed":0}
------------------------------
• [2.909 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:39:59.815
    Dec 30 04:39:59.815: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename watch 12/30/22 04:39:59.816
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:39:59.831
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:39:59.834
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 12/30/22 04:39:59.838
    STEP: starting a background goroutine to produce watch events 12/30/22 04:39:59.842
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 12/30/22 04:39:59.842
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Dec 30 04:40:02.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-2219" for this suite. 12/30/22 04:40:02.672
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:40:02.726
Dec 30 04:40:02.726: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename kubelet-test 12/30/22 04:40:02.727
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:40:02.744
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:40:02.747
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Dec 30 04:40:06.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4892" for this suite. 12/30/22 04:40:06.777
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","completed":292,"skipped":5354,"failed":0}
------------------------------
• [4.058 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:40:02.726
    Dec 30 04:40:02.726: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename kubelet-test 12/30/22 04:40:02.727
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:40:02.744
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:40:02.747
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Dec 30 04:40:06.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-4892" for this suite. 12/30/22 04:40:06.777
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:40:06.786
Dec 30 04:40:06.786: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename subpath 12/30/22 04:40:06.788
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:40:06.805
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:40:06.809
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 12/30/22 04:40:06.812
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-nlr7 12/30/22 04:40:06.822
STEP: Creating a pod to test atomic-volume-subpath 12/30/22 04:40:06.822
Dec 30 04:40:06.832: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-nlr7" in namespace "subpath-7338" to be "Succeeded or Failed"
Dec 30 04:40:06.835: INFO: Pod "pod-subpath-test-projected-nlr7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.635545ms
Dec 30 04:40:08.841: INFO: Pod "pod-subpath-test-projected-nlr7": Phase="Running", Reason="", readiness=true. Elapsed: 2.009628685s
Dec 30 04:40:10.841: INFO: Pod "pod-subpath-test-projected-nlr7": Phase="Running", Reason="", readiness=true. Elapsed: 4.009240611s
Dec 30 04:40:12.841: INFO: Pod "pod-subpath-test-projected-nlr7": Phase="Running", Reason="", readiness=true. Elapsed: 6.00921048s
Dec 30 04:40:14.841: INFO: Pod "pod-subpath-test-projected-nlr7": Phase="Running", Reason="", readiness=true. Elapsed: 8.009081761s
Dec 30 04:40:16.841: INFO: Pod "pod-subpath-test-projected-nlr7": Phase="Running", Reason="", readiness=true. Elapsed: 10.008780205s
Dec 30 04:40:18.843: INFO: Pod "pod-subpath-test-projected-nlr7": Phase="Running", Reason="", readiness=true. Elapsed: 12.010793823s
Dec 30 04:40:20.840: INFO: Pod "pod-subpath-test-projected-nlr7": Phase="Running", Reason="", readiness=true. Elapsed: 14.008198339s
Dec 30 04:40:22.841: INFO: Pod "pod-subpath-test-projected-nlr7": Phase="Running", Reason="", readiness=true. Elapsed: 16.00944337s
Dec 30 04:40:24.842: INFO: Pod "pod-subpath-test-projected-nlr7": Phase="Running", Reason="", readiness=true. Elapsed: 18.00993885s
Dec 30 04:40:26.840: INFO: Pod "pod-subpath-test-projected-nlr7": Phase="Running", Reason="", readiness=true. Elapsed: 20.00783268s
Dec 30 04:40:28.842: INFO: Pod "pod-subpath-test-projected-nlr7": Phase="Running", Reason="", readiness=false. Elapsed: 22.010468536s
Dec 30 04:40:30.841: INFO: Pod "pod-subpath-test-projected-nlr7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.009302204s
STEP: Saw pod success 12/30/22 04:40:30.841
Dec 30 04:40:30.841: INFO: Pod "pod-subpath-test-projected-nlr7" satisfied condition "Succeeded or Failed"
Dec 30 04:40:30.845: INFO: Trying to get logs from node k8s-mgmt01 pod pod-subpath-test-projected-nlr7 container test-container-subpath-projected-nlr7: <nil>
STEP: delete the pod 12/30/22 04:40:30.854
Dec 30 04:40:30.868: INFO: Waiting for pod pod-subpath-test-projected-nlr7 to disappear
Dec 30 04:40:30.871: INFO: Pod pod-subpath-test-projected-nlr7 no longer exists
STEP: Deleting pod pod-subpath-test-projected-nlr7 12/30/22 04:40:30.871
Dec 30 04:40:30.871: INFO: Deleting pod "pod-subpath-test-projected-nlr7" in namespace "subpath-7338"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Dec 30 04:40:30.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7338" for this suite. 12/30/22 04:40:30.881
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]","completed":293,"skipped":5374,"failed":0}
------------------------------
• [SLOW TEST] [24.102 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:40:06.786
    Dec 30 04:40:06.786: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename subpath 12/30/22 04:40:06.788
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:40:06.805
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:40:06.809
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 12/30/22 04:40:06.812
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-nlr7 12/30/22 04:40:06.822
    STEP: Creating a pod to test atomic-volume-subpath 12/30/22 04:40:06.822
    Dec 30 04:40:06.832: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-nlr7" in namespace "subpath-7338" to be "Succeeded or Failed"
    Dec 30 04:40:06.835: INFO: Pod "pod-subpath-test-projected-nlr7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.635545ms
    Dec 30 04:40:08.841: INFO: Pod "pod-subpath-test-projected-nlr7": Phase="Running", Reason="", readiness=true. Elapsed: 2.009628685s
    Dec 30 04:40:10.841: INFO: Pod "pod-subpath-test-projected-nlr7": Phase="Running", Reason="", readiness=true. Elapsed: 4.009240611s
    Dec 30 04:40:12.841: INFO: Pod "pod-subpath-test-projected-nlr7": Phase="Running", Reason="", readiness=true. Elapsed: 6.00921048s
    Dec 30 04:40:14.841: INFO: Pod "pod-subpath-test-projected-nlr7": Phase="Running", Reason="", readiness=true. Elapsed: 8.009081761s
    Dec 30 04:40:16.841: INFO: Pod "pod-subpath-test-projected-nlr7": Phase="Running", Reason="", readiness=true. Elapsed: 10.008780205s
    Dec 30 04:40:18.843: INFO: Pod "pod-subpath-test-projected-nlr7": Phase="Running", Reason="", readiness=true. Elapsed: 12.010793823s
    Dec 30 04:40:20.840: INFO: Pod "pod-subpath-test-projected-nlr7": Phase="Running", Reason="", readiness=true. Elapsed: 14.008198339s
    Dec 30 04:40:22.841: INFO: Pod "pod-subpath-test-projected-nlr7": Phase="Running", Reason="", readiness=true. Elapsed: 16.00944337s
    Dec 30 04:40:24.842: INFO: Pod "pod-subpath-test-projected-nlr7": Phase="Running", Reason="", readiness=true. Elapsed: 18.00993885s
    Dec 30 04:40:26.840: INFO: Pod "pod-subpath-test-projected-nlr7": Phase="Running", Reason="", readiness=true. Elapsed: 20.00783268s
    Dec 30 04:40:28.842: INFO: Pod "pod-subpath-test-projected-nlr7": Phase="Running", Reason="", readiness=false. Elapsed: 22.010468536s
    Dec 30 04:40:30.841: INFO: Pod "pod-subpath-test-projected-nlr7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.009302204s
    STEP: Saw pod success 12/30/22 04:40:30.841
    Dec 30 04:40:30.841: INFO: Pod "pod-subpath-test-projected-nlr7" satisfied condition "Succeeded or Failed"
    Dec 30 04:40:30.845: INFO: Trying to get logs from node k8s-mgmt01 pod pod-subpath-test-projected-nlr7 container test-container-subpath-projected-nlr7: <nil>
    STEP: delete the pod 12/30/22 04:40:30.854
    Dec 30 04:40:30.868: INFO: Waiting for pod pod-subpath-test-projected-nlr7 to disappear
    Dec 30 04:40:30.871: INFO: Pod pod-subpath-test-projected-nlr7 no longer exists
    STEP: Deleting pod pod-subpath-test-projected-nlr7 12/30/22 04:40:30.871
    Dec 30 04:40:30.871: INFO: Deleting pod "pod-subpath-test-projected-nlr7" in namespace "subpath-7338"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Dec 30 04:40:30.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-7338" for this suite. 12/30/22 04:40:30.881
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:40:30.888
Dec 30 04:40:30.890: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename emptydir-wrapper 12/30/22 04:40:30.893
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:40:30.909
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:40:30.912
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
Dec 30 04:40:30.934: INFO: Waiting up to 5m0s for pod "pod-secrets-8d9b40a2-7093-47c7-8764-6ac6110e74b9" in namespace "emptydir-wrapper-3215" to be "running and ready"
Dec 30 04:40:30.937: INFO: Pod "pod-secrets-8d9b40a2-7093-47c7-8764-6ac6110e74b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.917104ms
Dec 30 04:40:30.937: INFO: The phase of Pod pod-secrets-8d9b40a2-7093-47c7-8764-6ac6110e74b9 is Pending, waiting for it to be Running (with Ready = true)
Dec 30 04:40:32.943: INFO: Pod "pod-secrets-8d9b40a2-7093-47c7-8764-6ac6110e74b9": Phase="Running", Reason="", readiness=true. Elapsed: 2.008748309s
Dec 30 04:40:32.943: INFO: The phase of Pod pod-secrets-8d9b40a2-7093-47c7-8764-6ac6110e74b9 is Running (Ready = true)
Dec 30 04:40:32.943: INFO: Pod "pod-secrets-8d9b40a2-7093-47c7-8764-6ac6110e74b9" satisfied condition "running and ready"
STEP: Cleaning up the secret 12/30/22 04:40:32.947
STEP: Cleaning up the configmap 12/30/22 04:40:32.954
STEP: Cleaning up the pod 12/30/22 04:40:32.961
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
Dec 30 04:40:32.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-3215" for this suite. 12/30/22 04:40:32.978
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","completed":294,"skipped":5378,"failed":0}
------------------------------
• [2.097 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:40:30.888
    Dec 30 04:40:30.890: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename emptydir-wrapper 12/30/22 04:40:30.893
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:40:30.909
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:40:30.912
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    Dec 30 04:40:30.934: INFO: Waiting up to 5m0s for pod "pod-secrets-8d9b40a2-7093-47c7-8764-6ac6110e74b9" in namespace "emptydir-wrapper-3215" to be "running and ready"
    Dec 30 04:40:30.937: INFO: Pod "pod-secrets-8d9b40a2-7093-47c7-8764-6ac6110e74b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.917104ms
    Dec 30 04:40:30.937: INFO: The phase of Pod pod-secrets-8d9b40a2-7093-47c7-8764-6ac6110e74b9 is Pending, waiting for it to be Running (with Ready = true)
    Dec 30 04:40:32.943: INFO: Pod "pod-secrets-8d9b40a2-7093-47c7-8764-6ac6110e74b9": Phase="Running", Reason="", readiness=true. Elapsed: 2.008748309s
    Dec 30 04:40:32.943: INFO: The phase of Pod pod-secrets-8d9b40a2-7093-47c7-8764-6ac6110e74b9 is Running (Ready = true)
    Dec 30 04:40:32.943: INFO: Pod "pod-secrets-8d9b40a2-7093-47c7-8764-6ac6110e74b9" satisfied condition "running and ready"
    STEP: Cleaning up the secret 12/30/22 04:40:32.947
    STEP: Cleaning up the configmap 12/30/22 04:40:32.954
    STEP: Cleaning up the pod 12/30/22 04:40:32.961
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:187
    Dec 30 04:40:32.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-wrapper-3215" for this suite. 12/30/22 04:40:32.978
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:40:32.986
Dec 30 04:40:32.987: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename downward-api 12/30/22 04:40:32.988
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:40:33.007
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:40:33.01
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216
STEP: Creating a pod to test downward api env vars 12/30/22 04:40:33.014
Dec 30 04:40:33.024: INFO: Waiting up to 5m0s for pod "downward-api-2b45ffbc-6c7e-4e37-ae5c-fdceebb74ec4" in namespace "downward-api-9307" to be "Succeeded or Failed"
Dec 30 04:40:33.028: INFO: Pod "downward-api-2b45ffbc-6c7e-4e37-ae5c-fdceebb74ec4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.211326ms
Dec 30 04:40:35.032: INFO: Pod "downward-api-2b45ffbc-6c7e-4e37-ae5c-fdceebb74ec4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008548104s
Dec 30 04:40:37.034: INFO: Pod "downward-api-2b45ffbc-6c7e-4e37-ae5c-fdceebb74ec4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009692471s
STEP: Saw pod success 12/30/22 04:40:37.034
Dec 30 04:40:37.034: INFO: Pod "downward-api-2b45ffbc-6c7e-4e37-ae5c-fdceebb74ec4" satisfied condition "Succeeded or Failed"
Dec 30 04:40:37.038: INFO: Trying to get logs from node k8s-mgmt01 pod downward-api-2b45ffbc-6c7e-4e37-ae5c-fdceebb74ec4 container dapi-container: <nil>
STEP: delete the pod 12/30/22 04:40:37.046
Dec 30 04:40:37.056: INFO: Waiting for pod downward-api-2b45ffbc-6c7e-4e37-ae5c-fdceebb74ec4 to disappear
Dec 30 04:40:37.060: INFO: Pod downward-api-2b45ffbc-6c7e-4e37-ae5c-fdceebb74ec4 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Dec 30 04:40:37.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9307" for this suite. 12/30/22 04:40:37.066
{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","completed":295,"skipped":5384,"failed":0}
------------------------------
• [4.086 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:40:32.986
    Dec 30 04:40:32.987: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename downward-api 12/30/22 04:40:32.988
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:40:33.007
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:40:33.01
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:216
    STEP: Creating a pod to test downward api env vars 12/30/22 04:40:33.014
    Dec 30 04:40:33.024: INFO: Waiting up to 5m0s for pod "downward-api-2b45ffbc-6c7e-4e37-ae5c-fdceebb74ec4" in namespace "downward-api-9307" to be "Succeeded or Failed"
    Dec 30 04:40:33.028: INFO: Pod "downward-api-2b45ffbc-6c7e-4e37-ae5c-fdceebb74ec4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.211326ms
    Dec 30 04:40:35.032: INFO: Pod "downward-api-2b45ffbc-6c7e-4e37-ae5c-fdceebb74ec4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008548104s
    Dec 30 04:40:37.034: INFO: Pod "downward-api-2b45ffbc-6c7e-4e37-ae5c-fdceebb74ec4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009692471s
    STEP: Saw pod success 12/30/22 04:40:37.034
    Dec 30 04:40:37.034: INFO: Pod "downward-api-2b45ffbc-6c7e-4e37-ae5c-fdceebb74ec4" satisfied condition "Succeeded or Failed"
    Dec 30 04:40:37.038: INFO: Trying to get logs from node k8s-mgmt01 pod downward-api-2b45ffbc-6c7e-4e37-ae5c-fdceebb74ec4 container dapi-container: <nil>
    STEP: delete the pod 12/30/22 04:40:37.046
    Dec 30 04:40:37.056: INFO: Waiting for pod downward-api-2b45ffbc-6c7e-4e37-ae5c-fdceebb74ec4 to disappear
    Dec 30 04:40:37.060: INFO: Pod downward-api-2b45ffbc-6c7e-4e37-ae5c-fdceebb74ec4 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Dec 30 04:40:37.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-9307" for this suite. 12/30/22 04:40:37.066
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:40:37.074
Dec 30 04:40:37.074: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename dns 12/30/22 04:40:37.075
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:40:37.092
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:40:37.096
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 12/30/22 04:40:37.099
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3397.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-3397.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3397.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3397.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3397.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-3397.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3397.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-3397.svc.cluster.local;sleep 1; done
 12/30/22 04:40:37.104
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3397.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-3397.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3397.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-3397.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3397.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-3397.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3397.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-3397.svc.cluster.local;sleep 1; done
 12/30/22 04:40:37.105
STEP: creating a pod to probe DNS 12/30/22 04:40:37.105
STEP: submitting the pod to kubernetes 12/30/22 04:40:37.105
Dec 30 04:40:37.115: INFO: Waiting up to 15m0s for pod "dns-test-a8c0d16a-022e-4673-9f86-40ba72007930" in namespace "dns-3397" to be "running"
Dec 30 04:40:37.119: INFO: Pod "dns-test-a8c0d16a-022e-4673-9f86-40ba72007930": Phase="Pending", Reason="", readiness=false. Elapsed: 3.83073ms
Dec 30 04:40:39.126: INFO: Pod "dns-test-a8c0d16a-022e-4673-9f86-40ba72007930": Phase="Running", Reason="", readiness=true. Elapsed: 2.010759576s
Dec 30 04:40:39.126: INFO: Pod "dns-test-a8c0d16a-022e-4673-9f86-40ba72007930" satisfied condition "running"
STEP: retrieving the pod 12/30/22 04:40:39.126
STEP: looking for the results for each expected name from probers 12/30/22 04:40:39.13
Dec 30 04:40:39.136: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3397.svc.cluster.local from pod dns-3397/dns-test-a8c0d16a-022e-4673-9f86-40ba72007930: the server could not find the requested resource (get pods dns-test-a8c0d16a-022e-4673-9f86-40ba72007930)
Dec 30 04:40:39.141: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3397.svc.cluster.local from pod dns-3397/dns-test-a8c0d16a-022e-4673-9f86-40ba72007930: the server could not find the requested resource (get pods dns-test-a8c0d16a-022e-4673-9f86-40ba72007930)
Dec 30 04:40:39.145: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3397.svc.cluster.local from pod dns-3397/dns-test-a8c0d16a-022e-4673-9f86-40ba72007930: the server could not find the requested resource (get pods dns-test-a8c0d16a-022e-4673-9f86-40ba72007930)
Dec 30 04:40:39.150: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3397.svc.cluster.local from pod dns-3397/dns-test-a8c0d16a-022e-4673-9f86-40ba72007930: the server could not find the requested resource (get pods dns-test-a8c0d16a-022e-4673-9f86-40ba72007930)
Dec 30 04:40:39.155: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3397.svc.cluster.local from pod dns-3397/dns-test-a8c0d16a-022e-4673-9f86-40ba72007930: the server could not find the requested resource (get pods dns-test-a8c0d16a-022e-4673-9f86-40ba72007930)
Dec 30 04:40:39.159: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3397.svc.cluster.local from pod dns-3397/dns-test-a8c0d16a-022e-4673-9f86-40ba72007930: the server could not find the requested resource (get pods dns-test-a8c0d16a-022e-4673-9f86-40ba72007930)
Dec 30 04:40:39.164: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3397.svc.cluster.local from pod dns-3397/dns-test-a8c0d16a-022e-4673-9f86-40ba72007930: the server could not find the requested resource (get pods dns-test-a8c0d16a-022e-4673-9f86-40ba72007930)
Dec 30 04:40:39.168: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3397.svc.cluster.local from pod dns-3397/dns-test-a8c0d16a-022e-4673-9f86-40ba72007930: the server could not find the requested resource (get pods dns-test-a8c0d16a-022e-4673-9f86-40ba72007930)
Dec 30 04:40:39.168: INFO: Lookups using dns-3397/dns-test-a8c0d16a-022e-4673-9f86-40ba72007930 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3397.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3397.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3397.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3397.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3397.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3397.svc.cluster.local jessie_udp@dns-test-service-2.dns-3397.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3397.svc.cluster.local]

Dec 30 04:40:44.209: INFO: DNS probes using dns-3397/dns-test-a8c0d16a-022e-4673-9f86-40ba72007930 succeeded

STEP: deleting the pod 12/30/22 04:40:44.209
STEP: deleting the test headless service 12/30/22 04:40:44.224
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Dec 30 04:40:44.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3397" for this suite. 12/30/22 04:40:44.241
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","completed":296,"skipped":5396,"failed":0}
------------------------------
• [SLOW TEST] [7.174 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:40:37.074
    Dec 30 04:40:37.074: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename dns 12/30/22 04:40:37.075
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:40:37.092
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:40:37.096
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 12/30/22 04:40:37.099
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3397.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-3397.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3397.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3397.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3397.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-3397.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3397.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-3397.svc.cluster.local;sleep 1; done
     12/30/22 04:40:37.104
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3397.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-3397.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3397.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-3397.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3397.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-3397.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3397.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-3397.svc.cluster.local;sleep 1; done
     12/30/22 04:40:37.105
    STEP: creating a pod to probe DNS 12/30/22 04:40:37.105
    STEP: submitting the pod to kubernetes 12/30/22 04:40:37.105
    Dec 30 04:40:37.115: INFO: Waiting up to 15m0s for pod "dns-test-a8c0d16a-022e-4673-9f86-40ba72007930" in namespace "dns-3397" to be "running"
    Dec 30 04:40:37.119: INFO: Pod "dns-test-a8c0d16a-022e-4673-9f86-40ba72007930": Phase="Pending", Reason="", readiness=false. Elapsed: 3.83073ms
    Dec 30 04:40:39.126: INFO: Pod "dns-test-a8c0d16a-022e-4673-9f86-40ba72007930": Phase="Running", Reason="", readiness=true. Elapsed: 2.010759576s
    Dec 30 04:40:39.126: INFO: Pod "dns-test-a8c0d16a-022e-4673-9f86-40ba72007930" satisfied condition "running"
    STEP: retrieving the pod 12/30/22 04:40:39.126
    STEP: looking for the results for each expected name from probers 12/30/22 04:40:39.13
    Dec 30 04:40:39.136: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3397.svc.cluster.local from pod dns-3397/dns-test-a8c0d16a-022e-4673-9f86-40ba72007930: the server could not find the requested resource (get pods dns-test-a8c0d16a-022e-4673-9f86-40ba72007930)
    Dec 30 04:40:39.141: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3397.svc.cluster.local from pod dns-3397/dns-test-a8c0d16a-022e-4673-9f86-40ba72007930: the server could not find the requested resource (get pods dns-test-a8c0d16a-022e-4673-9f86-40ba72007930)
    Dec 30 04:40:39.145: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3397.svc.cluster.local from pod dns-3397/dns-test-a8c0d16a-022e-4673-9f86-40ba72007930: the server could not find the requested resource (get pods dns-test-a8c0d16a-022e-4673-9f86-40ba72007930)
    Dec 30 04:40:39.150: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3397.svc.cluster.local from pod dns-3397/dns-test-a8c0d16a-022e-4673-9f86-40ba72007930: the server could not find the requested resource (get pods dns-test-a8c0d16a-022e-4673-9f86-40ba72007930)
    Dec 30 04:40:39.155: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3397.svc.cluster.local from pod dns-3397/dns-test-a8c0d16a-022e-4673-9f86-40ba72007930: the server could not find the requested resource (get pods dns-test-a8c0d16a-022e-4673-9f86-40ba72007930)
    Dec 30 04:40:39.159: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3397.svc.cluster.local from pod dns-3397/dns-test-a8c0d16a-022e-4673-9f86-40ba72007930: the server could not find the requested resource (get pods dns-test-a8c0d16a-022e-4673-9f86-40ba72007930)
    Dec 30 04:40:39.164: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3397.svc.cluster.local from pod dns-3397/dns-test-a8c0d16a-022e-4673-9f86-40ba72007930: the server could not find the requested resource (get pods dns-test-a8c0d16a-022e-4673-9f86-40ba72007930)
    Dec 30 04:40:39.168: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3397.svc.cluster.local from pod dns-3397/dns-test-a8c0d16a-022e-4673-9f86-40ba72007930: the server could not find the requested resource (get pods dns-test-a8c0d16a-022e-4673-9f86-40ba72007930)
    Dec 30 04:40:39.168: INFO: Lookups using dns-3397/dns-test-a8c0d16a-022e-4673-9f86-40ba72007930 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3397.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3397.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3397.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3397.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3397.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3397.svc.cluster.local jessie_udp@dns-test-service-2.dns-3397.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3397.svc.cluster.local]

    Dec 30 04:40:44.209: INFO: DNS probes using dns-3397/dns-test-a8c0d16a-022e-4673-9f86-40ba72007930 succeeded

    STEP: deleting the pod 12/30/22 04:40:44.209
    STEP: deleting the test headless service 12/30/22 04:40:44.224
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Dec 30 04:40:44.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-3397" for this suite. 12/30/22 04:40:44.241
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:40:44.253
Dec 30 04:40:44.253: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename projected 12/30/22 04:40:44.254
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:40:44.27
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:40:44.274
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129
STEP: Creating the pod 12/30/22 04:40:44.277
Dec 30 04:40:44.292: INFO: Waiting up to 5m0s for pod "labelsupdate277218b6-bce1-424c-86d2-5c9dc0183a59" in namespace "projected-5148" to be "running and ready"
Dec 30 04:40:44.296: INFO: Pod "labelsupdate277218b6-bce1-424c-86d2-5c9dc0183a59": Phase="Pending", Reason="", readiness=false. Elapsed: 3.798598ms
Dec 30 04:40:44.296: INFO: The phase of Pod labelsupdate277218b6-bce1-424c-86d2-5c9dc0183a59 is Pending, waiting for it to be Running (with Ready = true)
Dec 30 04:40:46.301: INFO: Pod "labelsupdate277218b6-bce1-424c-86d2-5c9dc0183a59": Phase="Running", Reason="", readiness=true. Elapsed: 2.009223648s
Dec 30 04:40:46.301: INFO: The phase of Pod labelsupdate277218b6-bce1-424c-86d2-5c9dc0183a59 is Running (Ready = true)
Dec 30 04:40:46.301: INFO: Pod "labelsupdate277218b6-bce1-424c-86d2-5c9dc0183a59" satisfied condition "running and ready"
Dec 30 04:40:46.829: INFO: Successfully updated pod "labelsupdate277218b6-bce1-424c-86d2-5c9dc0183a59"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Dec 30 04:40:50.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5148" for this suite. 12/30/22 04:40:50.862
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","completed":297,"skipped":5477,"failed":0}
------------------------------
• [SLOW TEST] [6.617 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:40:44.253
    Dec 30 04:40:44.253: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename projected 12/30/22 04:40:44.254
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:40:44.27
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:40:44.274
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:129
    STEP: Creating the pod 12/30/22 04:40:44.277
    Dec 30 04:40:44.292: INFO: Waiting up to 5m0s for pod "labelsupdate277218b6-bce1-424c-86d2-5c9dc0183a59" in namespace "projected-5148" to be "running and ready"
    Dec 30 04:40:44.296: INFO: Pod "labelsupdate277218b6-bce1-424c-86d2-5c9dc0183a59": Phase="Pending", Reason="", readiness=false. Elapsed: 3.798598ms
    Dec 30 04:40:44.296: INFO: The phase of Pod labelsupdate277218b6-bce1-424c-86d2-5c9dc0183a59 is Pending, waiting for it to be Running (with Ready = true)
    Dec 30 04:40:46.301: INFO: Pod "labelsupdate277218b6-bce1-424c-86d2-5c9dc0183a59": Phase="Running", Reason="", readiness=true. Elapsed: 2.009223648s
    Dec 30 04:40:46.301: INFO: The phase of Pod labelsupdate277218b6-bce1-424c-86d2-5c9dc0183a59 is Running (Ready = true)
    Dec 30 04:40:46.301: INFO: Pod "labelsupdate277218b6-bce1-424c-86d2-5c9dc0183a59" satisfied condition "running and ready"
    Dec 30 04:40:46.829: INFO: Successfully updated pod "labelsupdate277218b6-bce1-424c-86d2-5c9dc0183a59"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Dec 30 04:40:50.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5148" for this suite. 12/30/22 04:40:50.862
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:40:50.871
Dec 30 04:40:50.871: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename sched-pred 12/30/22 04:40:50.873
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:40:50.889
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:40:50.893
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Dec 30 04:40:50.896: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 30 04:40:50.907: INFO: Waiting for terminating namespaces to be deleted...
Dec 30 04:40:50.911: INFO: 
Logging pods the apiserver thinks is on node k8s-mgmt01 before test
Dec 30 04:40:50.925: INFO: calico-node-6xm98 from kube-system started at 2022-12-28 02:19:33 +0000 UTC (1 container statuses recorded)
Dec 30 04:40:50.925: INFO: 	Container calico-node ready: true, restart count 0
Dec 30 04:40:50.925: INFO: kube-apiserver-k8s-mgmt01 from kube-system started at 2022-12-28 02:15:50 +0000 UTC (1 container statuses recorded)
Dec 30 04:40:50.925: INFO: 	Container kube-apiserver ready: true, restart count 1
Dec 30 04:40:50.925: INFO: kube-controller-manager-k8s-mgmt01 from kube-system started at 2022-12-28 02:15:49 +0000 UTC (1 container statuses recorded)
Dec 30 04:40:50.925: INFO: 	Container kube-controller-manager ready: true, restart count 1
Dec 30 04:40:50.925: INFO: kube-proxy-tcvcl from kube-system started at 2022-12-28 02:18:09 +0000 UTC (1 container statuses recorded)
Dec 30 04:40:50.925: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 30 04:40:50.925: INFO: kube-scheduler-k8s-mgmt01 from kube-system started at 2022-12-28 02:15:38 +0000 UTC (1 container statuses recorded)
Dec 30 04:40:50.925: INFO: 	Container kube-scheduler ready: true, restart count 1
Dec 30 04:40:50.925: INFO: nodelocaldns-n9jl8 from kube-system started at 2022-12-28 02:22:14 +0000 UTC (1 container statuses recorded)
Dec 30 04:40:50.925: INFO: 	Container node-cache ready: true, restart count 0
Dec 30 04:40:50.925: INFO: labelsupdate277218b6-bce1-424c-86d2-5c9dc0183a59 from projected-5148 started at 2022-12-30 04:40:44 +0000 UTC (1 container statuses recorded)
Dec 30 04:40:50.925: INFO: 	Container client-container ready: true, restart count 0
Dec 30 04:40:50.925: INFO: sonobuoy-systemd-logs-daemon-set-4da877126f61426e-qd9td from sonobuoy started at 2022-12-30 03:20:18 +0000 UTC (2 container statuses recorded)
Dec 30 04:40:50.925: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 30 04:40:50.925: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 30 04:40:50.925: INFO: 
Logging pods the apiserver thinks is on node k8s-mgmt02 before test
Dec 30 04:40:50.939: INFO: calico-node-gwlv6 from kube-system started at 2022-12-28 02:19:33 +0000 UTC (1 container statuses recorded)
Dec 30 04:40:50.939: INFO: 	Container calico-node ready: true, restart count 0
Dec 30 04:40:50.939: INFO: coredns-69dfc8446-74sw5 from kube-system started at 2022-12-30 04:17:48 +0000 UTC (1 container statuses recorded)
Dec 30 04:40:50.939: INFO: 	Container coredns ready: true, restart count 0
Dec 30 04:40:50.939: INFO: kube-apiserver-k8s-mgmt02 from kube-system started at 2022-12-28 02:16:10 +0000 UTC (1 container statuses recorded)
Dec 30 04:40:50.939: INFO: 	Container kube-apiserver ready: true, restart count 1
Dec 30 04:40:50.939: INFO: kube-controller-manager-k8s-mgmt02 from kube-system started at 2022-12-28 02:16:10 +0000 UTC (1 container statuses recorded)
Dec 30 04:40:50.939: INFO: 	Container kube-controller-manager ready: true, restart count 1
Dec 30 04:40:50.939: INFO: kube-proxy-5lvmg from kube-system started at 2022-12-28 02:18:08 +0000 UTC (1 container statuses recorded)
Dec 30 04:40:50.939: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 30 04:40:50.939: INFO: kube-scheduler-k8s-mgmt02 from kube-system started at 2022-12-28 02:16:10 +0000 UTC (1 container statuses recorded)
Dec 30 04:40:50.939: INFO: 	Container kube-scheduler ready: true, restart count 1
Dec 30 04:40:50.939: INFO: metrics-server-67f489ffcf-srn44 from kube-system started at 2022-12-28 02:22:51 +0000 UTC (1 container statuses recorded)
Dec 30 04:40:50.939: INFO: 	Container metrics-server ready: true, restart count 0
Dec 30 04:40:50.939: INFO: nodelocaldns-bwgsf from kube-system started at 2022-12-28 02:22:14 +0000 UTC (1 container statuses recorded)
Dec 30 04:40:50.939: INFO: 	Container node-cache ready: true, restart count 0
Dec 30 04:40:50.939: INFO: sonobuoy from sonobuoy started at 2022-12-30 03:20:11 +0000 UTC (1 container statuses recorded)
Dec 30 04:40:50.939: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec 30 04:40:50.939: INFO: sonobuoy-systemd-logs-daemon-set-4da877126f61426e-llwcr from sonobuoy started at 2022-12-30 03:20:18 +0000 UTC (2 container statuses recorded)
Dec 30 04:40:50.939: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 30 04:40:50.939: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 30 04:40:50.939: INFO: 
Logging pods the apiserver thinks is on node k8s-mgmt03 before test
Dec 30 04:40:50.952: INFO: calico-kube-controllers-d6484b75c-8gdnt from kube-system started at 2022-12-28 02:21:06 +0000 UTC (1 container statuses recorded)
Dec 30 04:40:50.952: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Dec 30 04:40:50.952: INFO: calico-node-lqstj from kube-system started at 2022-12-28 02:19:33 +0000 UTC (1 container statuses recorded)
Dec 30 04:40:50.952: INFO: 	Container calico-node ready: true, restart count 0
Dec 30 04:40:50.952: INFO: coredns-69dfc8446-6gk67 from kube-system started at 2022-12-28 02:22:13 +0000 UTC (1 container statuses recorded)
Dec 30 04:40:50.952: INFO: 	Container coredns ready: true, restart count 0
Dec 30 04:40:50.952: INFO: dns-autoscaler-5b9959d7fc-x5csc from kube-system started at 2022-12-30 04:17:48 +0000 UTC (1 container statuses recorded)
Dec 30 04:40:50.952: INFO: 	Container autoscaler ready: true, restart count 0
Dec 30 04:40:50.952: INFO: kube-apiserver-k8s-mgmt03 from kube-system started at 2022-12-28 02:16:20 +0000 UTC (1 container statuses recorded)
Dec 30 04:40:50.952: INFO: 	Container kube-apiserver ready: true, restart count 1
Dec 30 04:40:50.952: INFO: kube-controller-manager-k8s-mgmt03 from kube-system started at 2022-12-28 02:16:20 +0000 UTC (1 container statuses recorded)
Dec 30 04:40:50.952: INFO: 	Container kube-controller-manager ready: true, restart count 1
Dec 30 04:40:50.952: INFO: kube-proxy-rfm26 from kube-system started at 2022-12-28 02:18:08 +0000 UTC (1 container statuses recorded)
Dec 30 04:40:50.952: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 30 04:40:50.952: INFO: kube-scheduler-k8s-mgmt03 from kube-system started at 2022-12-28 02:17:11 +0000 UTC (1 container statuses recorded)
Dec 30 04:40:50.952: INFO: 	Container kube-scheduler ready: true, restart count 1
Dec 30 04:40:50.952: INFO: nodelocaldns-76d47 from kube-system started at 2022-12-28 02:22:14 +0000 UTC (1 container statuses recorded)
Dec 30 04:40:50.952: INFO: 	Container node-cache ready: true, restart count 0
Dec 30 04:40:50.952: INFO: sonobuoy-e2e-job-c5efb7b0ce524286 from sonobuoy started at 2022-12-30 03:20:18 +0000 UTC (2 container statuses recorded)
Dec 30 04:40:50.952: INFO: 	Container e2e ready: true, restart count 0
Dec 30 04:40:50.952: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 30 04:40:50.952: INFO: sonobuoy-systemd-logs-daemon-set-4da877126f61426e-wfvd8 from sonobuoy started at 2022-12-30 03:20:18 +0000 UTC (2 container statuses recorded)
Dec 30 04:40:50.952: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 30 04:40:50.952: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 30 04:40:50.952: INFO: 
Logging pods the apiserver thinks is on node k8s-worker01 before test
Dec 30 04:40:50.965: INFO: calico-node-dtvd6 from kube-system started at 2022-12-28 02:19:33 +0000 UTC (1 container statuses recorded)
Dec 30 04:40:50.965: INFO: 	Container calico-node ready: true, restart count 0
Dec 30 04:40:50.965: INFO: kube-proxy-m86bn from kube-system started at 2022-12-28 02:18:09 +0000 UTC (1 container statuses recorded)
Dec 30 04:40:50.965: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 30 04:40:50.965: INFO: nginx-proxy-k8s-worker01 from kube-system started at 2022-12-28 02:17:47 +0000 UTC (1 container statuses recorded)
Dec 30 04:40:50.965: INFO: 	Container nginx-proxy ready: true, restart count 0
Dec 30 04:40:50.965: INFO: nodelocaldns-8zm2b from kube-system started at 2022-12-28 02:22:14 +0000 UTC (1 container statuses recorded)
Dec 30 04:40:50.965: INFO: 	Container node-cache ready: true, restart count 0
Dec 30 04:40:50.965: INFO: sonobuoy-systemd-logs-daemon-set-4da877126f61426e-4wpvd from sonobuoy started at 2022-12-30 03:20:18 +0000 UTC (2 container statuses recorded)
Dec 30 04:40:50.965: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 30 04:40:50.965: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 30 04:40:50.965: INFO: 
Logging pods the apiserver thinks is on node k8s-worker02 before test
Dec 30 04:40:50.977: INFO: calico-node-s7npn from kube-system started at 2022-12-28 02:19:33 +0000 UTC (1 container statuses recorded)
Dec 30 04:40:50.977: INFO: 	Container calico-node ready: true, restart count 0
Dec 30 04:40:50.977: INFO: kube-proxy-8cg4d from kube-system started at 2022-12-28 02:18:09 +0000 UTC (1 container statuses recorded)
Dec 30 04:40:50.977: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 30 04:40:50.977: INFO: nginx-proxy-k8s-worker02 from kube-system started at 2022-12-28 02:17:48 +0000 UTC (1 container statuses recorded)
Dec 30 04:40:50.977: INFO: 	Container nginx-proxy ready: true, restart count 0
Dec 30 04:40:50.977: INFO: nodelocaldns-4hlqv from kube-system started at 2022-12-28 02:22:14 +0000 UTC (1 container statuses recorded)
Dec 30 04:40:50.977: INFO: 	Container node-cache ready: true, restart count 0
Dec 30 04:40:50.977: INFO: sonobuoy-systemd-logs-daemon-set-4da877126f61426e-7mpss from sonobuoy started at 2022-12-30 03:20:18 +0000 UTC (2 container statuses recorded)
Dec 30 04:40:50.977: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 30 04:40:50.977: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461
STEP: Trying to launch a pod without a label to get a node which can launch it. 12/30/22 04:40:50.977
Dec 30 04:40:50.987: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-8058" to be "running"
Dec 30 04:40:50.991: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 4.088896ms
Dec 30 04:40:52.996: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.009188243s
Dec 30 04:40:52.996: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 12/30/22 04:40:53
STEP: Trying to apply a random label on the found node. 12/30/22 04:40:53.014
STEP: verifying the node has the label kubernetes.io/e2e-b9cebf68-f4eb-4549-a406-f14b1cefc27a 42 12/30/22 04:40:53.027
STEP: Trying to relaunch the pod, now with labels. 12/30/22 04:40:53.031
Dec 30 04:40:53.036: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-8058" to be "not pending"
Dec 30 04:40:53.040: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 3.824148ms
Dec 30 04:40:55.045: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.008597176s
Dec 30 04:40:55.045: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-b9cebf68-f4eb-4549-a406-f14b1cefc27a off the node k8s-mgmt01 12/30/22 04:40:55.049
STEP: verifying the node doesn't have the label kubernetes.io/e2e-b9cebf68-f4eb-4549-a406-f14b1cefc27a 12/30/22 04:40:55.066
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Dec 30 04:40:55.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8058" for this suite. 12/30/22 04:40:55.077
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","completed":298,"skipped":5492,"failed":0}
------------------------------
• [4.213 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:40:50.871
    Dec 30 04:40:50.871: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename sched-pred 12/30/22 04:40:50.873
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:40:50.889
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:40:50.893
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Dec 30 04:40:50.896: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Dec 30 04:40:50.907: INFO: Waiting for terminating namespaces to be deleted...
    Dec 30 04:40:50.911: INFO: 
    Logging pods the apiserver thinks is on node k8s-mgmt01 before test
    Dec 30 04:40:50.925: INFO: calico-node-6xm98 from kube-system started at 2022-12-28 02:19:33 +0000 UTC (1 container statuses recorded)
    Dec 30 04:40:50.925: INFO: 	Container calico-node ready: true, restart count 0
    Dec 30 04:40:50.925: INFO: kube-apiserver-k8s-mgmt01 from kube-system started at 2022-12-28 02:15:50 +0000 UTC (1 container statuses recorded)
    Dec 30 04:40:50.925: INFO: 	Container kube-apiserver ready: true, restart count 1
    Dec 30 04:40:50.925: INFO: kube-controller-manager-k8s-mgmt01 from kube-system started at 2022-12-28 02:15:49 +0000 UTC (1 container statuses recorded)
    Dec 30 04:40:50.925: INFO: 	Container kube-controller-manager ready: true, restart count 1
    Dec 30 04:40:50.925: INFO: kube-proxy-tcvcl from kube-system started at 2022-12-28 02:18:09 +0000 UTC (1 container statuses recorded)
    Dec 30 04:40:50.925: INFO: 	Container kube-proxy ready: true, restart count 0
    Dec 30 04:40:50.925: INFO: kube-scheduler-k8s-mgmt01 from kube-system started at 2022-12-28 02:15:38 +0000 UTC (1 container statuses recorded)
    Dec 30 04:40:50.925: INFO: 	Container kube-scheduler ready: true, restart count 1
    Dec 30 04:40:50.925: INFO: nodelocaldns-n9jl8 from kube-system started at 2022-12-28 02:22:14 +0000 UTC (1 container statuses recorded)
    Dec 30 04:40:50.925: INFO: 	Container node-cache ready: true, restart count 0
    Dec 30 04:40:50.925: INFO: labelsupdate277218b6-bce1-424c-86d2-5c9dc0183a59 from projected-5148 started at 2022-12-30 04:40:44 +0000 UTC (1 container statuses recorded)
    Dec 30 04:40:50.925: INFO: 	Container client-container ready: true, restart count 0
    Dec 30 04:40:50.925: INFO: sonobuoy-systemd-logs-daemon-set-4da877126f61426e-qd9td from sonobuoy started at 2022-12-30 03:20:18 +0000 UTC (2 container statuses recorded)
    Dec 30 04:40:50.925: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Dec 30 04:40:50.925: INFO: 	Container systemd-logs ready: true, restart count 0
    Dec 30 04:40:50.925: INFO: 
    Logging pods the apiserver thinks is on node k8s-mgmt02 before test
    Dec 30 04:40:50.939: INFO: calico-node-gwlv6 from kube-system started at 2022-12-28 02:19:33 +0000 UTC (1 container statuses recorded)
    Dec 30 04:40:50.939: INFO: 	Container calico-node ready: true, restart count 0
    Dec 30 04:40:50.939: INFO: coredns-69dfc8446-74sw5 from kube-system started at 2022-12-30 04:17:48 +0000 UTC (1 container statuses recorded)
    Dec 30 04:40:50.939: INFO: 	Container coredns ready: true, restart count 0
    Dec 30 04:40:50.939: INFO: kube-apiserver-k8s-mgmt02 from kube-system started at 2022-12-28 02:16:10 +0000 UTC (1 container statuses recorded)
    Dec 30 04:40:50.939: INFO: 	Container kube-apiserver ready: true, restart count 1
    Dec 30 04:40:50.939: INFO: kube-controller-manager-k8s-mgmt02 from kube-system started at 2022-12-28 02:16:10 +0000 UTC (1 container statuses recorded)
    Dec 30 04:40:50.939: INFO: 	Container kube-controller-manager ready: true, restart count 1
    Dec 30 04:40:50.939: INFO: kube-proxy-5lvmg from kube-system started at 2022-12-28 02:18:08 +0000 UTC (1 container statuses recorded)
    Dec 30 04:40:50.939: INFO: 	Container kube-proxy ready: true, restart count 0
    Dec 30 04:40:50.939: INFO: kube-scheduler-k8s-mgmt02 from kube-system started at 2022-12-28 02:16:10 +0000 UTC (1 container statuses recorded)
    Dec 30 04:40:50.939: INFO: 	Container kube-scheduler ready: true, restart count 1
    Dec 30 04:40:50.939: INFO: metrics-server-67f489ffcf-srn44 from kube-system started at 2022-12-28 02:22:51 +0000 UTC (1 container statuses recorded)
    Dec 30 04:40:50.939: INFO: 	Container metrics-server ready: true, restart count 0
    Dec 30 04:40:50.939: INFO: nodelocaldns-bwgsf from kube-system started at 2022-12-28 02:22:14 +0000 UTC (1 container statuses recorded)
    Dec 30 04:40:50.939: INFO: 	Container node-cache ready: true, restart count 0
    Dec 30 04:40:50.939: INFO: sonobuoy from sonobuoy started at 2022-12-30 03:20:11 +0000 UTC (1 container statuses recorded)
    Dec 30 04:40:50.939: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Dec 30 04:40:50.939: INFO: sonobuoy-systemd-logs-daemon-set-4da877126f61426e-llwcr from sonobuoy started at 2022-12-30 03:20:18 +0000 UTC (2 container statuses recorded)
    Dec 30 04:40:50.939: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Dec 30 04:40:50.939: INFO: 	Container systemd-logs ready: true, restart count 0
    Dec 30 04:40:50.939: INFO: 
    Logging pods the apiserver thinks is on node k8s-mgmt03 before test
    Dec 30 04:40:50.952: INFO: calico-kube-controllers-d6484b75c-8gdnt from kube-system started at 2022-12-28 02:21:06 +0000 UTC (1 container statuses recorded)
    Dec 30 04:40:50.952: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Dec 30 04:40:50.952: INFO: calico-node-lqstj from kube-system started at 2022-12-28 02:19:33 +0000 UTC (1 container statuses recorded)
    Dec 30 04:40:50.952: INFO: 	Container calico-node ready: true, restart count 0
    Dec 30 04:40:50.952: INFO: coredns-69dfc8446-6gk67 from kube-system started at 2022-12-28 02:22:13 +0000 UTC (1 container statuses recorded)
    Dec 30 04:40:50.952: INFO: 	Container coredns ready: true, restart count 0
    Dec 30 04:40:50.952: INFO: dns-autoscaler-5b9959d7fc-x5csc from kube-system started at 2022-12-30 04:17:48 +0000 UTC (1 container statuses recorded)
    Dec 30 04:40:50.952: INFO: 	Container autoscaler ready: true, restart count 0
    Dec 30 04:40:50.952: INFO: kube-apiserver-k8s-mgmt03 from kube-system started at 2022-12-28 02:16:20 +0000 UTC (1 container statuses recorded)
    Dec 30 04:40:50.952: INFO: 	Container kube-apiserver ready: true, restart count 1
    Dec 30 04:40:50.952: INFO: kube-controller-manager-k8s-mgmt03 from kube-system started at 2022-12-28 02:16:20 +0000 UTC (1 container statuses recorded)
    Dec 30 04:40:50.952: INFO: 	Container kube-controller-manager ready: true, restart count 1
    Dec 30 04:40:50.952: INFO: kube-proxy-rfm26 from kube-system started at 2022-12-28 02:18:08 +0000 UTC (1 container statuses recorded)
    Dec 30 04:40:50.952: INFO: 	Container kube-proxy ready: true, restart count 0
    Dec 30 04:40:50.952: INFO: kube-scheduler-k8s-mgmt03 from kube-system started at 2022-12-28 02:17:11 +0000 UTC (1 container statuses recorded)
    Dec 30 04:40:50.952: INFO: 	Container kube-scheduler ready: true, restart count 1
    Dec 30 04:40:50.952: INFO: nodelocaldns-76d47 from kube-system started at 2022-12-28 02:22:14 +0000 UTC (1 container statuses recorded)
    Dec 30 04:40:50.952: INFO: 	Container node-cache ready: true, restart count 0
    Dec 30 04:40:50.952: INFO: sonobuoy-e2e-job-c5efb7b0ce524286 from sonobuoy started at 2022-12-30 03:20:18 +0000 UTC (2 container statuses recorded)
    Dec 30 04:40:50.952: INFO: 	Container e2e ready: true, restart count 0
    Dec 30 04:40:50.952: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Dec 30 04:40:50.952: INFO: sonobuoy-systemd-logs-daemon-set-4da877126f61426e-wfvd8 from sonobuoy started at 2022-12-30 03:20:18 +0000 UTC (2 container statuses recorded)
    Dec 30 04:40:50.952: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Dec 30 04:40:50.952: INFO: 	Container systemd-logs ready: true, restart count 0
    Dec 30 04:40:50.952: INFO: 
    Logging pods the apiserver thinks is on node k8s-worker01 before test
    Dec 30 04:40:50.965: INFO: calico-node-dtvd6 from kube-system started at 2022-12-28 02:19:33 +0000 UTC (1 container statuses recorded)
    Dec 30 04:40:50.965: INFO: 	Container calico-node ready: true, restart count 0
    Dec 30 04:40:50.965: INFO: kube-proxy-m86bn from kube-system started at 2022-12-28 02:18:09 +0000 UTC (1 container statuses recorded)
    Dec 30 04:40:50.965: INFO: 	Container kube-proxy ready: true, restart count 0
    Dec 30 04:40:50.965: INFO: nginx-proxy-k8s-worker01 from kube-system started at 2022-12-28 02:17:47 +0000 UTC (1 container statuses recorded)
    Dec 30 04:40:50.965: INFO: 	Container nginx-proxy ready: true, restart count 0
    Dec 30 04:40:50.965: INFO: nodelocaldns-8zm2b from kube-system started at 2022-12-28 02:22:14 +0000 UTC (1 container statuses recorded)
    Dec 30 04:40:50.965: INFO: 	Container node-cache ready: true, restart count 0
    Dec 30 04:40:50.965: INFO: sonobuoy-systemd-logs-daemon-set-4da877126f61426e-4wpvd from sonobuoy started at 2022-12-30 03:20:18 +0000 UTC (2 container statuses recorded)
    Dec 30 04:40:50.965: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Dec 30 04:40:50.965: INFO: 	Container systemd-logs ready: true, restart count 0
    Dec 30 04:40:50.965: INFO: 
    Logging pods the apiserver thinks is on node k8s-worker02 before test
    Dec 30 04:40:50.977: INFO: calico-node-s7npn from kube-system started at 2022-12-28 02:19:33 +0000 UTC (1 container statuses recorded)
    Dec 30 04:40:50.977: INFO: 	Container calico-node ready: true, restart count 0
    Dec 30 04:40:50.977: INFO: kube-proxy-8cg4d from kube-system started at 2022-12-28 02:18:09 +0000 UTC (1 container statuses recorded)
    Dec 30 04:40:50.977: INFO: 	Container kube-proxy ready: true, restart count 0
    Dec 30 04:40:50.977: INFO: nginx-proxy-k8s-worker02 from kube-system started at 2022-12-28 02:17:48 +0000 UTC (1 container statuses recorded)
    Dec 30 04:40:50.977: INFO: 	Container nginx-proxy ready: true, restart count 0
    Dec 30 04:40:50.977: INFO: nodelocaldns-4hlqv from kube-system started at 2022-12-28 02:22:14 +0000 UTC (1 container statuses recorded)
    Dec 30 04:40:50.977: INFO: 	Container node-cache ready: true, restart count 0
    Dec 30 04:40:50.977: INFO: sonobuoy-systemd-logs-daemon-set-4da877126f61426e-7mpss from sonobuoy started at 2022-12-30 03:20:18 +0000 UTC (2 container statuses recorded)
    Dec 30 04:40:50.977: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Dec 30 04:40:50.977: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:461
    STEP: Trying to launch a pod without a label to get a node which can launch it. 12/30/22 04:40:50.977
    Dec 30 04:40:50.987: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-8058" to be "running"
    Dec 30 04:40:50.991: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 4.088896ms
    Dec 30 04:40:52.996: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.009188243s
    Dec 30 04:40:52.996: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 12/30/22 04:40:53
    STEP: Trying to apply a random label on the found node. 12/30/22 04:40:53.014
    STEP: verifying the node has the label kubernetes.io/e2e-b9cebf68-f4eb-4549-a406-f14b1cefc27a 42 12/30/22 04:40:53.027
    STEP: Trying to relaunch the pod, now with labels. 12/30/22 04:40:53.031
    Dec 30 04:40:53.036: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-8058" to be "not pending"
    Dec 30 04:40:53.040: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 3.824148ms
    Dec 30 04:40:55.045: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.008597176s
    Dec 30 04:40:55.045: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-b9cebf68-f4eb-4549-a406-f14b1cefc27a off the node k8s-mgmt01 12/30/22 04:40:55.049
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-b9cebf68-f4eb-4549-a406-f14b1cefc27a 12/30/22 04:40:55.066
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Dec 30 04:40:55.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-8058" for this suite. 12/30/22 04:40:55.077
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:40:55.089
Dec 30 04:40:55.089: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename sched-preemption 12/30/22 04:40:55.091
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:40:55.108
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:40:55.112
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Dec 30 04:40:55.130: INFO: Waiting up to 1m0s for all nodes to be ready
Dec 30 04:41:55.189: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125
STEP: Create pods that use 4/5 of node resources. 12/30/22 04:41:55.194
Dec 30 04:41:55.222: INFO: Created pod: pod0-0-sched-preemption-low-priority
Dec 30 04:41:55.227: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Dec 30 04:41:55.245: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Dec 30 04:41:55.251: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Dec 30 04:41:55.270: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Dec 30 04:41:55.276: INFO: Created pod: pod2-1-sched-preemption-medium-priority
Dec 30 04:41:55.294: INFO: Created pod: pod3-0-sched-preemption-medium-priority
Dec 30 04:41:55.299: INFO: Created pod: pod3-1-sched-preemption-medium-priority
Dec 30 04:41:55.316: INFO: Created pod: pod4-0-sched-preemption-medium-priority
Dec 30 04:41:55.321: INFO: Created pod: pod4-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 12/30/22 04:41:55.321
Dec 30 04:41:55.321: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-5871" to be "running"
Dec 30 04:41:55.324: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 3.06076ms
Dec 30 04:41:57.342: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020593136s
Dec 30 04:41:59.330: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.009055713s
Dec 30 04:41:59.331: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Dec 30 04:41:59.331: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-5871" to be "running"
Dec 30 04:41:59.335: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.314336ms
Dec 30 04:41:59.335: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Dec 30 04:41:59.335: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-5871" to be "running"
Dec 30 04:41:59.338: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 3.468906ms
Dec 30 04:42:01.348: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013176328s
Dec 30 04:42:03.344: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.009146115s
Dec 30 04:42:03.344: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Dec 30 04:42:03.344: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-5871" to be "running"
Dec 30 04:42:03.348: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.122977ms
Dec 30 04:42:03.348: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Dec 30 04:42:03.348: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-5871" to be "running"
Dec 30 04:42:03.352: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.052686ms
Dec 30 04:42:03.352: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Dec 30 04:42:03.352: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-5871" to be "running"
Dec 30 04:42:03.356: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.597875ms
Dec 30 04:42:03.356: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
Dec 30 04:42:03.356: INFO: Waiting up to 5m0s for pod "pod3-0-sched-preemption-medium-priority" in namespace "sched-preemption-5871" to be "running"
Dec 30 04:42:03.360: INFO: Pod "pod3-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.037096ms
Dec 30 04:42:05.365: INFO: Pod "pod3-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.009044998s
Dec 30 04:42:05.365: INFO: Pod "pod3-0-sched-preemption-medium-priority" satisfied condition "running"
Dec 30 04:42:05.365: INFO: Waiting up to 5m0s for pod "pod3-1-sched-preemption-medium-priority" in namespace "sched-preemption-5871" to be "running"
Dec 30 04:42:05.369: INFO: Pod "pod3-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.945717ms
Dec 30 04:42:05.369: INFO: Pod "pod3-1-sched-preemption-medium-priority" satisfied condition "running"
Dec 30 04:42:05.369: INFO: Waiting up to 5m0s for pod "pod4-0-sched-preemption-medium-priority" in namespace "sched-preemption-5871" to be "running"
Dec 30 04:42:05.374: INFO: Pod "pod4-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.306675ms
Dec 30 04:42:05.374: INFO: Pod "pod4-0-sched-preemption-medium-priority" satisfied condition "running"
Dec 30 04:42:05.374: INFO: Waiting up to 5m0s for pod "pod4-1-sched-preemption-medium-priority" in namespace "sched-preemption-5871" to be "running"
Dec 30 04:42:05.378: INFO: Pod "pod4-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.190863ms
Dec 30 04:42:05.378: INFO: Pod "pod4-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 12/30/22 04:42:05.378
Dec 30 04:42:05.384: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-5871" to be "running"
Dec 30 04:42:05.388: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.259775ms
Dec 30 04:42:07.399: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015277838s
Dec 30 04:42:09.394: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.009669376s
Dec 30 04:42:09.394: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Dec 30 04:42:09.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-5871" for this suite. 12/30/22 04:42:09.443
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","completed":299,"skipped":5571,"failed":0}
------------------------------
• [SLOW TEST] [74.440 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:40:55.089
    Dec 30 04:40:55.089: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename sched-preemption 12/30/22 04:40:55.091
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:40:55.108
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:40:55.112
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Dec 30 04:40:55.130: INFO: Waiting up to 1m0s for all nodes to be ready
    Dec 30 04:41:55.189: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:125
    STEP: Create pods that use 4/5 of node resources. 12/30/22 04:41:55.194
    Dec 30 04:41:55.222: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Dec 30 04:41:55.227: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Dec 30 04:41:55.245: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Dec 30 04:41:55.251: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Dec 30 04:41:55.270: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Dec 30 04:41:55.276: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    Dec 30 04:41:55.294: INFO: Created pod: pod3-0-sched-preemption-medium-priority
    Dec 30 04:41:55.299: INFO: Created pod: pod3-1-sched-preemption-medium-priority
    Dec 30 04:41:55.316: INFO: Created pod: pod4-0-sched-preemption-medium-priority
    Dec 30 04:41:55.321: INFO: Created pod: pod4-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 12/30/22 04:41:55.321
    Dec 30 04:41:55.321: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-5871" to be "running"
    Dec 30 04:41:55.324: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 3.06076ms
    Dec 30 04:41:57.342: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020593136s
    Dec 30 04:41:59.330: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.009055713s
    Dec 30 04:41:59.331: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Dec 30 04:41:59.331: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-5871" to be "running"
    Dec 30 04:41:59.335: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.314336ms
    Dec 30 04:41:59.335: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Dec 30 04:41:59.335: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-5871" to be "running"
    Dec 30 04:41:59.338: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 3.468906ms
    Dec 30 04:42:01.348: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013176328s
    Dec 30 04:42:03.344: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.009146115s
    Dec 30 04:42:03.344: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Dec 30 04:42:03.344: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-5871" to be "running"
    Dec 30 04:42:03.348: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.122977ms
    Dec 30 04:42:03.348: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Dec 30 04:42:03.348: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-5871" to be "running"
    Dec 30 04:42:03.352: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.052686ms
    Dec 30 04:42:03.352: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Dec 30 04:42:03.352: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-5871" to be "running"
    Dec 30 04:42:03.356: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.597875ms
    Dec 30 04:42:03.356: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    Dec 30 04:42:03.356: INFO: Waiting up to 5m0s for pod "pod3-0-sched-preemption-medium-priority" in namespace "sched-preemption-5871" to be "running"
    Dec 30 04:42:03.360: INFO: Pod "pod3-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.037096ms
    Dec 30 04:42:05.365: INFO: Pod "pod3-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.009044998s
    Dec 30 04:42:05.365: INFO: Pod "pod3-0-sched-preemption-medium-priority" satisfied condition "running"
    Dec 30 04:42:05.365: INFO: Waiting up to 5m0s for pod "pod3-1-sched-preemption-medium-priority" in namespace "sched-preemption-5871" to be "running"
    Dec 30 04:42:05.369: INFO: Pod "pod3-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.945717ms
    Dec 30 04:42:05.369: INFO: Pod "pod3-1-sched-preemption-medium-priority" satisfied condition "running"
    Dec 30 04:42:05.369: INFO: Waiting up to 5m0s for pod "pod4-0-sched-preemption-medium-priority" in namespace "sched-preemption-5871" to be "running"
    Dec 30 04:42:05.374: INFO: Pod "pod4-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.306675ms
    Dec 30 04:42:05.374: INFO: Pod "pod4-0-sched-preemption-medium-priority" satisfied condition "running"
    Dec 30 04:42:05.374: INFO: Waiting up to 5m0s for pod "pod4-1-sched-preemption-medium-priority" in namespace "sched-preemption-5871" to be "running"
    Dec 30 04:42:05.378: INFO: Pod "pod4-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.190863ms
    Dec 30 04:42:05.378: INFO: Pod "pod4-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 12/30/22 04:42:05.378
    Dec 30 04:42:05.384: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-5871" to be "running"
    Dec 30 04:42:05.388: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.259775ms
    Dec 30 04:42:07.399: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015277838s
    Dec 30 04:42:09.394: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.009669376s
    Dec 30 04:42:09.394: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Dec 30 04:42:09.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-5871" for this suite. 12/30/22 04:42:09.443
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:42:09.531
Dec 30 04:42:09.531: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename dns 12/30/22 04:42:09.532
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:42:09.551
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:42:09.554
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 12/30/22 04:42:09.558
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2235.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-2235.svc.cluster.local; sleep 1; done
 12/30/22 04:42:09.563
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2235.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-2235.svc.cluster.local; sleep 1; done
 12/30/22 04:42:09.563
STEP: creating a pod to probe DNS 12/30/22 04:42:09.563
STEP: submitting the pod to kubernetes 12/30/22 04:42:09.563
Dec 30 04:42:09.573: INFO: Waiting up to 15m0s for pod "dns-test-975891f8-96c6-4ccf-b0bc-25596c44e4db" in namespace "dns-2235" to be "running"
Dec 30 04:42:09.577: INFO: Pod "dns-test-975891f8-96c6-4ccf-b0bc-25596c44e4db": Phase="Pending", Reason="", readiness=false. Elapsed: 4.07557ms
Dec 30 04:42:11.582: INFO: Pod "dns-test-975891f8-96c6-4ccf-b0bc-25596c44e4db": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009395949s
Dec 30 04:42:13.584: INFO: Pod "dns-test-975891f8-96c6-4ccf-b0bc-25596c44e4db": Phase="Running", Reason="", readiness=true. Elapsed: 4.011318169s
Dec 30 04:42:13.584: INFO: Pod "dns-test-975891f8-96c6-4ccf-b0bc-25596c44e4db" satisfied condition "running"
STEP: retrieving the pod 12/30/22 04:42:13.584
STEP: looking for the results for each expected name from probers 12/30/22 04:42:13.589
Dec 30 04:42:13.600: INFO: DNS probes using dns-test-975891f8-96c6-4ccf-b0bc-25596c44e4db succeeded

STEP: deleting the pod 12/30/22 04:42:13.6
STEP: changing the externalName to bar.example.com 12/30/22 04:42:13.614
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2235.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-2235.svc.cluster.local; sleep 1; done
 12/30/22 04:42:13.623
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2235.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-2235.svc.cluster.local; sleep 1; done
 12/30/22 04:42:13.624
STEP: creating a second pod to probe DNS 12/30/22 04:42:13.624
STEP: submitting the pod to kubernetes 12/30/22 04:42:13.624
Dec 30 04:42:13.630: INFO: Waiting up to 15m0s for pod "dns-test-13bede14-12d2-4bf8-b4f6-ab6a5d39267c" in namespace "dns-2235" to be "running"
Dec 30 04:42:13.634: INFO: Pod "dns-test-13bede14-12d2-4bf8-b4f6-ab6a5d39267c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.134294ms
Dec 30 04:42:15.639: INFO: Pod "dns-test-13bede14-12d2-4bf8-b4f6-ab6a5d39267c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00884438s
Dec 30 04:42:17.639: INFO: Pod "dns-test-13bede14-12d2-4bf8-b4f6-ab6a5d39267c": Phase="Running", Reason="", readiness=true. Elapsed: 4.009445582s
Dec 30 04:42:17.640: INFO: Pod "dns-test-13bede14-12d2-4bf8-b4f6-ab6a5d39267c" satisfied condition "running"
STEP: retrieving the pod 12/30/22 04:42:17.64
STEP: looking for the results for each expected name from probers 12/30/22 04:42:17.644
Dec 30 04:42:17.654: INFO: DNS probes using dns-test-13bede14-12d2-4bf8-b4f6-ab6a5d39267c succeeded

STEP: deleting the pod 12/30/22 04:42:17.655
STEP: changing the service to type=ClusterIP 12/30/22 04:42:17.665
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2235.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-2235.svc.cluster.local; sleep 1; done
 12/30/22 04:42:17.679
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2235.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-2235.svc.cluster.local; sleep 1; done
 12/30/22 04:42:17.679
STEP: creating a third pod to probe DNS 12/30/22 04:42:17.679
STEP: submitting the pod to kubernetes 12/30/22 04:42:17.683
Dec 30 04:42:17.690: INFO: Waiting up to 15m0s for pod "dns-test-fb4fdf5f-bd68-46fb-8d14-203056a85faa" in namespace "dns-2235" to be "running"
Dec 30 04:42:17.694: INFO: Pod "dns-test-fb4fdf5f-bd68-46fb-8d14-203056a85faa": Phase="Pending", Reason="", readiness=false. Elapsed: 3.979538ms
Dec 30 04:42:19.698: INFO: Pod "dns-test-fb4fdf5f-bd68-46fb-8d14-203056a85faa": Phase="Running", Reason="", readiness=true. Elapsed: 2.008593761s
Dec 30 04:42:19.698: INFO: Pod "dns-test-fb4fdf5f-bd68-46fb-8d14-203056a85faa" satisfied condition "running"
STEP: retrieving the pod 12/30/22 04:42:19.698
STEP: looking for the results for each expected name from probers 12/30/22 04:42:19.703
Dec 30 04:42:19.713: INFO: DNS probes using dns-test-fb4fdf5f-bd68-46fb-8d14-203056a85faa succeeded

STEP: deleting the pod 12/30/22 04:42:19.713
STEP: deleting the test externalName service 12/30/22 04:42:19.724
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Dec 30 04:42:19.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2235" for this suite. 12/30/22 04:42:19.745
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","completed":300,"skipped":5586,"failed":0}
------------------------------
• [SLOW TEST] [10.221 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:42:09.531
    Dec 30 04:42:09.531: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename dns 12/30/22 04:42:09.532
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:42:09.551
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:42:09.554
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 12/30/22 04:42:09.558
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2235.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-2235.svc.cluster.local; sleep 1; done
     12/30/22 04:42:09.563
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2235.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-2235.svc.cluster.local; sleep 1; done
     12/30/22 04:42:09.563
    STEP: creating a pod to probe DNS 12/30/22 04:42:09.563
    STEP: submitting the pod to kubernetes 12/30/22 04:42:09.563
    Dec 30 04:42:09.573: INFO: Waiting up to 15m0s for pod "dns-test-975891f8-96c6-4ccf-b0bc-25596c44e4db" in namespace "dns-2235" to be "running"
    Dec 30 04:42:09.577: INFO: Pod "dns-test-975891f8-96c6-4ccf-b0bc-25596c44e4db": Phase="Pending", Reason="", readiness=false. Elapsed: 4.07557ms
    Dec 30 04:42:11.582: INFO: Pod "dns-test-975891f8-96c6-4ccf-b0bc-25596c44e4db": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009395949s
    Dec 30 04:42:13.584: INFO: Pod "dns-test-975891f8-96c6-4ccf-b0bc-25596c44e4db": Phase="Running", Reason="", readiness=true. Elapsed: 4.011318169s
    Dec 30 04:42:13.584: INFO: Pod "dns-test-975891f8-96c6-4ccf-b0bc-25596c44e4db" satisfied condition "running"
    STEP: retrieving the pod 12/30/22 04:42:13.584
    STEP: looking for the results for each expected name from probers 12/30/22 04:42:13.589
    Dec 30 04:42:13.600: INFO: DNS probes using dns-test-975891f8-96c6-4ccf-b0bc-25596c44e4db succeeded

    STEP: deleting the pod 12/30/22 04:42:13.6
    STEP: changing the externalName to bar.example.com 12/30/22 04:42:13.614
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2235.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-2235.svc.cluster.local; sleep 1; done
     12/30/22 04:42:13.623
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2235.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-2235.svc.cluster.local; sleep 1; done
     12/30/22 04:42:13.624
    STEP: creating a second pod to probe DNS 12/30/22 04:42:13.624
    STEP: submitting the pod to kubernetes 12/30/22 04:42:13.624
    Dec 30 04:42:13.630: INFO: Waiting up to 15m0s for pod "dns-test-13bede14-12d2-4bf8-b4f6-ab6a5d39267c" in namespace "dns-2235" to be "running"
    Dec 30 04:42:13.634: INFO: Pod "dns-test-13bede14-12d2-4bf8-b4f6-ab6a5d39267c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.134294ms
    Dec 30 04:42:15.639: INFO: Pod "dns-test-13bede14-12d2-4bf8-b4f6-ab6a5d39267c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00884438s
    Dec 30 04:42:17.639: INFO: Pod "dns-test-13bede14-12d2-4bf8-b4f6-ab6a5d39267c": Phase="Running", Reason="", readiness=true. Elapsed: 4.009445582s
    Dec 30 04:42:17.640: INFO: Pod "dns-test-13bede14-12d2-4bf8-b4f6-ab6a5d39267c" satisfied condition "running"
    STEP: retrieving the pod 12/30/22 04:42:17.64
    STEP: looking for the results for each expected name from probers 12/30/22 04:42:17.644
    Dec 30 04:42:17.654: INFO: DNS probes using dns-test-13bede14-12d2-4bf8-b4f6-ab6a5d39267c succeeded

    STEP: deleting the pod 12/30/22 04:42:17.655
    STEP: changing the service to type=ClusterIP 12/30/22 04:42:17.665
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2235.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-2235.svc.cluster.local; sleep 1; done
     12/30/22 04:42:17.679
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2235.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-2235.svc.cluster.local; sleep 1; done
     12/30/22 04:42:17.679
    STEP: creating a third pod to probe DNS 12/30/22 04:42:17.679
    STEP: submitting the pod to kubernetes 12/30/22 04:42:17.683
    Dec 30 04:42:17.690: INFO: Waiting up to 15m0s for pod "dns-test-fb4fdf5f-bd68-46fb-8d14-203056a85faa" in namespace "dns-2235" to be "running"
    Dec 30 04:42:17.694: INFO: Pod "dns-test-fb4fdf5f-bd68-46fb-8d14-203056a85faa": Phase="Pending", Reason="", readiness=false. Elapsed: 3.979538ms
    Dec 30 04:42:19.698: INFO: Pod "dns-test-fb4fdf5f-bd68-46fb-8d14-203056a85faa": Phase="Running", Reason="", readiness=true. Elapsed: 2.008593761s
    Dec 30 04:42:19.698: INFO: Pod "dns-test-fb4fdf5f-bd68-46fb-8d14-203056a85faa" satisfied condition "running"
    STEP: retrieving the pod 12/30/22 04:42:19.698
    STEP: looking for the results for each expected name from probers 12/30/22 04:42:19.703
    Dec 30 04:42:19.713: INFO: DNS probes using dns-test-fb4fdf5f-bd68-46fb-8d14-203056a85faa succeeded

    STEP: deleting the pod 12/30/22 04:42:19.713
    STEP: deleting the test externalName service 12/30/22 04:42:19.724
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Dec 30 04:42:19.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-2235" for this suite. 12/30/22 04:42:19.745
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:42:19.752
Dec 30 04:42:19.753: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename pods 12/30/22 04:42:19.754
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:42:19.77
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:42:19.774
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203
STEP: creating pod 12/30/22 04:42:19.777
Dec 30 04:42:19.786: INFO: Waiting up to 5m0s for pod "pod-hostip-2bc2cccd-b0f8-42bc-9c0d-ba025b188afe" in namespace "pods-1326" to be "running and ready"
Dec 30 04:42:19.790: INFO: Pod "pod-hostip-2bc2cccd-b0f8-42bc-9c0d-ba025b188afe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.052766ms
Dec 30 04:42:19.790: INFO: The phase of Pod pod-hostip-2bc2cccd-b0f8-42bc-9c0d-ba025b188afe is Pending, waiting for it to be Running (with Ready = true)
Dec 30 04:42:21.795: INFO: Pod "pod-hostip-2bc2cccd-b0f8-42bc-9c0d-ba025b188afe": Phase="Running", Reason="", readiness=true. Elapsed: 2.009493006s
Dec 30 04:42:21.795: INFO: The phase of Pod pod-hostip-2bc2cccd-b0f8-42bc-9c0d-ba025b188afe is Running (Ready = true)
Dec 30 04:42:21.795: INFO: Pod "pod-hostip-2bc2cccd-b0f8-42bc-9c0d-ba025b188afe" satisfied condition "running and ready"
Dec 30 04:42:21.804: INFO: Pod pod-hostip-2bc2cccd-b0f8-42bc-9c0d-ba025b188afe has hostIP: 10.78.26.140
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Dec 30 04:42:21.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1326" for this suite. 12/30/22 04:42:21.809
{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","completed":301,"skipped":5586,"failed":0}
------------------------------
• [2.064 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:42:19.752
    Dec 30 04:42:19.753: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename pods 12/30/22 04:42:19.754
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:42:19.77
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:42:19.774
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:203
    STEP: creating pod 12/30/22 04:42:19.777
    Dec 30 04:42:19.786: INFO: Waiting up to 5m0s for pod "pod-hostip-2bc2cccd-b0f8-42bc-9c0d-ba025b188afe" in namespace "pods-1326" to be "running and ready"
    Dec 30 04:42:19.790: INFO: Pod "pod-hostip-2bc2cccd-b0f8-42bc-9c0d-ba025b188afe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.052766ms
    Dec 30 04:42:19.790: INFO: The phase of Pod pod-hostip-2bc2cccd-b0f8-42bc-9c0d-ba025b188afe is Pending, waiting for it to be Running (with Ready = true)
    Dec 30 04:42:21.795: INFO: Pod "pod-hostip-2bc2cccd-b0f8-42bc-9c0d-ba025b188afe": Phase="Running", Reason="", readiness=true. Elapsed: 2.009493006s
    Dec 30 04:42:21.795: INFO: The phase of Pod pod-hostip-2bc2cccd-b0f8-42bc-9c0d-ba025b188afe is Running (Ready = true)
    Dec 30 04:42:21.795: INFO: Pod "pod-hostip-2bc2cccd-b0f8-42bc-9c0d-ba025b188afe" satisfied condition "running and ready"
    Dec 30 04:42:21.804: INFO: Pod pod-hostip-2bc2cccd-b0f8-42bc-9c0d-ba025b188afe has hostIP: 10.78.26.140
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Dec 30 04:42:21.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-1326" for this suite. 12/30/22 04:42:21.809
  << End Captured GinkgoWriter Output
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:42:21.817
Dec 30 04:42:21.817: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename certificates 12/30/22 04:42:21.818
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:42:21.834
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:42:21.838
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 12/30/22 04:42:22.658
STEP: getting /apis/certificates.k8s.io 12/30/22 04:42:22.661
STEP: getting /apis/certificates.k8s.io/v1 12/30/22 04:42:22.662
STEP: creating 12/30/22 04:42:22.664
STEP: getting 12/30/22 04:42:22.681
STEP: listing 12/30/22 04:42:22.685
STEP: watching 12/30/22 04:42:22.689
Dec 30 04:42:22.689: INFO: starting watch
STEP: patching 12/30/22 04:42:22.69
STEP: updating 12/30/22 04:42:22.697
Dec 30 04:42:22.703: INFO: waiting for watch events with expected annotations
Dec 30 04:42:22.703: INFO: saw patched and updated annotations
STEP: getting /approval 12/30/22 04:42:22.703
STEP: patching /approval 12/30/22 04:42:22.707
STEP: updating /approval 12/30/22 04:42:22.714
STEP: getting /status 12/30/22 04:42:22.72
STEP: patching /status 12/30/22 04:42:22.723
STEP: updating /status 12/30/22 04:42:22.732
STEP: deleting 12/30/22 04:42:22.74
STEP: deleting a collection 12/30/22 04:42:22.754
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Dec 30 04:42:22.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-1357" for this suite. 12/30/22 04:42:22.776
{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","completed":302,"skipped":5586,"failed":0}
------------------------------
• [0.967 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:42:21.817
    Dec 30 04:42:21.817: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename certificates 12/30/22 04:42:21.818
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:42:21.834
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:42:21.838
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 12/30/22 04:42:22.658
    STEP: getting /apis/certificates.k8s.io 12/30/22 04:42:22.661
    STEP: getting /apis/certificates.k8s.io/v1 12/30/22 04:42:22.662
    STEP: creating 12/30/22 04:42:22.664
    STEP: getting 12/30/22 04:42:22.681
    STEP: listing 12/30/22 04:42:22.685
    STEP: watching 12/30/22 04:42:22.689
    Dec 30 04:42:22.689: INFO: starting watch
    STEP: patching 12/30/22 04:42:22.69
    STEP: updating 12/30/22 04:42:22.697
    Dec 30 04:42:22.703: INFO: waiting for watch events with expected annotations
    Dec 30 04:42:22.703: INFO: saw patched and updated annotations
    STEP: getting /approval 12/30/22 04:42:22.703
    STEP: patching /approval 12/30/22 04:42:22.707
    STEP: updating /approval 12/30/22 04:42:22.714
    STEP: getting /status 12/30/22 04:42:22.72
    STEP: patching /status 12/30/22 04:42:22.723
    STEP: updating /status 12/30/22 04:42:22.732
    STEP: deleting 12/30/22 04:42:22.74
    STEP: deleting a collection 12/30/22 04:42:22.754
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Dec 30 04:42:22.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "certificates-1357" for this suite. 12/30/22 04:42:22.776
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:42:22.784
Dec 30 04:42:22.784: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename custom-resource-definition 12/30/22 04:42:22.785
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:42:22.802
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:42:22.806
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
Dec 30 04:42:22.809: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Dec 30 04:43:24.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7454" for this suite. 12/30/22 04:43:24.099
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","completed":303,"skipped":5587,"failed":0}
------------------------------
• [SLOW TEST] [61.322 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:42:22.784
    Dec 30 04:42:22.784: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename custom-resource-definition 12/30/22 04:42:22.785
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:42:22.802
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:42:22.806
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    Dec 30 04:42:22.809: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Dec 30 04:43:24.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-7454" for this suite. 12/30/22 04:43:24.099
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:43:24.108
Dec 30 04:43:24.108: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename services 12/30/22 04:43:24.11
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:43:24.127
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:43:24.131
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157
STEP: creating service in namespace services-2375 12/30/22 04:43:24.134
STEP: creating service affinity-clusterip in namespace services-2375 12/30/22 04:43:24.135
STEP: creating replication controller affinity-clusterip in namespace services-2375 12/30/22 04:43:24.145
I1230 04:43:24.151605      25 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-2375, replica count: 3
I1230 04:43:27.203529      25 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 30 04:43:27.211: INFO: Creating new exec pod
Dec 30 04:43:27.220: INFO: Waiting up to 5m0s for pod "execpod-affinityvgxl8" in namespace "services-2375" to be "running"
Dec 30 04:43:27.223: INFO: Pod "execpod-affinityvgxl8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.185647ms
Dec 30 04:43:29.230: INFO: Pod "execpod-affinityvgxl8": Phase="Running", Reason="", readiness=true. Elapsed: 2.009342429s
Dec 30 04:43:29.230: INFO: Pod "execpod-affinityvgxl8" satisfied condition "running"
Dec 30 04:43:30.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-2375 exec execpod-affinityvgxl8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
Dec 30 04:43:30.473: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Dec 30 04:43:30.473: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 30 04:43:30.473: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-2375 exec execpod-affinityvgxl8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.28.65 80'
Dec 30 04:43:30.658: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.28.65 80\nConnection to 10.233.28.65 80 port [tcp/http] succeeded!\n"
Dec 30 04:43:30.658: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 30 04:43:30.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-2375 exec execpod-affinityvgxl8 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.28.65:80/ ; done'
Dec 30 04:43:30.970: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.28.65:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.28.65:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.28.65:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.28.65:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.28.65:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.28.65:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.28.65:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.28.65:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.28.65:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.28.65:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.28.65:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.28.65:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.28.65:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.28.65:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.28.65:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.28.65:80/\n"
Dec 30 04:43:30.970: INFO: stdout: "\naffinity-clusterip-shnf8\naffinity-clusterip-shnf8\naffinity-clusterip-shnf8\naffinity-clusterip-shnf8\naffinity-clusterip-shnf8\naffinity-clusterip-shnf8\naffinity-clusterip-shnf8\naffinity-clusterip-shnf8\naffinity-clusterip-shnf8\naffinity-clusterip-shnf8\naffinity-clusterip-shnf8\naffinity-clusterip-shnf8\naffinity-clusterip-shnf8\naffinity-clusterip-shnf8\naffinity-clusterip-shnf8\naffinity-clusterip-shnf8"
Dec 30 04:43:30.970: INFO: Received response from host: affinity-clusterip-shnf8
Dec 30 04:43:30.970: INFO: Received response from host: affinity-clusterip-shnf8
Dec 30 04:43:30.970: INFO: Received response from host: affinity-clusterip-shnf8
Dec 30 04:43:30.970: INFO: Received response from host: affinity-clusterip-shnf8
Dec 30 04:43:30.970: INFO: Received response from host: affinity-clusterip-shnf8
Dec 30 04:43:30.970: INFO: Received response from host: affinity-clusterip-shnf8
Dec 30 04:43:30.970: INFO: Received response from host: affinity-clusterip-shnf8
Dec 30 04:43:30.970: INFO: Received response from host: affinity-clusterip-shnf8
Dec 30 04:43:30.970: INFO: Received response from host: affinity-clusterip-shnf8
Dec 30 04:43:30.970: INFO: Received response from host: affinity-clusterip-shnf8
Dec 30 04:43:30.970: INFO: Received response from host: affinity-clusterip-shnf8
Dec 30 04:43:30.970: INFO: Received response from host: affinity-clusterip-shnf8
Dec 30 04:43:30.970: INFO: Received response from host: affinity-clusterip-shnf8
Dec 30 04:43:30.970: INFO: Received response from host: affinity-clusterip-shnf8
Dec 30 04:43:30.970: INFO: Received response from host: affinity-clusterip-shnf8
Dec 30 04:43:30.970: INFO: Received response from host: affinity-clusterip-shnf8
Dec 30 04:43:30.970: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-2375, will wait for the garbage collector to delete the pods 12/30/22 04:43:30.986
Dec 30 04:43:31.048: INFO: Deleting ReplicationController affinity-clusterip took: 7.918691ms
Dec 30 04:43:31.149: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.62226ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Dec 30 04:43:32.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2375" for this suite. 12/30/22 04:43:32.973
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","completed":304,"skipped":5613,"failed":0}
------------------------------
• [SLOW TEST] [8.872 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:43:24.108
    Dec 30 04:43:24.108: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename services 12/30/22 04:43:24.11
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:43:24.127
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:43:24.131
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2157
    STEP: creating service in namespace services-2375 12/30/22 04:43:24.134
    STEP: creating service affinity-clusterip in namespace services-2375 12/30/22 04:43:24.135
    STEP: creating replication controller affinity-clusterip in namespace services-2375 12/30/22 04:43:24.145
    I1230 04:43:24.151605      25 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-2375, replica count: 3
    I1230 04:43:27.203529      25 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Dec 30 04:43:27.211: INFO: Creating new exec pod
    Dec 30 04:43:27.220: INFO: Waiting up to 5m0s for pod "execpod-affinityvgxl8" in namespace "services-2375" to be "running"
    Dec 30 04:43:27.223: INFO: Pod "execpod-affinityvgxl8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.185647ms
    Dec 30 04:43:29.230: INFO: Pod "execpod-affinityvgxl8": Phase="Running", Reason="", readiness=true. Elapsed: 2.009342429s
    Dec 30 04:43:29.230: INFO: Pod "execpod-affinityvgxl8" satisfied condition "running"
    Dec 30 04:43:30.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-2375 exec execpod-affinityvgxl8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
    Dec 30 04:43:30.473: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    Dec 30 04:43:30.473: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Dec 30 04:43:30.473: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-2375 exec execpod-affinityvgxl8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.28.65 80'
    Dec 30 04:43:30.658: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.28.65 80\nConnection to 10.233.28.65 80 port [tcp/http] succeeded!\n"
    Dec 30 04:43:30.658: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Dec 30 04:43:30.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-2375 exec execpod-affinityvgxl8 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.28.65:80/ ; done'
    Dec 30 04:43:30.970: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.28.65:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.28.65:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.28.65:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.28.65:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.28.65:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.28.65:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.28.65:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.28.65:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.28.65:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.28.65:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.28.65:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.28.65:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.28.65:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.28.65:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.28.65:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.28.65:80/\n"
    Dec 30 04:43:30.970: INFO: stdout: "\naffinity-clusterip-shnf8\naffinity-clusterip-shnf8\naffinity-clusterip-shnf8\naffinity-clusterip-shnf8\naffinity-clusterip-shnf8\naffinity-clusterip-shnf8\naffinity-clusterip-shnf8\naffinity-clusterip-shnf8\naffinity-clusterip-shnf8\naffinity-clusterip-shnf8\naffinity-clusterip-shnf8\naffinity-clusterip-shnf8\naffinity-clusterip-shnf8\naffinity-clusterip-shnf8\naffinity-clusterip-shnf8\naffinity-clusterip-shnf8"
    Dec 30 04:43:30.970: INFO: Received response from host: affinity-clusterip-shnf8
    Dec 30 04:43:30.970: INFO: Received response from host: affinity-clusterip-shnf8
    Dec 30 04:43:30.970: INFO: Received response from host: affinity-clusterip-shnf8
    Dec 30 04:43:30.970: INFO: Received response from host: affinity-clusterip-shnf8
    Dec 30 04:43:30.970: INFO: Received response from host: affinity-clusterip-shnf8
    Dec 30 04:43:30.970: INFO: Received response from host: affinity-clusterip-shnf8
    Dec 30 04:43:30.970: INFO: Received response from host: affinity-clusterip-shnf8
    Dec 30 04:43:30.970: INFO: Received response from host: affinity-clusterip-shnf8
    Dec 30 04:43:30.970: INFO: Received response from host: affinity-clusterip-shnf8
    Dec 30 04:43:30.970: INFO: Received response from host: affinity-clusterip-shnf8
    Dec 30 04:43:30.970: INFO: Received response from host: affinity-clusterip-shnf8
    Dec 30 04:43:30.970: INFO: Received response from host: affinity-clusterip-shnf8
    Dec 30 04:43:30.970: INFO: Received response from host: affinity-clusterip-shnf8
    Dec 30 04:43:30.970: INFO: Received response from host: affinity-clusterip-shnf8
    Dec 30 04:43:30.970: INFO: Received response from host: affinity-clusterip-shnf8
    Dec 30 04:43:30.970: INFO: Received response from host: affinity-clusterip-shnf8
    Dec 30 04:43:30.970: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-2375, will wait for the garbage collector to delete the pods 12/30/22 04:43:30.986
    Dec 30 04:43:31.048: INFO: Deleting ReplicationController affinity-clusterip took: 7.918691ms
    Dec 30 04:43:31.149: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.62226ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Dec 30 04:43:32.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-2375" for this suite. 12/30/22 04:43:32.973
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:43:32.983
Dec 30 04:43:32.983: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename sched-pred 12/30/22 04:43:32.984
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:43:33.002
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:43:33.005
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Dec 30 04:43:33.009: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 30 04:43:33.020: INFO: Waiting for terminating namespaces to be deleted...
Dec 30 04:43:33.024: INFO: 
Logging pods the apiserver thinks is on node k8s-mgmt01 before test
Dec 30 04:43:33.038: INFO: calico-node-6xm98 from kube-system started at 2022-12-28 02:19:33 +0000 UTC (1 container statuses recorded)
Dec 30 04:43:33.038: INFO: 	Container calico-node ready: true, restart count 0
Dec 30 04:43:33.038: INFO: kube-apiserver-k8s-mgmt01 from kube-system started at 2022-12-28 02:15:50 +0000 UTC (1 container statuses recorded)
Dec 30 04:43:33.038: INFO: 	Container kube-apiserver ready: true, restart count 1
Dec 30 04:43:33.038: INFO: kube-controller-manager-k8s-mgmt01 from kube-system started at 2022-12-28 02:15:49 +0000 UTC (1 container statuses recorded)
Dec 30 04:43:33.038: INFO: 	Container kube-controller-manager ready: true, restart count 1
Dec 30 04:43:33.038: INFO: kube-proxy-tcvcl from kube-system started at 2022-12-28 02:18:09 +0000 UTC (1 container statuses recorded)
Dec 30 04:43:33.038: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 30 04:43:33.038: INFO: kube-scheduler-k8s-mgmt01 from kube-system started at 2022-12-28 02:15:38 +0000 UTC (1 container statuses recorded)
Dec 30 04:43:33.038: INFO: 	Container kube-scheduler ready: true, restart count 1
Dec 30 04:43:33.038: INFO: nodelocaldns-n9jl8 from kube-system started at 2022-12-28 02:22:14 +0000 UTC (1 container statuses recorded)
Dec 30 04:43:33.038: INFO: 	Container node-cache ready: true, restart count 0
Dec 30 04:43:33.038: INFO: sonobuoy-systemd-logs-daemon-set-4da877126f61426e-qd9td from sonobuoy started at 2022-12-30 03:20:18 +0000 UTC (2 container statuses recorded)
Dec 30 04:43:33.038: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 30 04:43:33.038: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 30 04:43:33.038: INFO: 
Logging pods the apiserver thinks is on node k8s-mgmt02 before test
Dec 30 04:43:33.052: INFO: calico-node-gwlv6 from kube-system started at 2022-12-28 02:19:33 +0000 UTC (1 container statuses recorded)
Dec 30 04:43:33.052: INFO: 	Container calico-node ready: true, restart count 0
Dec 30 04:43:33.052: INFO: coredns-69dfc8446-74sw5 from kube-system started at 2022-12-30 04:17:48 +0000 UTC (1 container statuses recorded)
Dec 30 04:43:33.052: INFO: 	Container coredns ready: true, restart count 0
Dec 30 04:43:33.052: INFO: kube-apiserver-k8s-mgmt02 from kube-system started at 2022-12-28 02:16:10 +0000 UTC (1 container statuses recorded)
Dec 30 04:43:33.052: INFO: 	Container kube-apiserver ready: true, restart count 1
Dec 30 04:43:33.052: INFO: kube-controller-manager-k8s-mgmt02 from kube-system started at 2022-12-28 02:16:10 +0000 UTC (1 container statuses recorded)
Dec 30 04:43:33.052: INFO: 	Container kube-controller-manager ready: true, restart count 1
Dec 30 04:43:33.052: INFO: kube-proxy-5lvmg from kube-system started at 2022-12-28 02:18:08 +0000 UTC (1 container statuses recorded)
Dec 30 04:43:33.052: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 30 04:43:33.052: INFO: kube-scheduler-k8s-mgmt02 from kube-system started at 2022-12-28 02:16:10 +0000 UTC (1 container statuses recorded)
Dec 30 04:43:33.052: INFO: 	Container kube-scheduler ready: true, restart count 1
Dec 30 04:43:33.052: INFO: metrics-server-67f489ffcf-srn44 from kube-system started at 2022-12-28 02:22:51 +0000 UTC (1 container statuses recorded)
Dec 30 04:43:33.052: INFO: 	Container metrics-server ready: true, restart count 0
Dec 30 04:43:33.052: INFO: nodelocaldns-bwgsf from kube-system started at 2022-12-28 02:22:14 +0000 UTC (1 container statuses recorded)
Dec 30 04:43:33.052: INFO: 	Container node-cache ready: true, restart count 0
Dec 30 04:43:33.052: INFO: sonobuoy from sonobuoy started at 2022-12-30 03:20:11 +0000 UTC (1 container statuses recorded)
Dec 30 04:43:33.052: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec 30 04:43:33.052: INFO: sonobuoy-systemd-logs-daemon-set-4da877126f61426e-llwcr from sonobuoy started at 2022-12-30 03:20:18 +0000 UTC (2 container statuses recorded)
Dec 30 04:43:33.052: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 30 04:43:33.052: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 30 04:43:33.052: INFO: 
Logging pods the apiserver thinks is on node k8s-mgmt03 before test
Dec 30 04:43:33.066: INFO: calico-kube-controllers-d6484b75c-8gdnt from kube-system started at 2022-12-28 02:21:06 +0000 UTC (1 container statuses recorded)
Dec 30 04:43:33.066: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Dec 30 04:43:33.066: INFO: calico-node-lqstj from kube-system started at 2022-12-28 02:19:33 +0000 UTC (1 container statuses recorded)
Dec 30 04:43:33.066: INFO: 	Container calico-node ready: true, restart count 0
Dec 30 04:43:33.066: INFO: coredns-69dfc8446-6gk67 from kube-system started at 2022-12-28 02:22:13 +0000 UTC (1 container statuses recorded)
Dec 30 04:43:33.066: INFO: 	Container coredns ready: true, restart count 0
Dec 30 04:43:33.066: INFO: dns-autoscaler-5b9959d7fc-x5csc from kube-system started at 2022-12-30 04:17:48 +0000 UTC (1 container statuses recorded)
Dec 30 04:43:33.066: INFO: 	Container autoscaler ready: true, restart count 0
Dec 30 04:43:33.066: INFO: kube-apiserver-k8s-mgmt03 from kube-system started at 2022-12-28 02:16:20 +0000 UTC (1 container statuses recorded)
Dec 30 04:43:33.066: INFO: 	Container kube-apiserver ready: true, restart count 1
Dec 30 04:43:33.066: INFO: kube-controller-manager-k8s-mgmt03 from kube-system started at 2022-12-28 02:16:20 +0000 UTC (1 container statuses recorded)
Dec 30 04:43:33.066: INFO: 	Container kube-controller-manager ready: true, restart count 1
Dec 30 04:43:33.066: INFO: kube-proxy-rfm26 from kube-system started at 2022-12-28 02:18:08 +0000 UTC (1 container statuses recorded)
Dec 30 04:43:33.066: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 30 04:43:33.066: INFO: kube-scheduler-k8s-mgmt03 from kube-system started at 2022-12-28 02:17:11 +0000 UTC (1 container statuses recorded)
Dec 30 04:43:33.066: INFO: 	Container kube-scheduler ready: true, restart count 1
Dec 30 04:43:33.066: INFO: nodelocaldns-76d47 from kube-system started at 2022-12-28 02:22:14 +0000 UTC (1 container statuses recorded)
Dec 30 04:43:33.066: INFO: 	Container node-cache ready: true, restart count 0
Dec 30 04:43:33.066: INFO: sonobuoy-e2e-job-c5efb7b0ce524286 from sonobuoy started at 2022-12-30 03:20:18 +0000 UTC (2 container statuses recorded)
Dec 30 04:43:33.066: INFO: 	Container e2e ready: true, restart count 0
Dec 30 04:43:33.066: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 30 04:43:33.066: INFO: sonobuoy-systemd-logs-daemon-set-4da877126f61426e-wfvd8 from sonobuoy started at 2022-12-30 03:20:18 +0000 UTC (2 container statuses recorded)
Dec 30 04:43:33.066: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 30 04:43:33.066: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 30 04:43:33.066: INFO: 
Logging pods the apiserver thinks is on node k8s-worker01 before test
Dec 30 04:43:33.079: INFO: calico-node-dtvd6 from kube-system started at 2022-12-28 02:19:33 +0000 UTC (1 container statuses recorded)
Dec 30 04:43:33.079: INFO: 	Container calico-node ready: true, restart count 0
Dec 30 04:43:33.079: INFO: kube-proxy-m86bn from kube-system started at 2022-12-28 02:18:09 +0000 UTC (1 container statuses recorded)
Dec 30 04:43:33.079: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 30 04:43:33.079: INFO: nginx-proxy-k8s-worker01 from kube-system started at 2022-12-28 02:17:47 +0000 UTC (1 container statuses recorded)
Dec 30 04:43:33.079: INFO: 	Container nginx-proxy ready: true, restart count 0
Dec 30 04:43:33.080: INFO: nodelocaldns-8zm2b from kube-system started at 2022-12-28 02:22:14 +0000 UTC (1 container statuses recorded)
Dec 30 04:43:33.080: INFO: 	Container node-cache ready: true, restart count 0
Dec 30 04:43:33.080: INFO: sonobuoy-systemd-logs-daemon-set-4da877126f61426e-4wpvd from sonobuoy started at 2022-12-30 03:20:18 +0000 UTC (2 container statuses recorded)
Dec 30 04:43:33.080: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 30 04:43:33.080: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 30 04:43:33.080: INFO: 
Logging pods the apiserver thinks is on node k8s-worker02 before test
Dec 30 04:43:33.091: INFO: calico-node-s7npn from kube-system started at 2022-12-28 02:19:33 +0000 UTC (1 container statuses recorded)
Dec 30 04:43:33.091: INFO: 	Container calico-node ready: true, restart count 0
Dec 30 04:43:33.092: INFO: kube-proxy-8cg4d from kube-system started at 2022-12-28 02:18:09 +0000 UTC (1 container statuses recorded)
Dec 30 04:43:33.092: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 30 04:43:33.092: INFO: nginx-proxy-k8s-worker02 from kube-system started at 2022-12-28 02:17:48 +0000 UTC (1 container statuses recorded)
Dec 30 04:43:33.092: INFO: 	Container nginx-proxy ready: true, restart count 0
Dec 30 04:43:33.092: INFO: nodelocaldns-4hlqv from kube-system started at 2022-12-28 02:22:14 +0000 UTC (1 container statuses recorded)
Dec 30 04:43:33.092: INFO: 	Container node-cache ready: true, restart count 0
Dec 30 04:43:33.092: INFO: sonobuoy-systemd-logs-daemon-set-4da877126f61426e-7mpss from sonobuoy started at 2022-12-30 03:20:18 +0000 UTC (2 container statuses recorded)
Dec 30 04:43:33.092: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 30 04:43:33.092: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699
STEP: Trying to launch a pod without a label to get a node which can launch it. 12/30/22 04:43:33.092
Dec 30 04:43:33.101: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-2519" to be "running"
Dec 30 04:43:33.105: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 3.848748ms
Dec 30 04:43:35.111: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.009227844s
Dec 30 04:43:35.111: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 12/30/22 04:43:35.115
STEP: Trying to apply a random label on the found node. 12/30/22 04:43:35.13
STEP: verifying the node has the label kubernetes.io/e2e-d9b27c1e-1c0b-42b3-9741-d96f13f78734 95 12/30/22 04:43:35.143
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 12/30/22 04:43:35.148
Dec 30 04:43:35.153: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-2519" to be "not pending"
Dec 30 04:43:35.157: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.065937ms
Dec 30 04:43:37.163: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.010086977s
Dec 30 04:43:37.163: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.78.26.140 on the node which pod4 resides and expect not scheduled 12/30/22 04:43:37.163
Dec 30 04:43:37.171: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-2519" to be "not pending"
Dec 30 04:43:37.174: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.30869ms
Dec 30 04:43:39.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008916988s
Dec 30 04:43:41.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009242752s
Dec 30 04:43:43.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.008090119s
Dec 30 04:43:45.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.008955365s
Dec 30 04:43:47.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.009142234s
Dec 30 04:43:49.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.009223802s
Dec 30 04:43:51.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.008408708s
Dec 30 04:43:53.182: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.011164299s
Dec 30 04:43:55.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.009227708s
Dec 30 04:43:57.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.009086251s
Dec 30 04:43:59.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.009586632s
Dec 30 04:44:01.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.00838009s
Dec 30 04:44:03.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.009333027s
Dec 30 04:44:05.181: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.010471706s
Dec 30 04:44:07.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.008637401s
Dec 30 04:44:09.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.008353826s
Dec 30 04:44:11.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.008209891s
Dec 30 04:44:13.181: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.009835273s
Dec 30 04:44:15.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.008690243s
Dec 30 04:44:17.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.009706517s
Dec 30 04:44:19.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.009105723s
Dec 30 04:44:21.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.008508173s
Dec 30 04:44:23.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.00917684s
Dec 30 04:44:25.178: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.007629816s
Dec 30 04:44:27.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.009386531s
Dec 30 04:44:29.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.009251428s
Dec 30 04:44:31.178: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.007499159s
Dec 30 04:44:33.181: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.01042802s
Dec 30 04:44:35.181: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.0101319s
Dec 30 04:44:37.178: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.007644375s
Dec 30 04:44:39.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.008926707s
Dec 30 04:44:41.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.009580761s
Dec 30 04:44:43.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.008700444s
Dec 30 04:44:45.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.008106993s
Dec 30 04:44:47.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.008333479s
Dec 30 04:44:49.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.009341506s
Dec 30 04:44:51.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.008861671s
Dec 30 04:44:53.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.009733288s
Dec 30 04:44:55.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.008823772s
Dec 30 04:44:57.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.008718404s
Dec 30 04:44:59.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.008772686s
Dec 30 04:45:01.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.008271645s
Dec 30 04:45:03.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.009171128s
Dec 30 04:45:05.181: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.010092654s
Dec 30 04:45:07.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.007910044s
Dec 30 04:45:09.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.009180222s
Dec 30 04:45:11.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.008021522s
Dec 30 04:45:13.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.009277458s
Dec 30 04:45:15.181: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.009890501s
Dec 30 04:45:17.181: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.00976191s
Dec 30 04:45:19.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.008705544s
Dec 30 04:45:21.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.00862265s
Dec 30 04:45:23.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.009311888s
Dec 30 04:45:25.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.009075761s
Dec 30 04:45:27.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.008518703s
Dec 30 04:45:29.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.009532269s
Dec 30 04:45:31.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.008044223s
Dec 30 04:45:33.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.00974431s
Dec 30 04:45:35.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.009208572s
Dec 30 04:45:37.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.008245537s
Dec 30 04:45:39.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.009326712s
Dec 30 04:45:41.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.009104743s
Dec 30 04:45:43.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.008896886s
Dec 30 04:45:45.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.009385311s
Dec 30 04:45:47.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.008650637s
Dec 30 04:45:49.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.008695s
Dec 30 04:45:51.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.00897896s
Dec 30 04:45:53.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.009106622s
Dec 30 04:45:55.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.008921544s
Dec 30 04:45:57.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.008492175s
Dec 30 04:45:59.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.007774198s
Dec 30 04:46:01.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.008727595s
Dec 30 04:46:03.181: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.010170336s
Dec 30 04:46:05.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.009012246s
Dec 30 04:46:07.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.008558741s
Dec 30 04:46:09.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.009612711s
Dec 30 04:46:11.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.008132853s
Dec 30 04:46:13.181: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.009772813s
Dec 30 04:46:15.181: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.009856651s
Dec 30 04:46:17.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.008428988s
Dec 30 04:46:19.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.009721s
Dec 30 04:46:21.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.008262669s
Dec 30 04:46:23.181: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.009913504s
Dec 30 04:46:25.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.009201006s
Dec 30 04:46:27.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.009516537s
Dec 30 04:46:29.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.009191146s
Dec 30 04:46:31.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.008201055s
Dec 30 04:46:33.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.009299192s
Dec 30 04:46:35.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.0097436s
Dec 30 04:46:37.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.008946556s
Dec 30 04:46:39.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.008085561s
Dec 30 04:46:41.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.008974379s
Dec 30 04:46:43.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.009592942s
Dec 30 04:46:45.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.008658181s
Dec 30 04:46:47.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.009038751s
Dec 30 04:46:49.181: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.009821707s
Dec 30 04:46:51.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.008256727s
Dec 30 04:46:53.181: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.010055376s
Dec 30 04:46:55.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.009474576s
Dec 30 04:46:57.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.008276097s
Dec 30 04:46:59.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.008683974s
Dec 30 04:47:01.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.009581388s
Dec 30 04:47:03.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.008752825s
Dec 30 04:47:05.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.00906823s
Dec 30 04:47:07.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.008812982s
Dec 30 04:47:09.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.008249278s
Dec 30 04:47:11.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.00832503s
Dec 30 04:47:13.181: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.010500825s
Dec 30 04:47:15.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.008527059s
Dec 30 04:47:17.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.008907015s
Dec 30 04:47:19.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.0093415s
Dec 30 04:47:21.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.007831942s
Dec 30 04:47:23.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.009564646s
Dec 30 04:47:25.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.009513493s
Dec 30 04:47:27.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.00827733s
Dec 30 04:47:29.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.008275741s
Dec 30 04:47:31.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.008108617s
Dec 30 04:47:33.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.009142756s
Dec 30 04:47:35.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.008628171s
Dec 30 04:47:37.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.008874527s
Dec 30 04:47:39.178: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.007662076s
Dec 30 04:47:41.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.008362007s
Dec 30 04:47:43.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.009143609s
Dec 30 04:47:45.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.009642371s
Dec 30 04:47:47.181: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.009853951s
Dec 30 04:47:49.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.009007515s
Dec 30 04:47:51.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.008769205s
Dec 30 04:47:53.181: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.009907262s
Dec 30 04:47:55.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.008759306s
Dec 30 04:47:57.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.008532772s
Dec 30 04:47:59.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.009087577s
Dec 30 04:48:01.178: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.007375639s
Dec 30 04:48:03.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.009299107s
Dec 30 04:48:05.181: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.009989164s
Dec 30 04:48:07.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.008437711s
Dec 30 04:48:09.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.008497742s
Dec 30 04:48:11.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.008694301s
Dec 30 04:48:13.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.008399535s
Dec 30 04:48:15.181: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.009922181s
Dec 30 04:48:17.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.008611296s
Dec 30 04:48:19.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.008576679s
Dec 30 04:48:21.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.008771459s
Dec 30 04:48:23.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.0096552s
Dec 30 04:48:25.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.008852551s
Dec 30 04:48:27.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.008437596s
Dec 30 04:48:29.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.008596892s
Dec 30 04:48:31.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.007989372s
Dec 30 04:48:33.182: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.010854151s
Dec 30 04:48:35.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.009479899s
Dec 30 04:48:37.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.008085891s
Dec 30 04:48:37.183: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.012286326s
STEP: removing the label kubernetes.io/e2e-d9b27c1e-1c0b-42b3-9741-d96f13f78734 off the node k8s-mgmt01 12/30/22 04:48:37.183
STEP: verifying the node doesn't have the label kubernetes.io/e2e-d9b27c1e-1c0b-42b3-9741-d96f13f78734 12/30/22 04:48:37.2
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Dec 30 04:48:37.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2519" for this suite. 12/30/22 04:48:37.211
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","completed":305,"skipped":5646,"failed":0}
------------------------------
• [SLOW TEST] [304.235 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:43:32.983
    Dec 30 04:43:32.983: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename sched-pred 12/30/22 04:43:32.984
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:43:33.002
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:43:33.005
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Dec 30 04:43:33.009: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Dec 30 04:43:33.020: INFO: Waiting for terminating namespaces to be deleted...
    Dec 30 04:43:33.024: INFO: 
    Logging pods the apiserver thinks is on node k8s-mgmt01 before test
    Dec 30 04:43:33.038: INFO: calico-node-6xm98 from kube-system started at 2022-12-28 02:19:33 +0000 UTC (1 container statuses recorded)
    Dec 30 04:43:33.038: INFO: 	Container calico-node ready: true, restart count 0
    Dec 30 04:43:33.038: INFO: kube-apiserver-k8s-mgmt01 from kube-system started at 2022-12-28 02:15:50 +0000 UTC (1 container statuses recorded)
    Dec 30 04:43:33.038: INFO: 	Container kube-apiserver ready: true, restart count 1
    Dec 30 04:43:33.038: INFO: kube-controller-manager-k8s-mgmt01 from kube-system started at 2022-12-28 02:15:49 +0000 UTC (1 container statuses recorded)
    Dec 30 04:43:33.038: INFO: 	Container kube-controller-manager ready: true, restart count 1
    Dec 30 04:43:33.038: INFO: kube-proxy-tcvcl from kube-system started at 2022-12-28 02:18:09 +0000 UTC (1 container statuses recorded)
    Dec 30 04:43:33.038: INFO: 	Container kube-proxy ready: true, restart count 0
    Dec 30 04:43:33.038: INFO: kube-scheduler-k8s-mgmt01 from kube-system started at 2022-12-28 02:15:38 +0000 UTC (1 container statuses recorded)
    Dec 30 04:43:33.038: INFO: 	Container kube-scheduler ready: true, restart count 1
    Dec 30 04:43:33.038: INFO: nodelocaldns-n9jl8 from kube-system started at 2022-12-28 02:22:14 +0000 UTC (1 container statuses recorded)
    Dec 30 04:43:33.038: INFO: 	Container node-cache ready: true, restart count 0
    Dec 30 04:43:33.038: INFO: sonobuoy-systemd-logs-daemon-set-4da877126f61426e-qd9td from sonobuoy started at 2022-12-30 03:20:18 +0000 UTC (2 container statuses recorded)
    Dec 30 04:43:33.038: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Dec 30 04:43:33.038: INFO: 	Container systemd-logs ready: true, restart count 0
    Dec 30 04:43:33.038: INFO: 
    Logging pods the apiserver thinks is on node k8s-mgmt02 before test
    Dec 30 04:43:33.052: INFO: calico-node-gwlv6 from kube-system started at 2022-12-28 02:19:33 +0000 UTC (1 container statuses recorded)
    Dec 30 04:43:33.052: INFO: 	Container calico-node ready: true, restart count 0
    Dec 30 04:43:33.052: INFO: coredns-69dfc8446-74sw5 from kube-system started at 2022-12-30 04:17:48 +0000 UTC (1 container statuses recorded)
    Dec 30 04:43:33.052: INFO: 	Container coredns ready: true, restart count 0
    Dec 30 04:43:33.052: INFO: kube-apiserver-k8s-mgmt02 from kube-system started at 2022-12-28 02:16:10 +0000 UTC (1 container statuses recorded)
    Dec 30 04:43:33.052: INFO: 	Container kube-apiserver ready: true, restart count 1
    Dec 30 04:43:33.052: INFO: kube-controller-manager-k8s-mgmt02 from kube-system started at 2022-12-28 02:16:10 +0000 UTC (1 container statuses recorded)
    Dec 30 04:43:33.052: INFO: 	Container kube-controller-manager ready: true, restart count 1
    Dec 30 04:43:33.052: INFO: kube-proxy-5lvmg from kube-system started at 2022-12-28 02:18:08 +0000 UTC (1 container statuses recorded)
    Dec 30 04:43:33.052: INFO: 	Container kube-proxy ready: true, restart count 0
    Dec 30 04:43:33.052: INFO: kube-scheduler-k8s-mgmt02 from kube-system started at 2022-12-28 02:16:10 +0000 UTC (1 container statuses recorded)
    Dec 30 04:43:33.052: INFO: 	Container kube-scheduler ready: true, restart count 1
    Dec 30 04:43:33.052: INFO: metrics-server-67f489ffcf-srn44 from kube-system started at 2022-12-28 02:22:51 +0000 UTC (1 container statuses recorded)
    Dec 30 04:43:33.052: INFO: 	Container metrics-server ready: true, restart count 0
    Dec 30 04:43:33.052: INFO: nodelocaldns-bwgsf from kube-system started at 2022-12-28 02:22:14 +0000 UTC (1 container statuses recorded)
    Dec 30 04:43:33.052: INFO: 	Container node-cache ready: true, restart count 0
    Dec 30 04:43:33.052: INFO: sonobuoy from sonobuoy started at 2022-12-30 03:20:11 +0000 UTC (1 container statuses recorded)
    Dec 30 04:43:33.052: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Dec 30 04:43:33.052: INFO: sonobuoy-systemd-logs-daemon-set-4da877126f61426e-llwcr from sonobuoy started at 2022-12-30 03:20:18 +0000 UTC (2 container statuses recorded)
    Dec 30 04:43:33.052: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Dec 30 04:43:33.052: INFO: 	Container systemd-logs ready: true, restart count 0
    Dec 30 04:43:33.052: INFO: 
    Logging pods the apiserver thinks is on node k8s-mgmt03 before test
    Dec 30 04:43:33.066: INFO: calico-kube-controllers-d6484b75c-8gdnt from kube-system started at 2022-12-28 02:21:06 +0000 UTC (1 container statuses recorded)
    Dec 30 04:43:33.066: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Dec 30 04:43:33.066: INFO: calico-node-lqstj from kube-system started at 2022-12-28 02:19:33 +0000 UTC (1 container statuses recorded)
    Dec 30 04:43:33.066: INFO: 	Container calico-node ready: true, restart count 0
    Dec 30 04:43:33.066: INFO: coredns-69dfc8446-6gk67 from kube-system started at 2022-12-28 02:22:13 +0000 UTC (1 container statuses recorded)
    Dec 30 04:43:33.066: INFO: 	Container coredns ready: true, restart count 0
    Dec 30 04:43:33.066: INFO: dns-autoscaler-5b9959d7fc-x5csc from kube-system started at 2022-12-30 04:17:48 +0000 UTC (1 container statuses recorded)
    Dec 30 04:43:33.066: INFO: 	Container autoscaler ready: true, restart count 0
    Dec 30 04:43:33.066: INFO: kube-apiserver-k8s-mgmt03 from kube-system started at 2022-12-28 02:16:20 +0000 UTC (1 container statuses recorded)
    Dec 30 04:43:33.066: INFO: 	Container kube-apiserver ready: true, restart count 1
    Dec 30 04:43:33.066: INFO: kube-controller-manager-k8s-mgmt03 from kube-system started at 2022-12-28 02:16:20 +0000 UTC (1 container statuses recorded)
    Dec 30 04:43:33.066: INFO: 	Container kube-controller-manager ready: true, restart count 1
    Dec 30 04:43:33.066: INFO: kube-proxy-rfm26 from kube-system started at 2022-12-28 02:18:08 +0000 UTC (1 container statuses recorded)
    Dec 30 04:43:33.066: INFO: 	Container kube-proxy ready: true, restart count 0
    Dec 30 04:43:33.066: INFO: kube-scheduler-k8s-mgmt03 from kube-system started at 2022-12-28 02:17:11 +0000 UTC (1 container statuses recorded)
    Dec 30 04:43:33.066: INFO: 	Container kube-scheduler ready: true, restart count 1
    Dec 30 04:43:33.066: INFO: nodelocaldns-76d47 from kube-system started at 2022-12-28 02:22:14 +0000 UTC (1 container statuses recorded)
    Dec 30 04:43:33.066: INFO: 	Container node-cache ready: true, restart count 0
    Dec 30 04:43:33.066: INFO: sonobuoy-e2e-job-c5efb7b0ce524286 from sonobuoy started at 2022-12-30 03:20:18 +0000 UTC (2 container statuses recorded)
    Dec 30 04:43:33.066: INFO: 	Container e2e ready: true, restart count 0
    Dec 30 04:43:33.066: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Dec 30 04:43:33.066: INFO: sonobuoy-systemd-logs-daemon-set-4da877126f61426e-wfvd8 from sonobuoy started at 2022-12-30 03:20:18 +0000 UTC (2 container statuses recorded)
    Dec 30 04:43:33.066: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Dec 30 04:43:33.066: INFO: 	Container systemd-logs ready: true, restart count 0
    Dec 30 04:43:33.066: INFO: 
    Logging pods the apiserver thinks is on node k8s-worker01 before test
    Dec 30 04:43:33.079: INFO: calico-node-dtvd6 from kube-system started at 2022-12-28 02:19:33 +0000 UTC (1 container statuses recorded)
    Dec 30 04:43:33.079: INFO: 	Container calico-node ready: true, restart count 0
    Dec 30 04:43:33.079: INFO: kube-proxy-m86bn from kube-system started at 2022-12-28 02:18:09 +0000 UTC (1 container statuses recorded)
    Dec 30 04:43:33.079: INFO: 	Container kube-proxy ready: true, restart count 0
    Dec 30 04:43:33.079: INFO: nginx-proxy-k8s-worker01 from kube-system started at 2022-12-28 02:17:47 +0000 UTC (1 container statuses recorded)
    Dec 30 04:43:33.079: INFO: 	Container nginx-proxy ready: true, restart count 0
    Dec 30 04:43:33.080: INFO: nodelocaldns-8zm2b from kube-system started at 2022-12-28 02:22:14 +0000 UTC (1 container statuses recorded)
    Dec 30 04:43:33.080: INFO: 	Container node-cache ready: true, restart count 0
    Dec 30 04:43:33.080: INFO: sonobuoy-systemd-logs-daemon-set-4da877126f61426e-4wpvd from sonobuoy started at 2022-12-30 03:20:18 +0000 UTC (2 container statuses recorded)
    Dec 30 04:43:33.080: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Dec 30 04:43:33.080: INFO: 	Container systemd-logs ready: true, restart count 0
    Dec 30 04:43:33.080: INFO: 
    Logging pods the apiserver thinks is on node k8s-worker02 before test
    Dec 30 04:43:33.091: INFO: calico-node-s7npn from kube-system started at 2022-12-28 02:19:33 +0000 UTC (1 container statuses recorded)
    Dec 30 04:43:33.091: INFO: 	Container calico-node ready: true, restart count 0
    Dec 30 04:43:33.092: INFO: kube-proxy-8cg4d from kube-system started at 2022-12-28 02:18:09 +0000 UTC (1 container statuses recorded)
    Dec 30 04:43:33.092: INFO: 	Container kube-proxy ready: true, restart count 0
    Dec 30 04:43:33.092: INFO: nginx-proxy-k8s-worker02 from kube-system started at 2022-12-28 02:17:48 +0000 UTC (1 container statuses recorded)
    Dec 30 04:43:33.092: INFO: 	Container nginx-proxy ready: true, restart count 0
    Dec 30 04:43:33.092: INFO: nodelocaldns-4hlqv from kube-system started at 2022-12-28 02:22:14 +0000 UTC (1 container statuses recorded)
    Dec 30 04:43:33.092: INFO: 	Container node-cache ready: true, restart count 0
    Dec 30 04:43:33.092: INFO: sonobuoy-systemd-logs-daemon-set-4da877126f61426e-7mpss from sonobuoy started at 2022-12-30 03:20:18 +0000 UTC (2 container statuses recorded)
    Dec 30 04:43:33.092: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Dec 30 04:43:33.092: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:699
    STEP: Trying to launch a pod without a label to get a node which can launch it. 12/30/22 04:43:33.092
    Dec 30 04:43:33.101: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-2519" to be "running"
    Dec 30 04:43:33.105: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 3.848748ms
    Dec 30 04:43:35.111: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.009227844s
    Dec 30 04:43:35.111: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 12/30/22 04:43:35.115
    STEP: Trying to apply a random label on the found node. 12/30/22 04:43:35.13
    STEP: verifying the node has the label kubernetes.io/e2e-d9b27c1e-1c0b-42b3-9741-d96f13f78734 95 12/30/22 04:43:35.143
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 12/30/22 04:43:35.148
    Dec 30 04:43:35.153: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-2519" to be "not pending"
    Dec 30 04:43:35.157: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.065937ms
    Dec 30 04:43:37.163: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.010086977s
    Dec 30 04:43:37.163: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.78.26.140 on the node which pod4 resides and expect not scheduled 12/30/22 04:43:37.163
    Dec 30 04:43:37.171: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-2519" to be "not pending"
    Dec 30 04:43:37.174: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.30869ms
    Dec 30 04:43:39.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008916988s
    Dec 30 04:43:41.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009242752s
    Dec 30 04:43:43.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.008090119s
    Dec 30 04:43:45.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.008955365s
    Dec 30 04:43:47.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.009142234s
    Dec 30 04:43:49.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.009223802s
    Dec 30 04:43:51.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.008408708s
    Dec 30 04:43:53.182: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.011164299s
    Dec 30 04:43:55.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.009227708s
    Dec 30 04:43:57.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.009086251s
    Dec 30 04:43:59.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.009586632s
    Dec 30 04:44:01.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.00838009s
    Dec 30 04:44:03.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.009333027s
    Dec 30 04:44:05.181: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.010471706s
    Dec 30 04:44:07.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.008637401s
    Dec 30 04:44:09.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.008353826s
    Dec 30 04:44:11.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.008209891s
    Dec 30 04:44:13.181: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.009835273s
    Dec 30 04:44:15.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.008690243s
    Dec 30 04:44:17.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.009706517s
    Dec 30 04:44:19.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.009105723s
    Dec 30 04:44:21.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.008508173s
    Dec 30 04:44:23.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.00917684s
    Dec 30 04:44:25.178: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.007629816s
    Dec 30 04:44:27.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.009386531s
    Dec 30 04:44:29.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.009251428s
    Dec 30 04:44:31.178: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.007499159s
    Dec 30 04:44:33.181: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.01042802s
    Dec 30 04:44:35.181: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.0101319s
    Dec 30 04:44:37.178: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.007644375s
    Dec 30 04:44:39.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.008926707s
    Dec 30 04:44:41.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.009580761s
    Dec 30 04:44:43.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.008700444s
    Dec 30 04:44:45.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.008106993s
    Dec 30 04:44:47.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.008333479s
    Dec 30 04:44:49.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.009341506s
    Dec 30 04:44:51.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.008861671s
    Dec 30 04:44:53.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.009733288s
    Dec 30 04:44:55.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.008823772s
    Dec 30 04:44:57.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.008718404s
    Dec 30 04:44:59.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.008772686s
    Dec 30 04:45:01.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.008271645s
    Dec 30 04:45:03.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.009171128s
    Dec 30 04:45:05.181: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.010092654s
    Dec 30 04:45:07.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.007910044s
    Dec 30 04:45:09.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.009180222s
    Dec 30 04:45:11.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.008021522s
    Dec 30 04:45:13.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.009277458s
    Dec 30 04:45:15.181: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.009890501s
    Dec 30 04:45:17.181: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.00976191s
    Dec 30 04:45:19.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.008705544s
    Dec 30 04:45:21.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.00862265s
    Dec 30 04:45:23.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.009311888s
    Dec 30 04:45:25.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.009075761s
    Dec 30 04:45:27.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.008518703s
    Dec 30 04:45:29.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.009532269s
    Dec 30 04:45:31.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.008044223s
    Dec 30 04:45:33.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.00974431s
    Dec 30 04:45:35.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.009208572s
    Dec 30 04:45:37.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.008245537s
    Dec 30 04:45:39.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.009326712s
    Dec 30 04:45:41.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.009104743s
    Dec 30 04:45:43.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.008896886s
    Dec 30 04:45:45.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.009385311s
    Dec 30 04:45:47.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.008650637s
    Dec 30 04:45:49.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.008695s
    Dec 30 04:45:51.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.00897896s
    Dec 30 04:45:53.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.009106622s
    Dec 30 04:45:55.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.008921544s
    Dec 30 04:45:57.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.008492175s
    Dec 30 04:45:59.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.007774198s
    Dec 30 04:46:01.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.008727595s
    Dec 30 04:46:03.181: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.010170336s
    Dec 30 04:46:05.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.009012246s
    Dec 30 04:46:07.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.008558741s
    Dec 30 04:46:09.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.009612711s
    Dec 30 04:46:11.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.008132853s
    Dec 30 04:46:13.181: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.009772813s
    Dec 30 04:46:15.181: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.009856651s
    Dec 30 04:46:17.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.008428988s
    Dec 30 04:46:19.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.009721s
    Dec 30 04:46:21.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.008262669s
    Dec 30 04:46:23.181: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.009913504s
    Dec 30 04:46:25.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.009201006s
    Dec 30 04:46:27.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.009516537s
    Dec 30 04:46:29.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.009191146s
    Dec 30 04:46:31.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.008201055s
    Dec 30 04:46:33.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.009299192s
    Dec 30 04:46:35.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.0097436s
    Dec 30 04:46:37.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.008946556s
    Dec 30 04:46:39.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.008085561s
    Dec 30 04:46:41.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.008974379s
    Dec 30 04:46:43.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.009592942s
    Dec 30 04:46:45.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.008658181s
    Dec 30 04:46:47.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.009038751s
    Dec 30 04:46:49.181: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.009821707s
    Dec 30 04:46:51.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.008256727s
    Dec 30 04:46:53.181: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.010055376s
    Dec 30 04:46:55.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.009474576s
    Dec 30 04:46:57.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.008276097s
    Dec 30 04:46:59.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.008683974s
    Dec 30 04:47:01.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.009581388s
    Dec 30 04:47:03.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.008752825s
    Dec 30 04:47:05.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.00906823s
    Dec 30 04:47:07.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.008812982s
    Dec 30 04:47:09.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.008249278s
    Dec 30 04:47:11.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.00832503s
    Dec 30 04:47:13.181: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.010500825s
    Dec 30 04:47:15.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.008527059s
    Dec 30 04:47:17.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.008907015s
    Dec 30 04:47:19.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.0093415s
    Dec 30 04:47:21.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.007831942s
    Dec 30 04:47:23.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.009564646s
    Dec 30 04:47:25.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.009513493s
    Dec 30 04:47:27.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.00827733s
    Dec 30 04:47:29.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.008275741s
    Dec 30 04:47:31.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.008108617s
    Dec 30 04:47:33.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.009142756s
    Dec 30 04:47:35.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.008628171s
    Dec 30 04:47:37.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.008874527s
    Dec 30 04:47:39.178: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.007662076s
    Dec 30 04:47:41.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.008362007s
    Dec 30 04:47:43.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.009143609s
    Dec 30 04:47:45.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.009642371s
    Dec 30 04:47:47.181: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.009853951s
    Dec 30 04:47:49.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.009007515s
    Dec 30 04:47:51.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.008769205s
    Dec 30 04:47:53.181: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.009907262s
    Dec 30 04:47:55.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.008759306s
    Dec 30 04:47:57.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.008532772s
    Dec 30 04:47:59.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.009087577s
    Dec 30 04:48:01.178: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.007375639s
    Dec 30 04:48:03.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.009299107s
    Dec 30 04:48:05.181: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.009989164s
    Dec 30 04:48:07.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.008437711s
    Dec 30 04:48:09.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.008497742s
    Dec 30 04:48:11.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.008694301s
    Dec 30 04:48:13.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.008399535s
    Dec 30 04:48:15.181: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.009922181s
    Dec 30 04:48:17.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.008611296s
    Dec 30 04:48:19.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.008576679s
    Dec 30 04:48:21.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.008771459s
    Dec 30 04:48:23.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.0096552s
    Dec 30 04:48:25.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.008852551s
    Dec 30 04:48:27.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.008437596s
    Dec 30 04:48:29.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.008596892s
    Dec 30 04:48:31.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.007989372s
    Dec 30 04:48:33.182: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.010854151s
    Dec 30 04:48:35.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.009479899s
    Dec 30 04:48:37.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.008085891s
    Dec 30 04:48:37.183: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.012286326s
    STEP: removing the label kubernetes.io/e2e-d9b27c1e-1c0b-42b3-9741-d96f13f78734 off the node k8s-mgmt01 12/30/22 04:48:37.183
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-d9b27c1e-1c0b-42b3-9741-d96f13f78734 12/30/22 04:48:37.2
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Dec 30 04:48:37.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-2519" for this suite. 12/30/22 04:48:37.211
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:608
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:48:37.219
Dec 30 04:48:37.219: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename security-context-test 12/30/22 04:48:37.221
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:48:37.239
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:48:37.242
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:608
Dec 30 04:48:37.255: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-de65da85-4beb-4b1f-a74a-dab1e3651edd" in namespace "security-context-test-3146" to be "Succeeded or Failed"
Dec 30 04:48:37.259: INFO: Pod "alpine-nnp-false-de65da85-4beb-4b1f-a74a-dab1e3651edd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.336379ms
Dec 30 04:48:39.265: INFO: Pod "alpine-nnp-false-de65da85-4beb-4b1f-a74a-dab1e3651edd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009015956s
Dec 30 04:48:41.265: INFO: Pod "alpine-nnp-false-de65da85-4beb-4b1f-a74a-dab1e3651edd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009249498s
Dec 30 04:48:43.266: INFO: Pod "alpine-nnp-false-de65da85-4beb-4b1f-a74a-dab1e3651edd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010766418s
Dec 30 04:48:43.266: INFO: Pod "alpine-nnp-false-de65da85-4beb-4b1f-a74a-dab1e3651edd" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Dec 30 04:48:43.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3146" for this suite. 12/30/22 04:48:43.294
{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","completed":306,"skipped":5648,"failed":0}
------------------------------
• [SLOW TEST] [6.081 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:554
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:608

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:48:37.219
    Dec 30 04:48:37.219: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename security-context-test 12/30/22 04:48:37.221
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:48:37.239
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:48:37.242
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:608
    Dec 30 04:48:37.255: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-de65da85-4beb-4b1f-a74a-dab1e3651edd" in namespace "security-context-test-3146" to be "Succeeded or Failed"
    Dec 30 04:48:37.259: INFO: Pod "alpine-nnp-false-de65da85-4beb-4b1f-a74a-dab1e3651edd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.336379ms
    Dec 30 04:48:39.265: INFO: Pod "alpine-nnp-false-de65da85-4beb-4b1f-a74a-dab1e3651edd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009015956s
    Dec 30 04:48:41.265: INFO: Pod "alpine-nnp-false-de65da85-4beb-4b1f-a74a-dab1e3651edd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009249498s
    Dec 30 04:48:43.266: INFO: Pod "alpine-nnp-false-de65da85-4beb-4b1f-a74a-dab1e3651edd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010766418s
    Dec 30 04:48:43.266: INFO: Pod "alpine-nnp-false-de65da85-4beb-4b1f-a74a-dab1e3651edd" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Dec 30 04:48:43.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-3146" for this suite. 12/30/22 04:48:43.294
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:48:43.301
Dec 30 04:48:43.301: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename replicaset 12/30/22 04:48:43.302
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:48:43.32
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:48:43.324
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 12/30/22 04:48:43.328
Dec 30 04:48:43.336: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-1065" to be "running and ready"
Dec 30 04:48:43.340: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 4.285942ms
Dec 30 04:48:43.340: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Dec 30 04:48:45.346: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.009567532s
Dec 30 04:48:45.346: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
Dec 30 04:48:45.346: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 12/30/22 04:48:45.35
STEP: Then the orphan pod is adopted 12/30/22 04:48:45.357
STEP: When the matched label of one of its pods change 12/30/22 04:48:46.366
Dec 30 04:48:46.370: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 12/30/22 04:48:46.383
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Dec 30 04:48:47.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1065" for this suite. 12/30/22 04:48:47.398
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","completed":307,"skipped":5648,"failed":0}
------------------------------
• [4.104 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:48:43.301
    Dec 30 04:48:43.301: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename replicaset 12/30/22 04:48:43.302
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:48:43.32
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:48:43.324
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 12/30/22 04:48:43.328
    Dec 30 04:48:43.336: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-1065" to be "running and ready"
    Dec 30 04:48:43.340: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 4.285942ms
    Dec 30 04:48:43.340: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Dec 30 04:48:45.346: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.009567532s
    Dec 30 04:48:45.346: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    Dec 30 04:48:45.346: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 12/30/22 04:48:45.35
    STEP: Then the orphan pod is adopted 12/30/22 04:48:45.357
    STEP: When the matched label of one of its pods change 12/30/22 04:48:46.366
    Dec 30 04:48:46.370: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 12/30/22 04:48:46.383
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Dec 30 04:48:47.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-1065" for this suite. 12/30/22 04:48:47.398
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:48:47.406
Dec 30 04:48:47.406: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename watch 12/30/22 04:48:47.408
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:48:47.428
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:48:47.431
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 12/30/22 04:48:47.434
STEP: creating a new configmap 12/30/22 04:48:47.436
STEP: modifying the configmap once 12/30/22 04:48:47.441
STEP: changing the label value of the configmap 12/30/22 04:48:47.45
STEP: Expecting to observe a delete notification for the watched object 12/30/22 04:48:47.459
Dec 30 04:48:47.459: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9816  55c2202c-9ca1-4c57-8417-338b2637f0f8 458101 0 2022-12-30 04:48:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-12-30 04:48:47 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 30 04:48:47.459: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9816  55c2202c-9ca1-4c57-8417-338b2637f0f8 458102 0 2022-12-30 04:48:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-12-30 04:48:47 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 30 04:48:47.459: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9816  55c2202c-9ca1-4c57-8417-338b2637f0f8 458103 0 2022-12-30 04:48:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-12-30 04:48:47 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 12/30/22 04:48:47.46
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 12/30/22 04:48:47.469
STEP: changing the label value of the configmap back 12/30/22 04:48:57.47
STEP: modifying the configmap a third time 12/30/22 04:48:57.48
STEP: deleting the configmap 12/30/22 04:48:57.488
STEP: Expecting to observe an add notification for the watched object when the label value was restored 12/30/22 04:48:57.495
Dec 30 04:48:57.496: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9816  55c2202c-9ca1-4c57-8417-338b2637f0f8 458170 0 2022-12-30 04:48:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-12-30 04:48:57 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 30 04:48:57.496: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9816  55c2202c-9ca1-4c57-8417-338b2637f0f8 458171 0 2022-12-30 04:48:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-12-30 04:48:57 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 30 04:48:57.496: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9816  55c2202c-9ca1-4c57-8417-338b2637f0f8 458172 0 2022-12-30 04:48:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-12-30 04:48:57 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Dec 30 04:48:57.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9816" for this suite. 12/30/22 04:48:57.502
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","completed":308,"skipped":5657,"failed":0}
------------------------------
• [SLOW TEST] [10.103 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:48:47.406
    Dec 30 04:48:47.406: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename watch 12/30/22 04:48:47.408
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:48:47.428
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:48:47.431
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 12/30/22 04:48:47.434
    STEP: creating a new configmap 12/30/22 04:48:47.436
    STEP: modifying the configmap once 12/30/22 04:48:47.441
    STEP: changing the label value of the configmap 12/30/22 04:48:47.45
    STEP: Expecting to observe a delete notification for the watched object 12/30/22 04:48:47.459
    Dec 30 04:48:47.459: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9816  55c2202c-9ca1-4c57-8417-338b2637f0f8 458101 0 2022-12-30 04:48:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-12-30 04:48:47 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Dec 30 04:48:47.459: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9816  55c2202c-9ca1-4c57-8417-338b2637f0f8 458102 0 2022-12-30 04:48:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-12-30 04:48:47 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Dec 30 04:48:47.459: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9816  55c2202c-9ca1-4c57-8417-338b2637f0f8 458103 0 2022-12-30 04:48:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-12-30 04:48:47 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 12/30/22 04:48:47.46
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 12/30/22 04:48:47.469
    STEP: changing the label value of the configmap back 12/30/22 04:48:57.47
    STEP: modifying the configmap a third time 12/30/22 04:48:57.48
    STEP: deleting the configmap 12/30/22 04:48:57.488
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 12/30/22 04:48:57.495
    Dec 30 04:48:57.496: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9816  55c2202c-9ca1-4c57-8417-338b2637f0f8 458170 0 2022-12-30 04:48:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-12-30 04:48:57 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Dec 30 04:48:57.496: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9816  55c2202c-9ca1-4c57-8417-338b2637f0f8 458171 0 2022-12-30 04:48:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-12-30 04:48:57 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    Dec 30 04:48:57.496: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9816  55c2202c-9ca1-4c57-8417-338b2637f0f8 458172 0 2022-12-30 04:48:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2022-12-30 04:48:57 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Dec 30 04:48:57.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-9816" for this suite. 12/30/22 04:48:57.502
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:48:57.511
Dec 30 04:48:57.511: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename taint-multiple-pods 12/30/22 04:48:57.513
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:48:57.529
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:48:57.533
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:348
Dec 30 04:48:57.536: INFO: Waiting up to 1m0s for all nodes to be ready
Dec 30 04:49:57.589: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420
Dec 30 04:49:57.593: INFO: Starting informer...
STEP: Starting pods... 12/30/22 04:49:57.593
Dec 30 04:49:57.817: INFO: Pod1 is running on k8s-mgmt01. Tainting Node
Dec 30 04:49:58.027: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-5998" to be "running"
Dec 30 04:49:58.031: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.547283ms
Dec 30 04:50:00.036: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.008449392s
Dec 30 04:50:00.036: INFO: Pod "taint-eviction-b1" satisfied condition "running"
Dec 30 04:50:00.036: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-5998" to be "running"
Dec 30 04:50:00.040: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 4.233986ms
Dec 30 04:50:00.040: INFO: Pod "taint-eviction-b2" satisfied condition "running"
Dec 30 04:50:00.040: INFO: Pod2 is running on k8s-mgmt01. Tainting Node
STEP: Trying to apply a taint on the Node 12/30/22 04:50:00.04
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 12/30/22 04:50:00.056
STEP: Waiting for Pod1 and Pod2 to be deleted 12/30/22 04:50:00.061
Dec 30 04:50:05.874: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Dec 30 04:50:25.935: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 12/30/22 04:50:25.952
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:187
Dec 30 04:50:25.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-5998" for this suite. 12/30/22 04:50:25.962
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","completed":309,"skipped":5673,"failed":0}
------------------------------
• [SLOW TEST] [88.458 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:48:57.511
    Dec 30 04:48:57.511: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename taint-multiple-pods 12/30/22 04:48:57.513
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:48:57.529
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:48:57.533
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:348
    Dec 30 04:48:57.536: INFO: Waiting up to 1m0s for all nodes to be ready
    Dec 30 04:49:57.589: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:420
    Dec 30 04:49:57.593: INFO: Starting informer...
    STEP: Starting pods... 12/30/22 04:49:57.593
    Dec 30 04:49:57.817: INFO: Pod1 is running on k8s-mgmt01. Tainting Node
    Dec 30 04:49:58.027: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-5998" to be "running"
    Dec 30 04:49:58.031: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.547283ms
    Dec 30 04:50:00.036: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.008449392s
    Dec 30 04:50:00.036: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    Dec 30 04:50:00.036: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-5998" to be "running"
    Dec 30 04:50:00.040: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 4.233986ms
    Dec 30 04:50:00.040: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    Dec 30 04:50:00.040: INFO: Pod2 is running on k8s-mgmt01. Tainting Node
    STEP: Trying to apply a taint on the Node 12/30/22 04:50:00.04
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 12/30/22 04:50:00.056
    STEP: Waiting for Pod1 and Pod2 to be deleted 12/30/22 04:50:00.061
    Dec 30 04:50:05.874: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    Dec 30 04:50:25.935: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 12/30/22 04:50:25.952
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/framework.go:187
    Dec 30 04:50:25.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "taint-multiple-pods-5998" for this suite. 12/30/22 04:50:25.962
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:50:25.97
Dec 30 04:50:25.970: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename job 12/30/22 04:50:25.972
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:50:25.988
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:50:25.992
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335
STEP: Creating a job 12/30/22 04:50:25.995
STEP: Ensuring active pods == parallelism 12/30/22 04:50:26.002
STEP: Orphaning one of the Job's Pods 12/30/22 04:50:28.008
Dec 30 04:50:28.527: INFO: Successfully updated pod "adopt-release-6tlbl"
STEP: Checking that the Job readopts the Pod 12/30/22 04:50:28.527
Dec 30 04:50:28.527: INFO: Waiting up to 15m0s for pod "adopt-release-6tlbl" in namespace "job-9761" to be "adopted"
Dec 30 04:50:28.531: INFO: Pod "adopt-release-6tlbl": Phase="Running", Reason="", readiness=true. Elapsed: 3.437583ms
Dec 30 04:50:30.537: INFO: Pod "adopt-release-6tlbl": Phase="Running", Reason="", readiness=true. Elapsed: 2.009584386s
Dec 30 04:50:30.537: INFO: Pod "adopt-release-6tlbl" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 12/30/22 04:50:30.537
Dec 30 04:50:31.051: INFO: Successfully updated pod "adopt-release-6tlbl"
STEP: Checking that the Job releases the Pod 12/30/22 04:50:31.051
Dec 30 04:50:31.052: INFO: Waiting up to 15m0s for pod "adopt-release-6tlbl" in namespace "job-9761" to be "released"
Dec 30 04:50:31.055: INFO: Pod "adopt-release-6tlbl": Phase="Running", Reason="", readiness=true. Elapsed: 3.803135ms
Dec 30 04:50:33.061: INFO: Pod "adopt-release-6tlbl": Phase="Running", Reason="", readiness=true. Elapsed: 2.009298547s
Dec 30 04:50:33.061: INFO: Pod "adopt-release-6tlbl" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Dec 30 04:50:33.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-9761" for this suite. 12/30/22 04:50:33.068
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","completed":310,"skipped":5677,"failed":0}
------------------------------
• [SLOW TEST] [7.105 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:50:25.97
    Dec 30 04:50:25.970: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename job 12/30/22 04:50:25.972
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:50:25.988
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:50:25.992
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:335
    STEP: Creating a job 12/30/22 04:50:25.995
    STEP: Ensuring active pods == parallelism 12/30/22 04:50:26.002
    STEP: Orphaning one of the Job's Pods 12/30/22 04:50:28.008
    Dec 30 04:50:28.527: INFO: Successfully updated pod "adopt-release-6tlbl"
    STEP: Checking that the Job readopts the Pod 12/30/22 04:50:28.527
    Dec 30 04:50:28.527: INFO: Waiting up to 15m0s for pod "adopt-release-6tlbl" in namespace "job-9761" to be "adopted"
    Dec 30 04:50:28.531: INFO: Pod "adopt-release-6tlbl": Phase="Running", Reason="", readiness=true. Elapsed: 3.437583ms
    Dec 30 04:50:30.537: INFO: Pod "adopt-release-6tlbl": Phase="Running", Reason="", readiness=true. Elapsed: 2.009584386s
    Dec 30 04:50:30.537: INFO: Pod "adopt-release-6tlbl" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 12/30/22 04:50:30.537
    Dec 30 04:50:31.051: INFO: Successfully updated pod "adopt-release-6tlbl"
    STEP: Checking that the Job releases the Pod 12/30/22 04:50:31.051
    Dec 30 04:50:31.052: INFO: Waiting up to 15m0s for pod "adopt-release-6tlbl" in namespace "job-9761" to be "released"
    Dec 30 04:50:31.055: INFO: Pod "adopt-release-6tlbl": Phase="Running", Reason="", readiness=true. Elapsed: 3.803135ms
    Dec 30 04:50:33.061: INFO: Pod "adopt-release-6tlbl": Phase="Running", Reason="", readiness=true. Elapsed: 2.009298547s
    Dec 30 04:50:33.061: INFO: Pod "adopt-release-6tlbl" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Dec 30 04:50:33.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-9761" for this suite. 12/30/22 04:50:33.068
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:50:33.076
Dec 30 04:50:33.076: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename emptydir 12/30/22 04:50:33.078
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:50:33.097
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:50:33.1
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206
STEP: Creating a pod to test emptydir 0666 on node default medium 12/30/22 04:50:33.104
Dec 30 04:50:33.113: INFO: Waiting up to 5m0s for pod "pod-b1970c31-92c6-4ec5-aa05-35e79b4021de" in namespace "emptydir-4135" to be "Succeeded or Failed"
Dec 30 04:50:33.117: INFO: Pod "pod-b1970c31-92c6-4ec5-aa05-35e79b4021de": Phase="Pending", Reason="", readiness=false. Elapsed: 4.172236ms
Dec 30 04:50:35.122: INFO: Pod "pod-b1970c31-92c6-4ec5-aa05-35e79b4021de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009758188s
Dec 30 04:50:37.123: INFO: Pod "pod-b1970c31-92c6-4ec5-aa05-35e79b4021de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010083396s
STEP: Saw pod success 12/30/22 04:50:37.123
Dec 30 04:50:37.123: INFO: Pod "pod-b1970c31-92c6-4ec5-aa05-35e79b4021de" satisfied condition "Succeeded or Failed"
Dec 30 04:50:37.127: INFO: Trying to get logs from node k8s-mgmt02 pod pod-b1970c31-92c6-4ec5-aa05-35e79b4021de container test-container: <nil>
STEP: delete the pod 12/30/22 04:50:37.148
Dec 30 04:50:37.171: INFO: Waiting for pod pod-b1970c31-92c6-4ec5-aa05-35e79b4021de to disappear
Dec 30 04:50:37.175: INFO: Pod pod-b1970c31-92c6-4ec5-aa05-35e79b4021de no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Dec 30 04:50:37.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4135" for this suite. 12/30/22 04:50:37.182
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":311,"skipped":5689,"failed":0}
------------------------------
• [4.112 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:50:33.076
    Dec 30 04:50:33.076: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename emptydir 12/30/22 04:50:33.078
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:50:33.097
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:50:33.1
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:206
    STEP: Creating a pod to test emptydir 0666 on node default medium 12/30/22 04:50:33.104
    Dec 30 04:50:33.113: INFO: Waiting up to 5m0s for pod "pod-b1970c31-92c6-4ec5-aa05-35e79b4021de" in namespace "emptydir-4135" to be "Succeeded or Failed"
    Dec 30 04:50:33.117: INFO: Pod "pod-b1970c31-92c6-4ec5-aa05-35e79b4021de": Phase="Pending", Reason="", readiness=false. Elapsed: 4.172236ms
    Dec 30 04:50:35.122: INFO: Pod "pod-b1970c31-92c6-4ec5-aa05-35e79b4021de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009758188s
    Dec 30 04:50:37.123: INFO: Pod "pod-b1970c31-92c6-4ec5-aa05-35e79b4021de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010083396s
    STEP: Saw pod success 12/30/22 04:50:37.123
    Dec 30 04:50:37.123: INFO: Pod "pod-b1970c31-92c6-4ec5-aa05-35e79b4021de" satisfied condition "Succeeded or Failed"
    Dec 30 04:50:37.127: INFO: Trying to get logs from node k8s-mgmt02 pod pod-b1970c31-92c6-4ec5-aa05-35e79b4021de container test-container: <nil>
    STEP: delete the pod 12/30/22 04:50:37.148
    Dec 30 04:50:37.171: INFO: Waiting for pod pod-b1970c31-92c6-4ec5-aa05-35e79b4021de to disappear
    Dec 30 04:50:37.175: INFO: Pod pod-b1970c31-92c6-4ec5-aa05-35e79b4021de no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Dec 30 04:50:37.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-4135" for this suite. 12/30/22 04:50:37.182
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:50:37.19
Dec 30 04:50:37.190: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename subpath 12/30/22 04:50:37.192
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:50:37.208
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:50:37.212
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 12/30/22 04:50:37.216
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-tztj 12/30/22 04:50:37.226
STEP: Creating a pod to test atomic-volume-subpath 12/30/22 04:50:37.226
Dec 30 04:50:37.236: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-tztj" in namespace "subpath-2983" to be "Succeeded or Failed"
Dec 30 04:50:37.240: INFO: Pod "pod-subpath-test-configmap-tztj": Phase="Pending", Reason="", readiness=false. Elapsed: 3.408156ms
Dec 30 04:50:39.245: INFO: Pod "pod-subpath-test-configmap-tztj": Phase="Running", Reason="", readiness=true. Elapsed: 2.009077248s
Dec 30 04:50:41.245: INFO: Pod "pod-subpath-test-configmap-tztj": Phase="Running", Reason="", readiness=true. Elapsed: 4.008458906s
Dec 30 04:50:43.245: INFO: Pod "pod-subpath-test-configmap-tztj": Phase="Running", Reason="", readiness=true. Elapsed: 6.009008056s
Dec 30 04:50:45.246: INFO: Pod "pod-subpath-test-configmap-tztj": Phase="Running", Reason="", readiness=true. Elapsed: 8.009215355s
Dec 30 04:50:47.246: INFO: Pod "pod-subpath-test-configmap-tztj": Phase="Running", Reason="", readiness=true. Elapsed: 10.009771578s
Dec 30 04:50:49.246: INFO: Pod "pod-subpath-test-configmap-tztj": Phase="Running", Reason="", readiness=true. Elapsed: 12.009363687s
Dec 30 04:50:51.244: INFO: Pod "pod-subpath-test-configmap-tztj": Phase="Running", Reason="", readiness=true. Elapsed: 14.007867177s
Dec 30 04:50:53.247: INFO: Pod "pod-subpath-test-configmap-tztj": Phase="Running", Reason="", readiness=true. Elapsed: 16.010208497s
Dec 30 04:50:55.246: INFO: Pod "pod-subpath-test-configmap-tztj": Phase="Running", Reason="", readiness=true. Elapsed: 18.009479176s
Dec 30 04:50:57.245: INFO: Pod "pod-subpath-test-configmap-tztj": Phase="Running", Reason="", readiness=true. Elapsed: 20.008699754s
Dec 30 04:50:59.245: INFO: Pod "pod-subpath-test-configmap-tztj": Phase="Running", Reason="", readiness=false. Elapsed: 22.008202198s
Dec 30 04:51:01.245: INFO: Pod "pod-subpath-test-configmap-tztj": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.008165389s
STEP: Saw pod success 12/30/22 04:51:01.245
Dec 30 04:51:01.245: INFO: Pod "pod-subpath-test-configmap-tztj" satisfied condition "Succeeded or Failed"
Dec 30 04:51:01.248: INFO: Trying to get logs from node k8s-mgmt03 pod pod-subpath-test-configmap-tztj container test-container-subpath-configmap-tztj: <nil>
STEP: delete the pod 12/30/22 04:51:01.27
Dec 30 04:51:01.283: INFO: Waiting for pod pod-subpath-test-configmap-tztj to disappear
Dec 30 04:51:01.287: INFO: Pod pod-subpath-test-configmap-tztj no longer exists
STEP: Deleting pod pod-subpath-test-configmap-tztj 12/30/22 04:51:01.287
Dec 30 04:51:01.287: INFO: Deleting pod "pod-subpath-test-configmap-tztj" in namespace "subpath-2983"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Dec 30 04:51:01.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2983" for this suite. 12/30/22 04:51:01.301
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]","completed":312,"skipped":5695,"failed":0}
------------------------------
• [SLOW TEST] [24.118 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:50:37.19
    Dec 30 04:50:37.190: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename subpath 12/30/22 04:50:37.192
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:50:37.208
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:50:37.212
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 12/30/22 04:50:37.216
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-tztj 12/30/22 04:50:37.226
    STEP: Creating a pod to test atomic-volume-subpath 12/30/22 04:50:37.226
    Dec 30 04:50:37.236: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-tztj" in namespace "subpath-2983" to be "Succeeded or Failed"
    Dec 30 04:50:37.240: INFO: Pod "pod-subpath-test-configmap-tztj": Phase="Pending", Reason="", readiness=false. Elapsed: 3.408156ms
    Dec 30 04:50:39.245: INFO: Pod "pod-subpath-test-configmap-tztj": Phase="Running", Reason="", readiness=true. Elapsed: 2.009077248s
    Dec 30 04:50:41.245: INFO: Pod "pod-subpath-test-configmap-tztj": Phase="Running", Reason="", readiness=true. Elapsed: 4.008458906s
    Dec 30 04:50:43.245: INFO: Pod "pod-subpath-test-configmap-tztj": Phase="Running", Reason="", readiness=true. Elapsed: 6.009008056s
    Dec 30 04:50:45.246: INFO: Pod "pod-subpath-test-configmap-tztj": Phase="Running", Reason="", readiness=true. Elapsed: 8.009215355s
    Dec 30 04:50:47.246: INFO: Pod "pod-subpath-test-configmap-tztj": Phase="Running", Reason="", readiness=true. Elapsed: 10.009771578s
    Dec 30 04:50:49.246: INFO: Pod "pod-subpath-test-configmap-tztj": Phase="Running", Reason="", readiness=true. Elapsed: 12.009363687s
    Dec 30 04:50:51.244: INFO: Pod "pod-subpath-test-configmap-tztj": Phase="Running", Reason="", readiness=true. Elapsed: 14.007867177s
    Dec 30 04:50:53.247: INFO: Pod "pod-subpath-test-configmap-tztj": Phase="Running", Reason="", readiness=true. Elapsed: 16.010208497s
    Dec 30 04:50:55.246: INFO: Pod "pod-subpath-test-configmap-tztj": Phase="Running", Reason="", readiness=true. Elapsed: 18.009479176s
    Dec 30 04:50:57.245: INFO: Pod "pod-subpath-test-configmap-tztj": Phase="Running", Reason="", readiness=true. Elapsed: 20.008699754s
    Dec 30 04:50:59.245: INFO: Pod "pod-subpath-test-configmap-tztj": Phase="Running", Reason="", readiness=false. Elapsed: 22.008202198s
    Dec 30 04:51:01.245: INFO: Pod "pod-subpath-test-configmap-tztj": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.008165389s
    STEP: Saw pod success 12/30/22 04:51:01.245
    Dec 30 04:51:01.245: INFO: Pod "pod-subpath-test-configmap-tztj" satisfied condition "Succeeded or Failed"
    Dec 30 04:51:01.248: INFO: Trying to get logs from node k8s-mgmt03 pod pod-subpath-test-configmap-tztj container test-container-subpath-configmap-tztj: <nil>
    STEP: delete the pod 12/30/22 04:51:01.27
    Dec 30 04:51:01.283: INFO: Waiting for pod pod-subpath-test-configmap-tztj to disappear
    Dec 30 04:51:01.287: INFO: Pod pod-subpath-test-configmap-tztj no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-tztj 12/30/22 04:51:01.287
    Dec 30 04:51:01.287: INFO: Deleting pod "pod-subpath-test-configmap-tztj" in namespace "subpath-2983"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Dec 30 04:51:01.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-2983" for this suite. 12/30/22 04:51:01.301
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:51:01.309
Dec 30 04:51:01.309: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename downward-api 12/30/22 04:51:01.311
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:51:01.328
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:51:01.332
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234
STEP: Creating a pod to test downward API volume plugin 12/30/22 04:51:01.336
Dec 30 04:51:01.347: INFO: Waiting up to 5m0s for pod "downwardapi-volume-012c11d7-a1c7-4873-9003-da7887fc8d45" in namespace "downward-api-8895" to be "Succeeded or Failed"
Dec 30 04:51:01.351: INFO: Pod "downwardapi-volume-012c11d7-a1c7-4873-9003-da7887fc8d45": Phase="Pending", Reason="", readiness=false. Elapsed: 3.715772ms
Dec 30 04:51:03.357: INFO: Pod "downwardapi-volume-012c11d7-a1c7-4873-9003-da7887fc8d45": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010079117s
Dec 30 04:51:05.357: INFO: Pod "downwardapi-volume-012c11d7-a1c7-4873-9003-da7887fc8d45": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010201591s
STEP: Saw pod success 12/30/22 04:51:05.357
Dec 30 04:51:05.358: INFO: Pod "downwardapi-volume-012c11d7-a1c7-4873-9003-da7887fc8d45" satisfied condition "Succeeded or Failed"
Dec 30 04:51:05.363: INFO: Trying to get logs from node k8s-mgmt01 pod downwardapi-volume-012c11d7-a1c7-4873-9003-da7887fc8d45 container client-container: <nil>
STEP: delete the pod 12/30/22 04:51:05.384
Dec 30 04:51:05.397: INFO: Waiting for pod downwardapi-volume-012c11d7-a1c7-4873-9003-da7887fc8d45 to disappear
Dec 30 04:51:05.400: INFO: Pod downwardapi-volume-012c11d7-a1c7-4873-9003-da7887fc8d45 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Dec 30 04:51:05.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8895" for this suite. 12/30/22 04:51:05.407
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","completed":313,"skipped":5700,"failed":0}
------------------------------
• [4.105 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:51:01.309
    Dec 30 04:51:01.309: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename downward-api 12/30/22 04:51:01.311
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:51:01.328
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:51:01.332
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:234
    STEP: Creating a pod to test downward API volume plugin 12/30/22 04:51:01.336
    Dec 30 04:51:01.347: INFO: Waiting up to 5m0s for pod "downwardapi-volume-012c11d7-a1c7-4873-9003-da7887fc8d45" in namespace "downward-api-8895" to be "Succeeded or Failed"
    Dec 30 04:51:01.351: INFO: Pod "downwardapi-volume-012c11d7-a1c7-4873-9003-da7887fc8d45": Phase="Pending", Reason="", readiness=false. Elapsed: 3.715772ms
    Dec 30 04:51:03.357: INFO: Pod "downwardapi-volume-012c11d7-a1c7-4873-9003-da7887fc8d45": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010079117s
    Dec 30 04:51:05.357: INFO: Pod "downwardapi-volume-012c11d7-a1c7-4873-9003-da7887fc8d45": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010201591s
    STEP: Saw pod success 12/30/22 04:51:05.357
    Dec 30 04:51:05.358: INFO: Pod "downwardapi-volume-012c11d7-a1c7-4873-9003-da7887fc8d45" satisfied condition "Succeeded or Failed"
    Dec 30 04:51:05.363: INFO: Trying to get logs from node k8s-mgmt01 pod downwardapi-volume-012c11d7-a1c7-4873-9003-da7887fc8d45 container client-container: <nil>
    STEP: delete the pod 12/30/22 04:51:05.384
    Dec 30 04:51:05.397: INFO: Waiting for pod downwardapi-volume-012c11d7-a1c7-4873-9003-da7887fc8d45 to disappear
    Dec 30 04:51:05.400: INFO: Pod downwardapi-volume-012c11d7-a1c7-4873-9003-da7887fc8d45 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Dec 30 04:51:05.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-8895" for this suite. 12/30/22 04:51:05.407
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:51:05.42
Dec 30 04:51:05.420: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename crd-webhook 12/30/22 04:51:05.421
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:51:05.438
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:51:05.441
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 12/30/22 04:51:05.445
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 12/30/22 04:51:06.479
STEP: Deploying the custom resource conversion webhook pod 12/30/22 04:51:06.487
STEP: Wait for the deployment to be ready 12/30/22 04:51:06.498
Dec 30 04:51:06.506: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 12/30/22 04:51:08.52
STEP: Verifying the service has paired with the endpoint 12/30/22 04:51:08.531
Dec 30 04:51:09.532: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
Dec 30 04:51:09.537: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Creating a v1 custom resource 12/30/22 04:51:17.182
STEP: v2 custom resource should be converted 12/30/22 04:51:17.188
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Dec 30 04:51:17.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-1539" for this suite. 12/30/22 04:51:17.714
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","completed":314,"skipped":5783,"failed":0}
------------------------------
• [SLOW TEST] [12.340 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:51:05.42
    Dec 30 04:51:05.420: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename crd-webhook 12/30/22 04:51:05.421
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:51:05.438
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:51:05.441
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 12/30/22 04:51:05.445
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 12/30/22 04:51:06.479
    STEP: Deploying the custom resource conversion webhook pod 12/30/22 04:51:06.487
    STEP: Wait for the deployment to be ready 12/30/22 04:51:06.498
    Dec 30 04:51:06.506: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 12/30/22 04:51:08.52
    STEP: Verifying the service has paired with the endpoint 12/30/22 04:51:08.531
    Dec 30 04:51:09.532: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    Dec 30 04:51:09.537: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Creating a v1 custom resource 12/30/22 04:51:17.182
    STEP: v2 custom resource should be converted 12/30/22 04:51:17.188
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Dec 30 04:51:17.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-webhook-1539" for this suite. 12/30/22 04:51:17.714
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:51:17.762
Dec 30 04:51:17.762: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename ingress 12/30/22 04:51:17.763
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:51:17.779
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:51:17.783
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 12/30/22 04:51:17.786
STEP: getting /apis/networking.k8s.io 12/30/22 04:51:17.789
STEP: getting /apis/networking.k8s.iov1 12/30/22 04:51:17.791
STEP: creating 12/30/22 04:51:17.792
STEP: getting 12/30/22 04:51:17.808
STEP: listing 12/30/22 04:51:17.812
STEP: watching 12/30/22 04:51:17.816
Dec 30 04:51:17.816: INFO: starting watch
STEP: cluster-wide listing 12/30/22 04:51:17.817
STEP: cluster-wide watching 12/30/22 04:51:17.821
Dec 30 04:51:17.822: INFO: starting watch
STEP: patching 12/30/22 04:51:17.823
STEP: updating 12/30/22 04:51:17.829
Dec 30 04:51:17.838: INFO: waiting for watch events with expected annotations
Dec 30 04:51:17.838: INFO: saw patched and updated annotations
STEP: patching /status 12/30/22 04:51:17.839
STEP: updating /status 12/30/22 04:51:17.845
STEP: get /status 12/30/22 04:51:17.856
STEP: deleting 12/30/22 04:51:17.863
STEP: deleting a collection 12/30/22 04:51:17.876
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:187
Dec 30 04:51:17.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-1538" for this suite. 12/30/22 04:51:17.895
{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","completed":315,"skipped":5806,"failed":0}
------------------------------
• [0.140 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:51:17.762
    Dec 30 04:51:17.762: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename ingress 12/30/22 04:51:17.763
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:51:17.779
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:51:17.783
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 12/30/22 04:51:17.786
    STEP: getting /apis/networking.k8s.io 12/30/22 04:51:17.789
    STEP: getting /apis/networking.k8s.iov1 12/30/22 04:51:17.791
    STEP: creating 12/30/22 04:51:17.792
    STEP: getting 12/30/22 04:51:17.808
    STEP: listing 12/30/22 04:51:17.812
    STEP: watching 12/30/22 04:51:17.816
    Dec 30 04:51:17.816: INFO: starting watch
    STEP: cluster-wide listing 12/30/22 04:51:17.817
    STEP: cluster-wide watching 12/30/22 04:51:17.821
    Dec 30 04:51:17.822: INFO: starting watch
    STEP: patching 12/30/22 04:51:17.823
    STEP: updating 12/30/22 04:51:17.829
    Dec 30 04:51:17.838: INFO: waiting for watch events with expected annotations
    Dec 30 04:51:17.838: INFO: saw patched and updated annotations
    STEP: patching /status 12/30/22 04:51:17.839
    STEP: updating /status 12/30/22 04:51:17.845
    STEP: get /status 12/30/22 04:51:17.856
    STEP: deleting 12/30/22 04:51:17.863
    STEP: deleting a collection 12/30/22 04:51:17.876
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/framework.go:187
    Dec 30 04:51:17.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ingress-1538" for this suite. 12/30/22 04:51:17.895
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:51:17.903
Dec 30 04:51:17.903: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename projected 12/30/22 04:51:17.904
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:51:17.917
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:51:17.921
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248
STEP: Creating a pod to test downward API volume plugin 12/30/22 04:51:17.924
Dec 30 04:51:17.933: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c703ae6b-4c11-462a-8e52-0a300e8a7479" in namespace "projected-461" to be "Succeeded or Failed"
Dec 30 04:51:17.936: INFO: Pod "downwardapi-volume-c703ae6b-4c11-462a-8e52-0a300e8a7479": Phase="Pending", Reason="", readiness=false. Elapsed: 3.433881ms
Dec 30 04:51:19.942: INFO: Pod "downwardapi-volume-c703ae6b-4c11-462a-8e52-0a300e8a7479": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009799842s
Dec 30 04:51:21.941: INFO: Pod "downwardapi-volume-c703ae6b-4c11-462a-8e52-0a300e8a7479": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008535347s
STEP: Saw pod success 12/30/22 04:51:21.941
Dec 30 04:51:21.941: INFO: Pod "downwardapi-volume-c703ae6b-4c11-462a-8e52-0a300e8a7479" satisfied condition "Succeeded or Failed"
Dec 30 04:51:21.945: INFO: Trying to get logs from node k8s-mgmt01 pod downwardapi-volume-c703ae6b-4c11-462a-8e52-0a300e8a7479 container client-container: <nil>
STEP: delete the pod 12/30/22 04:51:21.954
Dec 30 04:51:21.968: INFO: Waiting for pod downwardapi-volume-c703ae6b-4c11-462a-8e52-0a300e8a7479 to disappear
Dec 30 04:51:21.971: INFO: Pod downwardapi-volume-c703ae6b-4c11-462a-8e52-0a300e8a7479 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Dec 30 04:51:21.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-461" for this suite. 12/30/22 04:51:21.977
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","completed":316,"skipped":5814,"failed":0}
------------------------------
• [4.081 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:51:17.903
    Dec 30 04:51:17.903: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename projected 12/30/22 04:51:17.904
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:51:17.917
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:51:17.921
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:248
    STEP: Creating a pod to test downward API volume plugin 12/30/22 04:51:17.924
    Dec 30 04:51:17.933: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c703ae6b-4c11-462a-8e52-0a300e8a7479" in namespace "projected-461" to be "Succeeded or Failed"
    Dec 30 04:51:17.936: INFO: Pod "downwardapi-volume-c703ae6b-4c11-462a-8e52-0a300e8a7479": Phase="Pending", Reason="", readiness=false. Elapsed: 3.433881ms
    Dec 30 04:51:19.942: INFO: Pod "downwardapi-volume-c703ae6b-4c11-462a-8e52-0a300e8a7479": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009799842s
    Dec 30 04:51:21.941: INFO: Pod "downwardapi-volume-c703ae6b-4c11-462a-8e52-0a300e8a7479": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008535347s
    STEP: Saw pod success 12/30/22 04:51:21.941
    Dec 30 04:51:21.941: INFO: Pod "downwardapi-volume-c703ae6b-4c11-462a-8e52-0a300e8a7479" satisfied condition "Succeeded or Failed"
    Dec 30 04:51:21.945: INFO: Trying to get logs from node k8s-mgmt01 pod downwardapi-volume-c703ae6b-4c11-462a-8e52-0a300e8a7479 container client-container: <nil>
    STEP: delete the pod 12/30/22 04:51:21.954
    Dec 30 04:51:21.968: INFO: Waiting for pod downwardapi-volume-c703ae6b-4c11-462a-8e52-0a300e8a7479 to disappear
    Dec 30 04:51:21.971: INFO: Pod downwardapi-volume-c703ae6b-4c11-462a-8e52-0a300e8a7479 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Dec 30 04:51:21.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-461" for this suite. 12/30/22 04:51:21.977
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:315
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:51:21.986
Dec 30 04:51:21.986: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename statefulset 12/30/22 04:51:21.988
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:51:22.007
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:51:22.011
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-2275 12/30/22 04:51:22.015
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:315
STEP: Creating a new StatefulSet 12/30/22 04:51:22.02
Dec 30 04:51:22.031: INFO: Found 0 stateful pods, waiting for 3
Dec 30 04:51:32.038: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 30 04:51:32.038: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 30 04:51:32.038: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 12/30/22 04:51:32.052
Dec 30 04:51:32.074: INFO: Updating stateful set ss2
STEP: Creating a new revision 12/30/22 04:51:32.074
STEP: Not applying an update when the partition is greater than the number of replicas 12/30/22 04:51:42.094
STEP: Performing a canary update 12/30/22 04:51:42.094
Dec 30 04:51:42.117: INFO: Updating stateful set ss2
Dec 30 04:51:42.125: INFO: Waiting for Pod statefulset-2275/ss2-2 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
STEP: Restoring Pods to the correct revision when they are deleted 12/30/22 04:51:52.137
Dec 30 04:51:52.163: INFO: Found 1 stateful pods, waiting for 3
Dec 30 04:52:02.170: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 30 04:52:02.170: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 30 04:52:02.170: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 12/30/22 04:52:02.179
Dec 30 04:52:02.201: INFO: Updating stateful set ss2
Dec 30 04:52:02.209: INFO: Waiting for Pod statefulset-2275/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
Dec 30 04:52:12.243: INFO: Updating stateful set ss2
Dec 30 04:52:12.252: INFO: Waiting for StatefulSet statefulset-2275/ss2 to complete update
Dec 30 04:52:12.252: INFO: Waiting for Pod statefulset-2275/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Dec 30 04:52:22.264: INFO: Deleting all statefulset in ns statefulset-2275
Dec 30 04:52:22.269: INFO: Scaling statefulset ss2 to 0
Dec 30 04:52:32.297: INFO: Waiting for statefulset status.replicas updated to 0
Dec 30 04:52:32.300: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Dec 30 04:52:32.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2275" for this suite. 12/30/22 04:52:32.321
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","completed":317,"skipped":5824,"failed":0}
------------------------------
• [SLOW TEST] [70.342 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:315

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:51:21.986
    Dec 30 04:51:21.986: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename statefulset 12/30/22 04:51:21.988
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:51:22.007
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:51:22.011
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-2275 12/30/22 04:51:22.015
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:315
    STEP: Creating a new StatefulSet 12/30/22 04:51:22.02
    Dec 30 04:51:22.031: INFO: Found 0 stateful pods, waiting for 3
    Dec 30 04:51:32.038: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Dec 30 04:51:32.038: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Dec 30 04:51:32.038: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 12/30/22 04:51:32.052
    Dec 30 04:51:32.074: INFO: Updating stateful set ss2
    STEP: Creating a new revision 12/30/22 04:51:32.074
    STEP: Not applying an update when the partition is greater than the number of replicas 12/30/22 04:51:42.094
    STEP: Performing a canary update 12/30/22 04:51:42.094
    Dec 30 04:51:42.117: INFO: Updating stateful set ss2
    Dec 30 04:51:42.125: INFO: Waiting for Pod statefulset-2275/ss2-2 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    STEP: Restoring Pods to the correct revision when they are deleted 12/30/22 04:51:52.137
    Dec 30 04:51:52.163: INFO: Found 1 stateful pods, waiting for 3
    Dec 30 04:52:02.170: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Dec 30 04:52:02.170: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Dec 30 04:52:02.170: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 12/30/22 04:52:02.179
    Dec 30 04:52:02.201: INFO: Updating stateful set ss2
    Dec 30 04:52:02.209: INFO: Waiting for Pod statefulset-2275/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    Dec 30 04:52:12.243: INFO: Updating stateful set ss2
    Dec 30 04:52:12.252: INFO: Waiting for StatefulSet statefulset-2275/ss2 to complete update
    Dec 30 04:52:12.252: INFO: Waiting for Pod statefulset-2275/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Dec 30 04:52:22.264: INFO: Deleting all statefulset in ns statefulset-2275
    Dec 30 04:52:22.269: INFO: Scaling statefulset ss2 to 0
    Dec 30 04:52:32.297: INFO: Waiting for statefulset status.replicas updated to 0
    Dec 30 04:52:32.300: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Dec 30 04:52:32.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-2275" for this suite. 12/30/22 04:52:32.321
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:52:32.332
Dec 30 04:52:32.332: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename lease-test 12/30/22 04:52:32.333
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:52:32.351
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:52:32.354
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/framework.go:187
Dec 30 04:52:32.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-1022" for this suite. 12/30/22 04:52:32.426
{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","completed":318,"skipped":5862,"failed":0}
------------------------------
• [0.101 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:52:32.332
    Dec 30 04:52:32.332: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename lease-test 12/30/22 04:52:32.333
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:52:32.351
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:52:32.354
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/framework.go:187
    Dec 30 04:52:32.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "lease-test-1022" for this suite. 12/30/22 04:52:32.426
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:52:32.44
Dec 30 04:52:32.440: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename pod-network-test 12/30/22 04:52:32.442
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:52:32.457
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:52:32.46
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-677 12/30/22 04:52:32.464
STEP: creating a selector 12/30/22 04:52:32.464
STEP: Creating the service pods in kubernetes 12/30/22 04:52:32.464
Dec 30 04:52:32.464: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Dec 30 04:52:32.507: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-677" to be "running and ready"
Dec 30 04:52:32.511: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.261926ms
Dec 30 04:52:32.511: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec 30 04:52:34.517: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.010380425s
Dec 30 04:52:34.517: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 30 04:52:36.516: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.009428773s
Dec 30 04:52:36.516: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 30 04:52:38.516: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.009067962s
Dec 30 04:52:38.516: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 30 04:52:40.518: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.010877679s
Dec 30 04:52:40.518: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 30 04:52:42.517: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.010681505s
Dec 30 04:52:42.517: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 30 04:52:44.516: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.009653257s
Dec 30 04:52:44.516: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 30 04:52:46.517: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.010278229s
Dec 30 04:52:46.517: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 30 04:52:48.518: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.010975447s
Dec 30 04:52:48.518: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 30 04:52:50.517: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.010093424s
Dec 30 04:52:50.517: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 30 04:52:52.517: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.010521118s
Dec 30 04:52:52.517: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Dec 30 04:52:54.518: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.011695864s
Dec 30 04:52:54.518: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Dec 30 04:52:54.518: INFO: Pod "netserver-0" satisfied condition "running and ready"
Dec 30 04:52:54.523: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-677" to be "running and ready"
Dec 30 04:52:54.526: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.665068ms
Dec 30 04:52:54.526: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Dec 30 04:52:54.526: INFO: Pod "netserver-1" satisfied condition "running and ready"
Dec 30 04:52:54.530: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-677" to be "running and ready"
Dec 30 04:52:54.535: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 4.284381ms
Dec 30 04:52:54.535: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Dec 30 04:52:54.535: INFO: Pod "netserver-2" satisfied condition "running and ready"
Dec 30 04:52:54.538: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-677" to be "running and ready"
Dec 30 04:52:54.542: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 4.027656ms
Dec 30 04:52:54.542: INFO: The phase of Pod netserver-3 is Running (Ready = true)
Dec 30 04:52:54.542: INFO: Pod "netserver-3" satisfied condition "running and ready"
Dec 30 04:52:54.546: INFO: Waiting up to 5m0s for pod "netserver-4" in namespace "pod-network-test-677" to be "running and ready"
Dec 30 04:52:54.550: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=true. Elapsed: 3.608911ms
Dec 30 04:52:54.550: INFO: The phase of Pod netserver-4 is Running (Ready = true)
Dec 30 04:52:54.550: INFO: Pod "netserver-4" satisfied condition "running and ready"
STEP: Creating test pods 12/30/22 04:52:54.554
Dec 30 04:52:54.560: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-677" to be "running"
Dec 30 04:52:54.564: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.470655ms
Dec 30 04:52:56.569: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008644123s
Dec 30 04:52:56.569: INFO: Pod "test-container-pod" satisfied condition "running"
Dec 30 04:52:56.573: INFO: Setting MaxTries for pod polling to 55 for networking test based on endpoint count 5
Dec 30 04:52:56.573: INFO: Breadth first check of 10.233.112.188 on host 10.78.26.140...
Dec 30 04:52:56.577: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.112.136:9080/dial?request=hostname&protocol=http&host=10.233.112.188&port=8083&tries=1'] Namespace:pod-network-test-677 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 30 04:52:56.577: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
Dec 30 04:52:56.578: INFO: ExecWithOptions: Clientset creation
Dec 30 04:52:56.578: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-677/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.112.136%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.112.188%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Dec 30 04:52:56.708: INFO: Waiting for responses: map[]
Dec 30 04:52:56.708: INFO: reached 10.233.112.188 after 0/1 tries
Dec 30 04:52:56.708: INFO: Breadth first check of 10.233.125.222 on host 10.78.26.141...
Dec 30 04:52:56.712: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.112.136:9080/dial?request=hostname&protocol=http&host=10.233.125.222&port=8083&tries=1'] Namespace:pod-network-test-677 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 30 04:52:56.712: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
Dec 30 04:52:56.713: INFO: ExecWithOptions: Clientset creation
Dec 30 04:52:56.713: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-677/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.112.136%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.125.222%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Dec 30 04:52:56.816: INFO: Waiting for responses: map[]
Dec 30 04:52:56.816: INFO: reached 10.233.125.222 after 0/1 tries
Dec 30 04:52:56.816: INFO: Breadth first check of 10.233.78.156 on host 10.78.26.142...
Dec 30 04:52:56.819: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.112.136:9080/dial?request=hostname&protocol=http&host=10.233.78.156&port=8083&tries=1'] Namespace:pod-network-test-677 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 30 04:52:56.819: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
Dec 30 04:52:56.820: INFO: ExecWithOptions: Clientset creation
Dec 30 04:52:56.820: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-677/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.112.136%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.78.156%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Dec 30 04:52:56.924: INFO: Waiting for responses: map[]
Dec 30 04:52:56.924: INFO: reached 10.233.78.156 after 0/1 tries
Dec 30 04:52:56.924: INFO: Breadth first check of 10.233.79.78 on host 10.78.26.194...
Dec 30 04:52:56.937: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.112.136:9080/dial?request=hostname&protocol=http&host=10.233.79.78&port=8083&tries=1'] Namespace:pod-network-test-677 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 30 04:52:56.937: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
Dec 30 04:52:56.938: INFO: ExecWithOptions: Clientset creation
Dec 30 04:52:56.938: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-677/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.112.136%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.79.78%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Dec 30 04:52:57.040: INFO: Waiting for responses: map[]
Dec 30 04:52:57.040: INFO: reached 10.233.79.78 after 0/1 tries
Dec 30 04:52:57.040: INFO: Breadth first check of 10.233.109.93 on host 10.78.26.195...
Dec 30 04:52:57.044: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.112.136:9080/dial?request=hostname&protocol=http&host=10.233.109.93&port=8083&tries=1'] Namespace:pod-network-test-677 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 30 04:52:57.044: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
Dec 30 04:52:57.045: INFO: ExecWithOptions: Clientset creation
Dec 30 04:52:57.045: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-677/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.112.136%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.109.93%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Dec 30 04:52:57.143: INFO: Waiting for responses: map[]
Dec 30 04:52:57.144: INFO: reached 10.233.109.93 after 0/1 tries
Dec 30 04:52:57.144: INFO: Going to retry 0 out of 5 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Dec 30 04:52:57.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-677" for this suite. 12/30/22 04:52:57.15
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","completed":319,"skipped":5953,"failed":0}
------------------------------
• [SLOW TEST] [24.717 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:52:32.44
    Dec 30 04:52:32.440: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename pod-network-test 12/30/22 04:52:32.442
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:52:32.457
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:52:32.46
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-677 12/30/22 04:52:32.464
    STEP: creating a selector 12/30/22 04:52:32.464
    STEP: Creating the service pods in kubernetes 12/30/22 04:52:32.464
    Dec 30 04:52:32.464: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Dec 30 04:52:32.507: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-677" to be "running and ready"
    Dec 30 04:52:32.511: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.261926ms
    Dec 30 04:52:32.511: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Dec 30 04:52:34.517: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.010380425s
    Dec 30 04:52:34.517: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 30 04:52:36.516: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.009428773s
    Dec 30 04:52:36.516: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 30 04:52:38.516: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.009067962s
    Dec 30 04:52:38.516: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 30 04:52:40.518: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.010877679s
    Dec 30 04:52:40.518: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 30 04:52:42.517: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.010681505s
    Dec 30 04:52:42.517: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 30 04:52:44.516: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.009653257s
    Dec 30 04:52:44.516: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 30 04:52:46.517: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.010278229s
    Dec 30 04:52:46.517: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 30 04:52:48.518: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.010975447s
    Dec 30 04:52:48.518: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 30 04:52:50.517: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.010093424s
    Dec 30 04:52:50.517: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 30 04:52:52.517: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.010521118s
    Dec 30 04:52:52.517: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Dec 30 04:52:54.518: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.011695864s
    Dec 30 04:52:54.518: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Dec 30 04:52:54.518: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Dec 30 04:52:54.523: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-677" to be "running and ready"
    Dec 30 04:52:54.526: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.665068ms
    Dec 30 04:52:54.526: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Dec 30 04:52:54.526: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Dec 30 04:52:54.530: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-677" to be "running and ready"
    Dec 30 04:52:54.535: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 4.284381ms
    Dec 30 04:52:54.535: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Dec 30 04:52:54.535: INFO: Pod "netserver-2" satisfied condition "running and ready"
    Dec 30 04:52:54.538: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-677" to be "running and ready"
    Dec 30 04:52:54.542: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 4.027656ms
    Dec 30 04:52:54.542: INFO: The phase of Pod netserver-3 is Running (Ready = true)
    Dec 30 04:52:54.542: INFO: Pod "netserver-3" satisfied condition "running and ready"
    Dec 30 04:52:54.546: INFO: Waiting up to 5m0s for pod "netserver-4" in namespace "pod-network-test-677" to be "running and ready"
    Dec 30 04:52:54.550: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=true. Elapsed: 3.608911ms
    Dec 30 04:52:54.550: INFO: The phase of Pod netserver-4 is Running (Ready = true)
    Dec 30 04:52:54.550: INFO: Pod "netserver-4" satisfied condition "running and ready"
    STEP: Creating test pods 12/30/22 04:52:54.554
    Dec 30 04:52:54.560: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-677" to be "running"
    Dec 30 04:52:54.564: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.470655ms
    Dec 30 04:52:56.569: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008644123s
    Dec 30 04:52:56.569: INFO: Pod "test-container-pod" satisfied condition "running"
    Dec 30 04:52:56.573: INFO: Setting MaxTries for pod polling to 55 for networking test based on endpoint count 5
    Dec 30 04:52:56.573: INFO: Breadth first check of 10.233.112.188 on host 10.78.26.140...
    Dec 30 04:52:56.577: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.112.136:9080/dial?request=hostname&protocol=http&host=10.233.112.188&port=8083&tries=1'] Namespace:pod-network-test-677 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 30 04:52:56.577: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    Dec 30 04:52:56.578: INFO: ExecWithOptions: Clientset creation
    Dec 30 04:52:56.578: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-677/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.112.136%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.112.188%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Dec 30 04:52:56.708: INFO: Waiting for responses: map[]
    Dec 30 04:52:56.708: INFO: reached 10.233.112.188 after 0/1 tries
    Dec 30 04:52:56.708: INFO: Breadth first check of 10.233.125.222 on host 10.78.26.141...
    Dec 30 04:52:56.712: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.112.136:9080/dial?request=hostname&protocol=http&host=10.233.125.222&port=8083&tries=1'] Namespace:pod-network-test-677 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 30 04:52:56.712: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    Dec 30 04:52:56.713: INFO: ExecWithOptions: Clientset creation
    Dec 30 04:52:56.713: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-677/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.112.136%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.125.222%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Dec 30 04:52:56.816: INFO: Waiting for responses: map[]
    Dec 30 04:52:56.816: INFO: reached 10.233.125.222 after 0/1 tries
    Dec 30 04:52:56.816: INFO: Breadth first check of 10.233.78.156 on host 10.78.26.142...
    Dec 30 04:52:56.819: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.112.136:9080/dial?request=hostname&protocol=http&host=10.233.78.156&port=8083&tries=1'] Namespace:pod-network-test-677 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 30 04:52:56.819: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    Dec 30 04:52:56.820: INFO: ExecWithOptions: Clientset creation
    Dec 30 04:52:56.820: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-677/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.112.136%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.78.156%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Dec 30 04:52:56.924: INFO: Waiting for responses: map[]
    Dec 30 04:52:56.924: INFO: reached 10.233.78.156 after 0/1 tries
    Dec 30 04:52:56.924: INFO: Breadth first check of 10.233.79.78 on host 10.78.26.194...
    Dec 30 04:52:56.937: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.112.136:9080/dial?request=hostname&protocol=http&host=10.233.79.78&port=8083&tries=1'] Namespace:pod-network-test-677 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 30 04:52:56.937: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    Dec 30 04:52:56.938: INFO: ExecWithOptions: Clientset creation
    Dec 30 04:52:56.938: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-677/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.112.136%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.79.78%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Dec 30 04:52:57.040: INFO: Waiting for responses: map[]
    Dec 30 04:52:57.040: INFO: reached 10.233.79.78 after 0/1 tries
    Dec 30 04:52:57.040: INFO: Breadth first check of 10.233.109.93 on host 10.78.26.195...
    Dec 30 04:52:57.044: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.112.136:9080/dial?request=hostname&protocol=http&host=10.233.109.93&port=8083&tries=1'] Namespace:pod-network-test-677 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 30 04:52:57.044: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    Dec 30 04:52:57.045: INFO: ExecWithOptions: Clientset creation
    Dec 30 04:52:57.045: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-677/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.112.136%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.109.93%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Dec 30 04:52:57.143: INFO: Waiting for responses: map[]
    Dec 30 04:52:57.144: INFO: reached 10.233.109.93 after 0/1 tries
    Dec 30 04:52:57.144: INFO: Going to retry 0 out of 5 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Dec 30 04:52:57.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-677" for this suite. 12/30/22 04:52:57.15
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:52:57.158
Dec 30 04:52:57.159: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename services 12/30/22 04:52:57.16
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:52:57.184
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:52:57.188
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204
STEP: creating service in namespace services-9609 12/30/22 04:52:57.191
STEP: creating service affinity-nodeport in namespace services-9609 12/30/22 04:52:57.191
STEP: creating replication controller affinity-nodeport in namespace services-9609 12/30/22 04:52:57.207
I1230 04:52:57.212917      25 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-9609, replica count: 3
I1230 04:53:00.264039      25 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 30 04:53:00.278: INFO: Creating new exec pod
Dec 30 04:53:00.286: INFO: Waiting up to 5m0s for pod "execpod-affinityntk6q" in namespace "services-9609" to be "running"
Dec 30 04:53:00.291: INFO: Pod "execpod-affinityntk6q": Phase="Pending", Reason="", readiness=false. Elapsed: 4.354049ms
Dec 30 04:53:02.301: INFO: Pod "execpod-affinityntk6q": Phase="Running", Reason="", readiness=true. Elapsed: 2.014430371s
Dec 30 04:53:02.301: INFO: Pod "execpod-affinityntk6q" satisfied condition "running"
Dec 30 04:53:03.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-9609 exec execpod-affinityntk6q -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
Dec 30 04:53:03.515: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Dec 30 04:53:03.515: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 30 04:53:03.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-9609 exec execpod-affinityntk6q -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.20.50 80'
Dec 30 04:53:03.725: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.20.50 80\nConnection to 10.233.20.50 80 port [tcp/http] succeeded!\n"
Dec 30 04:53:03.725: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 30 04:53:03.725: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-9609 exec execpod-affinityntk6q -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.78.26.142 32469'
Dec 30 04:53:03.925: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.78.26.142 32469\nConnection to 10.78.26.142 32469 port [tcp/*] succeeded!\n"
Dec 30 04:53:03.925: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 30 04:53:03.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-9609 exec execpod-affinityntk6q -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.78.26.140 32469'
Dec 30 04:53:04.128: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.78.26.140 32469\nConnection to 10.78.26.140 32469 port [tcp/*] succeeded!\n"
Dec 30 04:53:04.128: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 30 04:53:04.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-9609 exec execpod-affinityntk6q -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.78.26.140:32469/ ; done'
Dec 30 04:53:04.443: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:32469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:32469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:32469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:32469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:32469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:32469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:32469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:32469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:32469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:32469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:32469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:32469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:32469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:32469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:32469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:32469/\n"
Dec 30 04:53:04.443: INFO: stdout: "\naffinity-nodeport-44mvk\naffinity-nodeport-44mvk\naffinity-nodeport-44mvk\naffinity-nodeport-44mvk\naffinity-nodeport-44mvk\naffinity-nodeport-44mvk\naffinity-nodeport-44mvk\naffinity-nodeport-44mvk\naffinity-nodeport-44mvk\naffinity-nodeport-44mvk\naffinity-nodeport-44mvk\naffinity-nodeport-44mvk\naffinity-nodeport-44mvk\naffinity-nodeport-44mvk\naffinity-nodeport-44mvk\naffinity-nodeport-44mvk"
Dec 30 04:53:04.443: INFO: Received response from host: affinity-nodeport-44mvk
Dec 30 04:53:04.443: INFO: Received response from host: affinity-nodeport-44mvk
Dec 30 04:53:04.443: INFO: Received response from host: affinity-nodeport-44mvk
Dec 30 04:53:04.443: INFO: Received response from host: affinity-nodeport-44mvk
Dec 30 04:53:04.443: INFO: Received response from host: affinity-nodeport-44mvk
Dec 30 04:53:04.443: INFO: Received response from host: affinity-nodeport-44mvk
Dec 30 04:53:04.443: INFO: Received response from host: affinity-nodeport-44mvk
Dec 30 04:53:04.443: INFO: Received response from host: affinity-nodeport-44mvk
Dec 30 04:53:04.443: INFO: Received response from host: affinity-nodeport-44mvk
Dec 30 04:53:04.443: INFO: Received response from host: affinity-nodeport-44mvk
Dec 30 04:53:04.443: INFO: Received response from host: affinity-nodeport-44mvk
Dec 30 04:53:04.443: INFO: Received response from host: affinity-nodeport-44mvk
Dec 30 04:53:04.443: INFO: Received response from host: affinity-nodeport-44mvk
Dec 30 04:53:04.443: INFO: Received response from host: affinity-nodeport-44mvk
Dec 30 04:53:04.443: INFO: Received response from host: affinity-nodeport-44mvk
Dec 30 04:53:04.443: INFO: Received response from host: affinity-nodeport-44mvk
Dec 30 04:53:04.443: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-9609, will wait for the garbage collector to delete the pods 12/30/22 04:53:04.459
Dec 30 04:53:04.522: INFO: Deleting ReplicationController affinity-nodeport took: 8.026675ms
Dec 30 04:53:04.623: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.240667ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Dec 30 04:53:06.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9609" for this suite. 12/30/22 04:53:06.452
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","completed":320,"skipped":5960,"failed":0}
------------------------------
• [SLOW TEST] [9.300 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:52:57.158
    Dec 30 04:52:57.159: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename services 12/30/22 04:52:57.16
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:52:57.184
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:52:57.188
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2204
    STEP: creating service in namespace services-9609 12/30/22 04:52:57.191
    STEP: creating service affinity-nodeport in namespace services-9609 12/30/22 04:52:57.191
    STEP: creating replication controller affinity-nodeport in namespace services-9609 12/30/22 04:52:57.207
    I1230 04:52:57.212917      25 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-9609, replica count: 3
    I1230 04:53:00.264039      25 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Dec 30 04:53:00.278: INFO: Creating new exec pod
    Dec 30 04:53:00.286: INFO: Waiting up to 5m0s for pod "execpod-affinityntk6q" in namespace "services-9609" to be "running"
    Dec 30 04:53:00.291: INFO: Pod "execpod-affinityntk6q": Phase="Pending", Reason="", readiness=false. Elapsed: 4.354049ms
    Dec 30 04:53:02.301: INFO: Pod "execpod-affinityntk6q": Phase="Running", Reason="", readiness=true. Elapsed: 2.014430371s
    Dec 30 04:53:02.301: INFO: Pod "execpod-affinityntk6q" satisfied condition "running"
    Dec 30 04:53:03.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-9609 exec execpod-affinityntk6q -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
    Dec 30 04:53:03.515: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    Dec 30 04:53:03.515: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Dec 30 04:53:03.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-9609 exec execpod-affinityntk6q -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.20.50 80'
    Dec 30 04:53:03.725: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.20.50 80\nConnection to 10.233.20.50 80 port [tcp/http] succeeded!\n"
    Dec 30 04:53:03.725: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Dec 30 04:53:03.725: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-9609 exec execpod-affinityntk6q -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.78.26.142 32469'
    Dec 30 04:53:03.925: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.78.26.142 32469\nConnection to 10.78.26.142 32469 port [tcp/*] succeeded!\n"
    Dec 30 04:53:03.925: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Dec 30 04:53:03.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-9609 exec execpod-affinityntk6q -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.78.26.140 32469'
    Dec 30 04:53:04.128: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.78.26.140 32469\nConnection to 10.78.26.140 32469 port [tcp/*] succeeded!\n"
    Dec 30 04:53:04.128: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Dec 30 04:53:04.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-9609 exec execpod-affinityntk6q -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.78.26.140:32469/ ; done'
    Dec 30 04:53:04.443: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:32469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:32469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:32469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:32469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:32469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:32469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:32469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:32469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:32469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:32469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:32469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:32469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:32469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:32469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:32469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.78.26.140:32469/\n"
    Dec 30 04:53:04.443: INFO: stdout: "\naffinity-nodeport-44mvk\naffinity-nodeport-44mvk\naffinity-nodeport-44mvk\naffinity-nodeport-44mvk\naffinity-nodeport-44mvk\naffinity-nodeport-44mvk\naffinity-nodeport-44mvk\naffinity-nodeport-44mvk\naffinity-nodeport-44mvk\naffinity-nodeport-44mvk\naffinity-nodeport-44mvk\naffinity-nodeport-44mvk\naffinity-nodeport-44mvk\naffinity-nodeport-44mvk\naffinity-nodeport-44mvk\naffinity-nodeport-44mvk"
    Dec 30 04:53:04.443: INFO: Received response from host: affinity-nodeport-44mvk
    Dec 30 04:53:04.443: INFO: Received response from host: affinity-nodeport-44mvk
    Dec 30 04:53:04.443: INFO: Received response from host: affinity-nodeport-44mvk
    Dec 30 04:53:04.443: INFO: Received response from host: affinity-nodeport-44mvk
    Dec 30 04:53:04.443: INFO: Received response from host: affinity-nodeport-44mvk
    Dec 30 04:53:04.443: INFO: Received response from host: affinity-nodeport-44mvk
    Dec 30 04:53:04.443: INFO: Received response from host: affinity-nodeport-44mvk
    Dec 30 04:53:04.443: INFO: Received response from host: affinity-nodeport-44mvk
    Dec 30 04:53:04.443: INFO: Received response from host: affinity-nodeport-44mvk
    Dec 30 04:53:04.443: INFO: Received response from host: affinity-nodeport-44mvk
    Dec 30 04:53:04.443: INFO: Received response from host: affinity-nodeport-44mvk
    Dec 30 04:53:04.443: INFO: Received response from host: affinity-nodeport-44mvk
    Dec 30 04:53:04.443: INFO: Received response from host: affinity-nodeport-44mvk
    Dec 30 04:53:04.443: INFO: Received response from host: affinity-nodeport-44mvk
    Dec 30 04:53:04.443: INFO: Received response from host: affinity-nodeport-44mvk
    Dec 30 04:53:04.443: INFO: Received response from host: affinity-nodeport-44mvk
    Dec 30 04:53:04.443: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-9609, will wait for the garbage collector to delete the pods 12/30/22 04:53:04.459
    Dec 30 04:53:04.522: INFO: Deleting ReplicationController affinity-nodeport took: 8.026675ms
    Dec 30 04:53:04.623: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.240667ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Dec 30 04:53:06.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-9609" for this suite. 12/30/22 04:53:06.452
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:53:06.461
Dec 30 04:53:06.462: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename crd-publish-openapi 12/30/22 04:53:06.463
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:53:06.477
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:53:06.48
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 12/30/22 04:53:06.483
Dec 30 04:53:06.484: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
Dec 30 04:53:16.819: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Dec 30 04:53:36.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5441" for this suite. 12/30/22 04:53:36.529
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","completed":321,"skipped":5980,"failed":0}
------------------------------
• [SLOW TEST] [30.075 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:53:06.461
    Dec 30 04:53:06.462: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename crd-publish-openapi 12/30/22 04:53:06.463
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:53:06.477
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:53:06.48
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:356
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 12/30/22 04:53:06.483
    Dec 30 04:53:06.484: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    Dec 30 04:53:16.819: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Dec 30 04:53:36.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-5441" for this suite. 12/30/22 04:53:36.529
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:53:36.537
Dec 30 04:53:36.537: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename deployment 12/30/22 04:53:36.539
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:53:36.557
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:53:36.56
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 12/30/22 04:53:36.569
STEP: waiting for Deployment to be created 12/30/22 04:53:36.575
STEP: waiting for all Replicas to be Ready 12/30/22 04:53:36.577
Dec 30 04:53:36.579: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec 30 04:53:36.579: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec 30 04:53:36.588: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec 30 04:53:36.588: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec 30 04:53:36.601: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec 30 04:53:36.601: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec 30 04:53:36.615: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec 30 04:53:36.615: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec 30 04:53:38.182: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Dec 30 04:53:38.182: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Dec 30 04:53:38.483: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 12/30/22 04:53:38.483
W1230 04:53:38.501899      25 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Dec 30 04:53:38.503: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 12/30/22 04:53:38.503
Dec 30 04:53:38.505: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 0
Dec 30 04:53:38.505: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 0
Dec 30 04:53:38.505: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 0
Dec 30 04:53:38.505: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 0
Dec 30 04:53:38.505: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 0
Dec 30 04:53:38.505: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 0
Dec 30 04:53:38.505: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 0
Dec 30 04:53:38.505: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 0
Dec 30 04:53:38.505: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 1
Dec 30 04:53:38.505: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 1
Dec 30 04:53:38.505: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 2
Dec 30 04:53:38.505: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 2
Dec 30 04:53:38.505: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 2
Dec 30 04:53:38.505: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 2
Dec 30 04:53:38.517: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 2
Dec 30 04:53:38.517: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 2
Dec 30 04:53:38.535: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 2
Dec 30 04:53:38.535: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 2
Dec 30 04:53:38.545: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 2
Dec 30 04:53:38.545: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 2
Dec 30 04:53:38.558: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 1
Dec 30 04:53:38.558: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 1
Dec 30 04:53:39.496: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 2
Dec 30 04:53:39.496: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 2
Dec 30 04:53:39.526: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 1
STEP: listing Deployments 12/30/22 04:53:39.526
Dec 30 04:53:39.532: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 12/30/22 04:53:39.532
Dec 30 04:53:39.546: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 12/30/22 04:53:39.546
Dec 30 04:53:39.556: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Dec 30 04:53:39.557: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Dec 30 04:53:39.576: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Dec 30 04:53:39.587: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Dec 30 04:53:39.597: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Dec 30 04:53:41.228: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Dec 30 04:53:41.248: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Dec 30 04:53:41.262: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Dec 30 04:53:42.392: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 12/30/22 04:53:42.413
STEP: fetching the DeploymentStatus 12/30/22 04:53:42.424
Dec 30 04:53:42.431: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 1
Dec 30 04:53:42.431: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 1
Dec 30 04:53:42.431: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 1
Dec 30 04:53:42.431: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 1
Dec 30 04:53:42.431: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 1
Dec 30 04:53:42.431: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 2
Dec 30 04:53:42.431: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 2
Dec 30 04:53:42.432: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 2
Dec 30 04:53:42.432: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 3
STEP: deleting the Deployment 12/30/22 04:53:42.432
Dec 30 04:53:42.443: INFO: observed event type MODIFIED
Dec 30 04:53:42.443: INFO: observed event type MODIFIED
Dec 30 04:53:42.443: INFO: observed event type MODIFIED
Dec 30 04:53:42.443: INFO: observed event type MODIFIED
Dec 30 04:53:42.443: INFO: observed event type MODIFIED
Dec 30 04:53:42.444: INFO: observed event type MODIFIED
Dec 30 04:53:42.444: INFO: observed event type MODIFIED
Dec 30 04:53:42.444: INFO: observed event type MODIFIED
Dec 30 04:53:42.444: INFO: observed event type MODIFIED
Dec 30 04:53:42.444: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Dec 30 04:53:42.448: INFO: Log out all the ReplicaSets if there is no deployment created
Dec 30 04:53:42.452: INFO: ReplicaSet "test-deployment-54cc775c4b":
&ReplicaSet{ObjectMeta:{test-deployment-54cc775c4b  deployment-944  960b5e53-9a68-4c2f-af2e-3965a38c108c 460122 4 2022-12-30 04:53:38 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 33297eca-80c8-4a22-901c-8e9d41dd81bb 0xc005dd0d27 0xc005dd0d28}] [] [{kube-controller-manager Update apps/v1 2022-12-30 04:53:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"33297eca-80c8-4a22-901c-8e9d41dd81bb\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-30 04:53:42 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 54cc775c4b,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005dd0db0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Dec 30 04:53:42.458: INFO: pod: "test-deployment-54cc775c4b-45tvq":
&Pod{ObjectMeta:{test-deployment-54cc775c4b-45tvq test-deployment-54cc775c4b- deployment-944  edc5945b-4c7a-4474-8fbe-101ac1761b88 460118 0 2022-12-30 04:53:38 +0000 UTC 2022-12-30 04:53:43 +0000 UTC 0xc0046a4108 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[cni.projectcalico.org/containerID:84252fe01a8ab12a47e5995386687c508ea777cdc711a06993cf330de4c0fb31 cni.projectcalico.org/podIP:10.233.112.140/32 cni.projectcalico.org/podIPs:10.233.112.140/32] [{apps/v1 ReplicaSet test-deployment-54cc775c4b 960b5e53-9a68-4c2f-af2e-3965a38c108c 0xc0046a4157 0xc0046a4158}] [] [{kube-controller-manager Update v1 2022-12-30 04:53:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"960b5e53-9a68-4c2f-af2e-3965a38c108c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-12-30 04:53:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-30 04:53:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.112.140\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-skng7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-skng7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-mgmt01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 04:53:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 04:53:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 04:53:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 04:53:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.140,PodIP:10.233.112.140,StartTime:2022-12-30 04:53:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-30 04:53:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d,ContainerID:containerd://d8fd3816cd117fa0abfcd4081b58b4d7867ff3e5629945940cc781fe9735d898,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.112.140,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Dec 30 04:53:42.459: INFO: pod: "test-deployment-54cc775c4b-f2jlx":
&Pod{ObjectMeta:{test-deployment-54cc775c4b-f2jlx test-deployment-54cc775c4b- deployment-944  97fdf628-6a21-4112-ab48-c6526fbf2a17 460093 0 2022-12-30 04:53:39 +0000 UTC 2022-12-30 04:53:42 +0000 UTC 0xc0046a43a0 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[cni.projectcalico.org/containerID:1c5059d79ccab41be6ca7102a944fc2e79208c39bd7ea23a64ce7e4be1e0ef4d cni.projectcalico.org/podIP:10.233.125.237/32 cni.projectcalico.org/podIPs:10.233.125.237/32] [{apps/v1 ReplicaSet test-deployment-54cc775c4b 960b5e53-9a68-4c2f-af2e-3965a38c108c 0xc0046a43d7 0xc0046a43d8}] [] [{kube-controller-manager Update v1 2022-12-30 04:53:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"960b5e53-9a68-4c2f-af2e-3965a38c108c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-12-30 04:53:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-30 04:53:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.125.237\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l6fj6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l6fj6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-mgmt02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 04:53:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 04:53:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 04:53:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 04:53:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.141,PodIP:10.233.125.237,StartTime:2022-12-30 04:53:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-30 04:53:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d,ContainerID:containerd://4906124d0cbbc5791836576eee1d17cf0bf0b4cf630c1d16cdc69e2b583b297b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.125.237,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Dec 30 04:53:42.459: INFO: ReplicaSet "test-deployment-7c7d8d58c8":
&ReplicaSet{ObjectMeta:{test-deployment-7c7d8d58c8  deployment-944  5034f083-c01b-4dfb-9dd4-ed330b66ca5e 460114 2 2022-12-30 04:53:39 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 33297eca-80c8-4a22-901c-8e9d41dd81bb 0xc005dd0e17 0xc005dd0e18}] [] [{kube-controller-manager Update apps/v1 2022-12-30 04:53:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"33297eca-80c8-4a22-901c-8e9d41dd81bb\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-30 04:53:42 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7c7d8d58c8,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005dd0ea0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Dec 30 04:53:42.464: INFO: pod: "test-deployment-7c7d8d58c8-6hlml":
&Pod{ObjectMeta:{test-deployment-7c7d8d58c8-6hlml test-deployment-7c7d8d58c8- deployment-944  c4abc94f-11a8-44fe-b9a6-1ad3e769f010 460072 0 2022-12-30 04:53:39 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[cni.projectcalico.org/containerID:c72491453f3f610dd35cda6f9947001301445e7c68ea80ed7aebce05ec9f821c cni.projectcalico.org/podIP:10.233.78.169/32 cni.projectcalico.org/podIPs:10.233.78.169/32] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 5034f083-c01b-4dfb-9dd4-ed330b66ca5e 0xc0046a5c47 0xc0046a5c48}] [] [{kube-controller-manager Update v1 2022-12-30 04:53:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5034f083-c01b-4dfb-9dd4-ed330b66ca5e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-12-30 04:53:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-30 04:53:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.78.169\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f89hv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f89hv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-mgmt03,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 04:53:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 04:53:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 04:53:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 04:53:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.142,PodIP:10.233.78.169,StartTime:2022-12-30 04:53:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-30 04:53:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://ef929d40da688810e98d8fb6e20ab400582b9d1ec3236e8c768385fd7dd39021,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.78.169,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Dec 30 04:53:42.464: INFO: pod: "test-deployment-7c7d8d58c8-lj6m2":
&Pod{ObjectMeta:{test-deployment-7c7d8d58c8-lj6m2 test-deployment-7c7d8d58c8- deployment-944  1340a3c6-1cb1-4cf4-b66f-99dd3f25971d 460113 0 2022-12-30 04:53:41 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[cni.projectcalico.org/containerID:81ab35c22493794e1f64e4d74f33638d9595559a07262fc3d18feff491b559af cni.projectcalico.org/podIP:10.233.125.219/32 cni.projectcalico.org/podIPs:10.233.125.219/32] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 5034f083-c01b-4dfb-9dd4-ed330b66ca5e 0xc0046a5eb7 0xc0046a5eb8}] [] [{Go-http-client Update v1 2022-12-30 04:53:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-12-30 04:53:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5034f083-c01b-4dfb-9dd4-ed330b66ca5e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-30 04:53:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.125.219\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dp94j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dp94j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-mgmt02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 04:53:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 04:53:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 04:53:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 04:53:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.141,PodIP:10.233.125.219,StartTime:2022-12-30 04:53:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-30 04:53:42 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://894f2d591ad6ef4ddebc96108198eeffb8d5421bd4de8aa6020f1e20aa7c2d98,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.125.219,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Dec 30 04:53:42.464: INFO: ReplicaSet "test-deployment-8594bb6fdd":
&ReplicaSet{ObjectMeta:{test-deployment-8594bb6fdd  deployment-944  b4973cf9-3458-413d-85d5-953c5de7627f 460018 3 2022-12-30 04:53:36 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 33297eca-80c8-4a22-901c-8e9d41dd81bb 0xc005dd0f47 0xc005dd0f48}] [] [{kube-controller-manager Update apps/v1 2022-12-30 04:53:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"33297eca-80c8-4a22-901c-8e9d41dd81bb\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-30 04:53:39 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 8594bb6fdd,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005dd0ff0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Dec 30 04:53:42.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-944" for this suite. 12/30/22 04:53:42.475
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","completed":322,"skipped":5984,"failed":0}
------------------------------
• [SLOW TEST] [5.944 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:53:36.537
    Dec 30 04:53:36.537: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename deployment 12/30/22 04:53:36.539
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:53:36.557
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:53:36.56
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 12/30/22 04:53:36.569
    STEP: waiting for Deployment to be created 12/30/22 04:53:36.575
    STEP: waiting for all Replicas to be Ready 12/30/22 04:53:36.577
    Dec 30 04:53:36.579: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Dec 30 04:53:36.579: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Dec 30 04:53:36.588: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Dec 30 04:53:36.588: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Dec 30 04:53:36.601: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Dec 30 04:53:36.601: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Dec 30 04:53:36.615: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Dec 30 04:53:36.615: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Dec 30 04:53:38.182: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Dec 30 04:53:38.182: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Dec 30 04:53:38.483: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 12/30/22 04:53:38.483
    W1230 04:53:38.501899      25 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Dec 30 04:53:38.503: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 12/30/22 04:53:38.503
    Dec 30 04:53:38.505: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 0
    Dec 30 04:53:38.505: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 0
    Dec 30 04:53:38.505: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 0
    Dec 30 04:53:38.505: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 0
    Dec 30 04:53:38.505: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 0
    Dec 30 04:53:38.505: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 0
    Dec 30 04:53:38.505: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 0
    Dec 30 04:53:38.505: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 0
    Dec 30 04:53:38.505: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 1
    Dec 30 04:53:38.505: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 1
    Dec 30 04:53:38.505: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 2
    Dec 30 04:53:38.505: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 2
    Dec 30 04:53:38.505: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 2
    Dec 30 04:53:38.505: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 2
    Dec 30 04:53:38.517: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 2
    Dec 30 04:53:38.517: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 2
    Dec 30 04:53:38.535: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 2
    Dec 30 04:53:38.535: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 2
    Dec 30 04:53:38.545: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 2
    Dec 30 04:53:38.545: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 2
    Dec 30 04:53:38.558: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 1
    Dec 30 04:53:38.558: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 1
    Dec 30 04:53:39.496: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 2
    Dec 30 04:53:39.496: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 2
    Dec 30 04:53:39.526: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 1
    STEP: listing Deployments 12/30/22 04:53:39.526
    Dec 30 04:53:39.532: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 12/30/22 04:53:39.532
    Dec 30 04:53:39.546: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 12/30/22 04:53:39.546
    Dec 30 04:53:39.556: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Dec 30 04:53:39.557: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Dec 30 04:53:39.576: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Dec 30 04:53:39.587: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Dec 30 04:53:39.597: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Dec 30 04:53:41.228: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Dec 30 04:53:41.248: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Dec 30 04:53:41.262: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Dec 30 04:53:42.392: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 12/30/22 04:53:42.413
    STEP: fetching the DeploymentStatus 12/30/22 04:53:42.424
    Dec 30 04:53:42.431: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 1
    Dec 30 04:53:42.431: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 1
    Dec 30 04:53:42.431: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 1
    Dec 30 04:53:42.431: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 1
    Dec 30 04:53:42.431: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 1
    Dec 30 04:53:42.431: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 2
    Dec 30 04:53:42.431: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 2
    Dec 30 04:53:42.432: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 2
    Dec 30 04:53:42.432: INFO: observed Deployment test-deployment in namespace deployment-944 with ReadyReplicas 3
    STEP: deleting the Deployment 12/30/22 04:53:42.432
    Dec 30 04:53:42.443: INFO: observed event type MODIFIED
    Dec 30 04:53:42.443: INFO: observed event type MODIFIED
    Dec 30 04:53:42.443: INFO: observed event type MODIFIED
    Dec 30 04:53:42.443: INFO: observed event type MODIFIED
    Dec 30 04:53:42.443: INFO: observed event type MODIFIED
    Dec 30 04:53:42.444: INFO: observed event type MODIFIED
    Dec 30 04:53:42.444: INFO: observed event type MODIFIED
    Dec 30 04:53:42.444: INFO: observed event type MODIFIED
    Dec 30 04:53:42.444: INFO: observed event type MODIFIED
    Dec 30 04:53:42.444: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Dec 30 04:53:42.448: INFO: Log out all the ReplicaSets if there is no deployment created
    Dec 30 04:53:42.452: INFO: ReplicaSet "test-deployment-54cc775c4b":
    &ReplicaSet{ObjectMeta:{test-deployment-54cc775c4b  deployment-944  960b5e53-9a68-4c2f-af2e-3965a38c108c 460122 4 2022-12-30 04:53:38 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 33297eca-80c8-4a22-901c-8e9d41dd81bb 0xc005dd0d27 0xc005dd0d28}] [] [{kube-controller-manager Update apps/v1 2022-12-30 04:53:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"33297eca-80c8-4a22-901c-8e9d41dd81bb\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-30 04:53:42 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 54cc775c4b,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005dd0db0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    Dec 30 04:53:42.458: INFO: pod: "test-deployment-54cc775c4b-45tvq":
    &Pod{ObjectMeta:{test-deployment-54cc775c4b-45tvq test-deployment-54cc775c4b- deployment-944  edc5945b-4c7a-4474-8fbe-101ac1761b88 460118 0 2022-12-30 04:53:38 +0000 UTC 2022-12-30 04:53:43 +0000 UTC 0xc0046a4108 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[cni.projectcalico.org/containerID:84252fe01a8ab12a47e5995386687c508ea777cdc711a06993cf330de4c0fb31 cni.projectcalico.org/podIP:10.233.112.140/32 cni.projectcalico.org/podIPs:10.233.112.140/32] [{apps/v1 ReplicaSet test-deployment-54cc775c4b 960b5e53-9a68-4c2f-af2e-3965a38c108c 0xc0046a4157 0xc0046a4158}] [] [{kube-controller-manager Update v1 2022-12-30 04:53:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"960b5e53-9a68-4c2f-af2e-3965a38c108c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-12-30 04:53:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-30 04:53:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.112.140\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-skng7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-skng7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-mgmt01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 04:53:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 04:53:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 04:53:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 04:53:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.140,PodIP:10.233.112.140,StartTime:2022-12-30 04:53:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-30 04:53:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d,ContainerID:containerd://d8fd3816cd117fa0abfcd4081b58b4d7867ff3e5629945940cc781fe9735d898,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.112.140,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Dec 30 04:53:42.459: INFO: pod: "test-deployment-54cc775c4b-f2jlx":
    &Pod{ObjectMeta:{test-deployment-54cc775c4b-f2jlx test-deployment-54cc775c4b- deployment-944  97fdf628-6a21-4112-ab48-c6526fbf2a17 460093 0 2022-12-30 04:53:39 +0000 UTC 2022-12-30 04:53:42 +0000 UTC 0xc0046a43a0 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[cni.projectcalico.org/containerID:1c5059d79ccab41be6ca7102a944fc2e79208c39bd7ea23a64ce7e4be1e0ef4d cni.projectcalico.org/podIP:10.233.125.237/32 cni.projectcalico.org/podIPs:10.233.125.237/32] [{apps/v1 ReplicaSet test-deployment-54cc775c4b 960b5e53-9a68-4c2f-af2e-3965a38c108c 0xc0046a43d7 0xc0046a43d8}] [] [{kube-controller-manager Update v1 2022-12-30 04:53:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"960b5e53-9a68-4c2f-af2e-3965a38c108c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-12-30 04:53:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-30 04:53:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.125.237\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l6fj6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l6fj6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-mgmt02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 04:53:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 04:53:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 04:53:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 04:53:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.141,PodIP:10.233.125.237,StartTime:2022-12-30 04:53:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-30 04:53:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d,ContainerID:containerd://4906124d0cbbc5791836576eee1d17cf0bf0b4cf630c1d16cdc69e2b583b297b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.125.237,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Dec 30 04:53:42.459: INFO: ReplicaSet "test-deployment-7c7d8d58c8":
    &ReplicaSet{ObjectMeta:{test-deployment-7c7d8d58c8  deployment-944  5034f083-c01b-4dfb-9dd4-ed330b66ca5e 460114 2 2022-12-30 04:53:39 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 33297eca-80c8-4a22-901c-8e9d41dd81bb 0xc005dd0e17 0xc005dd0e18}] [] [{kube-controller-manager Update apps/v1 2022-12-30 04:53:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"33297eca-80c8-4a22-901c-8e9d41dd81bb\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-30 04:53:42 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7c7d8d58c8,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005dd0ea0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

    Dec 30 04:53:42.464: INFO: pod: "test-deployment-7c7d8d58c8-6hlml":
    &Pod{ObjectMeta:{test-deployment-7c7d8d58c8-6hlml test-deployment-7c7d8d58c8- deployment-944  c4abc94f-11a8-44fe-b9a6-1ad3e769f010 460072 0 2022-12-30 04:53:39 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[cni.projectcalico.org/containerID:c72491453f3f610dd35cda6f9947001301445e7c68ea80ed7aebce05ec9f821c cni.projectcalico.org/podIP:10.233.78.169/32 cni.projectcalico.org/podIPs:10.233.78.169/32] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 5034f083-c01b-4dfb-9dd4-ed330b66ca5e 0xc0046a5c47 0xc0046a5c48}] [] [{kube-controller-manager Update v1 2022-12-30 04:53:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5034f083-c01b-4dfb-9dd4-ed330b66ca5e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2022-12-30 04:53:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-12-30 04:53:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.78.169\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f89hv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f89hv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-mgmt03,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 04:53:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 04:53:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 04:53:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 04:53:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.142,PodIP:10.233.78.169,StartTime:2022-12-30 04:53:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-30 04:53:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://ef929d40da688810e98d8fb6e20ab400582b9d1ec3236e8c768385fd7dd39021,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.78.169,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Dec 30 04:53:42.464: INFO: pod: "test-deployment-7c7d8d58c8-lj6m2":
    &Pod{ObjectMeta:{test-deployment-7c7d8d58c8-lj6m2 test-deployment-7c7d8d58c8- deployment-944  1340a3c6-1cb1-4cf4-b66f-99dd3f25971d 460113 0 2022-12-30 04:53:41 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[cni.projectcalico.org/containerID:81ab35c22493794e1f64e4d74f33638d9595559a07262fc3d18feff491b559af cni.projectcalico.org/podIP:10.233.125.219/32 cni.projectcalico.org/podIPs:10.233.125.219/32] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 5034f083-c01b-4dfb-9dd4-ed330b66ca5e 0xc0046a5eb7 0xc0046a5eb8}] [] [{Go-http-client Update v1 2022-12-30 04:53:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-12-30 04:53:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5034f083-c01b-4dfb-9dd4-ed330b66ca5e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-30 04:53:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.125.219\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dp94j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dp94j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-mgmt02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 04:53:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 04:53:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 04:53:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 04:53:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.141,PodIP:10.233.125.219,StartTime:2022-12-30 04:53:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-30 04:53:42 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://894f2d591ad6ef4ddebc96108198eeffb8d5421bd4de8aa6020f1e20aa7c2d98,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.125.219,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Dec 30 04:53:42.464: INFO: ReplicaSet "test-deployment-8594bb6fdd":
    &ReplicaSet{ObjectMeta:{test-deployment-8594bb6fdd  deployment-944  b4973cf9-3458-413d-85d5-953c5de7627f 460018 3 2022-12-30 04:53:36 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 33297eca-80c8-4a22-901c-8e9d41dd81bb 0xc005dd0f47 0xc005dd0f48}] [] [{kube-controller-manager Update apps/v1 2022-12-30 04:53:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"33297eca-80c8-4a22-901c-8e9d41dd81bb\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-30 04:53:39 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 8594bb6fdd,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005dd0ff0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Dec 30 04:53:42.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-944" for this suite. 12/30/22 04:53:42.475
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:53:42.484
Dec 30 04:53:42.485: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename var-expansion 12/30/22 04:53:42.486
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:53:42.503
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:53:42.507
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111
STEP: Creating a pod to test substitution in volume subpath 12/30/22 04:53:42.51
Dec 30 04:53:42.520: INFO: Waiting up to 5m0s for pod "var-expansion-a020f030-4571-4a6f-89bd-f036f03c1951" in namespace "var-expansion-767" to be "Succeeded or Failed"
Dec 30 04:53:42.524: INFO: Pod "var-expansion-a020f030-4571-4a6f-89bd-f036f03c1951": Phase="Pending", Reason="", readiness=false. Elapsed: 3.488465ms
Dec 30 04:53:44.529: INFO: Pod "var-expansion-a020f030-4571-4a6f-89bd-f036f03c1951": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008513638s
Dec 30 04:53:46.530: INFO: Pod "var-expansion-a020f030-4571-4a6f-89bd-f036f03c1951": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009876413s
STEP: Saw pod success 12/30/22 04:53:46.53
Dec 30 04:53:46.530: INFO: Pod "var-expansion-a020f030-4571-4a6f-89bd-f036f03c1951" satisfied condition "Succeeded or Failed"
Dec 30 04:53:46.534: INFO: Trying to get logs from node k8s-mgmt01 pod var-expansion-a020f030-4571-4a6f-89bd-f036f03c1951 container dapi-container: <nil>
STEP: delete the pod 12/30/22 04:53:46.556
Dec 30 04:53:46.569: INFO: Waiting for pod var-expansion-a020f030-4571-4a6f-89bd-f036f03c1951 to disappear
Dec 30 04:53:46.573: INFO: Pod var-expansion-a020f030-4571-4a6f-89bd-f036f03c1951 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Dec 30 04:53:46.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-767" for this suite. 12/30/22 04:53:46.579
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","completed":323,"skipped":6013,"failed":0}
------------------------------
• [4.101 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:53:42.484
    Dec 30 04:53:42.485: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename var-expansion 12/30/22 04:53:42.486
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:53:42.503
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:53:42.507
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:111
    STEP: Creating a pod to test substitution in volume subpath 12/30/22 04:53:42.51
    Dec 30 04:53:42.520: INFO: Waiting up to 5m0s for pod "var-expansion-a020f030-4571-4a6f-89bd-f036f03c1951" in namespace "var-expansion-767" to be "Succeeded or Failed"
    Dec 30 04:53:42.524: INFO: Pod "var-expansion-a020f030-4571-4a6f-89bd-f036f03c1951": Phase="Pending", Reason="", readiness=false. Elapsed: 3.488465ms
    Dec 30 04:53:44.529: INFO: Pod "var-expansion-a020f030-4571-4a6f-89bd-f036f03c1951": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008513638s
    Dec 30 04:53:46.530: INFO: Pod "var-expansion-a020f030-4571-4a6f-89bd-f036f03c1951": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009876413s
    STEP: Saw pod success 12/30/22 04:53:46.53
    Dec 30 04:53:46.530: INFO: Pod "var-expansion-a020f030-4571-4a6f-89bd-f036f03c1951" satisfied condition "Succeeded or Failed"
    Dec 30 04:53:46.534: INFO: Trying to get logs from node k8s-mgmt01 pod var-expansion-a020f030-4571-4a6f-89bd-f036f03c1951 container dapi-container: <nil>
    STEP: delete the pod 12/30/22 04:53:46.556
    Dec 30 04:53:46.569: INFO: Waiting for pod var-expansion-a020f030-4571-4a6f-89bd-f036f03c1951 to disappear
    Dec 30 04:53:46.573: INFO: Pod var-expansion-a020f030-4571-4a6f-89bd-f036f03c1951 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Dec 30 04:53:46.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-767" for this suite. 12/30/22 04:53:46.579
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:53:46.586
Dec 30 04:53:46.586: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename kubelet-test 12/30/22 04:53:46.588
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:53:46.605
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:53:46.608
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 12/30/22 04:53:46.621
Dec 30 04:53:46.621: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases61417e38-6bd1-4298-9215-dfe5c3f3e755" in namespace "kubelet-test-3808" to be "completed"
Dec 30 04:53:46.626: INFO: Pod "agnhost-host-aliases61417e38-6bd1-4298-9215-dfe5c3f3e755": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006421ms
Dec 30 04:53:48.631: INFO: Pod "agnhost-host-aliases61417e38-6bd1-4298-9215-dfe5c3f3e755": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009501675s
Dec 30 04:53:50.631: INFO: Pod "agnhost-host-aliases61417e38-6bd1-4298-9215-dfe5c3f3e755": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009470389s
Dec 30 04:53:50.631: INFO: Pod "agnhost-host-aliases61417e38-6bd1-4298-9215-dfe5c3f3e755" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Dec 30 04:53:50.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3808" for this suite. 12/30/22 04:53:50.646
{"msg":"PASSED [sig-node] Kubelet when scheduling an agnhost Pod with hostAliases should write entries to /etc/hosts [NodeConformance] [Conformance]","completed":324,"skipped":6019,"failed":0}
------------------------------
• [4.067 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:53:46.586
    Dec 30 04:53:46.586: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename kubelet-test 12/30/22 04:53:46.588
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:53:46.605
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:53:46.608
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 12/30/22 04:53:46.621
    Dec 30 04:53:46.621: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases61417e38-6bd1-4298-9215-dfe5c3f3e755" in namespace "kubelet-test-3808" to be "completed"
    Dec 30 04:53:46.626: INFO: Pod "agnhost-host-aliases61417e38-6bd1-4298-9215-dfe5c3f3e755": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006421ms
    Dec 30 04:53:48.631: INFO: Pod "agnhost-host-aliases61417e38-6bd1-4298-9215-dfe5c3f3e755": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009501675s
    Dec 30 04:53:50.631: INFO: Pod "agnhost-host-aliases61417e38-6bd1-4298-9215-dfe5c3f3e755": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009470389s
    Dec 30 04:53:50.631: INFO: Pod "agnhost-host-aliases61417e38-6bd1-4298-9215-dfe5c3f3e755" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Dec 30 04:53:50.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-3808" for this suite. 12/30/22 04:53:50.646
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:53:50.655
Dec 30 04:53:50.656: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename configmap 12/30/22 04:53:50.657
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:53:50.674
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:53:50.678
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46
STEP: Creating configMap with name configmap-test-volume-d98c9ea7-1eff-484b-8652-9fa04ed766aa 12/30/22 04:53:50.681
STEP: Creating a pod to test consume configMaps 12/30/22 04:53:50.686
Dec 30 04:53:50.696: INFO: Waiting up to 5m0s for pod "pod-configmaps-33e7ed52-28f4-4b62-89d3-ece5486359b7" in namespace "configmap-9483" to be "Succeeded or Failed"
Dec 30 04:53:50.699: INFO: Pod "pod-configmaps-33e7ed52-28f4-4b62-89d3-ece5486359b7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.50209ms
Dec 30 04:53:52.704: INFO: Pod "pod-configmaps-33e7ed52-28f4-4b62-89d3-ece5486359b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00854899s
Dec 30 04:53:54.706: INFO: Pod "pod-configmaps-33e7ed52-28f4-4b62-89d3-ece5486359b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010171966s
STEP: Saw pod success 12/30/22 04:53:54.706
Dec 30 04:53:54.706: INFO: Pod "pod-configmaps-33e7ed52-28f4-4b62-89d3-ece5486359b7" satisfied condition "Succeeded or Failed"
Dec 30 04:53:54.710: INFO: Trying to get logs from node k8s-mgmt01 pod pod-configmaps-33e7ed52-28f4-4b62-89d3-ece5486359b7 container agnhost-container: <nil>
STEP: delete the pod 12/30/22 04:53:54.719
Dec 30 04:53:54.733: INFO: Waiting for pod pod-configmaps-33e7ed52-28f4-4b62-89d3-ece5486359b7 to disappear
Dec 30 04:53:54.736: INFO: Pod pod-configmaps-33e7ed52-28f4-4b62-89d3-ece5486359b7 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Dec 30 04:53:54.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9483" for this suite. 12/30/22 04:53:54.743
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","completed":325,"skipped":6043,"failed":0}
------------------------------
• [4.094 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:53:50.655
    Dec 30 04:53:50.656: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename configmap 12/30/22 04:53:50.657
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:53:50.674
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:53:50.678
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:46
    STEP: Creating configMap with name configmap-test-volume-d98c9ea7-1eff-484b-8652-9fa04ed766aa 12/30/22 04:53:50.681
    STEP: Creating a pod to test consume configMaps 12/30/22 04:53:50.686
    Dec 30 04:53:50.696: INFO: Waiting up to 5m0s for pod "pod-configmaps-33e7ed52-28f4-4b62-89d3-ece5486359b7" in namespace "configmap-9483" to be "Succeeded or Failed"
    Dec 30 04:53:50.699: INFO: Pod "pod-configmaps-33e7ed52-28f4-4b62-89d3-ece5486359b7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.50209ms
    Dec 30 04:53:52.704: INFO: Pod "pod-configmaps-33e7ed52-28f4-4b62-89d3-ece5486359b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00854899s
    Dec 30 04:53:54.706: INFO: Pod "pod-configmaps-33e7ed52-28f4-4b62-89d3-ece5486359b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010171966s
    STEP: Saw pod success 12/30/22 04:53:54.706
    Dec 30 04:53:54.706: INFO: Pod "pod-configmaps-33e7ed52-28f4-4b62-89d3-ece5486359b7" satisfied condition "Succeeded or Failed"
    Dec 30 04:53:54.710: INFO: Trying to get logs from node k8s-mgmt01 pod pod-configmaps-33e7ed52-28f4-4b62-89d3-ece5486359b7 container agnhost-container: <nil>
    STEP: delete the pod 12/30/22 04:53:54.719
    Dec 30 04:53:54.733: INFO: Waiting for pod pod-configmaps-33e7ed52-28f4-4b62-89d3-ece5486359b7 to disappear
    Dec 30 04:53:54.736: INFO: Pod pod-configmaps-33e7ed52-28f4-4b62-89d3-ece5486359b7 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Dec 30 04:53:54.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-9483" for this suite. 12/30/22 04:53:54.743
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:53:54.751
Dec 30 04:53:54.751: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename resourcequota 12/30/22 04:53:54.753
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:53:54.771
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:53:54.774
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90
STEP: Counting existing ResourceQuota 12/30/22 04:53:54.777
STEP: Creating a ResourceQuota 12/30/22 04:53:59.783
STEP: Ensuring resource quota status is calculated 12/30/22 04:53:59.789
STEP: Creating a Service 12/30/22 04:54:01.794
STEP: Creating a NodePort Service 12/30/22 04:54:01.815
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 12/30/22 04:54:01.837
STEP: Ensuring resource quota status captures service creation 12/30/22 04:54:01.857
STEP: Deleting Services 12/30/22 04:54:03.863
STEP: Ensuring resource quota status released usage 12/30/22 04:54:03.895
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Dec 30 04:54:05.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6108" for this suite. 12/30/22 04:54:05.908
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","completed":326,"skipped":6067,"failed":0}
------------------------------
• [SLOW TEST] [11.163 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:53:54.751
    Dec 30 04:53:54.751: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename resourcequota 12/30/22 04:53:54.753
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:53:54.771
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:53:54.774
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:90
    STEP: Counting existing ResourceQuota 12/30/22 04:53:54.777
    STEP: Creating a ResourceQuota 12/30/22 04:53:59.783
    STEP: Ensuring resource quota status is calculated 12/30/22 04:53:59.789
    STEP: Creating a Service 12/30/22 04:54:01.794
    STEP: Creating a NodePort Service 12/30/22 04:54:01.815
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 12/30/22 04:54:01.837
    STEP: Ensuring resource quota status captures service creation 12/30/22 04:54:01.857
    STEP: Deleting Services 12/30/22 04:54:03.863
    STEP: Ensuring resource quota status released usage 12/30/22 04:54:03.895
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Dec 30 04:54:05.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-6108" for this suite. 12/30/22 04:54:05.908
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:54:05.917
Dec 30 04:54:05.917: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename kubelet-test 12/30/22 04:54:05.919
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:54:05.937
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:54:05.94
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
Dec 30 04:54:05.958: INFO: Waiting up to 5m0s for pod "busybox-scheduling-939832d8-4229-4e25-8a86-7bf53be24ee7" in namespace "kubelet-test-575" to be "running and ready"
Dec 30 04:54:05.962: INFO: Pod "busybox-scheduling-939832d8-4229-4e25-8a86-7bf53be24ee7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.264434ms
Dec 30 04:54:05.962: INFO: The phase of Pod busybox-scheduling-939832d8-4229-4e25-8a86-7bf53be24ee7 is Pending, waiting for it to be Running (with Ready = true)
Dec 30 04:54:07.967: INFO: Pod "busybox-scheduling-939832d8-4229-4e25-8a86-7bf53be24ee7": Phase="Running", Reason="", readiness=true. Elapsed: 2.009441712s
Dec 30 04:54:07.967: INFO: The phase of Pod busybox-scheduling-939832d8-4229-4e25-8a86-7bf53be24ee7 is Running (Ready = true)
Dec 30 04:54:07.967: INFO: Pod "busybox-scheduling-939832d8-4229-4e25-8a86-7bf53be24ee7" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Dec 30 04:54:07.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-575" for this suite. 12/30/22 04:54:07.985
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","completed":327,"skipped":6088,"failed":0}
------------------------------
• [2.075 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:54:05.917
    Dec 30 04:54:05.917: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename kubelet-test 12/30/22 04:54:05.919
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:54:05.937
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:54:05.94
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    Dec 30 04:54:05.958: INFO: Waiting up to 5m0s for pod "busybox-scheduling-939832d8-4229-4e25-8a86-7bf53be24ee7" in namespace "kubelet-test-575" to be "running and ready"
    Dec 30 04:54:05.962: INFO: Pod "busybox-scheduling-939832d8-4229-4e25-8a86-7bf53be24ee7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.264434ms
    Dec 30 04:54:05.962: INFO: The phase of Pod busybox-scheduling-939832d8-4229-4e25-8a86-7bf53be24ee7 is Pending, waiting for it to be Running (with Ready = true)
    Dec 30 04:54:07.967: INFO: Pod "busybox-scheduling-939832d8-4229-4e25-8a86-7bf53be24ee7": Phase="Running", Reason="", readiness=true. Elapsed: 2.009441712s
    Dec 30 04:54:07.967: INFO: The phase of Pod busybox-scheduling-939832d8-4229-4e25-8a86-7bf53be24ee7 is Running (Ready = true)
    Dec 30 04:54:07.967: INFO: Pod "busybox-scheduling-939832d8-4229-4e25-8a86-7bf53be24ee7" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Dec 30 04:54:07.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-575" for this suite. 12/30/22 04:54:07.985
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:54:07.994
Dec 30 04:54:07.994: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename csistoragecapacity 12/30/22 04:54:07.995
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:54:08.012
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:54:08.015
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 12/30/22 04:54:08.019
STEP: getting /apis/storage.k8s.io 12/30/22 04:54:08.022
STEP: getting /apis/storage.k8s.io/v1 12/30/22 04:54:08.023
STEP: creating 12/30/22 04:54:08.024
STEP: watching 12/30/22 04:54:08.041
Dec 30 04:54:08.041: INFO: starting watch
STEP: getting 12/30/22 04:54:08.049
STEP: listing in namespace 12/30/22 04:54:08.053
STEP: listing across namespaces 12/30/22 04:54:08.057
STEP: patching 12/30/22 04:54:08.06
STEP: updating 12/30/22 04:54:08.065
Dec 30 04:54:08.071: INFO: waiting for watch events with expected annotations in namespace
Dec 30 04:54:08.071: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 12/30/22 04:54:08.071
STEP: deleting a collection 12/30/22 04:54:08.084
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:187
Dec 30 04:54:08.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "csistoragecapacity-319" for this suite. 12/30/22 04:54:08.107
{"msg":"PASSED [sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]","completed":328,"skipped":6098,"failed":0}
------------------------------
• [0.120 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:54:07.994
    Dec 30 04:54:07.994: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename csistoragecapacity 12/30/22 04:54:07.995
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:54:08.012
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:54:08.015
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 12/30/22 04:54:08.019
    STEP: getting /apis/storage.k8s.io 12/30/22 04:54:08.022
    STEP: getting /apis/storage.k8s.io/v1 12/30/22 04:54:08.023
    STEP: creating 12/30/22 04:54:08.024
    STEP: watching 12/30/22 04:54:08.041
    Dec 30 04:54:08.041: INFO: starting watch
    STEP: getting 12/30/22 04:54:08.049
    STEP: listing in namespace 12/30/22 04:54:08.053
    STEP: listing across namespaces 12/30/22 04:54:08.057
    STEP: patching 12/30/22 04:54:08.06
    STEP: updating 12/30/22 04:54:08.065
    Dec 30 04:54:08.071: INFO: waiting for watch events with expected annotations in namespace
    Dec 30 04:54:08.071: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 12/30/22 04:54:08.071
    STEP: deleting a collection 12/30/22 04:54:08.084
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/framework.go:187
    Dec 30 04:54:08.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "csistoragecapacity-319" for this suite. 12/30/22 04:54:08.107
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:54:08.116
Dec 30 04:54:08.116: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename containers 12/30/22 04:54:08.118
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:54:08.133
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:54:08.137
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86
STEP: Creating a pod to test override all 12/30/22 04:54:08.141
Dec 30 04:54:08.150: INFO: Waiting up to 5m0s for pod "client-containers-65bbf217-8762-4e52-a94d-bbdd1a53f866" in namespace "containers-3320" to be "Succeeded or Failed"
Dec 30 04:54:08.153: INFO: Pod "client-containers-65bbf217-8762-4e52-a94d-bbdd1a53f866": Phase="Pending", Reason="", readiness=false. Elapsed: 3.252509ms
Dec 30 04:54:10.158: INFO: Pod "client-containers-65bbf217-8762-4e52-a94d-bbdd1a53f866": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008498017s
Dec 30 04:54:12.158: INFO: Pod "client-containers-65bbf217-8762-4e52-a94d-bbdd1a53f866": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008165251s
STEP: Saw pod success 12/30/22 04:54:12.158
Dec 30 04:54:12.158: INFO: Pod "client-containers-65bbf217-8762-4e52-a94d-bbdd1a53f866" satisfied condition "Succeeded or Failed"
Dec 30 04:54:12.163: INFO: Trying to get logs from node k8s-mgmt01 pod client-containers-65bbf217-8762-4e52-a94d-bbdd1a53f866 container agnhost-container: <nil>
STEP: delete the pod 12/30/22 04:54:12.172
Dec 30 04:54:12.185: INFO: Waiting for pod client-containers-65bbf217-8762-4e52-a94d-bbdd1a53f866 to disappear
Dec 30 04:54:12.189: INFO: Pod client-containers-65bbf217-8762-4e52-a94d-bbdd1a53f866 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Dec 30 04:54:12.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3320" for this suite. 12/30/22 04:54:12.195
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","completed":329,"skipped":6125,"failed":0}
------------------------------
• [4.085 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:54:08.116
    Dec 30 04:54:08.116: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename containers 12/30/22 04:54:08.118
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:54:08.133
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:54:08.137
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:86
    STEP: Creating a pod to test override all 12/30/22 04:54:08.141
    Dec 30 04:54:08.150: INFO: Waiting up to 5m0s for pod "client-containers-65bbf217-8762-4e52-a94d-bbdd1a53f866" in namespace "containers-3320" to be "Succeeded or Failed"
    Dec 30 04:54:08.153: INFO: Pod "client-containers-65bbf217-8762-4e52-a94d-bbdd1a53f866": Phase="Pending", Reason="", readiness=false. Elapsed: 3.252509ms
    Dec 30 04:54:10.158: INFO: Pod "client-containers-65bbf217-8762-4e52-a94d-bbdd1a53f866": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008498017s
    Dec 30 04:54:12.158: INFO: Pod "client-containers-65bbf217-8762-4e52-a94d-bbdd1a53f866": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008165251s
    STEP: Saw pod success 12/30/22 04:54:12.158
    Dec 30 04:54:12.158: INFO: Pod "client-containers-65bbf217-8762-4e52-a94d-bbdd1a53f866" satisfied condition "Succeeded or Failed"
    Dec 30 04:54:12.163: INFO: Trying to get logs from node k8s-mgmt01 pod client-containers-65bbf217-8762-4e52-a94d-bbdd1a53f866 container agnhost-container: <nil>
    STEP: delete the pod 12/30/22 04:54:12.172
    Dec 30 04:54:12.185: INFO: Waiting for pod client-containers-65bbf217-8762-4e52-a94d-bbdd1a53f866 to disappear
    Dec 30 04:54:12.189: INFO: Pod client-containers-65bbf217-8762-4e52-a94d-bbdd1a53f866 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Dec 30 04:54:12.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-3320" for this suite. 12/30/22 04:54:12.195
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:54:12.203
Dec 30 04:54:12.203: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename disruption 12/30/22 04:54:12.205
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:54:12.22
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:54:12.224
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346
STEP: Creating a pdb that targets all three pods in a test replica set 12/30/22 04:54:12.227
STEP: Waiting for the pdb to be processed 12/30/22 04:54:12.232
STEP: First trying to evict a pod which shouldn't be evictable 12/30/22 04:54:14.249
STEP: Waiting for all pods to be running 12/30/22 04:54:14.249
Dec 30 04:54:14.254: INFO: pods: 0 < 3
STEP: locating a running pod 12/30/22 04:54:16.259
STEP: Updating the pdb to allow a pod to be evicted 12/30/22 04:54:16.272
STEP: Waiting for the pdb to be processed 12/30/22 04:54:16.283
STEP: Trying to evict the same pod we tried earlier which should now be evictable 12/30/22 04:54:18.292
STEP: Waiting for all pods to be running 12/30/22 04:54:18.292
STEP: Waiting for the pdb to observed all healthy pods 12/30/22 04:54:18.297
STEP: Patching the pdb to disallow a pod to be evicted 12/30/22 04:54:18.319
STEP: Waiting for the pdb to be processed 12/30/22 04:54:18.331
STEP: Waiting for all pods to be running 12/30/22 04:54:20.34
STEP: locating a running pod 12/30/22 04:54:20.345
STEP: Deleting the pdb to allow a pod to be evicted 12/30/22 04:54:20.358
STEP: Waiting for the pdb to be deleted 12/30/22 04:54:20.365
STEP: Trying to evict the same pod we tried earlier which should now be evictable 12/30/22 04:54:20.368
STEP: Waiting for all pods to be running 12/30/22 04:54:20.368
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Dec 30 04:54:20.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-6157" for this suite. 12/30/22 04:54:20.392
{"msg":"PASSED [sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]","completed":330,"skipped":6145,"failed":0}
------------------------------
• [SLOW TEST] [8.196 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:54:12.203
    Dec 30 04:54:12.203: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename disruption 12/30/22 04:54:12.205
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:54:12.22
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:54:12.224
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:346
    STEP: Creating a pdb that targets all three pods in a test replica set 12/30/22 04:54:12.227
    STEP: Waiting for the pdb to be processed 12/30/22 04:54:12.232
    STEP: First trying to evict a pod which shouldn't be evictable 12/30/22 04:54:14.249
    STEP: Waiting for all pods to be running 12/30/22 04:54:14.249
    Dec 30 04:54:14.254: INFO: pods: 0 < 3
    STEP: locating a running pod 12/30/22 04:54:16.259
    STEP: Updating the pdb to allow a pod to be evicted 12/30/22 04:54:16.272
    STEP: Waiting for the pdb to be processed 12/30/22 04:54:16.283
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 12/30/22 04:54:18.292
    STEP: Waiting for all pods to be running 12/30/22 04:54:18.292
    STEP: Waiting for the pdb to observed all healthy pods 12/30/22 04:54:18.297
    STEP: Patching the pdb to disallow a pod to be evicted 12/30/22 04:54:18.319
    STEP: Waiting for the pdb to be processed 12/30/22 04:54:18.331
    STEP: Waiting for all pods to be running 12/30/22 04:54:20.34
    STEP: locating a running pod 12/30/22 04:54:20.345
    STEP: Deleting the pdb to allow a pod to be evicted 12/30/22 04:54:20.358
    STEP: Waiting for the pdb to be deleted 12/30/22 04:54:20.365
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 12/30/22 04:54:20.368
    STEP: Waiting for all pods to be running 12/30/22 04:54:20.368
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Dec 30 04:54:20.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-6157" for this suite. 12/30/22 04:54:20.392
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:54:20.4
Dec 30 04:54:20.400: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename init-container 12/30/22 04:54:20.401
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:54:20.417
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:54:20.421
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333
STEP: creating the pod 12/30/22 04:54:20.424
Dec 30 04:54:20.424: INFO: PodSpec: initContainers in spec.initContainers
Dec 30 04:55:01.757: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-09fda0e2-2830-43ea-82a7-ad3d48edfdcf", GenerateName:"", Namespace:"init-container-6823", SelfLink:"", UID:"a0e014ad-c0f2-4817-a9f4-b8eb77f2084a", ResourceVersion:"460780", Generation:0, CreationTimestamp:time.Date(2022, time.December, 30, 4, 54, 20, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"424909719"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"88f66f258ee7ba4ae544b69f29e2e6466de6d7ab114ccfc7440c63f84329bff3", "cni.projectcalico.org/podIP":"10.233.112.148/32", "cni.projectcalico.org/podIPs":"10.233.112.148/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.December, 30, 4, 54, 20, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0069e6f78), Subresource:""}, v1.ManagedFieldsEntry{Manager:"Go-http-client", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.December, 30, 4, 54, 21, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0069e6fa8), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.December, 30, 4, 55, 1, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0069e6fd8), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-bmxm8", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc006ef4a80), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-bmxm8", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-bmxm8", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.8", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-bmxm8", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc007e09c98), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"k8s-mgmt01", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc004024af0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc007e09da0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc007e09dc0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc007e09dc8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc007e09dcc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc000194470), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.December, 30, 4, 54, 20, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.December, 30, 4, 54, 20, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.December, 30, 4, 54, 20, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.December, 30, 4, 54, 20, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.78.26.140", PodIP:"10.233.112.148", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.233.112.148"}}, StartTime:time.Date(2022, time.December, 30, 4, 54, 20, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc004024bd0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc004024c40)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"containerd://e25d0e30f10a23d27cee79ead042e0907589acdc391a0afd718a839a0c00fbf6", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc006ef4b00), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc006ef4ae0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.8", ImageID:"", ContainerID:"", Started:(*bool)(0xc007e09ecf)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Dec 30 04:55:01.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6823" for this suite. 12/30/22 04:55:01.765
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","completed":331,"skipped":6147,"failed":0}
------------------------------
• [SLOW TEST] [41.372 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:54:20.4
    Dec 30 04:54:20.400: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename init-container 12/30/22 04:54:20.401
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:54:20.417
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:54:20.421
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:333
    STEP: creating the pod 12/30/22 04:54:20.424
    Dec 30 04:54:20.424: INFO: PodSpec: initContainers in spec.initContainers
    Dec 30 04:55:01.757: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-09fda0e2-2830-43ea-82a7-ad3d48edfdcf", GenerateName:"", Namespace:"init-container-6823", SelfLink:"", UID:"a0e014ad-c0f2-4817-a9f4-b8eb77f2084a", ResourceVersion:"460780", Generation:0, CreationTimestamp:time.Date(2022, time.December, 30, 4, 54, 20, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"424909719"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"88f66f258ee7ba4ae544b69f29e2e6466de6d7ab114ccfc7440c63f84329bff3", "cni.projectcalico.org/podIP":"10.233.112.148/32", "cni.projectcalico.org/podIPs":"10.233.112.148/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.December, 30, 4, 54, 20, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0069e6f78), Subresource:""}, v1.ManagedFieldsEntry{Manager:"Go-http-client", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.December, 30, 4, 54, 21, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0069e6fa8), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.December, 30, 4, 55, 1, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0069e6fd8), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-bmxm8", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc006ef4a80), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-bmxm8", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-bmxm8", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.8", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-bmxm8", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc007e09c98), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"k8s-mgmt01", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc004024af0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc007e09da0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc007e09dc0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc007e09dc8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc007e09dcc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc000194470), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.December, 30, 4, 54, 20, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.December, 30, 4, 54, 20, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.December, 30, 4, 54, 20, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.December, 30, 4, 54, 20, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.78.26.140", PodIP:"10.233.112.148", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.233.112.148"}}, StartTime:time.Date(2022, time.December, 30, 4, 54, 20, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc004024bd0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc004024c40)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"containerd://e25d0e30f10a23d27cee79ead042e0907589acdc391a0afd718a839a0c00fbf6", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc006ef4b00), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc006ef4ae0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.8", ImageID:"", ContainerID:"", Started:(*bool)(0xc007e09ecf)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Dec 30 04:55:01.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-6823" for this suite. 12/30/22 04:55:01.765
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:55:01.776
Dec 30 04:55:01.776: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename configmap 12/30/22 04:55:01.777
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:55:01.794
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:55:01.798
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Dec 30 04:55:01.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5610" for this suite. 12/30/22 04:55:01.849
{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","completed":332,"skipped":6185,"failed":0}
------------------------------
• [0.080 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:55:01.776
    Dec 30 04:55:01.776: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename configmap 12/30/22 04:55:01.777
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:55:01.794
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:55:01.798
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:503
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Dec 30 04:55:01.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-5610" for this suite. 12/30/22 04:55:01.849
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:304
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:55:01.858
Dec 30 04:55:01.858: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename statefulset 12/30/22 04:55:01.859
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:55:01.875
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:55:01.878
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-7185 12/30/22 04:55:01.882
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:304
STEP: Creating a new StatefulSet 12/30/22 04:55:01.888
Dec 30 04:55:01.898: INFO: Found 0 stateful pods, waiting for 3
Dec 30 04:55:11.909: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 30 04:55:11.909: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 30 04:55:11.909: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Dec 30 04:55:11.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=statefulset-7185 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 30 04:55:12.150: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 30 04:55:12.150: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 30 04:55:12.150: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 12/30/22 04:55:22.17
Dec 30 04:55:22.192: INFO: Updating stateful set ss2
STEP: Creating a new revision 12/30/22 04:55:22.192
STEP: Updating Pods in reverse ordinal order 12/30/22 04:55:32.211
Dec 30 04:55:32.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=statefulset-7185 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 30 04:55:32.421: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 30 04:55:32.421: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 30 04:55:32.421: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision 12/30/22 04:55:42.45
Dec 30 04:55:42.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=statefulset-7185 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 30 04:55:42.672: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 30 04:55:42.672: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 30 04:55:42.672: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 30 04:55:52.715: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 12/30/22 04:56:02.736
Dec 30 04:56:02.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=statefulset-7185 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 30 04:56:02.914: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 30 04:56:02.914: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 30 04:56:02.914: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Dec 30 04:56:12.949: INFO: Deleting all statefulset in ns statefulset-7185
Dec 30 04:56:12.953: INFO: Scaling statefulset ss2 to 0
Dec 30 04:56:22.976: INFO: Waiting for statefulset status.replicas updated to 0
Dec 30 04:56:22.980: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Dec 30 04:56:22.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7185" for this suite. 12/30/22 04:56:23.001
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","completed":333,"skipped":6209,"failed":0}
------------------------------
• [SLOW TEST] [81.150 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:304

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:55:01.858
    Dec 30 04:55:01.858: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename statefulset 12/30/22 04:55:01.859
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:55:01.875
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:55:01.878
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-7185 12/30/22 04:55:01.882
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:304
    STEP: Creating a new StatefulSet 12/30/22 04:55:01.888
    Dec 30 04:55:01.898: INFO: Found 0 stateful pods, waiting for 3
    Dec 30 04:55:11.909: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Dec 30 04:55:11.909: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Dec 30 04:55:11.909: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    Dec 30 04:55:11.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=statefulset-7185 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Dec 30 04:55:12.150: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Dec 30 04:55:12.150: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Dec 30 04:55:12.150: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 12/30/22 04:55:22.17
    Dec 30 04:55:22.192: INFO: Updating stateful set ss2
    STEP: Creating a new revision 12/30/22 04:55:22.192
    STEP: Updating Pods in reverse ordinal order 12/30/22 04:55:32.211
    Dec 30 04:55:32.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=statefulset-7185 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Dec 30 04:55:32.421: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Dec 30 04:55:32.421: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Dec 30 04:55:32.421: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    STEP: Rolling back to a previous revision 12/30/22 04:55:42.45
    Dec 30 04:55:42.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=statefulset-7185 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Dec 30 04:55:42.672: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Dec 30 04:55:42.672: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Dec 30 04:55:42.672: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Dec 30 04:55:52.715: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 12/30/22 04:56:02.736
    Dec 30 04:56:02.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=statefulset-7185 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Dec 30 04:56:02.914: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Dec 30 04:56:02.914: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Dec 30 04:56:02.914: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Dec 30 04:56:12.949: INFO: Deleting all statefulset in ns statefulset-7185
    Dec 30 04:56:12.953: INFO: Scaling statefulset ss2 to 0
    Dec 30 04:56:22.976: INFO: Waiting for statefulset status.replicas updated to 0
    Dec 30 04:56:22.980: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Dec 30 04:56:22.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-7185" for this suite. 12/30/22 04:56:23.001
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:56:23.011
Dec 30 04:56:23.012: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename dns 12/30/22 04:56:23.013
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:56:23.033
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:56:23.037
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 12/30/22 04:56:23.04
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-325.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-325.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 12/30/22 04:56:23.045
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-325.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-325.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 12/30/22 04:56:23.046
STEP: creating a pod to probe DNS 12/30/22 04:56:23.046
STEP: submitting the pod to kubernetes 12/30/22 04:56:23.046
Dec 30 04:56:23.057: INFO: Waiting up to 15m0s for pod "dns-test-008bcd7f-b236-4468-8f36-0a4496b96497" in namespace "dns-325" to be "running"
Dec 30 04:56:23.061: INFO: Pod "dns-test-008bcd7f-b236-4468-8f36-0a4496b96497": Phase="Pending", Reason="", readiness=false. Elapsed: 4.346506ms
Dec 30 04:56:25.068: INFO: Pod "dns-test-008bcd7f-b236-4468-8f36-0a4496b96497": Phase="Running", Reason="", readiness=true. Elapsed: 2.010838928s
Dec 30 04:56:25.068: INFO: Pod "dns-test-008bcd7f-b236-4468-8f36-0a4496b96497" satisfied condition "running"
STEP: retrieving the pod 12/30/22 04:56:25.068
STEP: looking for the results for each expected name from probers 12/30/22 04:56:25.072
Dec 30 04:56:25.092: INFO: DNS probes using dns-325/dns-test-008bcd7f-b236-4468-8f36-0a4496b96497 succeeded

STEP: deleting the pod 12/30/22 04:56:25.092
STEP: deleting the test headless service 12/30/22 04:56:25.106
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Dec 30 04:56:25.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-325" for this suite. 12/30/22 04:56:25.124
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [Conformance]","completed":334,"skipped":6242,"failed":0}
------------------------------
• [2.119 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:56:23.011
    Dec 30 04:56:23.012: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename dns 12/30/22 04:56:23.013
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:56:23.033
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:56:23.037
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 12/30/22 04:56:23.04
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-325.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-325.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     12/30/22 04:56:23.045
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-325.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-325.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     12/30/22 04:56:23.046
    STEP: creating a pod to probe DNS 12/30/22 04:56:23.046
    STEP: submitting the pod to kubernetes 12/30/22 04:56:23.046
    Dec 30 04:56:23.057: INFO: Waiting up to 15m0s for pod "dns-test-008bcd7f-b236-4468-8f36-0a4496b96497" in namespace "dns-325" to be "running"
    Dec 30 04:56:23.061: INFO: Pod "dns-test-008bcd7f-b236-4468-8f36-0a4496b96497": Phase="Pending", Reason="", readiness=false. Elapsed: 4.346506ms
    Dec 30 04:56:25.068: INFO: Pod "dns-test-008bcd7f-b236-4468-8f36-0a4496b96497": Phase="Running", Reason="", readiness=true. Elapsed: 2.010838928s
    Dec 30 04:56:25.068: INFO: Pod "dns-test-008bcd7f-b236-4468-8f36-0a4496b96497" satisfied condition "running"
    STEP: retrieving the pod 12/30/22 04:56:25.068
    STEP: looking for the results for each expected name from probers 12/30/22 04:56:25.072
    Dec 30 04:56:25.092: INFO: DNS probes using dns-325/dns-test-008bcd7f-b236-4468-8f36-0a4496b96497 succeeded

    STEP: deleting the pod 12/30/22 04:56:25.092
    STEP: deleting the test headless service 12/30/22 04:56:25.106
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Dec 30 04:56:25.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-325" for this suite. 12/30/22 04:56:25.124
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:56:25.133
Dec 30 04:56:25.133: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename services 12/30/22 04:56:25.134
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:56:25.152
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:56:25.156
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523
STEP: creating a service nodeport-service with the type=NodePort in namespace services-8263 12/30/22 04:56:25.159
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 12/30/22 04:56:25.182
STEP: creating service externalsvc in namespace services-8263 12/30/22 04:56:25.182
STEP: creating replication controller externalsvc in namespace services-8263 12/30/22 04:56:25.192
I1230 04:56:25.198296      25 runners.go:193] Created replication controller with name: externalsvc, namespace: services-8263, replica count: 2
I1230 04:56:28.249624      25 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 12/30/22 04:56:28.253
Dec 30 04:56:28.272: INFO: Creating new exec pod
Dec 30 04:56:28.287: INFO: Waiting up to 5m0s for pod "execpodtw7tn" in namespace "services-8263" to be "running"
Dec 30 04:56:28.292: INFO: Pod "execpodtw7tn": Phase="Pending", Reason="", readiness=false. Elapsed: 4.963801ms
Dec 30 04:56:30.298: INFO: Pod "execpodtw7tn": Phase="Running", Reason="", readiness=true. Elapsed: 2.010698127s
Dec 30 04:56:30.298: INFO: Pod "execpodtw7tn" satisfied condition "running"
Dec 30 04:56:30.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-8263 exec execpodtw7tn -- /bin/sh -x -c nslookup nodeport-service.services-8263.svc.cluster.local'
Dec 30 04:56:30.543: INFO: stderr: "+ nslookup nodeport-service.services-8263.svc.cluster.local\n"
Dec 30 04:56:30.543: INFO: stdout: "Server:\t\t169.254.25.10\nAddress:\t169.254.25.10#53\n\nnodeport-service.services-8263.svc.cluster.local\tcanonical name = externalsvc.services-8263.svc.cluster.local.\nName:\texternalsvc.services-8263.svc.cluster.local\nAddress: 10.233.61.195\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-8263, will wait for the garbage collector to delete the pods 12/30/22 04:56:30.543
Dec 30 04:56:30.605: INFO: Deleting ReplicationController externalsvc took: 6.721708ms
Dec 30 04:56:30.705: INFO: Terminating ReplicationController externalsvc pods took: 100.2251ms
Dec 30 04:56:33.122: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Dec 30 04:56:33.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8263" for this suite. 12/30/22 04:56:33.14
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","completed":335,"skipped":6255,"failed":0}
------------------------------
• [SLOW TEST] [8.014 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:56:25.133
    Dec 30 04:56:25.133: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename services 12/30/22 04:56:25.134
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:56:25.152
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:56:25.156
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1523
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-8263 12/30/22 04:56:25.159
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 12/30/22 04:56:25.182
    STEP: creating service externalsvc in namespace services-8263 12/30/22 04:56:25.182
    STEP: creating replication controller externalsvc in namespace services-8263 12/30/22 04:56:25.192
    I1230 04:56:25.198296      25 runners.go:193] Created replication controller with name: externalsvc, namespace: services-8263, replica count: 2
    I1230 04:56:28.249624      25 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 12/30/22 04:56:28.253
    Dec 30 04:56:28.272: INFO: Creating new exec pod
    Dec 30 04:56:28.287: INFO: Waiting up to 5m0s for pod "execpodtw7tn" in namespace "services-8263" to be "running"
    Dec 30 04:56:28.292: INFO: Pod "execpodtw7tn": Phase="Pending", Reason="", readiness=false. Elapsed: 4.963801ms
    Dec 30 04:56:30.298: INFO: Pod "execpodtw7tn": Phase="Running", Reason="", readiness=true. Elapsed: 2.010698127s
    Dec 30 04:56:30.298: INFO: Pod "execpodtw7tn" satisfied condition "running"
    Dec 30 04:56:30.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2341570513 --namespace=services-8263 exec execpodtw7tn -- /bin/sh -x -c nslookup nodeport-service.services-8263.svc.cluster.local'
    Dec 30 04:56:30.543: INFO: stderr: "+ nslookup nodeport-service.services-8263.svc.cluster.local\n"
    Dec 30 04:56:30.543: INFO: stdout: "Server:\t\t169.254.25.10\nAddress:\t169.254.25.10#53\n\nnodeport-service.services-8263.svc.cluster.local\tcanonical name = externalsvc.services-8263.svc.cluster.local.\nName:\texternalsvc.services-8263.svc.cluster.local\nAddress: 10.233.61.195\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-8263, will wait for the garbage collector to delete the pods 12/30/22 04:56:30.543
    Dec 30 04:56:30.605: INFO: Deleting ReplicationController externalsvc took: 6.721708ms
    Dec 30 04:56:30.705: INFO: Terminating ReplicationController externalsvc pods took: 100.2251ms
    Dec 30 04:56:33.122: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Dec 30 04:56:33.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-8263" for this suite. 12/30/22 04:56:33.14
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:56:33.148
Dec 30 04:56:33.148: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename custom-resource-definition 12/30/22 04:56:33.149
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:56:33.167
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:56:33.171
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
Dec 30 04:56:33.175: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Dec 30 04:56:38.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2767" for this suite. 12/30/22 04:56:38.743
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","completed":336,"skipped":6262,"failed":0}
------------------------------
• [SLOW TEST] [5.601 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:56:33.148
    Dec 30 04:56:33.148: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename custom-resource-definition 12/30/22 04:56:33.149
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:56:33.167
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:56:33.171
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    Dec 30 04:56:33.175: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Dec 30 04:56:38.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-2767" for this suite. 12/30/22 04:56:38.743
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:56:38.749
Dec 30 04:56:38.749: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename configmap 12/30/22 04:56:38.751
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:56:38.764
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:56:38.766
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98
STEP: Creating configMap with name configmap-test-volume-map-7186f7f0-17e5-4e39-9fa3-f5bd71bf93a4 12/30/22 04:56:38.77
STEP: Creating a pod to test consume configMaps 12/30/22 04:56:38.774
Dec 30 04:56:38.783: INFO: Waiting up to 5m0s for pod "pod-configmaps-69eda89b-e3bf-4347-a0d8-2a7951912784" in namespace "configmap-4329" to be "Succeeded or Failed"
Dec 30 04:56:38.786: INFO: Pod "pod-configmaps-69eda89b-e3bf-4347-a0d8-2a7951912784": Phase="Pending", Reason="", readiness=false. Elapsed: 2.976757ms
Dec 30 04:56:40.791: INFO: Pod "pod-configmaps-69eda89b-e3bf-4347-a0d8-2a7951912784": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008495313s
Dec 30 04:56:42.792: INFO: Pod "pod-configmaps-69eda89b-e3bf-4347-a0d8-2a7951912784": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009045092s
STEP: Saw pod success 12/30/22 04:56:42.792
Dec 30 04:56:42.792: INFO: Pod "pod-configmaps-69eda89b-e3bf-4347-a0d8-2a7951912784" satisfied condition "Succeeded or Failed"
Dec 30 04:56:42.796: INFO: Trying to get logs from node k8s-mgmt01 pod pod-configmaps-69eda89b-e3bf-4347-a0d8-2a7951912784 container agnhost-container: <nil>
STEP: delete the pod 12/30/22 04:56:42.817
Dec 30 04:56:42.830: INFO: Waiting for pod pod-configmaps-69eda89b-e3bf-4347-a0d8-2a7951912784 to disappear
Dec 30 04:56:42.835: INFO: Pod pod-configmaps-69eda89b-e3bf-4347-a0d8-2a7951912784 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Dec 30 04:56:42.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4329" for this suite. 12/30/22 04:56:42.84
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":337,"skipped":6266,"failed":0}
------------------------------
• [4.098 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:56:38.749
    Dec 30 04:56:38.749: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename configmap 12/30/22 04:56:38.751
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:56:38.764
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:56:38.766
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:98
    STEP: Creating configMap with name configmap-test-volume-map-7186f7f0-17e5-4e39-9fa3-f5bd71bf93a4 12/30/22 04:56:38.77
    STEP: Creating a pod to test consume configMaps 12/30/22 04:56:38.774
    Dec 30 04:56:38.783: INFO: Waiting up to 5m0s for pod "pod-configmaps-69eda89b-e3bf-4347-a0d8-2a7951912784" in namespace "configmap-4329" to be "Succeeded or Failed"
    Dec 30 04:56:38.786: INFO: Pod "pod-configmaps-69eda89b-e3bf-4347-a0d8-2a7951912784": Phase="Pending", Reason="", readiness=false. Elapsed: 2.976757ms
    Dec 30 04:56:40.791: INFO: Pod "pod-configmaps-69eda89b-e3bf-4347-a0d8-2a7951912784": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008495313s
    Dec 30 04:56:42.792: INFO: Pod "pod-configmaps-69eda89b-e3bf-4347-a0d8-2a7951912784": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009045092s
    STEP: Saw pod success 12/30/22 04:56:42.792
    Dec 30 04:56:42.792: INFO: Pod "pod-configmaps-69eda89b-e3bf-4347-a0d8-2a7951912784" satisfied condition "Succeeded or Failed"
    Dec 30 04:56:42.796: INFO: Trying to get logs from node k8s-mgmt01 pod pod-configmaps-69eda89b-e3bf-4347-a0d8-2a7951912784 container agnhost-container: <nil>
    STEP: delete the pod 12/30/22 04:56:42.817
    Dec 30 04:56:42.830: INFO: Waiting for pod pod-configmaps-69eda89b-e3bf-4347-a0d8-2a7951912784 to disappear
    Dec 30 04:56:42.835: INFO: Pod pod-configmaps-69eda89b-e3bf-4347-a0d8-2a7951912784 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Dec 30 04:56:42.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-4329" for this suite. 12/30/22 04:56:42.84
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:56:42.849
Dec 30 04:56:42.849: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename pods 12/30/22 04:56:42.851
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:56:42.867
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:56:42.87
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844
STEP: Create set of pods 12/30/22 04:56:42.874
Dec 30 04:56:42.884: INFO: created test-pod-1
Dec 30 04:56:42.889: INFO: created test-pod-2
Dec 30 04:56:42.895: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 12/30/22 04:56:42.895
Dec 30 04:56:42.896: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-7680' to be running and ready
Dec 30 04:56:42.908: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Dec 30 04:56:42.908: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Dec 30 04:56:42.908: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Dec 30 04:56:42.908: INFO: 0 / 3 pods in namespace 'pods-7680' are running and ready (0 seconds elapsed)
Dec 30 04:56:42.908: INFO: expected 0 pod replicas in namespace 'pods-7680', 0 are Running and Ready.
Dec 30 04:56:42.908: INFO: POD         NODE        PHASE    GRACE  CONDITIONS
Dec 30 04:56:42.908: INFO: test-pod-1  k8s-mgmt01  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-30 04:56:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-30 04:56:42 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-30 04:56:42 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-30 04:56:42 +0000 UTC  }]
Dec 30 04:56:42.908: INFO: test-pod-2  k8s-mgmt01  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-30 04:56:42 +0000 UTC  }]
Dec 30 04:56:42.908: INFO: test-pod-3  k8s-mgmt03  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-30 04:56:42 +0000 UTC  }]
Dec 30 04:56:42.908: INFO: 
Dec 30 04:56:44.923: INFO: 3 / 3 pods in namespace 'pods-7680' are running and ready (2 seconds elapsed)
Dec 30 04:56:44.923: INFO: expected 0 pod replicas in namespace 'pods-7680', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 12/30/22 04:56:44.949
Dec 30 04:56:44.955: INFO: Pod quantity 3 is different from expected quantity 0
Dec 30 04:56:45.964: INFO: Pod quantity 3 is different from expected quantity 0
Dec 30 04:56:46.964: INFO: Pod quantity 3 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Dec 30 04:56:47.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7680" for this suite. 12/30/22 04:56:47.97
{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","completed":338,"skipped":6281,"failed":0}
------------------------------
• [SLOW TEST] [5.128 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:56:42.849
    Dec 30 04:56:42.849: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename pods 12/30/22 04:56:42.851
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:56:42.867
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:56:42.87
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:844
    STEP: Create set of pods 12/30/22 04:56:42.874
    Dec 30 04:56:42.884: INFO: created test-pod-1
    Dec 30 04:56:42.889: INFO: created test-pod-2
    Dec 30 04:56:42.895: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 12/30/22 04:56:42.895
    Dec 30 04:56:42.896: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-7680' to be running and ready
    Dec 30 04:56:42.908: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Dec 30 04:56:42.908: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Dec 30 04:56:42.908: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Dec 30 04:56:42.908: INFO: 0 / 3 pods in namespace 'pods-7680' are running and ready (0 seconds elapsed)
    Dec 30 04:56:42.908: INFO: expected 0 pod replicas in namespace 'pods-7680', 0 are Running and Ready.
    Dec 30 04:56:42.908: INFO: POD         NODE        PHASE    GRACE  CONDITIONS
    Dec 30 04:56:42.908: INFO: test-pod-1  k8s-mgmt01  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-30 04:56:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-30 04:56:42 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-30 04:56:42 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-30 04:56:42 +0000 UTC  }]
    Dec 30 04:56:42.908: INFO: test-pod-2  k8s-mgmt01  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-30 04:56:42 +0000 UTC  }]
    Dec 30 04:56:42.908: INFO: test-pod-3  k8s-mgmt03  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-30 04:56:42 +0000 UTC  }]
    Dec 30 04:56:42.908: INFO: 
    Dec 30 04:56:44.923: INFO: 3 / 3 pods in namespace 'pods-7680' are running and ready (2 seconds elapsed)
    Dec 30 04:56:44.923: INFO: expected 0 pod replicas in namespace 'pods-7680', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 12/30/22 04:56:44.949
    Dec 30 04:56:44.955: INFO: Pod quantity 3 is different from expected quantity 0
    Dec 30 04:56:45.964: INFO: Pod quantity 3 is different from expected quantity 0
    Dec 30 04:56:46.964: INFO: Pod quantity 3 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Dec 30 04:56:47.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-7680" for this suite. 12/30/22 04:56:47.97
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:56:47.981
Dec 30 04:56:47.981: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename deployment 12/30/22 04:56:47.983
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:56:47.998
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:56:48.001
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
Dec 30 04:56:48.005: INFO: Creating deployment "test-recreate-deployment"
Dec 30 04:56:48.011: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Dec 30 04:56:48.019: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Dec 30 04:56:50.028: INFO: Waiting deployment "test-recreate-deployment" to complete
Dec 30 04:56:50.033: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Dec 30 04:56:50.044: INFO: Updating deployment test-recreate-deployment
Dec 30 04:56:50.044: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Dec 30 04:56:50.123: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-4008  4d626acb-ce82-436f-8435-1976b00d3629 461875 2 2022-12-30 04:56:48 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2022-12-30 04:56:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-30 04:56:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003d913a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-12-30 04:56:50 +0000 UTC,LastTransitionTime:2022-12-30 04:56:50 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-9d58999df" is progressing.,LastUpdateTime:2022-12-30 04:56:50 +0000 UTC,LastTransitionTime:2022-12-30 04:56:48 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Dec 30 04:56:50.127: INFO: New ReplicaSet "test-recreate-deployment-9d58999df" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-9d58999df  deployment-4008  20cd44ba-2abf-47c5-8a98-dd7e2f018951 461872 1 2022-12-30 04:56:50 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 4d626acb-ce82-436f-8435-1976b00d3629 0xc005f9dbb0 0xc005f9dbb1}] [] [{kube-controller-manager Update apps/v1 2022-12-30 04:56:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4d626acb-ce82-436f-8435-1976b00d3629\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-30 04:56:50 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 9d58999df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005f9dc48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 30 04:56:50.127: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Dec 30 04:56:50.127: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d8b6f647f  deployment-4008  6f73512e-0e37-48b9-be6d-6c58e5a800d1 461863 2 2022-12-30 04:56:48 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 4d626acb-ce82-436f-8435-1976b00d3629 0xc005f9da97 0xc005f9da98}] [] [{kube-controller-manager Update apps/v1 2022-12-30 04:56:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4d626acb-ce82-436f-8435-1976b00d3629\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-30 04:56:50 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d8b6f647f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005f9db48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 30 04:56:50.132: INFO: Pod "test-recreate-deployment-9d58999df-jvq2c" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-9d58999df-jvq2c test-recreate-deployment-9d58999df- deployment-4008  11be61b9-d14e-4f5d-a0be-500662248939 461874 0 2022-12-30 04:56:50 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [{apps/v1 ReplicaSet test-recreate-deployment-9d58999df 20cd44ba-2abf-47c5-8a98-dd7e2f018951 0xc00101e2e0 0xc00101e2e1}] [] [{kube-controller-manager Update v1 2022-12-30 04:56:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"20cd44ba-2abf-47c5-8a98-dd7e2f018951\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-30 04:56:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vc5p9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vc5p9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-mgmt01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 04:56:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 04:56:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 04:56:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 04:56:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.140,PodIP:,StartTime:2022-12-30 04:56:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Dec 30 04:56:50.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4008" for this suite. 12/30/22 04:56:50.138
{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","completed":339,"skipped":6316,"failed":0}
------------------------------
• [2.163 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:56:47.981
    Dec 30 04:56:47.981: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename deployment 12/30/22 04:56:47.983
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:56:47.998
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:56:48.001
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    Dec 30 04:56:48.005: INFO: Creating deployment "test-recreate-deployment"
    Dec 30 04:56:48.011: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    Dec 30 04:56:48.019: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
    Dec 30 04:56:50.028: INFO: Waiting deployment "test-recreate-deployment" to complete
    Dec 30 04:56:50.033: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    Dec 30 04:56:50.044: INFO: Updating deployment test-recreate-deployment
    Dec 30 04:56:50.044: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Dec 30 04:56:50.123: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-4008  4d626acb-ce82-436f-8435-1976b00d3629 461875 2 2022-12-30 04:56:48 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2022-12-30 04:56:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-30 04:56:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003d913a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-12-30 04:56:50 +0000 UTC,LastTransitionTime:2022-12-30 04:56:50 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-9d58999df" is progressing.,LastUpdateTime:2022-12-30 04:56:50 +0000 UTC,LastTransitionTime:2022-12-30 04:56:48 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Dec 30 04:56:50.127: INFO: New ReplicaSet "test-recreate-deployment-9d58999df" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-9d58999df  deployment-4008  20cd44ba-2abf-47c5-8a98-dd7e2f018951 461872 1 2022-12-30 04:56:50 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 4d626acb-ce82-436f-8435-1976b00d3629 0xc005f9dbb0 0xc005f9dbb1}] [] [{kube-controller-manager Update apps/v1 2022-12-30 04:56:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4d626acb-ce82-436f-8435-1976b00d3629\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-30 04:56:50 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 9d58999df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005f9dc48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Dec 30 04:56:50.127: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    Dec 30 04:56:50.127: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d8b6f647f  deployment-4008  6f73512e-0e37-48b9-be6d-6c58e5a800d1 461863 2 2022-12-30 04:56:48 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 4d626acb-ce82-436f-8435-1976b00d3629 0xc005f9da97 0xc005f9da98}] [] [{kube-controller-manager Update apps/v1 2022-12-30 04:56:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4d626acb-ce82-436f-8435-1976b00d3629\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-30 04:56:50 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d8b6f647f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005f9db48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Dec 30 04:56:50.132: INFO: Pod "test-recreate-deployment-9d58999df-jvq2c" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-9d58999df-jvq2c test-recreate-deployment-9d58999df- deployment-4008  11be61b9-d14e-4f5d-a0be-500662248939 461874 0 2022-12-30 04:56:50 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [{apps/v1 ReplicaSet test-recreate-deployment-9d58999df 20cd44ba-2abf-47c5-8a98-dd7e2f018951 0xc00101e2e0 0xc00101e2e1}] [] [{kube-controller-manager Update v1 2022-12-30 04:56:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"20cd44ba-2abf-47c5-8a98-dd7e2f018951\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-30 04:56:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vc5p9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vc5p9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-mgmt01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 04:56:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 04:56:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 04:56:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-30 04:56:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.78.26.140,PodIP:,StartTime:2022-12-30 04:56:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Dec 30 04:56:50.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-4008" for this suite. 12/30/22 04:56:50.138
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:56:50.145
Dec 30 04:56:50.145: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename var-expansion 12/30/22 04:56:50.147
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:56:50.165
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:56:50.168
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91
STEP: Creating a pod to test substitution in container's args 12/30/22 04:56:50.172
Dec 30 04:56:50.181: INFO: Waiting up to 5m0s for pod "var-expansion-d6eefa16-3048-41a3-a25d-4dbc8ee1930f" in namespace "var-expansion-1078" to be "Succeeded or Failed"
Dec 30 04:56:50.185: INFO: Pod "var-expansion-d6eefa16-3048-41a3-a25d-4dbc8ee1930f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.284003ms
Dec 30 04:56:52.190: INFO: Pod "var-expansion-d6eefa16-3048-41a3-a25d-4dbc8ee1930f": Phase="Running", Reason="", readiness=false. Elapsed: 2.008145394s
Dec 30 04:56:54.190: INFO: Pod "var-expansion-d6eefa16-3048-41a3-a25d-4dbc8ee1930f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008909069s
STEP: Saw pod success 12/30/22 04:56:54.19
Dec 30 04:56:54.191: INFO: Pod "var-expansion-d6eefa16-3048-41a3-a25d-4dbc8ee1930f" satisfied condition "Succeeded or Failed"
Dec 30 04:56:54.194: INFO: Trying to get logs from node k8s-mgmt01 pod var-expansion-d6eefa16-3048-41a3-a25d-4dbc8ee1930f container dapi-container: <nil>
STEP: delete the pod 12/30/22 04:56:54.208
Dec 30 04:56:54.221: INFO: Waiting for pod var-expansion-d6eefa16-3048-41a3-a25d-4dbc8ee1930f to disappear
Dec 30 04:56:54.225: INFO: Pod var-expansion-d6eefa16-3048-41a3-a25d-4dbc8ee1930f no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Dec 30 04:56:54.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1078" for this suite. 12/30/22 04:56:54.231
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","completed":340,"skipped":6323,"failed":0}
------------------------------
• [4.092 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:56:50.145
    Dec 30 04:56:50.145: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename var-expansion 12/30/22 04:56:50.147
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:56:50.165
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:56:50.168
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:91
    STEP: Creating a pod to test substitution in container's args 12/30/22 04:56:50.172
    Dec 30 04:56:50.181: INFO: Waiting up to 5m0s for pod "var-expansion-d6eefa16-3048-41a3-a25d-4dbc8ee1930f" in namespace "var-expansion-1078" to be "Succeeded or Failed"
    Dec 30 04:56:50.185: INFO: Pod "var-expansion-d6eefa16-3048-41a3-a25d-4dbc8ee1930f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.284003ms
    Dec 30 04:56:52.190: INFO: Pod "var-expansion-d6eefa16-3048-41a3-a25d-4dbc8ee1930f": Phase="Running", Reason="", readiness=false. Elapsed: 2.008145394s
    Dec 30 04:56:54.190: INFO: Pod "var-expansion-d6eefa16-3048-41a3-a25d-4dbc8ee1930f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008909069s
    STEP: Saw pod success 12/30/22 04:56:54.19
    Dec 30 04:56:54.191: INFO: Pod "var-expansion-d6eefa16-3048-41a3-a25d-4dbc8ee1930f" satisfied condition "Succeeded or Failed"
    Dec 30 04:56:54.194: INFO: Trying to get logs from node k8s-mgmt01 pod var-expansion-d6eefa16-3048-41a3-a25d-4dbc8ee1930f container dapi-container: <nil>
    STEP: delete the pod 12/30/22 04:56:54.208
    Dec 30 04:56:54.221: INFO: Waiting for pod var-expansion-d6eefa16-3048-41a3-a25d-4dbc8ee1930f to disappear
    Dec 30 04:56:54.225: INFO: Pod var-expansion-d6eefa16-3048-41a3-a25d-4dbc8ee1930f no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Dec 30 04:56:54.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-1078" for this suite. 12/30/22 04:56:54.231
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:56:54.238
Dec 30 04:56:54.238: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename runtimeclass 12/30/22 04:56:54.24
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:56:54.269
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:56:54.273
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 12/30/22 04:56:54.276
STEP: getting /apis/node.k8s.io 12/30/22 04:56:54.279
STEP: getting /apis/node.k8s.io/v1 12/30/22 04:56:54.281
STEP: creating 12/30/22 04:56:54.282
STEP: watching 12/30/22 04:56:54.299
Dec 30 04:56:54.299: INFO: starting watch
STEP: getting 12/30/22 04:56:54.304
STEP: listing 12/30/22 04:56:54.308
STEP: patching 12/30/22 04:56:54.311
STEP: updating 12/30/22 04:56:54.317
Dec 30 04:56:54.322: INFO: waiting for watch events with expected annotations
STEP: deleting 12/30/22 04:56:54.322
STEP: deleting a collection 12/30/22 04:56:54.336
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Dec 30 04:56:54.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-2340" for this suite. 12/30/22 04:56:54.359
{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","completed":341,"skipped":6330,"failed":0}
------------------------------
• [0.127 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:56:54.238
    Dec 30 04:56:54.238: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename runtimeclass 12/30/22 04:56:54.24
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:56:54.269
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:56:54.273
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 12/30/22 04:56:54.276
    STEP: getting /apis/node.k8s.io 12/30/22 04:56:54.279
    STEP: getting /apis/node.k8s.io/v1 12/30/22 04:56:54.281
    STEP: creating 12/30/22 04:56:54.282
    STEP: watching 12/30/22 04:56:54.299
    Dec 30 04:56:54.299: INFO: starting watch
    STEP: getting 12/30/22 04:56:54.304
    STEP: listing 12/30/22 04:56:54.308
    STEP: patching 12/30/22 04:56:54.311
    STEP: updating 12/30/22 04:56:54.317
    Dec 30 04:56:54.322: INFO: waiting for watch events with expected annotations
    STEP: deleting 12/30/22 04:56:54.322
    STEP: deleting a collection 12/30/22 04:56:54.336
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Dec 30 04:56:54.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-2340" for this suite. 12/30/22 04:56:54.359
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:56:54.368
Dec 30 04:56:54.368: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename replicaset 12/30/22 04:56:54.369
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:56:54.39
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:56:54.394
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
Dec 30 04:56:54.411: INFO: Pod name sample-pod: Found 0 pods out of 1
Dec 30 04:56:59.416: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 12/30/22 04:56:59.416
STEP: Scaling up "test-rs" replicaset  12/30/22 04:56:59.416
Dec 30 04:56:59.426: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 12/30/22 04:56:59.426
W1230 04:56:59.443689      25 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Dec 30 04:56:59.445: INFO: observed ReplicaSet test-rs in namespace replicaset-2720 with ReadyReplicas 1, AvailableReplicas 1
Dec 30 04:56:59.454: INFO: observed ReplicaSet test-rs in namespace replicaset-2720 with ReadyReplicas 1, AvailableReplicas 1
Dec 30 04:56:59.476: INFO: observed ReplicaSet test-rs in namespace replicaset-2720 with ReadyReplicas 1, AvailableReplicas 1
Dec 30 04:56:59.489: INFO: observed ReplicaSet test-rs in namespace replicaset-2720 with ReadyReplicas 1, AvailableReplicas 1
Dec 30 04:57:00.768: INFO: observed ReplicaSet test-rs in namespace replicaset-2720 with ReadyReplicas 2, AvailableReplicas 2
Dec 30 04:57:00.938: INFO: observed Replicaset test-rs in namespace replicaset-2720 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Dec 30 04:57:00.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2720" for this suite. 12/30/22 04:57:00.944
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","completed":342,"skipped":6359,"failed":0}
------------------------------
• [SLOW TEST] [6.583 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:56:54.368
    Dec 30 04:56:54.368: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename replicaset 12/30/22 04:56:54.369
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:56:54.39
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:56:54.394
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    Dec 30 04:56:54.411: INFO: Pod name sample-pod: Found 0 pods out of 1
    Dec 30 04:56:59.416: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 12/30/22 04:56:59.416
    STEP: Scaling up "test-rs" replicaset  12/30/22 04:56:59.416
    Dec 30 04:56:59.426: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 12/30/22 04:56:59.426
    W1230 04:56:59.443689      25 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Dec 30 04:56:59.445: INFO: observed ReplicaSet test-rs in namespace replicaset-2720 with ReadyReplicas 1, AvailableReplicas 1
    Dec 30 04:56:59.454: INFO: observed ReplicaSet test-rs in namespace replicaset-2720 with ReadyReplicas 1, AvailableReplicas 1
    Dec 30 04:56:59.476: INFO: observed ReplicaSet test-rs in namespace replicaset-2720 with ReadyReplicas 1, AvailableReplicas 1
    Dec 30 04:56:59.489: INFO: observed ReplicaSet test-rs in namespace replicaset-2720 with ReadyReplicas 1, AvailableReplicas 1
    Dec 30 04:57:00.768: INFO: observed ReplicaSet test-rs in namespace replicaset-2720 with ReadyReplicas 2, AvailableReplicas 2
    Dec 30 04:57:00.938: INFO: observed Replicaset test-rs in namespace replicaset-2720 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Dec 30 04:57:00.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-2720" for this suite. 12/30/22 04:57:00.944
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:57:00.952
Dec 30 04:57:00.952: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename sched-pred 12/30/22 04:57:00.953
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:57:00.968
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:57:00.972
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Dec 30 04:57:00.975: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 30 04:57:00.986: INFO: Waiting for terminating namespaces to be deleted...
Dec 30 04:57:00.990: INFO: 
Logging pods the apiserver thinks is on node k8s-mgmt01 before test
Dec 30 04:57:01.004: INFO: calico-node-6xm98 from kube-system started at 2022-12-28 02:19:33 +0000 UTC (1 container statuses recorded)
Dec 30 04:57:01.004: INFO: 	Container calico-node ready: true, restart count 0
Dec 30 04:57:01.004: INFO: kube-apiserver-k8s-mgmt01 from kube-system started at 2022-12-28 02:15:50 +0000 UTC (1 container statuses recorded)
Dec 30 04:57:01.004: INFO: 	Container kube-apiserver ready: true, restart count 1
Dec 30 04:57:01.004: INFO: kube-controller-manager-k8s-mgmt01 from kube-system started at 2022-12-28 02:15:49 +0000 UTC (1 container statuses recorded)
Dec 30 04:57:01.004: INFO: 	Container kube-controller-manager ready: true, restart count 1
Dec 30 04:57:01.004: INFO: kube-proxy-tcvcl from kube-system started at 2022-12-28 02:18:09 +0000 UTC (1 container statuses recorded)
Dec 30 04:57:01.004: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 30 04:57:01.004: INFO: kube-scheduler-k8s-mgmt01 from kube-system started at 2022-12-28 02:15:38 +0000 UTC (1 container statuses recorded)
Dec 30 04:57:01.004: INFO: 	Container kube-scheduler ready: true, restart count 1
Dec 30 04:57:01.004: INFO: nodelocaldns-n9jl8 from kube-system started at 2022-12-28 02:22:14 +0000 UTC (1 container statuses recorded)
Dec 30 04:57:01.004: INFO: 	Container node-cache ready: true, restart count 0
Dec 30 04:57:01.004: INFO: test-rs-bcdsd from replicaset-2720 started at 2022-12-30 04:56:54 +0000 UTC (1 container statuses recorded)
Dec 30 04:57:01.004: INFO: 	Container httpd ready: true, restart count 0
Dec 30 04:57:01.004: INFO: sonobuoy-systemd-logs-daemon-set-4da877126f61426e-qd9td from sonobuoy started at 2022-12-30 03:20:18 +0000 UTC (2 container statuses recorded)
Dec 30 04:57:01.004: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 30 04:57:01.004: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 30 04:57:01.004: INFO: 
Logging pods the apiserver thinks is on node k8s-mgmt02 before test
Dec 30 04:57:01.017: INFO: calico-node-gwlv6 from kube-system started at 2022-12-28 02:19:33 +0000 UTC (1 container statuses recorded)
Dec 30 04:57:01.017: INFO: 	Container calico-node ready: true, restart count 0
Dec 30 04:57:01.017: INFO: coredns-69dfc8446-74sw5 from kube-system started at 2022-12-30 04:17:48 +0000 UTC (1 container statuses recorded)
Dec 30 04:57:01.017: INFO: 	Container coredns ready: true, restart count 0
Dec 30 04:57:01.017: INFO: kube-apiserver-k8s-mgmt02 from kube-system started at 2022-12-28 02:16:10 +0000 UTC (1 container statuses recorded)
Dec 30 04:57:01.017: INFO: 	Container kube-apiserver ready: true, restart count 1
Dec 30 04:57:01.017: INFO: kube-controller-manager-k8s-mgmt02 from kube-system started at 2022-12-28 02:16:10 +0000 UTC (1 container statuses recorded)
Dec 30 04:57:01.017: INFO: 	Container kube-controller-manager ready: true, restart count 1
Dec 30 04:57:01.017: INFO: kube-proxy-5lvmg from kube-system started at 2022-12-28 02:18:08 +0000 UTC (1 container statuses recorded)
Dec 30 04:57:01.017: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 30 04:57:01.017: INFO: kube-scheduler-k8s-mgmt02 from kube-system started at 2022-12-28 02:16:10 +0000 UTC (1 container statuses recorded)
Dec 30 04:57:01.017: INFO: 	Container kube-scheduler ready: true, restart count 1
Dec 30 04:57:01.017: INFO: metrics-server-67f489ffcf-srn44 from kube-system started at 2022-12-28 02:22:51 +0000 UTC (1 container statuses recorded)
Dec 30 04:57:01.017: INFO: 	Container metrics-server ready: true, restart count 0
Dec 30 04:57:01.017: INFO: nodelocaldns-bwgsf from kube-system started at 2022-12-28 02:22:14 +0000 UTC (1 container statuses recorded)
Dec 30 04:57:01.017: INFO: 	Container node-cache ready: true, restart count 0
Dec 30 04:57:01.017: INFO: test-rs-ns9t4 from replicaset-2720 started at 2022-12-30 04:56:59 +0000 UTC (2 container statuses recorded)
Dec 30 04:57:01.017: INFO: 	Container httpd ready: true, restart count 0
Dec 30 04:57:01.017: INFO: 	Container test-rs ready: true, restart count 0
Dec 30 04:57:01.017: INFO: sonobuoy from sonobuoy started at 2022-12-30 03:20:11 +0000 UTC (1 container statuses recorded)
Dec 30 04:57:01.017: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec 30 04:57:01.017: INFO: sonobuoy-systemd-logs-daemon-set-4da877126f61426e-llwcr from sonobuoy started at 2022-12-30 03:20:18 +0000 UTC (2 container statuses recorded)
Dec 30 04:57:01.017: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 30 04:57:01.017: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 30 04:57:01.017: INFO: 
Logging pods the apiserver thinks is on node k8s-mgmt03 before test
Dec 30 04:57:01.030: INFO: calico-kube-controllers-d6484b75c-8gdnt from kube-system started at 2022-12-28 02:21:06 +0000 UTC (1 container statuses recorded)
Dec 30 04:57:01.030: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Dec 30 04:57:01.030: INFO: calico-node-lqstj from kube-system started at 2022-12-28 02:19:33 +0000 UTC (1 container statuses recorded)
Dec 30 04:57:01.030: INFO: 	Container calico-node ready: true, restart count 0
Dec 30 04:57:01.030: INFO: coredns-69dfc8446-6gk67 from kube-system started at 2022-12-28 02:22:13 +0000 UTC (1 container statuses recorded)
Dec 30 04:57:01.030: INFO: 	Container coredns ready: true, restart count 0
Dec 30 04:57:01.030: INFO: dns-autoscaler-5b9959d7fc-x5csc from kube-system started at 2022-12-30 04:17:48 +0000 UTC (1 container statuses recorded)
Dec 30 04:57:01.030: INFO: 	Container autoscaler ready: true, restart count 0
Dec 30 04:57:01.030: INFO: kube-apiserver-k8s-mgmt03 from kube-system started at 2022-12-28 02:16:20 +0000 UTC (1 container statuses recorded)
Dec 30 04:57:01.030: INFO: 	Container kube-apiserver ready: true, restart count 1
Dec 30 04:57:01.030: INFO: kube-controller-manager-k8s-mgmt03 from kube-system started at 2022-12-28 02:16:20 +0000 UTC (1 container statuses recorded)
Dec 30 04:57:01.030: INFO: 	Container kube-controller-manager ready: true, restart count 1
Dec 30 04:57:01.031: INFO: kube-proxy-rfm26 from kube-system started at 2022-12-28 02:18:08 +0000 UTC (1 container statuses recorded)
Dec 30 04:57:01.031: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 30 04:57:01.031: INFO: kube-scheduler-k8s-mgmt03 from kube-system started at 2022-12-28 02:17:11 +0000 UTC (1 container statuses recorded)
Dec 30 04:57:01.031: INFO: 	Container kube-scheduler ready: true, restart count 1
Dec 30 04:57:01.031: INFO: nodelocaldns-76d47 from kube-system started at 2022-12-28 02:22:14 +0000 UTC (1 container statuses recorded)
Dec 30 04:57:01.031: INFO: 	Container node-cache ready: true, restart count 0
Dec 30 04:57:01.031: INFO: test-rs-7wrg6 from replicaset-2720 started at 2022-12-30 04:56:59 +0000 UTC (1 container statuses recorded)
Dec 30 04:57:01.031: INFO: 	Container httpd ready: true, restart count 0
Dec 30 04:57:01.031: INFO: sonobuoy-e2e-job-c5efb7b0ce524286 from sonobuoy started at 2022-12-30 03:20:18 +0000 UTC (2 container statuses recorded)
Dec 30 04:57:01.031: INFO: 	Container e2e ready: true, restart count 0
Dec 30 04:57:01.031: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 30 04:57:01.031: INFO: sonobuoy-systemd-logs-daemon-set-4da877126f61426e-wfvd8 from sonobuoy started at 2022-12-30 03:20:18 +0000 UTC (2 container statuses recorded)
Dec 30 04:57:01.031: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 30 04:57:01.031: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 30 04:57:01.031: INFO: 
Logging pods the apiserver thinks is on node k8s-worker01 before test
Dec 30 04:57:01.041: INFO: calico-node-dtvd6 from kube-system started at 2022-12-28 02:19:33 +0000 UTC (1 container statuses recorded)
Dec 30 04:57:01.041: INFO: 	Container calico-node ready: true, restart count 0
Dec 30 04:57:01.041: INFO: kube-proxy-m86bn from kube-system started at 2022-12-28 02:18:09 +0000 UTC (1 container statuses recorded)
Dec 30 04:57:01.041: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 30 04:57:01.041: INFO: nginx-proxy-k8s-worker01 from kube-system started at 2022-12-28 02:17:47 +0000 UTC (1 container statuses recorded)
Dec 30 04:57:01.041: INFO: 	Container nginx-proxy ready: true, restart count 0
Dec 30 04:57:01.041: INFO: nodelocaldns-8zm2b from kube-system started at 2022-12-28 02:22:14 +0000 UTC (1 container statuses recorded)
Dec 30 04:57:01.041: INFO: 	Container node-cache ready: true, restart count 0
Dec 30 04:57:01.041: INFO: sonobuoy-systemd-logs-daemon-set-4da877126f61426e-4wpvd from sonobuoy started at 2022-12-30 03:20:18 +0000 UTC (2 container statuses recorded)
Dec 30 04:57:01.041: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 30 04:57:01.041: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 30 04:57:01.041: INFO: 
Logging pods the apiserver thinks is on node k8s-worker02 before test
Dec 30 04:57:01.053: INFO: calico-node-s7npn from kube-system started at 2022-12-28 02:19:33 +0000 UTC (1 container statuses recorded)
Dec 30 04:57:01.053: INFO: 	Container calico-node ready: true, restart count 0
Dec 30 04:57:01.053: INFO: kube-proxy-8cg4d from kube-system started at 2022-12-28 02:18:09 +0000 UTC (1 container statuses recorded)
Dec 30 04:57:01.053: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 30 04:57:01.053: INFO: nginx-proxy-k8s-worker02 from kube-system started at 2022-12-28 02:17:48 +0000 UTC (1 container statuses recorded)
Dec 30 04:57:01.053: INFO: 	Container nginx-proxy ready: true, restart count 0
Dec 30 04:57:01.053: INFO: nodelocaldns-4hlqv from kube-system started at 2022-12-28 02:22:14 +0000 UTC (1 container statuses recorded)
Dec 30 04:57:01.053: INFO: 	Container node-cache ready: true, restart count 0
Dec 30 04:57:01.053: INFO: sonobuoy-systemd-logs-daemon-set-4da877126f61426e-7mpss from sonobuoy started at 2022-12-30 03:20:18 +0000 UTC (2 container statuses recorded)
Dec 30 04:57:01.053: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 30 04:57:01.053: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438
STEP: Trying to schedule Pod with nonempty NodeSelector. 12/30/22 04:57:01.053
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.1735794ffce9b118], Reason = [FailedScheduling], Message = [0/5 nodes are available: 5 node(s) didn't match Pod's node affinity/selector. preemption: 0/5 nodes are available: 5 Preemption is not helpful for scheduling.] 12/30/22 04:57:01.093
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Dec 30 04:57:02.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7915" for this suite. 12/30/22 04:57:02.099
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","completed":343,"skipped":6361,"failed":0}
------------------------------
• [1.155 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:57:00.952
    Dec 30 04:57:00.952: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename sched-pred 12/30/22 04:57:00.953
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:57:00.968
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:57:00.972
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Dec 30 04:57:00.975: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Dec 30 04:57:00.986: INFO: Waiting for terminating namespaces to be deleted...
    Dec 30 04:57:00.990: INFO: 
    Logging pods the apiserver thinks is on node k8s-mgmt01 before test
    Dec 30 04:57:01.004: INFO: calico-node-6xm98 from kube-system started at 2022-12-28 02:19:33 +0000 UTC (1 container statuses recorded)
    Dec 30 04:57:01.004: INFO: 	Container calico-node ready: true, restart count 0
    Dec 30 04:57:01.004: INFO: kube-apiserver-k8s-mgmt01 from kube-system started at 2022-12-28 02:15:50 +0000 UTC (1 container statuses recorded)
    Dec 30 04:57:01.004: INFO: 	Container kube-apiserver ready: true, restart count 1
    Dec 30 04:57:01.004: INFO: kube-controller-manager-k8s-mgmt01 from kube-system started at 2022-12-28 02:15:49 +0000 UTC (1 container statuses recorded)
    Dec 30 04:57:01.004: INFO: 	Container kube-controller-manager ready: true, restart count 1
    Dec 30 04:57:01.004: INFO: kube-proxy-tcvcl from kube-system started at 2022-12-28 02:18:09 +0000 UTC (1 container statuses recorded)
    Dec 30 04:57:01.004: INFO: 	Container kube-proxy ready: true, restart count 0
    Dec 30 04:57:01.004: INFO: kube-scheduler-k8s-mgmt01 from kube-system started at 2022-12-28 02:15:38 +0000 UTC (1 container statuses recorded)
    Dec 30 04:57:01.004: INFO: 	Container kube-scheduler ready: true, restart count 1
    Dec 30 04:57:01.004: INFO: nodelocaldns-n9jl8 from kube-system started at 2022-12-28 02:22:14 +0000 UTC (1 container statuses recorded)
    Dec 30 04:57:01.004: INFO: 	Container node-cache ready: true, restart count 0
    Dec 30 04:57:01.004: INFO: test-rs-bcdsd from replicaset-2720 started at 2022-12-30 04:56:54 +0000 UTC (1 container statuses recorded)
    Dec 30 04:57:01.004: INFO: 	Container httpd ready: true, restart count 0
    Dec 30 04:57:01.004: INFO: sonobuoy-systemd-logs-daemon-set-4da877126f61426e-qd9td from sonobuoy started at 2022-12-30 03:20:18 +0000 UTC (2 container statuses recorded)
    Dec 30 04:57:01.004: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Dec 30 04:57:01.004: INFO: 	Container systemd-logs ready: true, restart count 0
    Dec 30 04:57:01.004: INFO: 
    Logging pods the apiserver thinks is on node k8s-mgmt02 before test
    Dec 30 04:57:01.017: INFO: calico-node-gwlv6 from kube-system started at 2022-12-28 02:19:33 +0000 UTC (1 container statuses recorded)
    Dec 30 04:57:01.017: INFO: 	Container calico-node ready: true, restart count 0
    Dec 30 04:57:01.017: INFO: coredns-69dfc8446-74sw5 from kube-system started at 2022-12-30 04:17:48 +0000 UTC (1 container statuses recorded)
    Dec 30 04:57:01.017: INFO: 	Container coredns ready: true, restart count 0
    Dec 30 04:57:01.017: INFO: kube-apiserver-k8s-mgmt02 from kube-system started at 2022-12-28 02:16:10 +0000 UTC (1 container statuses recorded)
    Dec 30 04:57:01.017: INFO: 	Container kube-apiserver ready: true, restart count 1
    Dec 30 04:57:01.017: INFO: kube-controller-manager-k8s-mgmt02 from kube-system started at 2022-12-28 02:16:10 +0000 UTC (1 container statuses recorded)
    Dec 30 04:57:01.017: INFO: 	Container kube-controller-manager ready: true, restart count 1
    Dec 30 04:57:01.017: INFO: kube-proxy-5lvmg from kube-system started at 2022-12-28 02:18:08 +0000 UTC (1 container statuses recorded)
    Dec 30 04:57:01.017: INFO: 	Container kube-proxy ready: true, restart count 0
    Dec 30 04:57:01.017: INFO: kube-scheduler-k8s-mgmt02 from kube-system started at 2022-12-28 02:16:10 +0000 UTC (1 container statuses recorded)
    Dec 30 04:57:01.017: INFO: 	Container kube-scheduler ready: true, restart count 1
    Dec 30 04:57:01.017: INFO: metrics-server-67f489ffcf-srn44 from kube-system started at 2022-12-28 02:22:51 +0000 UTC (1 container statuses recorded)
    Dec 30 04:57:01.017: INFO: 	Container metrics-server ready: true, restart count 0
    Dec 30 04:57:01.017: INFO: nodelocaldns-bwgsf from kube-system started at 2022-12-28 02:22:14 +0000 UTC (1 container statuses recorded)
    Dec 30 04:57:01.017: INFO: 	Container node-cache ready: true, restart count 0
    Dec 30 04:57:01.017: INFO: test-rs-ns9t4 from replicaset-2720 started at 2022-12-30 04:56:59 +0000 UTC (2 container statuses recorded)
    Dec 30 04:57:01.017: INFO: 	Container httpd ready: true, restart count 0
    Dec 30 04:57:01.017: INFO: 	Container test-rs ready: true, restart count 0
    Dec 30 04:57:01.017: INFO: sonobuoy from sonobuoy started at 2022-12-30 03:20:11 +0000 UTC (1 container statuses recorded)
    Dec 30 04:57:01.017: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Dec 30 04:57:01.017: INFO: sonobuoy-systemd-logs-daemon-set-4da877126f61426e-llwcr from sonobuoy started at 2022-12-30 03:20:18 +0000 UTC (2 container statuses recorded)
    Dec 30 04:57:01.017: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Dec 30 04:57:01.017: INFO: 	Container systemd-logs ready: true, restart count 0
    Dec 30 04:57:01.017: INFO: 
    Logging pods the apiserver thinks is on node k8s-mgmt03 before test
    Dec 30 04:57:01.030: INFO: calico-kube-controllers-d6484b75c-8gdnt from kube-system started at 2022-12-28 02:21:06 +0000 UTC (1 container statuses recorded)
    Dec 30 04:57:01.030: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Dec 30 04:57:01.030: INFO: calico-node-lqstj from kube-system started at 2022-12-28 02:19:33 +0000 UTC (1 container statuses recorded)
    Dec 30 04:57:01.030: INFO: 	Container calico-node ready: true, restart count 0
    Dec 30 04:57:01.030: INFO: coredns-69dfc8446-6gk67 from kube-system started at 2022-12-28 02:22:13 +0000 UTC (1 container statuses recorded)
    Dec 30 04:57:01.030: INFO: 	Container coredns ready: true, restart count 0
    Dec 30 04:57:01.030: INFO: dns-autoscaler-5b9959d7fc-x5csc from kube-system started at 2022-12-30 04:17:48 +0000 UTC (1 container statuses recorded)
    Dec 30 04:57:01.030: INFO: 	Container autoscaler ready: true, restart count 0
    Dec 30 04:57:01.030: INFO: kube-apiserver-k8s-mgmt03 from kube-system started at 2022-12-28 02:16:20 +0000 UTC (1 container statuses recorded)
    Dec 30 04:57:01.030: INFO: 	Container kube-apiserver ready: true, restart count 1
    Dec 30 04:57:01.030: INFO: kube-controller-manager-k8s-mgmt03 from kube-system started at 2022-12-28 02:16:20 +0000 UTC (1 container statuses recorded)
    Dec 30 04:57:01.030: INFO: 	Container kube-controller-manager ready: true, restart count 1
    Dec 30 04:57:01.031: INFO: kube-proxy-rfm26 from kube-system started at 2022-12-28 02:18:08 +0000 UTC (1 container statuses recorded)
    Dec 30 04:57:01.031: INFO: 	Container kube-proxy ready: true, restart count 0
    Dec 30 04:57:01.031: INFO: kube-scheduler-k8s-mgmt03 from kube-system started at 2022-12-28 02:17:11 +0000 UTC (1 container statuses recorded)
    Dec 30 04:57:01.031: INFO: 	Container kube-scheduler ready: true, restart count 1
    Dec 30 04:57:01.031: INFO: nodelocaldns-76d47 from kube-system started at 2022-12-28 02:22:14 +0000 UTC (1 container statuses recorded)
    Dec 30 04:57:01.031: INFO: 	Container node-cache ready: true, restart count 0
    Dec 30 04:57:01.031: INFO: test-rs-7wrg6 from replicaset-2720 started at 2022-12-30 04:56:59 +0000 UTC (1 container statuses recorded)
    Dec 30 04:57:01.031: INFO: 	Container httpd ready: true, restart count 0
    Dec 30 04:57:01.031: INFO: sonobuoy-e2e-job-c5efb7b0ce524286 from sonobuoy started at 2022-12-30 03:20:18 +0000 UTC (2 container statuses recorded)
    Dec 30 04:57:01.031: INFO: 	Container e2e ready: true, restart count 0
    Dec 30 04:57:01.031: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Dec 30 04:57:01.031: INFO: sonobuoy-systemd-logs-daemon-set-4da877126f61426e-wfvd8 from sonobuoy started at 2022-12-30 03:20:18 +0000 UTC (2 container statuses recorded)
    Dec 30 04:57:01.031: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Dec 30 04:57:01.031: INFO: 	Container systemd-logs ready: true, restart count 0
    Dec 30 04:57:01.031: INFO: 
    Logging pods the apiserver thinks is on node k8s-worker01 before test
    Dec 30 04:57:01.041: INFO: calico-node-dtvd6 from kube-system started at 2022-12-28 02:19:33 +0000 UTC (1 container statuses recorded)
    Dec 30 04:57:01.041: INFO: 	Container calico-node ready: true, restart count 0
    Dec 30 04:57:01.041: INFO: kube-proxy-m86bn from kube-system started at 2022-12-28 02:18:09 +0000 UTC (1 container statuses recorded)
    Dec 30 04:57:01.041: INFO: 	Container kube-proxy ready: true, restart count 0
    Dec 30 04:57:01.041: INFO: nginx-proxy-k8s-worker01 from kube-system started at 2022-12-28 02:17:47 +0000 UTC (1 container statuses recorded)
    Dec 30 04:57:01.041: INFO: 	Container nginx-proxy ready: true, restart count 0
    Dec 30 04:57:01.041: INFO: nodelocaldns-8zm2b from kube-system started at 2022-12-28 02:22:14 +0000 UTC (1 container statuses recorded)
    Dec 30 04:57:01.041: INFO: 	Container node-cache ready: true, restart count 0
    Dec 30 04:57:01.041: INFO: sonobuoy-systemd-logs-daemon-set-4da877126f61426e-4wpvd from sonobuoy started at 2022-12-30 03:20:18 +0000 UTC (2 container statuses recorded)
    Dec 30 04:57:01.041: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Dec 30 04:57:01.041: INFO: 	Container systemd-logs ready: true, restart count 0
    Dec 30 04:57:01.041: INFO: 
    Logging pods the apiserver thinks is on node k8s-worker02 before test
    Dec 30 04:57:01.053: INFO: calico-node-s7npn from kube-system started at 2022-12-28 02:19:33 +0000 UTC (1 container statuses recorded)
    Dec 30 04:57:01.053: INFO: 	Container calico-node ready: true, restart count 0
    Dec 30 04:57:01.053: INFO: kube-proxy-8cg4d from kube-system started at 2022-12-28 02:18:09 +0000 UTC (1 container statuses recorded)
    Dec 30 04:57:01.053: INFO: 	Container kube-proxy ready: true, restart count 0
    Dec 30 04:57:01.053: INFO: nginx-proxy-k8s-worker02 from kube-system started at 2022-12-28 02:17:48 +0000 UTC (1 container statuses recorded)
    Dec 30 04:57:01.053: INFO: 	Container nginx-proxy ready: true, restart count 0
    Dec 30 04:57:01.053: INFO: nodelocaldns-4hlqv from kube-system started at 2022-12-28 02:22:14 +0000 UTC (1 container statuses recorded)
    Dec 30 04:57:01.053: INFO: 	Container node-cache ready: true, restart count 0
    Dec 30 04:57:01.053: INFO: sonobuoy-systemd-logs-daemon-set-4da877126f61426e-7mpss from sonobuoy started at 2022-12-30 03:20:18 +0000 UTC (2 container statuses recorded)
    Dec 30 04:57:01.053: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Dec 30 04:57:01.053: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:438
    STEP: Trying to schedule Pod with nonempty NodeSelector. 12/30/22 04:57:01.053
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.1735794ffce9b118], Reason = [FailedScheduling], Message = [0/5 nodes are available: 5 node(s) didn't match Pod's node affinity/selector. preemption: 0/5 nodes are available: 5 Preemption is not helpful for scheduling.] 12/30/22 04:57:01.093
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Dec 30 04:57:02.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-7915" for this suite. 12/30/22 04:57:02.099
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:57:02.108
Dec 30 04:57:02.108: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename replicaset 12/30/22 04:57:02.11
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:57:02.131
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:57:02.134
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 12/30/22 04:57:02.138
Dec 30 04:57:02.147: INFO: Pod name sample-pod: Found 0 pods out of 1
Dec 30 04:57:07.153: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 12/30/22 04:57:07.153
STEP: getting scale subresource 12/30/22 04:57:07.153
STEP: updating a scale subresource 12/30/22 04:57:07.157
STEP: verifying the replicaset Spec.Replicas was modified 12/30/22 04:57:07.163
STEP: Patch a scale subresource 12/30/22 04:57:07.167
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Dec 30 04:57:07.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-5208" for this suite. 12/30/22 04:57:07.183
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","completed":344,"skipped":6374,"failed":0}
------------------------------
• [SLOW TEST] [5.081 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:57:02.108
    Dec 30 04:57:02.108: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename replicaset 12/30/22 04:57:02.11
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:57:02.131
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:57:02.134
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 12/30/22 04:57:02.138
    Dec 30 04:57:02.147: INFO: Pod name sample-pod: Found 0 pods out of 1
    Dec 30 04:57:07.153: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 12/30/22 04:57:07.153
    STEP: getting scale subresource 12/30/22 04:57:07.153
    STEP: updating a scale subresource 12/30/22 04:57:07.157
    STEP: verifying the replicaset Spec.Replicas was modified 12/30/22 04:57:07.163
    STEP: Patch a scale subresource 12/30/22 04:57:07.167
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Dec 30 04:57:07.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-5208" for this suite. 12/30/22 04:57:07.183
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:57:07.191
Dec 30 04:57:07.191: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename container-probe 12/30/22 04:57:07.193
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:57:07.208
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:57:07.212
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131
STEP: Creating pod busybox-0f2e3de3-76b9-4e86-af89-cadf95256958 in namespace container-probe-1515 12/30/22 04:57:07.216
Dec 30 04:57:07.224: INFO: Waiting up to 5m0s for pod "busybox-0f2e3de3-76b9-4e86-af89-cadf95256958" in namespace "container-probe-1515" to be "not pending"
Dec 30 04:57:07.228: INFO: Pod "busybox-0f2e3de3-76b9-4e86-af89-cadf95256958": Phase="Pending", Reason="", readiness=false. Elapsed: 3.727412ms
Dec 30 04:57:09.234: INFO: Pod "busybox-0f2e3de3-76b9-4e86-af89-cadf95256958": Phase="Running", Reason="", readiness=true. Elapsed: 2.009466913s
Dec 30 04:57:09.234: INFO: Pod "busybox-0f2e3de3-76b9-4e86-af89-cadf95256958" satisfied condition "not pending"
Dec 30 04:57:09.234: INFO: Started pod busybox-0f2e3de3-76b9-4e86-af89-cadf95256958 in namespace container-probe-1515
STEP: checking the pod's current state and verifying that restartCount is present 12/30/22 04:57:09.234
Dec 30 04:57:09.237: INFO: Initial restart count of pod busybox-0f2e3de3-76b9-4e86-af89-cadf95256958 is 0
Dec 30 04:57:59.380: INFO: Restart count of pod container-probe-1515/busybox-0f2e3de3-76b9-4e86-af89-cadf95256958 is now 1 (50.142714723s elapsed)
STEP: deleting the pod 12/30/22 04:57:59.38
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Dec 30 04:57:59.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1515" for this suite. 12/30/22 04:57:59.397
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","completed":345,"skipped":6396,"failed":0}
------------------------------
• [SLOW TEST] [52.213 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:57:07.191
    Dec 30 04:57:07.191: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename container-probe 12/30/22 04:57:07.193
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:57:07.208
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:57:07.212
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:131
    STEP: Creating pod busybox-0f2e3de3-76b9-4e86-af89-cadf95256958 in namespace container-probe-1515 12/30/22 04:57:07.216
    Dec 30 04:57:07.224: INFO: Waiting up to 5m0s for pod "busybox-0f2e3de3-76b9-4e86-af89-cadf95256958" in namespace "container-probe-1515" to be "not pending"
    Dec 30 04:57:07.228: INFO: Pod "busybox-0f2e3de3-76b9-4e86-af89-cadf95256958": Phase="Pending", Reason="", readiness=false. Elapsed: 3.727412ms
    Dec 30 04:57:09.234: INFO: Pod "busybox-0f2e3de3-76b9-4e86-af89-cadf95256958": Phase="Running", Reason="", readiness=true. Elapsed: 2.009466913s
    Dec 30 04:57:09.234: INFO: Pod "busybox-0f2e3de3-76b9-4e86-af89-cadf95256958" satisfied condition "not pending"
    Dec 30 04:57:09.234: INFO: Started pod busybox-0f2e3de3-76b9-4e86-af89-cadf95256958 in namespace container-probe-1515
    STEP: checking the pod's current state and verifying that restartCount is present 12/30/22 04:57:09.234
    Dec 30 04:57:09.237: INFO: Initial restart count of pod busybox-0f2e3de3-76b9-4e86-af89-cadf95256958 is 0
    Dec 30 04:57:59.380: INFO: Restart count of pod container-probe-1515/busybox-0f2e3de3-76b9-4e86-af89-cadf95256958 is now 1 (50.142714723s elapsed)
    STEP: deleting the pod 12/30/22 04:57:59.38
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Dec 30 04:57:59.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-1515" for this suite. 12/30/22 04:57:59.397
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:57:59.405
Dec 30 04:57:59.405: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename containers 12/30/22 04:57:59.407
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:57:59.424
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:57:59.427
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38
Dec 30 04:57:59.440: INFO: Waiting up to 5m0s for pod "client-containers-5e62dcf4-17c6-4d48-bbee-f5c1f1566581" in namespace "containers-1792" to be "running"
Dec 30 04:57:59.444: INFO: Pod "client-containers-5e62dcf4-17c6-4d48-bbee-f5c1f1566581": Phase="Pending", Reason="", readiness=false. Elapsed: 4.084395ms
Dec 30 04:58:01.449: INFO: Pod "client-containers-5e62dcf4-17c6-4d48-bbee-f5c1f1566581": Phase="Running", Reason="", readiness=true. Elapsed: 2.008834438s
Dec 30 04:58:01.449: INFO: Pod "client-containers-5e62dcf4-17c6-4d48-bbee-f5c1f1566581" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Dec 30 04:58:01.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1792" for this suite. 12/30/22 04:58:01.463
{"msg":"PASSED [sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","completed":346,"skipped":6400,"failed":0}
------------------------------
• [2.067 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:57:59.405
    Dec 30 04:57:59.405: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename containers 12/30/22 04:57:59.407
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:57:59.424
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:57:59.427
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:38
    Dec 30 04:57:59.440: INFO: Waiting up to 5m0s for pod "client-containers-5e62dcf4-17c6-4d48-bbee-f5c1f1566581" in namespace "containers-1792" to be "running"
    Dec 30 04:57:59.444: INFO: Pod "client-containers-5e62dcf4-17c6-4d48-bbee-f5c1f1566581": Phase="Pending", Reason="", readiness=false. Elapsed: 4.084395ms
    Dec 30 04:58:01.449: INFO: Pod "client-containers-5e62dcf4-17c6-4d48-bbee-f5c1f1566581": Phase="Running", Reason="", readiness=true. Elapsed: 2.008834438s
    Dec 30 04:58:01.449: INFO: Pod "client-containers-5e62dcf4-17c6-4d48-bbee-f5c1f1566581" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Dec 30 04:58:01.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-1792" for this suite. 12/30/22 04:58:01.463
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:58:01.478
Dec 30 04:58:01.478: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename ingressclass 12/30/22 04:58:01.48
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:58:01.496
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:58:01.499
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 12/30/22 04:58:01.503
STEP: getting /apis/networking.k8s.io 12/30/22 04:58:01.505
STEP: getting /apis/networking.k8s.iov1 12/30/22 04:58:01.507
STEP: creating 12/30/22 04:58:01.508
STEP: getting 12/30/22 04:58:01.523
STEP: listing 12/30/22 04:58:01.527
STEP: watching 12/30/22 04:58:01.531
Dec 30 04:58:01.531: INFO: starting watch
STEP: patching 12/30/22 04:58:01.532
STEP: updating 12/30/22 04:58:01.538
Dec 30 04:58:01.543: INFO: waiting for watch events with expected annotations
Dec 30 04:58:01.543: INFO: saw patched and updated annotations
STEP: deleting 12/30/22 04:58:01.543
STEP: deleting a collection 12/30/22 04:58:01.557
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:187
Dec 30 04:58:01.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-3102" for this suite. 12/30/22 04:58:01.581
{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","completed":347,"skipped":6483,"failed":0}
------------------------------
• [0.109 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:58:01.478
    Dec 30 04:58:01.478: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename ingressclass 12/30/22 04:58:01.48
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:58:01.496
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:58:01.499
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 12/30/22 04:58:01.503
    STEP: getting /apis/networking.k8s.io 12/30/22 04:58:01.505
    STEP: getting /apis/networking.k8s.iov1 12/30/22 04:58:01.507
    STEP: creating 12/30/22 04:58:01.508
    STEP: getting 12/30/22 04:58:01.523
    STEP: listing 12/30/22 04:58:01.527
    STEP: watching 12/30/22 04:58:01.531
    Dec 30 04:58:01.531: INFO: starting watch
    STEP: patching 12/30/22 04:58:01.532
    STEP: updating 12/30/22 04:58:01.538
    Dec 30 04:58:01.543: INFO: waiting for watch events with expected annotations
    Dec 30 04:58:01.543: INFO: saw patched and updated annotations
    STEP: deleting 12/30/22 04:58:01.543
    STEP: deleting a collection 12/30/22 04:58:01.557
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/framework.go:187
    Dec 30 04:58:01.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ingressclass-3102" for this suite. 12/30/22 04:58:01.581
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:58:01.589
Dec 30 04:58:01.589: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename dns 12/30/22 04:58:01.59
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:58:01.607
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:58:01.61
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 12/30/22 04:58:01.614
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 12/30/22 04:58:01.614
STEP: creating a pod to probe DNS 12/30/22 04:58:01.614
STEP: submitting the pod to kubernetes 12/30/22 04:58:01.614
Dec 30 04:58:01.623: INFO: Waiting up to 15m0s for pod "dns-test-ae4426eb-e6ad-4394-9b91-b47b782e4a81" in namespace "dns-694" to be "running"
Dec 30 04:58:01.627: INFO: Pod "dns-test-ae4426eb-e6ad-4394-9b91-b47b782e4a81": Phase="Pending", Reason="", readiness=false. Elapsed: 3.80615ms
Dec 30 04:58:03.633: INFO: Pod "dns-test-ae4426eb-e6ad-4394-9b91-b47b782e4a81": Phase="Running", Reason="", readiness=true. Elapsed: 2.01017717s
Dec 30 04:58:03.633: INFO: Pod "dns-test-ae4426eb-e6ad-4394-9b91-b47b782e4a81" satisfied condition "running"
STEP: retrieving the pod 12/30/22 04:58:03.633
STEP: looking for the results for each expected name from probers 12/30/22 04:58:03.638
Dec 30 04:58:03.657: INFO: DNS probes using dns-694/dns-test-ae4426eb-e6ad-4394-9b91-b47b782e4a81 succeeded

STEP: deleting the pod 12/30/22 04:58:03.657
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Dec 30 04:58:03.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-694" for this suite. 12/30/22 04:58:03.678
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","completed":348,"skipped":6491,"failed":0}
------------------------------
• [2.096 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:58:01.589
    Dec 30 04:58:01.589: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename dns 12/30/22 04:58:01.59
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:58:01.607
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:58:01.61
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     12/30/22 04:58:01.614
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     12/30/22 04:58:01.614
    STEP: creating a pod to probe DNS 12/30/22 04:58:01.614
    STEP: submitting the pod to kubernetes 12/30/22 04:58:01.614
    Dec 30 04:58:01.623: INFO: Waiting up to 15m0s for pod "dns-test-ae4426eb-e6ad-4394-9b91-b47b782e4a81" in namespace "dns-694" to be "running"
    Dec 30 04:58:01.627: INFO: Pod "dns-test-ae4426eb-e6ad-4394-9b91-b47b782e4a81": Phase="Pending", Reason="", readiness=false. Elapsed: 3.80615ms
    Dec 30 04:58:03.633: INFO: Pod "dns-test-ae4426eb-e6ad-4394-9b91-b47b782e4a81": Phase="Running", Reason="", readiness=true. Elapsed: 2.01017717s
    Dec 30 04:58:03.633: INFO: Pod "dns-test-ae4426eb-e6ad-4394-9b91-b47b782e4a81" satisfied condition "running"
    STEP: retrieving the pod 12/30/22 04:58:03.633
    STEP: looking for the results for each expected name from probers 12/30/22 04:58:03.638
    Dec 30 04:58:03.657: INFO: DNS probes using dns-694/dns-test-ae4426eb-e6ad-4394-9b91-b47b782e4a81 succeeded

    STEP: deleting the pod 12/30/22 04:58:03.657
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Dec 30 04:58:03.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-694" for this suite. 12/30/22 04:58:03.678
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:58:03.687
Dec 30 04:58:03.688: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename services 12/30/22 04:58:03.689
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:58:03.706
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:58:03.71
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641
STEP: creating a collection of services 12/30/22 04:58:03.713
Dec 30 04:58:03.713: INFO: Creating e2e-svc-a-l484t
Dec 30 04:58:03.723: INFO: Creating e2e-svc-b-bjr86
Dec 30 04:58:03.734: INFO: Creating e2e-svc-c-zfg92
STEP: deleting service collection 12/30/22 04:58:03.747
Dec 30 04:58:03.778: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Dec 30 04:58:03.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6765" for this suite. 12/30/22 04:58:03.784
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should delete a collection of services [Conformance]","completed":349,"skipped":6521,"failed":0}
------------------------------
• [0.104 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:58:03.687
    Dec 30 04:58:03.688: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename services 12/30/22 04:58:03.689
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:58:03.706
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:58:03.71
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3641
    STEP: creating a collection of services 12/30/22 04:58:03.713
    Dec 30 04:58:03.713: INFO: Creating e2e-svc-a-l484t
    Dec 30 04:58:03.723: INFO: Creating e2e-svc-b-bjr86
    Dec 30 04:58:03.734: INFO: Creating e2e-svc-c-zfg92
    STEP: deleting service collection 12/30/22 04:58:03.747
    Dec 30 04:58:03.778: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Dec 30 04:58:03.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-6765" for this suite. 12/30/22 04:58:03.784
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:58:03.791
Dec 30 04:58:03.791: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename hostport 12/30/22 04:58:03.793
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:58:03.809
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:58:03.812
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 12/30/22 04:58:03.822
Dec 30 04:58:03.832: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-9962" to be "running and ready"
Dec 30 04:58:03.836: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.352133ms
Dec 30 04:58:03.836: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Dec 30 04:58:05.841: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.009037511s
Dec 30 04:58:05.841: INFO: The phase of Pod pod1 is Running (Ready = true)
Dec 30 04:58:05.841: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.78.26.141 on the node which pod1 resides and expect scheduled 12/30/22 04:58:05.841
Dec 30 04:58:05.848: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-9962" to be "running and ready"
Dec 30 04:58:05.852: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020717ms
Dec 30 04:58:05.852: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Dec 30 04:58:07.858: INFO: Pod "pod2": Phase="Running", Reason="", readiness=false. Elapsed: 2.010197006s
Dec 30 04:58:07.858: INFO: The phase of Pod pod2 is Running (Ready = false)
Dec 30 04:58:09.857: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.008999699s
Dec 30 04:58:09.857: INFO: The phase of Pod pod2 is Running (Ready = true)
Dec 30 04:58:09.857: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.78.26.141 but use UDP protocol on the node which pod2 resides 12/30/22 04:58:09.857
Dec 30 04:58:09.863: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-9962" to be "running and ready"
Dec 30 04:58:09.867: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.984037ms
Dec 30 04:58:09.867: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Dec 30 04:58:11.873: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.010095175s
Dec 30 04:58:11.873: INFO: The phase of Pod pod3 is Running (Ready = true)
Dec 30 04:58:11.873: INFO: Pod "pod3" satisfied condition "running and ready"
Dec 30 04:58:11.880: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-9962" to be "running and ready"
Dec 30 04:58:11.884: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 4.050394ms
Dec 30 04:58:11.884: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Dec 30 04:58:13.889: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.009251046s
Dec 30 04:58:13.889: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
Dec 30 04:58:13.889: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 12/30/22 04:58:13.893
Dec 30 04:58:13.893: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.78.26.141 http://127.0.0.1:54323/hostname] Namespace:hostport-9962 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 30 04:58:13.893: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
Dec 30 04:58:13.894: INFO: ExecWithOptions: Clientset creation
Dec 30 04:58:13.894: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-9962/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.78.26.141+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.78.26.141, port: 54323 12/30/22 04:58:13.991
Dec 30 04:58:13.991: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.78.26.141:54323/hostname] Namespace:hostport-9962 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 30 04:58:13.991: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
Dec 30 04:58:13.992: INFO: ExecWithOptions: Clientset creation
Dec 30 04:58:13.992: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-9962/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.78.26.141%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.78.26.141, port: 54323 UDP 12/30/22 04:58:14.11
Dec 30 04:58:14.110: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.78.26.141 54323] Namespace:hostport-9962 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 30 04:58:14.110: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
Dec 30 04:58:14.111: INFO: ExecWithOptions: Clientset creation
Dec 30 04:58:14.111: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-9962/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.78.26.141+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/framework.go:187
Dec 30 04:58:19.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-9962" for this suite. 12/30/22 04:58:19.214
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","completed":350,"skipped":6521,"failed":0}
------------------------------
• [SLOW TEST] [15.430 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:58:03.791
    Dec 30 04:58:03.791: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename hostport 12/30/22 04:58:03.793
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:58:03.809
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:58:03.812
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 12/30/22 04:58:03.822
    Dec 30 04:58:03.832: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-9962" to be "running and ready"
    Dec 30 04:58:03.836: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.352133ms
    Dec 30 04:58:03.836: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Dec 30 04:58:05.841: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.009037511s
    Dec 30 04:58:05.841: INFO: The phase of Pod pod1 is Running (Ready = true)
    Dec 30 04:58:05.841: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.78.26.141 on the node which pod1 resides and expect scheduled 12/30/22 04:58:05.841
    Dec 30 04:58:05.848: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-9962" to be "running and ready"
    Dec 30 04:58:05.852: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020717ms
    Dec 30 04:58:05.852: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Dec 30 04:58:07.858: INFO: Pod "pod2": Phase="Running", Reason="", readiness=false. Elapsed: 2.010197006s
    Dec 30 04:58:07.858: INFO: The phase of Pod pod2 is Running (Ready = false)
    Dec 30 04:58:09.857: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.008999699s
    Dec 30 04:58:09.857: INFO: The phase of Pod pod2 is Running (Ready = true)
    Dec 30 04:58:09.857: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.78.26.141 but use UDP protocol on the node which pod2 resides 12/30/22 04:58:09.857
    Dec 30 04:58:09.863: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-9962" to be "running and ready"
    Dec 30 04:58:09.867: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.984037ms
    Dec 30 04:58:09.867: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Dec 30 04:58:11.873: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.010095175s
    Dec 30 04:58:11.873: INFO: The phase of Pod pod3 is Running (Ready = true)
    Dec 30 04:58:11.873: INFO: Pod "pod3" satisfied condition "running and ready"
    Dec 30 04:58:11.880: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-9962" to be "running and ready"
    Dec 30 04:58:11.884: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 4.050394ms
    Dec 30 04:58:11.884: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Dec 30 04:58:13.889: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.009251046s
    Dec 30 04:58:13.889: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    Dec 30 04:58:13.889: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 12/30/22 04:58:13.893
    Dec 30 04:58:13.893: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.78.26.141 http://127.0.0.1:54323/hostname] Namespace:hostport-9962 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 30 04:58:13.893: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    Dec 30 04:58:13.894: INFO: ExecWithOptions: Clientset creation
    Dec 30 04:58:13.894: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-9962/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.78.26.141+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.78.26.141, port: 54323 12/30/22 04:58:13.991
    Dec 30 04:58:13.991: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.78.26.141:54323/hostname] Namespace:hostport-9962 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 30 04:58:13.991: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    Dec 30 04:58:13.992: INFO: ExecWithOptions: Clientset creation
    Dec 30 04:58:13.992: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-9962/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.78.26.141%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.78.26.141, port: 54323 UDP 12/30/22 04:58:14.11
    Dec 30 04:58:14.110: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.78.26.141 54323] Namespace:hostport-9962 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Dec 30 04:58:14.110: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    Dec 30 04:58:14.111: INFO: ExecWithOptions: Clientset creation
    Dec 30 04:58:14.111: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-9962/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.78.26.141+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/framework.go:187
    Dec 30 04:58:19.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "hostport-9962" for this suite. 12/30/22 04:58:19.214
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:485
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:58:19.223
Dec 30 04:58:19.223: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename security-context-test 12/30/22 04:58:19.224
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:58:19.242
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:58:19.245
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:485
Dec 30 04:58:19.259: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-9c5e5dbf-daaf-44d2-911e-5ce8ce6f9b16" in namespace "security-context-test-7397" to be "Succeeded or Failed"
Dec 30 04:58:19.262: INFO: Pod "busybox-readonly-false-9c5e5dbf-daaf-44d2-911e-5ce8ce6f9b16": Phase="Pending", Reason="", readiness=false. Elapsed: 3.467879ms
Dec 30 04:58:21.268: INFO: Pod "busybox-readonly-false-9c5e5dbf-daaf-44d2-911e-5ce8ce6f9b16": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009045777s
Dec 30 04:58:23.268: INFO: Pod "busybox-readonly-false-9c5e5dbf-daaf-44d2-911e-5ce8ce6f9b16": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009798156s
Dec 30 04:58:23.269: INFO: Pod "busybox-readonly-false-9c5e5dbf-daaf-44d2-911e-5ce8ce6f9b16" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Dec 30 04:58:23.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-7397" for this suite. 12/30/22 04:58:23.275
{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","completed":351,"skipped":6524,"failed":0}
------------------------------
• [4.061 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:429
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:485

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:58:19.223
    Dec 30 04:58:19.223: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename security-context-test 12/30/22 04:58:19.224
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:58:19.242
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:58:19.245
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:485
    Dec 30 04:58:19.259: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-9c5e5dbf-daaf-44d2-911e-5ce8ce6f9b16" in namespace "security-context-test-7397" to be "Succeeded or Failed"
    Dec 30 04:58:19.262: INFO: Pod "busybox-readonly-false-9c5e5dbf-daaf-44d2-911e-5ce8ce6f9b16": Phase="Pending", Reason="", readiness=false. Elapsed: 3.467879ms
    Dec 30 04:58:21.268: INFO: Pod "busybox-readonly-false-9c5e5dbf-daaf-44d2-911e-5ce8ce6f9b16": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009045777s
    Dec 30 04:58:23.268: INFO: Pod "busybox-readonly-false-9c5e5dbf-daaf-44d2-911e-5ce8ce6f9b16": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009798156s
    Dec 30 04:58:23.269: INFO: Pod "busybox-readonly-false-9c5e5dbf-daaf-44d2-911e-5ce8ce6f9b16" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Dec 30 04:58:23.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-7397" for this suite. 12/30/22 04:58:23.275
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 04:58:23.286
Dec 30 04:58:23.286: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename var-expansion 12/30/22 04:58:23.288
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:58:23.305
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:58:23.308
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224
STEP: creating the pod with failed condition 12/30/22 04:58:23.312
Dec 30 04:58:23.321: INFO: Waiting up to 2m0s for pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45" in namespace "var-expansion-1082" to be "running"
Dec 30 04:58:23.325: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007758ms
Dec 30 04:58:25.331: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009445163s
Dec 30 04:58:27.332: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010737702s
Dec 30 04:58:29.330: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009255334s
Dec 30 04:58:31.331: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 8.009486161s
Dec 30 04:58:33.331: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 10.009453615s
Dec 30 04:58:35.331: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 12.010287545s
Dec 30 04:58:37.332: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 14.010337996s
Dec 30 04:58:39.332: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 16.010470052s
Dec 30 04:58:41.331: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 18.01012837s
Dec 30 04:58:43.332: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 20.010382172s
Dec 30 04:58:45.330: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 22.008641298s
Dec 30 04:58:47.331: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 24.010305757s
Dec 30 04:58:49.331: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 26.010269133s
Dec 30 04:58:51.331: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 28.009327677s
Dec 30 04:58:53.331: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 30.009913363s
Dec 30 04:58:55.332: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 32.010370789s
Dec 30 04:58:57.331: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 34.009374596s
Dec 30 04:58:59.330: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 36.008942539s
Dec 30 04:59:01.330: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 38.008528943s
Dec 30 04:59:03.331: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 40.010028132s
Dec 30 04:59:05.332: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 42.010511146s
Dec 30 04:59:07.331: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 44.009387692s
Dec 30 04:59:09.330: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 46.008509813s
Dec 30 04:59:11.330: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 48.009285792s
Dec 30 04:59:13.332: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 50.010403751s
Dec 30 04:59:15.332: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 52.010501357s
Dec 30 04:59:17.331: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 54.009347038s
Dec 30 04:59:19.330: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 56.009092523s
Dec 30 04:59:21.330: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 58.008408136s
Dec 30 04:59:23.331: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.009863444s
Dec 30 04:59:25.331: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.010253441s
Dec 30 04:59:27.331: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.009930062s
Dec 30 04:59:29.331: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.010203988s
Dec 30 04:59:31.330: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.009244458s
Dec 30 04:59:33.331: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.010258277s
Dec 30 04:59:35.330: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.008613366s
Dec 30 04:59:37.331: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.010072109s
Dec 30 04:59:39.331: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.009677986s
Dec 30 04:59:41.330: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.009250637s
Dec 30 04:59:43.331: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.010112582s
Dec 30 04:59:45.330: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.008658037s
Dec 30 04:59:47.331: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.010180317s
Dec 30 04:59:49.332: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.010373099s
Dec 30 04:59:51.331: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.009474986s
Dec 30 04:59:53.330: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.008592346s
Dec 30 04:59:55.331: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.009541727s
Dec 30 04:59:57.330: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.009302493s
Dec 30 04:59:59.331: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.009615604s
Dec 30 05:00:01.331: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.010211144s
Dec 30 05:00:03.332: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.010449766s
Dec 30 05:00:05.331: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.01016955s
Dec 30 05:00:07.330: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.008829134s
Dec 30 05:00:09.330: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.009247654s
Dec 30 05:00:11.330: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.008559994s
Dec 30 05:00:13.331: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.009787268s
Dec 30 05:00:15.331: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.010165054s
Dec 30 05:00:17.331: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.009336238s
Dec 30 05:00:19.332: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.010599669s
Dec 30 05:00:21.331: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.009621793s
Dec 30 05:00:23.331: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.010181448s
Dec 30 05:00:23.335: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.014223802s
STEP: updating the pod 12/30/22 05:00:23.335
Dec 30 05:00:23.850: INFO: Successfully updated pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45"
STEP: waiting for pod running 12/30/22 05:00:23.851
Dec 30 05:00:23.851: INFO: Waiting up to 2m0s for pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45" in namespace "var-expansion-1082" to be "running"
Dec 30 05:00:23.855: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 3.792091ms
Dec 30 05:00:25.859: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Running", Reason="", readiness=true. Elapsed: 2.007972412s
Dec 30 05:00:25.859: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45" satisfied condition "running"
STEP: deleting the pod gracefully 12/30/22 05:00:25.859
Dec 30 05:00:25.859: INFO: Deleting pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45" in namespace "var-expansion-1082"
Dec 30 05:00:25.867: INFO: Wait up to 5m0s for pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Dec 30 05:00:57.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1082" for this suite. 12/30/22 05:00:57.883
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","completed":352,"skipped":6548,"failed":0}
------------------------------
• [SLOW TEST] [154.604 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 04:58:23.286
    Dec 30 04:58:23.286: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename var-expansion 12/30/22 04:58:23.288
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 04:58:23.305
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 04:58:23.308
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:224
    STEP: creating the pod with failed condition 12/30/22 04:58:23.312
    Dec 30 04:58:23.321: INFO: Waiting up to 2m0s for pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45" in namespace "var-expansion-1082" to be "running"
    Dec 30 04:58:23.325: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007758ms
    Dec 30 04:58:25.331: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009445163s
    Dec 30 04:58:27.332: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010737702s
    Dec 30 04:58:29.330: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009255334s
    Dec 30 04:58:31.331: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 8.009486161s
    Dec 30 04:58:33.331: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 10.009453615s
    Dec 30 04:58:35.331: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 12.010287545s
    Dec 30 04:58:37.332: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 14.010337996s
    Dec 30 04:58:39.332: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 16.010470052s
    Dec 30 04:58:41.331: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 18.01012837s
    Dec 30 04:58:43.332: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 20.010382172s
    Dec 30 04:58:45.330: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 22.008641298s
    Dec 30 04:58:47.331: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 24.010305757s
    Dec 30 04:58:49.331: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 26.010269133s
    Dec 30 04:58:51.331: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 28.009327677s
    Dec 30 04:58:53.331: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 30.009913363s
    Dec 30 04:58:55.332: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 32.010370789s
    Dec 30 04:58:57.331: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 34.009374596s
    Dec 30 04:58:59.330: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 36.008942539s
    Dec 30 04:59:01.330: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 38.008528943s
    Dec 30 04:59:03.331: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 40.010028132s
    Dec 30 04:59:05.332: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 42.010511146s
    Dec 30 04:59:07.331: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 44.009387692s
    Dec 30 04:59:09.330: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 46.008509813s
    Dec 30 04:59:11.330: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 48.009285792s
    Dec 30 04:59:13.332: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 50.010403751s
    Dec 30 04:59:15.332: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 52.010501357s
    Dec 30 04:59:17.331: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 54.009347038s
    Dec 30 04:59:19.330: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 56.009092523s
    Dec 30 04:59:21.330: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 58.008408136s
    Dec 30 04:59:23.331: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.009863444s
    Dec 30 04:59:25.331: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.010253441s
    Dec 30 04:59:27.331: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.009930062s
    Dec 30 04:59:29.331: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.010203988s
    Dec 30 04:59:31.330: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.009244458s
    Dec 30 04:59:33.331: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.010258277s
    Dec 30 04:59:35.330: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.008613366s
    Dec 30 04:59:37.331: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.010072109s
    Dec 30 04:59:39.331: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.009677986s
    Dec 30 04:59:41.330: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.009250637s
    Dec 30 04:59:43.331: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.010112582s
    Dec 30 04:59:45.330: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.008658037s
    Dec 30 04:59:47.331: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.010180317s
    Dec 30 04:59:49.332: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.010373099s
    Dec 30 04:59:51.331: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.009474986s
    Dec 30 04:59:53.330: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.008592346s
    Dec 30 04:59:55.331: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.009541727s
    Dec 30 04:59:57.330: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.009302493s
    Dec 30 04:59:59.331: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.009615604s
    Dec 30 05:00:01.331: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.010211144s
    Dec 30 05:00:03.332: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.010449766s
    Dec 30 05:00:05.331: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.01016955s
    Dec 30 05:00:07.330: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.008829134s
    Dec 30 05:00:09.330: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.009247654s
    Dec 30 05:00:11.330: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.008559994s
    Dec 30 05:00:13.331: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.009787268s
    Dec 30 05:00:15.331: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.010165054s
    Dec 30 05:00:17.331: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.009336238s
    Dec 30 05:00:19.332: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.010599669s
    Dec 30 05:00:21.331: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.009621793s
    Dec 30 05:00:23.331: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.010181448s
    Dec 30 05:00:23.335: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.014223802s
    STEP: updating the pod 12/30/22 05:00:23.335
    Dec 30 05:00:23.850: INFO: Successfully updated pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45"
    STEP: waiting for pod running 12/30/22 05:00:23.851
    Dec 30 05:00:23.851: INFO: Waiting up to 2m0s for pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45" in namespace "var-expansion-1082" to be "running"
    Dec 30 05:00:23.855: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Pending", Reason="", readiness=false. Elapsed: 3.792091ms
    Dec 30 05:00:25.859: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45": Phase="Running", Reason="", readiness=true. Elapsed: 2.007972412s
    Dec 30 05:00:25.859: INFO: Pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45" satisfied condition "running"
    STEP: deleting the pod gracefully 12/30/22 05:00:25.859
    Dec 30 05:00:25.859: INFO: Deleting pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45" in namespace "var-expansion-1082"
    Dec 30 05:00:25.867: INFO: Wait up to 5m0s for pod "var-expansion-a9d94381-3ccf-45f1-a844-47d0ab029b45" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Dec 30 05:00:57.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-1082" for this suite. 12/30/22 05:00:57.883
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:247
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 05:00:57.891
Dec 30 05:00:57.891: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename container-runtime 12/30/22 05:00:57.893
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 05:00:57.912
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 05:00:57.916
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:247
STEP: create the container 12/30/22 05:00:57.919
STEP: wait for the container to reach Succeeded 12/30/22 05:00:57.928
STEP: get the container status 12/30/22 05:01:01.954
STEP: the container should be terminated 12/30/22 05:01:01.958
STEP: the termination message should be set 12/30/22 05:01:01.958
Dec 30 05:01:01.958: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 12/30/22 05:01:01.958
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Dec 30 05:01:01.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1662" for this suite. 12/30/22 05:01:01.982
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":353,"skipped":6558,"failed":0}
------------------------------
• [4.097 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:247

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 05:00:57.891
    Dec 30 05:00:57.891: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename container-runtime 12/30/22 05:00:57.893
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 05:00:57.912
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 05:00:57.916
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:247
    STEP: create the container 12/30/22 05:00:57.919
    STEP: wait for the container to reach Succeeded 12/30/22 05:00:57.928
    STEP: get the container status 12/30/22 05:01:01.954
    STEP: the container should be terminated 12/30/22 05:01:01.958
    STEP: the termination message should be set 12/30/22 05:01:01.958
    Dec 30 05:01:01.958: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 12/30/22 05:01:01.958
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Dec 30 05:01:01.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-1662" for this suite. 12/30/22 05:01:01.982
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 05:01:01.99
Dec 30 05:01:01.990: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename webhook 12/30/22 05:01:01.991
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 05:01:02.007
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 05:01:02.011
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 12/30/22 05:01:02.028
STEP: Create role binding to let webhook read extension-apiserver-authentication 12/30/22 05:01:02.364
STEP: Deploying the webhook pod 12/30/22 05:01:02.373
STEP: Wait for the deployment to be ready 12/30/22 05:01:02.385
Dec 30 05:01:02.393: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 12/30/22 05:01:04.406
STEP: Verifying the service has paired with the endpoint 12/30/22 05:01:04.416
Dec 30 05:01:05.417: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 12/30/22 05:01:05.421
STEP: create a configmap that should be updated by the webhook 12/30/22 05:01:05.44
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Dec 30 05:01:05.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3164" for this suite. 12/30/22 05:01:05.469
STEP: Destroying namespace "webhook-3164-markers" for this suite. 12/30/22 05:01:05.476
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","completed":354,"skipped":6564,"failed":0}
------------------------------
• [3.528 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 05:01:01.99
    Dec 30 05:01:01.990: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename webhook 12/30/22 05:01:01.991
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 05:01:02.007
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 05:01:02.011
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 12/30/22 05:01:02.028
    STEP: Create role binding to let webhook read extension-apiserver-authentication 12/30/22 05:01:02.364
    STEP: Deploying the webhook pod 12/30/22 05:01:02.373
    STEP: Wait for the deployment to be ready 12/30/22 05:01:02.385
    Dec 30 05:01:02.393: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 12/30/22 05:01:04.406
    STEP: Verifying the service has paired with the endpoint 12/30/22 05:01:04.416
    Dec 30 05:01:05.417: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:251
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 12/30/22 05:01:05.421
    STEP: create a configmap that should be updated by the webhook 12/30/22 05:01:05.44
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Dec 30 05:01:05.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-3164" for this suite. 12/30/22 05:01:05.469
    STEP: Destroying namespace "webhook-3164-markers" for this suite. 12/30/22 05:01:05.476
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 05:01:05.519
Dec 30 05:01:05.519: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename projected 12/30/22 05:01:05.52
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 05:01:05.536
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 05:01:05.54
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220
STEP: Creating a pod to test downward API volume plugin 12/30/22 05:01:05.543
Dec 30 05:01:05.553: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f042017a-5ffe-47e0-88d8-34be69f5e1cb" in namespace "projected-698" to be "Succeeded or Failed"
Dec 30 05:01:05.557: INFO: Pod "downwardapi-volume-f042017a-5ffe-47e0-88d8-34be69f5e1cb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.488076ms
Dec 30 05:01:07.564: INFO: Pod "downwardapi-volume-f042017a-5ffe-47e0-88d8-34be69f5e1cb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010453137s
Dec 30 05:01:09.563: INFO: Pod "downwardapi-volume-f042017a-5ffe-47e0-88d8-34be69f5e1cb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009598723s
STEP: Saw pod success 12/30/22 05:01:09.563
Dec 30 05:01:09.563: INFO: Pod "downwardapi-volume-f042017a-5ffe-47e0-88d8-34be69f5e1cb" satisfied condition "Succeeded or Failed"
Dec 30 05:01:09.568: INFO: Trying to get logs from node k8s-mgmt01 pod downwardapi-volume-f042017a-5ffe-47e0-88d8-34be69f5e1cb container client-container: <nil>
STEP: delete the pod 12/30/22 05:01:09.589
Dec 30 05:01:09.603: INFO: Waiting for pod downwardapi-volume-f042017a-5ffe-47e0-88d8-34be69f5e1cb to disappear
Dec 30 05:01:09.607: INFO: Pod downwardapi-volume-f042017a-5ffe-47e0-88d8-34be69f5e1cb no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Dec 30 05:01:09.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-698" for this suite. 12/30/22 05:01:09.612
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","completed":355,"skipped":6570,"failed":0}
------------------------------
• [4.100 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 05:01:05.519
    Dec 30 05:01:05.519: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename projected 12/30/22 05:01:05.52
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 05:01:05.536
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 05:01:05.54
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:220
    STEP: Creating a pod to test downward API volume plugin 12/30/22 05:01:05.543
    Dec 30 05:01:05.553: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f042017a-5ffe-47e0-88d8-34be69f5e1cb" in namespace "projected-698" to be "Succeeded or Failed"
    Dec 30 05:01:05.557: INFO: Pod "downwardapi-volume-f042017a-5ffe-47e0-88d8-34be69f5e1cb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.488076ms
    Dec 30 05:01:07.564: INFO: Pod "downwardapi-volume-f042017a-5ffe-47e0-88d8-34be69f5e1cb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010453137s
    Dec 30 05:01:09.563: INFO: Pod "downwardapi-volume-f042017a-5ffe-47e0-88d8-34be69f5e1cb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009598723s
    STEP: Saw pod success 12/30/22 05:01:09.563
    Dec 30 05:01:09.563: INFO: Pod "downwardapi-volume-f042017a-5ffe-47e0-88d8-34be69f5e1cb" satisfied condition "Succeeded or Failed"
    Dec 30 05:01:09.568: INFO: Trying to get logs from node k8s-mgmt01 pod downwardapi-volume-f042017a-5ffe-47e0-88d8-34be69f5e1cb container client-container: <nil>
    STEP: delete the pod 12/30/22 05:01:09.589
    Dec 30 05:01:09.603: INFO: Waiting for pod downwardapi-volume-f042017a-5ffe-47e0-88d8-34be69f5e1cb to disappear
    Dec 30 05:01:09.607: INFO: Pod downwardapi-volume-f042017a-5ffe-47e0-88d8-34be69f5e1cb no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Dec 30 05:01:09.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-698" for this suite. 12/30/22 05:01:09.612
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 05:01:09.621
Dec 30 05:01:09.621: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename projected 12/30/22 05:01:09.622
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 05:01:09.64
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 05:01:09.643
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77
STEP: Creating projection with secret that has name projected-secret-test-map-12321ad1-941f-4d7b-a495-80dc2197137c 12/30/22 05:01:09.647
STEP: Creating a pod to test consume secrets 12/30/22 05:01:09.652
Dec 30 05:01:09.660: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-73e32f5e-eaa1-44b9-9760-9bb02b3a0aa4" in namespace "projected-8366" to be "Succeeded or Failed"
Dec 30 05:01:09.665: INFO: Pod "pod-projected-secrets-73e32f5e-eaa1-44b9-9760-9bb02b3a0aa4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.135627ms
Dec 30 05:01:11.669: INFO: Pod "pod-projected-secrets-73e32f5e-eaa1-44b9-9760-9bb02b3a0aa4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008857748s
Dec 30 05:01:13.671: INFO: Pod "pod-projected-secrets-73e32f5e-eaa1-44b9-9760-9bb02b3a0aa4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009991956s
STEP: Saw pod success 12/30/22 05:01:13.671
Dec 30 05:01:13.671: INFO: Pod "pod-projected-secrets-73e32f5e-eaa1-44b9-9760-9bb02b3a0aa4" satisfied condition "Succeeded or Failed"
Dec 30 05:01:13.675: INFO: Trying to get logs from node k8s-mgmt01 pod pod-projected-secrets-73e32f5e-eaa1-44b9-9760-9bb02b3a0aa4 container projected-secret-volume-test: <nil>
STEP: delete the pod 12/30/22 05:01:13.684
Dec 30 05:01:13.697: INFO: Waiting for pod pod-projected-secrets-73e32f5e-eaa1-44b9-9760-9bb02b3a0aa4 to disappear
Dec 30 05:01:13.701: INFO: Pod pod-projected-secrets-73e32f5e-eaa1-44b9-9760-9bb02b3a0aa4 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Dec 30 05:01:13.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8366" for this suite. 12/30/22 05:01:13.707
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":356,"skipped":6591,"failed":0}
------------------------------
• [4.093 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 05:01:09.621
    Dec 30 05:01:09.621: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename projected 12/30/22 05:01:09.622
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 05:01:09.64
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 05:01:09.643
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:77
    STEP: Creating projection with secret that has name projected-secret-test-map-12321ad1-941f-4d7b-a495-80dc2197137c 12/30/22 05:01:09.647
    STEP: Creating a pod to test consume secrets 12/30/22 05:01:09.652
    Dec 30 05:01:09.660: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-73e32f5e-eaa1-44b9-9760-9bb02b3a0aa4" in namespace "projected-8366" to be "Succeeded or Failed"
    Dec 30 05:01:09.665: INFO: Pod "pod-projected-secrets-73e32f5e-eaa1-44b9-9760-9bb02b3a0aa4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.135627ms
    Dec 30 05:01:11.669: INFO: Pod "pod-projected-secrets-73e32f5e-eaa1-44b9-9760-9bb02b3a0aa4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008857748s
    Dec 30 05:01:13.671: INFO: Pod "pod-projected-secrets-73e32f5e-eaa1-44b9-9760-9bb02b3a0aa4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009991956s
    STEP: Saw pod success 12/30/22 05:01:13.671
    Dec 30 05:01:13.671: INFO: Pod "pod-projected-secrets-73e32f5e-eaa1-44b9-9760-9bb02b3a0aa4" satisfied condition "Succeeded or Failed"
    Dec 30 05:01:13.675: INFO: Trying to get logs from node k8s-mgmt01 pod pod-projected-secrets-73e32f5e-eaa1-44b9-9760-9bb02b3a0aa4 container projected-secret-volume-test: <nil>
    STEP: delete the pod 12/30/22 05:01:13.684
    Dec 30 05:01:13.697: INFO: Waiting for pod pod-projected-secrets-73e32f5e-eaa1-44b9-9760-9bb02b3a0aa4 to disappear
    Dec 30 05:01:13.701: INFO: Pod pod-projected-secrets-73e32f5e-eaa1-44b9-9760-9bb02b3a0aa4 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Dec 30 05:01:13.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8366" for this suite. 12/30/22 05:01:13.707
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 05:01:13.715
Dec 30 05:01:13.715: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename daemonsets 12/30/22 05:01:13.716
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 05:01:13.733
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 05:01:13.737
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861
STEP: Creating simple DaemonSet "daemon-set" 12/30/22 05:01:13.777
STEP: Check that daemon pods launch on every node of the cluster. 12/30/22 05:01:13.783
Dec 30 05:01:13.792: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 30 05:01:13.793: INFO: Node k8s-mgmt01 is running 0 daemon pod, expected 1
Dec 30 05:01:14.805: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 30 05:01:14.805: INFO: Node k8s-mgmt01 is running 0 daemon pod, expected 1
Dec 30 05:01:15.805: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Dec 30 05:01:15.805: INFO: Node k8s-worker01 is running 0 daemon pod, expected 1
Dec 30 05:01:16.805: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
Dec 30 05:01:16.805: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
STEP: Getting /status 12/30/22 05:01:16.809
Dec 30 05:01:16.814: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 12/30/22 05:01:16.814
Dec 30 05:01:16.825: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 12/30/22 05:01:16.825
Dec 30 05:01:16.828: INFO: Observed &DaemonSet event: ADDED
Dec 30 05:01:16.828: INFO: Observed &DaemonSet event: MODIFIED
Dec 30 05:01:16.828: INFO: Observed &DaemonSet event: MODIFIED
Dec 30 05:01:16.829: INFO: Observed &DaemonSet event: MODIFIED
Dec 30 05:01:16.829: INFO: Observed &DaemonSet event: MODIFIED
Dec 30 05:01:16.829: INFO: Observed &DaemonSet event: MODIFIED
Dec 30 05:01:16.829: INFO: Observed &DaemonSet event: MODIFIED
Dec 30 05:01:16.829: INFO: Found daemon set daemon-set in namespace daemonsets-6686 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Dec 30 05:01:16.829: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 12/30/22 05:01:16.829
STEP: watching for the daemon set status to be patched 12/30/22 05:01:16.839
Dec 30 05:01:16.841: INFO: Observed &DaemonSet event: ADDED
Dec 30 05:01:16.841: INFO: Observed &DaemonSet event: MODIFIED
Dec 30 05:01:16.841: INFO: Observed &DaemonSet event: MODIFIED
Dec 30 05:01:16.841: INFO: Observed &DaemonSet event: MODIFIED
Dec 30 05:01:16.841: INFO: Observed &DaemonSet event: MODIFIED
Dec 30 05:01:16.842: INFO: Observed &DaemonSet event: MODIFIED
Dec 30 05:01:16.842: INFO: Observed &DaemonSet event: MODIFIED
Dec 30 05:01:16.842: INFO: Observed daemon set daemon-set in namespace daemonsets-6686 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Dec 30 05:01:16.842: INFO: Observed &DaemonSet event: MODIFIED
Dec 30 05:01:16.842: INFO: Found daemon set daemon-set in namespace daemonsets-6686 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Dec 30 05:01:16.842: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 12/30/22 05:01:16.847
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6686, will wait for the garbage collector to delete the pods 12/30/22 05:01:16.847
Dec 30 05:01:16.908: INFO: Deleting DaemonSet.extensions daemon-set took: 7.371555ms
Dec 30 05:01:17.009: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.962793ms
Dec 30 05:01:20.114: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 30 05:01:20.114: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Dec 30 05:01:20.118: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"463363"},"items":null}

Dec 30 05:01:20.122: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"463363"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Dec 30 05:01:20.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6686" for this suite. 12/30/22 05:01:20.155
{"msg":"PASSED [sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]","completed":357,"skipped":6596,"failed":0}
------------------------------
• [SLOW TEST] [6.447 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 05:01:13.715
    Dec 30 05:01:13.715: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename daemonsets 12/30/22 05:01:13.716
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 05:01:13.733
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 05:01:13.737
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:861
    STEP: Creating simple DaemonSet "daemon-set" 12/30/22 05:01:13.777
    STEP: Check that daemon pods launch on every node of the cluster. 12/30/22 05:01:13.783
    Dec 30 05:01:13.792: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 30 05:01:13.793: INFO: Node k8s-mgmt01 is running 0 daemon pod, expected 1
    Dec 30 05:01:14.805: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 30 05:01:14.805: INFO: Node k8s-mgmt01 is running 0 daemon pod, expected 1
    Dec 30 05:01:15.805: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Dec 30 05:01:15.805: INFO: Node k8s-worker01 is running 0 daemon pod, expected 1
    Dec 30 05:01:16.805: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
    Dec 30 05:01:16.805: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
    STEP: Getting /status 12/30/22 05:01:16.809
    Dec 30 05:01:16.814: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 12/30/22 05:01:16.814
    Dec 30 05:01:16.825: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 12/30/22 05:01:16.825
    Dec 30 05:01:16.828: INFO: Observed &DaemonSet event: ADDED
    Dec 30 05:01:16.828: INFO: Observed &DaemonSet event: MODIFIED
    Dec 30 05:01:16.828: INFO: Observed &DaemonSet event: MODIFIED
    Dec 30 05:01:16.829: INFO: Observed &DaemonSet event: MODIFIED
    Dec 30 05:01:16.829: INFO: Observed &DaemonSet event: MODIFIED
    Dec 30 05:01:16.829: INFO: Observed &DaemonSet event: MODIFIED
    Dec 30 05:01:16.829: INFO: Observed &DaemonSet event: MODIFIED
    Dec 30 05:01:16.829: INFO: Found daemon set daemon-set in namespace daemonsets-6686 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Dec 30 05:01:16.829: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 12/30/22 05:01:16.829
    STEP: watching for the daemon set status to be patched 12/30/22 05:01:16.839
    Dec 30 05:01:16.841: INFO: Observed &DaemonSet event: ADDED
    Dec 30 05:01:16.841: INFO: Observed &DaemonSet event: MODIFIED
    Dec 30 05:01:16.841: INFO: Observed &DaemonSet event: MODIFIED
    Dec 30 05:01:16.841: INFO: Observed &DaemonSet event: MODIFIED
    Dec 30 05:01:16.841: INFO: Observed &DaemonSet event: MODIFIED
    Dec 30 05:01:16.842: INFO: Observed &DaemonSet event: MODIFIED
    Dec 30 05:01:16.842: INFO: Observed &DaemonSet event: MODIFIED
    Dec 30 05:01:16.842: INFO: Observed daemon set daemon-set in namespace daemonsets-6686 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Dec 30 05:01:16.842: INFO: Observed &DaemonSet event: MODIFIED
    Dec 30 05:01:16.842: INFO: Found daemon set daemon-set in namespace daemonsets-6686 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Dec 30 05:01:16.842: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 12/30/22 05:01:16.847
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6686, will wait for the garbage collector to delete the pods 12/30/22 05:01:16.847
    Dec 30 05:01:16.908: INFO: Deleting DaemonSet.extensions daemon-set took: 7.371555ms
    Dec 30 05:01:17.009: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.962793ms
    Dec 30 05:01:20.114: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Dec 30 05:01:20.114: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Dec 30 05:01:20.118: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"463363"},"items":null}

    Dec 30 05:01:20.122: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"463363"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Dec 30 05:01:20.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-6686" for this suite. 12/30/22 05:01:20.155
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 05:01:20.165
Dec 30 05:01:20.165: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename crd-publish-openapi 12/30/22 05:01:20.167
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 05:01:20.185
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 05:01:20.188
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390
STEP: set up a multi version CRD 12/30/22 05:01:20.191
Dec 30 05:01:20.192: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: rename a version 12/30/22 05:01:35.195
STEP: check the new version name is served 12/30/22 05:01:35.218
STEP: check the old version name is removed 12/30/22 05:01:39.476
STEP: check the other version is not changed 12/30/22 05:01:41.224
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Dec 30 05:01:49.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9362" for this suite. 12/30/22 05:01:49.399
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","completed":358,"skipped":6640,"failed":0}
------------------------------
• [SLOW TEST] [29.241 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 05:01:20.165
    Dec 30 05:01:20.165: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename crd-publish-openapi 12/30/22 05:01:20.167
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 05:01:20.185
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 05:01:20.188
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:390
    STEP: set up a multi version CRD 12/30/22 05:01:20.191
    Dec 30 05:01:20.192: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: rename a version 12/30/22 05:01:35.195
    STEP: check the new version name is served 12/30/22 05:01:35.218
    STEP: check the old version name is removed 12/30/22 05:01:39.476
    STEP: check the other version is not changed 12/30/22 05:01:41.224
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Dec 30 05:01:49.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-9362" for this suite. 12/30/22 05:01:49.399
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 05:01:49.407
Dec 30 05:01:49.408: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename kubelet-test 12/30/22 05:01:49.409
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 05:01:49.427
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 05:01:49.431
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
Dec 30 05:01:49.444: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs9aad3cbe-b2d5-4097-b5ba-2634e3ad3c7d" in namespace "kubelet-test-7718" to be "running and ready"
Dec 30 05:01:49.448: INFO: Pod "busybox-readonly-fs9aad3cbe-b2d5-4097-b5ba-2634e3ad3c7d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.640298ms
Dec 30 05:01:49.448: INFO: The phase of Pod busybox-readonly-fs9aad3cbe-b2d5-4097-b5ba-2634e3ad3c7d is Pending, waiting for it to be Running (with Ready = true)
Dec 30 05:01:51.453: INFO: Pod "busybox-readonly-fs9aad3cbe-b2d5-4097-b5ba-2634e3ad3c7d": Phase="Running", Reason="", readiness=true. Elapsed: 2.008993707s
Dec 30 05:01:51.453: INFO: The phase of Pod busybox-readonly-fs9aad3cbe-b2d5-4097-b5ba-2634e3ad3c7d is Running (Ready = true)
Dec 30 05:01:51.453: INFO: Pod "busybox-readonly-fs9aad3cbe-b2d5-4097-b5ba-2634e3ad3c7d" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Dec 30 05:01:51.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7718" for this suite. 12/30/22 05:01:51.472
{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","completed":359,"skipped":6643,"failed":0}
------------------------------
• [2.071 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 05:01:49.407
    Dec 30 05:01:49.408: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename kubelet-test 12/30/22 05:01:49.409
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 05:01:49.427
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 05:01:49.431
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    Dec 30 05:01:49.444: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs9aad3cbe-b2d5-4097-b5ba-2634e3ad3c7d" in namespace "kubelet-test-7718" to be "running and ready"
    Dec 30 05:01:49.448: INFO: Pod "busybox-readonly-fs9aad3cbe-b2d5-4097-b5ba-2634e3ad3c7d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.640298ms
    Dec 30 05:01:49.448: INFO: The phase of Pod busybox-readonly-fs9aad3cbe-b2d5-4097-b5ba-2634e3ad3c7d is Pending, waiting for it to be Running (with Ready = true)
    Dec 30 05:01:51.453: INFO: Pod "busybox-readonly-fs9aad3cbe-b2d5-4097-b5ba-2634e3ad3c7d": Phase="Running", Reason="", readiness=true. Elapsed: 2.008993707s
    Dec 30 05:01:51.453: INFO: The phase of Pod busybox-readonly-fs9aad3cbe-b2d5-4097-b5ba-2634e3ad3c7d is Running (Ready = true)
    Dec 30 05:01:51.453: INFO: Pod "busybox-readonly-fs9aad3cbe-b2d5-4097-b5ba-2634e3ad3c7d" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Dec 30 05:01:51.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-7718" for this suite. 12/30/22 05:01:51.472
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:906
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 05:01:51.481
Dec 30 05:01:51.481: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename statefulset 12/30/22 05:01:51.483
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 05:01:51.5
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 05:01:51.504
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-9843 12/30/22 05:01:51.507
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:906
Dec 30 05:01:51.522: INFO: Found 0 stateful pods, waiting for 1
Dec 30 05:02:01.527: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 12/30/22 05:02:01.535
W1230 05:02:01.543826      25 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Dec 30 05:02:01.552: INFO: Found 1 stateful pods, waiting for 2
Dec 30 05:02:11.558: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 30 05:02:11.558: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 12/30/22 05:02:11.567
STEP: Delete all of the StatefulSets 12/30/22 05:02:11.571
STEP: Verify that StatefulSets have been deleted 12/30/22 05:02:11.58
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Dec 30 05:02:11.584: INFO: Deleting all statefulset in ns statefulset-9843
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Dec 30 05:02:11.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9843" for this suite. 12/30/22 05:02:11.601
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]","completed":360,"skipped":6665,"failed":0}
------------------------------
• [SLOW TEST] [20.130 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:906

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 05:01:51.481
    Dec 30 05:01:51.481: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename statefulset 12/30/22 05:01:51.483
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 05:01:51.5
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 05:01:51.504
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-9843 12/30/22 05:01:51.507
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:906
    Dec 30 05:01:51.522: INFO: Found 0 stateful pods, waiting for 1
    Dec 30 05:02:01.527: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 12/30/22 05:02:01.535
    W1230 05:02:01.543826      25 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Dec 30 05:02:01.552: INFO: Found 1 stateful pods, waiting for 2
    Dec 30 05:02:11.558: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Dec 30 05:02:11.558: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 12/30/22 05:02:11.567
    STEP: Delete all of the StatefulSets 12/30/22 05:02:11.571
    STEP: Verify that StatefulSets have been deleted 12/30/22 05:02:11.58
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Dec 30 05:02:11.584: INFO: Deleting all statefulset in ns statefulset-9843
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Dec 30 05:02:11.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-9843" for this suite. 12/30/22 05:02:11.601
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 05:02:11.613
Dec 30 05:02:11.613: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename pods 12/30/22 05:02:11.614
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 05:02:11.629
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 05:02:11.632
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 12/30/22 05:02:11.635
STEP: submitting the pod to kubernetes 12/30/22 05:02:11.635
STEP: verifying QOS class is set on the pod 12/30/22 05:02:11.645
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:187
Dec 30 05:02:11.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1263" for this suite. 12/30/22 05:02:11.654
{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","completed":361,"skipped":6679,"failed":0}
------------------------------
• [0.047 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 05:02:11.613
    Dec 30 05:02:11.613: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename pods 12/30/22 05:02:11.614
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 05:02:11.629
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 05:02:11.632
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 12/30/22 05:02:11.635
    STEP: submitting the pod to kubernetes 12/30/22 05:02:11.635
    STEP: verifying QOS class is set on the pod 12/30/22 05:02:11.645
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/framework.go:187
    Dec 30 05:02:11.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-1263" for this suite. 12/30/22 05:02:11.654
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 12/30/22 05:02:11.662
Dec 30 05:02:11.662: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
STEP: Building a namespace api object, basename configmap 12/30/22 05:02:11.663
STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 05:02:11.681
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 05:02:11.684
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73
STEP: Creating configMap with name configmap-test-volume-f1b3ec8f-2e02-44bd-86c5-9bcbd92c6fc7 12/30/22 05:02:11.688
STEP: Creating a pod to test consume configMaps 12/30/22 05:02:11.693
Dec 30 05:02:11.702: INFO: Waiting up to 5m0s for pod "pod-configmaps-9ae50300-ad3f-463e-96aa-45857e653aab" in namespace "configmap-2104" to be "Succeeded or Failed"
Dec 30 05:02:11.706: INFO: Pod "pod-configmaps-9ae50300-ad3f-463e-96aa-45857e653aab": Phase="Pending", Reason="", readiness=false. Elapsed: 4.193441ms
Dec 30 05:02:13.712: INFO: Pod "pod-configmaps-9ae50300-ad3f-463e-96aa-45857e653aab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010131561s
Dec 30 05:02:15.711: INFO: Pod "pod-configmaps-9ae50300-ad3f-463e-96aa-45857e653aab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009408173s
STEP: Saw pod success 12/30/22 05:02:15.711
Dec 30 05:02:15.711: INFO: Pod "pod-configmaps-9ae50300-ad3f-463e-96aa-45857e653aab" satisfied condition "Succeeded or Failed"
Dec 30 05:02:15.716: INFO: Trying to get logs from node k8s-mgmt02 pod pod-configmaps-9ae50300-ad3f-463e-96aa-45857e653aab container agnhost-container: <nil>
STEP: delete the pod 12/30/22 05:02:15.737
Dec 30 05:02:15.751: INFO: Waiting for pod pod-configmaps-9ae50300-ad3f-463e-96aa-45857e653aab to disappear
Dec 30 05:02:15.755: INFO: Pod pod-configmaps-9ae50300-ad3f-463e-96aa-45857e653aab no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Dec 30 05:02:15.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2104" for this suite. 12/30/22 05:02:15.761
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","completed":362,"skipped":6705,"failed":0}
------------------------------
• [4.105 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 12/30/22 05:02:11.662
    Dec 30 05:02:11.662: INFO: >>> kubeConfig: /tmp/kubeconfig-2341570513
    STEP: Building a namespace api object, basename configmap 12/30/22 05:02:11.663
    STEP: Waiting for a default service account to be provisioned in namespace 12/30/22 05:02:11.681
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 12/30/22 05:02:11.684
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:73
    STEP: Creating configMap with name configmap-test-volume-f1b3ec8f-2e02-44bd-86c5-9bcbd92c6fc7 12/30/22 05:02:11.688
    STEP: Creating a pod to test consume configMaps 12/30/22 05:02:11.693
    Dec 30 05:02:11.702: INFO: Waiting up to 5m0s for pod "pod-configmaps-9ae50300-ad3f-463e-96aa-45857e653aab" in namespace "configmap-2104" to be "Succeeded or Failed"
    Dec 30 05:02:11.706: INFO: Pod "pod-configmaps-9ae50300-ad3f-463e-96aa-45857e653aab": Phase="Pending", Reason="", readiness=false. Elapsed: 4.193441ms
    Dec 30 05:02:13.712: INFO: Pod "pod-configmaps-9ae50300-ad3f-463e-96aa-45857e653aab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010131561s
    Dec 30 05:02:15.711: INFO: Pod "pod-configmaps-9ae50300-ad3f-463e-96aa-45857e653aab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009408173s
    STEP: Saw pod success 12/30/22 05:02:15.711
    Dec 30 05:02:15.711: INFO: Pod "pod-configmaps-9ae50300-ad3f-463e-96aa-45857e653aab" satisfied condition "Succeeded or Failed"
    Dec 30 05:02:15.716: INFO: Trying to get logs from node k8s-mgmt02 pod pod-configmaps-9ae50300-ad3f-463e-96aa-45857e653aab container agnhost-container: <nil>
    STEP: delete the pod 12/30/22 05:02:15.737
    Dec 30 05:02:15.751: INFO: Waiting for pod pod-configmaps-9ae50300-ad3f-463e-96aa-45857e653aab to disappear
    Dec 30 05:02:15.755: INFO: Pod pod-configmaps-9ae50300-ad3f-463e-96aa-45857e653aab no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Dec 30 05:02:15.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-2104" for this suite. 12/30/22 05:02:15.761
  << End Captured GinkgoWriter Output
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:87
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:87
{"msg":"Test Suite completed","completed":362,"skipped":6705,"failed":0}
Dec 30 05:02:15.769: INFO: Running AfterSuite actions on all nodes
Dec 30 05:02:15.769: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func20.2
Dec 30 05:02:15.769: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func10.2
Dec 30 05:02:15.769: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
Dec 30 05:02:15.769: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
Dec 30 05:02:15.769: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
Dec 30 05:02:15.769: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
Dec 30 05:02:15.769: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:87
Dec 30 05:02:15.769: INFO: Running AfterSuite actions on node 1
Dec 30 05:02:15.769: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.000 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:87

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:87
    Dec 30 05:02:15.769: INFO: Running AfterSuite actions on all nodes
    Dec 30 05:02:15.769: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func20.2
    Dec 30 05:02:15.769: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func10.2
    Dec 30 05:02:15.769: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
    Dec 30 05:02:15.769: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
    Dec 30 05:02:15.769: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
    Dec 30 05:02:15.769: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
    Dec 30 05:02:15.769: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:87
    Dec 30 05:02:15.769: INFO: Running AfterSuite actions on node 1
    Dec 30 05:02:15.769: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:146
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:146
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:146

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:146
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:559
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:559
------------------------------
[ReportAfterSuite] PASSED [0.087 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:559

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:559
  << End Captured GinkgoWriter Output
------------------------------

Ran 362 of 7067 Specs in 6094.842 seconds
SUCCESS! -- 362 Passed | 0 Failed | 0 Pending | 6705 Skipped
PASS

Ginkgo ran 1 suite in 1h41m35.165516457s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.1.6[0m

