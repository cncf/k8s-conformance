I0117 14:47:32.979020      22 e2e.go:116] Starting e2e run "277599aa-dbb9-45ba-ba19-6ee6b0fa425c" on Ginkgo node 1
Jan 17 14:47:32.992: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1673966852 - will randomize all specs

Will run 362 of 7067 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:76
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:76
{"msg":"Test Suite starting","completed":0,"skipped":0,"failed":0}
Jan 17 14:47:33.068: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
E0117 14:47:33.069088      22 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
E0117 14:47:33.069088      22 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
Jan 17 14:47:33.069: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jan 17 14:47:33.085: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jan 17 14:47:33.094: INFO: 0 / 0 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jan 17 14:47:33.094: INFO: expected 0 pod replicas in namespace 'kube-system', 0 are Running and Ready.
Jan 17 14:47:33.094: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jan 17 14:47:33.098: INFO: e2e test version: v1.25.4
Jan 17 14:47:33.099: INFO: kube-apiserver version: v1.25.4+77bec7a
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:76
Jan 17 14:47:33.099: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
Jan 17 14:47:33.104: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.036 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:76

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:76
    Jan 17 14:47:33.068: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    E0117 14:47:33.069088      22 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
    Jan 17 14:47:33.069: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    Jan 17 14:47:33.085: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    Jan 17 14:47:33.094: INFO: 0 / 0 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    Jan 17 14:47:33.094: INFO: expected 0 pod replicas in namespace 'kube-system', 0 are Running and Ready.
    Jan 17 14:47:33.094: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Jan 17 14:47:33.098: INFO: e2e test version: v1.25.4
    Jan 17 14:47:33.099: INFO: kube-apiserver version: v1.25.4+77bec7a
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:76
    Jan 17 14:47:33.099: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    Jan 17 14:47:33.104: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:152
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 14:47:33.12
Jan 17 14:47:33.120: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename container-lifecycle-hook 01/17/23 14:47:33.12
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 14:47:33.151
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 14:47:33.153
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 01/17/23 14:47:33.162
Jan 17 14:47:33.198: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4214" to be "running and ready"
Jan 17 14:47:33.201: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.713312ms
Jan 17 14:47:33.201: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 17 14:47:35.205: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00668696s
Jan 17 14:47:35.205: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 17 14:47:37.205: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.0065643s
Jan 17 14:47:37.205: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan 17 14:47:37.205: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:152
STEP: create the pod with lifecycle hook 01/17/23 14:47:37.208
Jan 17 14:47:37.220: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-4214" to be "running and ready"
Jan 17 14:47:37.223: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.066908ms
Jan 17 14:47:37.223: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 17 14:47:39.227: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.007169522s
Jan 17 14:47:39.227: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
Jan 17 14:47:39.227: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 01/17/23 14:47:39.23
Jan 17 14:47:39.242: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 17 14:47:39.245: INFO: Pod pod-with-prestop-http-hook still exists
Jan 17 14:47:41.245: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 17 14:47:41.249: INFO: Pod pod-with-prestop-http-hook still exists
Jan 17 14:47:43.245: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 17 14:47:43.248: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 01/17/23 14:47:43.248
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Jan 17 14:47:43.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4214" for this suite. 01/17/23 14:47:43.264
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","completed":1,"skipped":12,"failed":0}
------------------------------
• [SLOW TEST] [10.149 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 14:47:33.12
    Jan 17 14:47:33.120: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/17/23 14:47:33.12
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 14:47:33.151
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 14:47:33.153
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 01/17/23 14:47:33.162
    Jan 17 14:47:33.198: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4214" to be "running and ready"
    Jan 17 14:47:33.201: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.713312ms
    Jan 17 14:47:33.201: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 14:47:35.205: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00668696s
    Jan 17 14:47:35.205: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 14:47:37.205: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.0065643s
    Jan 17 14:47:37.205: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan 17 14:47:37.205: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:152
    STEP: create the pod with lifecycle hook 01/17/23 14:47:37.208
    Jan 17 14:47:37.220: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-4214" to be "running and ready"
    Jan 17 14:47:37.223: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.066908ms
    Jan 17 14:47:37.223: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 14:47:39.227: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.007169522s
    Jan 17 14:47:39.227: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    Jan 17 14:47:39.227: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 01/17/23 14:47:39.23
    Jan 17 14:47:39.242: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jan 17 14:47:39.245: INFO: Pod pod-with-prestop-http-hook still exists
    Jan 17 14:47:41.245: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jan 17 14:47:41.249: INFO: Pod pod-with-prestop-http-hook still exists
    Jan 17 14:47:43.245: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jan 17 14:47:43.248: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 01/17/23 14:47:43.248
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Jan 17 14:47:43.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-4214" for this suite. 01/17/23 14:47:43.264
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 14:47:43.27
Jan 17 14:47:43.270: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename cronjob 01/17/23 14:47:43.27
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 14:47:43.292
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 14:47:43.294
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 01/17/23 14:47:43.296
STEP: creating 01/17/23 14:47:43.296
W0117 14:47:43.308102      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: getting 01/17/23 14:47:43.308
STEP: listing 01/17/23 14:47:43.32
STEP: watching 01/17/23 14:47:43.326
Jan 17 14:47:43.326: INFO: starting watch
STEP: cluster-wide listing 01/17/23 14:47:43.327
STEP: cluster-wide watching 01/17/23 14:47:43.338
Jan 17 14:47:43.338: INFO: starting watch
STEP: patching 01/17/23 14:47:43.339
STEP: updating 01/17/23 14:47:43.349
Jan 17 14:47:43.362: INFO: waiting for watch events with expected annotations
Jan 17 14:47:43.362: INFO: saw patched and updated annotations
STEP: patching /status 01/17/23 14:47:43.362
STEP: updating /status 01/17/23 14:47:43.37
STEP: get /status 01/17/23 14:47:43.385
STEP: deleting 01/17/23 14:47:43.387
STEP: deleting a collection 01/17/23 14:47:43.402
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Jan 17 14:47:43.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-1269" for this suite. 01/17/23 14:47:43.415
{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","completed":2,"skipped":25,"failed":0}
------------------------------
• [0.150 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 14:47:43.27
    Jan 17 14:47:43.270: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename cronjob 01/17/23 14:47:43.27
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 14:47:43.292
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 14:47:43.294
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 01/17/23 14:47:43.296
    STEP: creating 01/17/23 14:47:43.296
    W0117 14:47:43.308102      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: getting 01/17/23 14:47:43.308
    STEP: listing 01/17/23 14:47:43.32
    STEP: watching 01/17/23 14:47:43.326
    Jan 17 14:47:43.326: INFO: starting watch
    STEP: cluster-wide listing 01/17/23 14:47:43.327
    STEP: cluster-wide watching 01/17/23 14:47:43.338
    Jan 17 14:47:43.338: INFO: starting watch
    STEP: patching 01/17/23 14:47:43.339
    STEP: updating 01/17/23 14:47:43.349
    Jan 17 14:47:43.362: INFO: waiting for watch events with expected annotations
    Jan 17 14:47:43.362: INFO: saw patched and updated annotations
    STEP: patching /status 01/17/23 14:47:43.362
    STEP: updating /status 01/17/23 14:47:43.37
    STEP: get /status 01/17/23 14:47:43.385
    STEP: deleting 01/17/23 14:47:43.387
    STEP: deleting a collection 01/17/23 14:47:43.402
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Jan 17 14:47:43.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-1269" for this suite. 01/17/23 14:47:43.415
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 14:47:43.421
Jan 17 14:47:43.421: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename proxy 01/17/23 14:47:43.421
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 14:47:43.447
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 14:47:43.449
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
Jan 17 14:47:43.451: INFO: Creating pod...
Jan 17 14:47:43.476: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-8601" to be "running"
Jan 17 14:47:43.479: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 3.148142ms
Jan 17 14:47:45.483: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006616489s
Jan 17 14:47:47.484: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 4.007481849s
Jan 17 14:47:47.484: INFO: Pod "agnhost" satisfied condition "running"
Jan 17 14:47:47.484: INFO: Creating service...
Jan 17 14:47:47.493: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-8601/pods/agnhost/proxy?method=DELETE
Jan 17 14:47:47.499: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 17 14:47:47.500: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-8601/pods/agnhost/proxy?method=OPTIONS
Jan 17 14:47:47.506: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 17 14:47:47.506: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-8601/pods/agnhost/proxy?method=PATCH
Jan 17 14:47:47.509: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 17 14:47:47.509: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-8601/pods/agnhost/proxy?method=POST
Jan 17 14:47:47.512: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 17 14:47:47.512: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-8601/pods/agnhost/proxy?method=PUT
Jan 17 14:47:47.517: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan 17 14:47:47.517: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-8601/services/e2e-proxy-test-service/proxy?method=DELETE
Jan 17 14:47:47.533: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 17 14:47:47.533: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-8601/services/e2e-proxy-test-service/proxy?method=OPTIONS
Jan 17 14:47:47.542: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 17 14:47:47.542: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-8601/services/e2e-proxy-test-service/proxy?method=PATCH
Jan 17 14:47:47.546: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 17 14:47:47.546: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-8601/services/e2e-proxy-test-service/proxy?method=POST
Jan 17 14:47:47.551: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 17 14:47:47.551: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-8601/services/e2e-proxy-test-service/proxy?method=PUT
Jan 17 14:47:47.557: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan 17 14:47:47.557: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-8601/pods/agnhost/proxy?method=GET
Jan 17 14:47:47.559: INFO: http.Client request:GET StatusCode:301
Jan 17 14:47:47.559: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-8601/services/e2e-proxy-test-service/proxy?method=GET
Jan 17 14:47:47.563: INFO: http.Client request:GET StatusCode:301
Jan 17 14:47:47.563: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-8601/pods/agnhost/proxy?method=HEAD
Jan 17 14:47:47.566: INFO: http.Client request:HEAD StatusCode:301
Jan 17 14:47:47.566: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-8601/services/e2e-proxy-test-service/proxy?method=HEAD
Jan 17 14:47:47.570: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Jan 17 14:47:47.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-8601" for this suite. 01/17/23 14:47:47.574
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]","completed":3,"skipped":31,"failed":0}
------------------------------
• [4.158 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 14:47:43.421
    Jan 17 14:47:43.421: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename proxy 01/17/23 14:47:43.421
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 14:47:43.447
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 14:47:43.449
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    Jan 17 14:47:43.451: INFO: Creating pod...
    Jan 17 14:47:43.476: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-8601" to be "running"
    Jan 17 14:47:43.479: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 3.148142ms
    Jan 17 14:47:45.483: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006616489s
    Jan 17 14:47:47.484: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 4.007481849s
    Jan 17 14:47:47.484: INFO: Pod "agnhost" satisfied condition "running"
    Jan 17 14:47:47.484: INFO: Creating service...
    Jan 17 14:47:47.493: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-8601/pods/agnhost/proxy?method=DELETE
    Jan 17 14:47:47.499: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan 17 14:47:47.500: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-8601/pods/agnhost/proxy?method=OPTIONS
    Jan 17 14:47:47.506: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan 17 14:47:47.506: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-8601/pods/agnhost/proxy?method=PATCH
    Jan 17 14:47:47.509: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan 17 14:47:47.509: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-8601/pods/agnhost/proxy?method=POST
    Jan 17 14:47:47.512: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan 17 14:47:47.512: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-8601/pods/agnhost/proxy?method=PUT
    Jan 17 14:47:47.517: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jan 17 14:47:47.517: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-8601/services/e2e-proxy-test-service/proxy?method=DELETE
    Jan 17 14:47:47.533: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan 17 14:47:47.533: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-8601/services/e2e-proxy-test-service/proxy?method=OPTIONS
    Jan 17 14:47:47.542: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan 17 14:47:47.542: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-8601/services/e2e-proxy-test-service/proxy?method=PATCH
    Jan 17 14:47:47.546: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan 17 14:47:47.546: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-8601/services/e2e-proxy-test-service/proxy?method=POST
    Jan 17 14:47:47.551: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan 17 14:47:47.551: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-8601/services/e2e-proxy-test-service/proxy?method=PUT
    Jan 17 14:47:47.557: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jan 17 14:47:47.557: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-8601/pods/agnhost/proxy?method=GET
    Jan 17 14:47:47.559: INFO: http.Client request:GET StatusCode:301
    Jan 17 14:47:47.559: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-8601/services/e2e-proxy-test-service/proxy?method=GET
    Jan 17 14:47:47.563: INFO: http.Client request:GET StatusCode:301
    Jan 17 14:47:47.563: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-8601/pods/agnhost/proxy?method=HEAD
    Jan 17 14:47:47.566: INFO: http.Client request:HEAD StatusCode:301
    Jan 17 14:47:47.566: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-8601/services/e2e-proxy-test-service/proxy?method=HEAD
    Jan 17 14:47:47.570: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Jan 17 14:47:47.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-8601" for this suite. 01/17/23 14:47:47.574
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 14:47:47.58
Jan 17 14:47:47.580: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename container-probe 01/17/23 14:47:47.58
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 14:47:47.609
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 14:47:47.611
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan 17 14:48:47.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-977" for this suite. 01/17/23 14:48:47.649
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","completed":4,"skipped":53,"failed":0}
------------------------------
• [SLOW TEST] [60.077 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 14:47:47.58
    Jan 17 14:47:47.580: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename container-probe 01/17/23 14:47:47.58
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 14:47:47.609
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 14:47:47.611
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:104
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan 17 14:48:47.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-977" for this suite. 01/17/23 14:48:47.649
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 14:48:47.657
Jan 17 14:48:47.657: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename endpointslice 01/17/23 14:48:47.658
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 14:48:47.679
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 14:48:47.682
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Jan 17 14:48:51.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-3354" for this suite. 01/17/23 14:48:51.758
{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","completed":5,"skipped":62,"failed":0}
------------------------------
• [4.108 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 14:48:47.657
    Jan 17 14:48:47.657: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename endpointslice 01/17/23 14:48:47.658
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 14:48:47.679
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 14:48:47.682
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:101
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Jan 17 14:48:51.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-3354" for this suite. 01/17/23 14:48:51.758
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 14:48:51.765
Jan 17 14:48:51.765: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename container-probe 01/17/23 14:48:51.766
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 14:48:51.799
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 14:48:51.801
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211
STEP: Creating pod test-webserver-cefe6db6-b58d-4c76-bdca-869484dea953 in namespace container-probe-7684 01/17/23 14:48:51.804
Jan 17 14:48:51.855: INFO: Waiting up to 5m0s for pod "test-webserver-cefe6db6-b58d-4c76-bdca-869484dea953" in namespace "container-probe-7684" to be "not pending"
Jan 17 14:48:51.860: INFO: Pod "test-webserver-cefe6db6-b58d-4c76-bdca-869484dea953": Phase="Pending", Reason="", readiness=false. Elapsed: 5.12896ms
Jan 17 14:48:53.864: INFO: Pod "test-webserver-cefe6db6-b58d-4c76-bdca-869484dea953": Phase="Running", Reason="", readiness=true. Elapsed: 2.009239679s
Jan 17 14:48:53.864: INFO: Pod "test-webserver-cefe6db6-b58d-4c76-bdca-869484dea953" satisfied condition "not pending"
Jan 17 14:48:53.864: INFO: Started pod test-webserver-cefe6db6-b58d-4c76-bdca-869484dea953 in namespace container-probe-7684
STEP: checking the pod's current state and verifying that restartCount is present 01/17/23 14:48:53.864
Jan 17 14:48:53.867: INFO: Initial restart count of pod test-webserver-cefe6db6-b58d-4c76-bdca-869484dea953 is 0
STEP: deleting the pod 01/17/23 14:52:54.377
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan 17 14:52:54.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7684" for this suite. 01/17/23 14:52:54.394
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","completed":6,"skipped":65,"failed":0}
------------------------------
• [SLOW TEST] [242.635 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 14:48:51.765
    Jan 17 14:48:51.765: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename container-probe 01/17/23 14:48:51.766
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 14:48:51.799
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 14:48:51.801
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:211
    STEP: Creating pod test-webserver-cefe6db6-b58d-4c76-bdca-869484dea953 in namespace container-probe-7684 01/17/23 14:48:51.804
    Jan 17 14:48:51.855: INFO: Waiting up to 5m0s for pod "test-webserver-cefe6db6-b58d-4c76-bdca-869484dea953" in namespace "container-probe-7684" to be "not pending"
    Jan 17 14:48:51.860: INFO: Pod "test-webserver-cefe6db6-b58d-4c76-bdca-869484dea953": Phase="Pending", Reason="", readiness=false. Elapsed: 5.12896ms
    Jan 17 14:48:53.864: INFO: Pod "test-webserver-cefe6db6-b58d-4c76-bdca-869484dea953": Phase="Running", Reason="", readiness=true. Elapsed: 2.009239679s
    Jan 17 14:48:53.864: INFO: Pod "test-webserver-cefe6db6-b58d-4c76-bdca-869484dea953" satisfied condition "not pending"
    Jan 17 14:48:53.864: INFO: Started pod test-webserver-cefe6db6-b58d-4c76-bdca-869484dea953 in namespace container-probe-7684
    STEP: checking the pod's current state and verifying that restartCount is present 01/17/23 14:48:53.864
    Jan 17 14:48:53.867: INFO: Initial restart count of pod test-webserver-cefe6db6-b58d-4c76-bdca-869484dea953 is 0
    STEP: deleting the pod 01/17/23 14:52:54.377
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan 17 14:52:54.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-7684" for this suite. 01/17/23 14:52:54.394
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 14:52:54.402
Jan 17 14:52:54.402: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename var-expansion 01/17/23 14:52:54.403
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 14:52:54.438
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 14:52:54.44
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224
STEP: creating the pod with failed condition 01/17/23 14:52:54.442
Jan 17 14:52:54.480: INFO: Waiting up to 2m0s for pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8" in namespace "var-expansion-7909" to be "running"
Jan 17 14:52:54.485: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.257345ms
Jan 17 14:52:56.490: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009825443s
Jan 17 14:52:58.490: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00953972s
Jan 17 14:53:00.490: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009759414s
Jan 17 14:53:02.489: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.00862331s
Jan 17 14:53:04.490: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 10.009374659s
Jan 17 14:53:06.490: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 12.009608683s
Jan 17 14:53:08.490: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 14.009591213s
Jan 17 14:53:10.490: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 16.010001835s
Jan 17 14:53:12.490: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 18.010082207s
Jan 17 14:53:14.490: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 20.009589103s
Jan 17 14:53:16.490: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 22.009755092s
Jan 17 14:53:18.490: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 24.0093353s
Jan 17 14:53:20.491: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 26.010388804s
Jan 17 14:53:22.490: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 28.009650669s
Jan 17 14:53:24.490: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 30.010119559s
Jan 17 14:53:26.491: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 32.010481226s
Jan 17 14:53:28.489: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 34.008483639s
Jan 17 14:53:30.491: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 36.010411601s
Jan 17 14:53:32.490: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 38.009981818s
Jan 17 14:53:34.490: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 40.009491353s
Jan 17 14:53:36.490: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 42.009743751s
Jan 17 14:53:38.489: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 44.008777059s
Jan 17 14:53:40.490: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 46.009784923s
Jan 17 14:53:42.491: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 48.01067709s
Jan 17 14:53:44.491: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 50.010334081s
Jan 17 14:53:46.490: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 52.010236974s
Jan 17 14:53:48.490: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 54.009889212s
Jan 17 14:53:50.490: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 56.010099759s
Jan 17 14:53:52.490: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 58.010183835s
Jan 17 14:53:54.490: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.00939932s
Jan 17 14:53:56.491: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.010278634s
Jan 17 14:53:58.490: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.010088313s
Jan 17 14:54:00.490: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.009266667s
Jan 17 14:54:02.490: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.010203181s
Jan 17 14:54:04.493: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.012600962s
Jan 17 14:54:06.490: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.009684102s
Jan 17 14:54:08.489: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.009007277s
Jan 17 14:54:10.491: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.010491456s
Jan 17 14:54:12.490: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.01022882s
Jan 17 14:54:14.490: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.010120722s
Jan 17 14:54:16.491: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.01027001s
Jan 17 14:54:18.490: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.009576548s
Jan 17 14:54:20.490: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.009747396s
Jan 17 14:54:22.491: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.010456561s
Jan 17 14:54:24.489: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.008902587s
Jan 17 14:54:26.491: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.01078347s
Jan 17 14:54:28.490: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.009709014s
Jan 17 14:54:30.491: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.010541389s
Jan 17 14:54:32.490: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.009800951s
Jan 17 14:54:34.491: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.010608303s
Jan 17 14:54:36.490: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.00997006s
Jan 17 14:54:38.494: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.013969132s
Jan 17 14:54:40.491: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.010627378s
Jan 17 14:54:42.490: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.009957545s
Jan 17 14:54:44.490: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.010140324s
Jan 17 14:54:46.491: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.01059758s
Jan 17 14:54:48.490: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.010001745s
Jan 17 14:54:50.491: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.010703468s
Jan 17 14:54:52.490: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.010063461s
Jan 17 14:54:54.490: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.009839074s
Jan 17 14:54:54.494: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.013340074s
STEP: updating the pod 01/17/23 14:54:54.494
Jan 17 14:54:55.012: INFO: Successfully updated pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8"
STEP: waiting for pod running 01/17/23 14:54:55.012
Jan 17 14:54:55.012: INFO: Waiting up to 2m0s for pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8" in namespace "var-expansion-7909" to be "running"
Jan 17 14:54:55.015: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.277649ms
Jan 17 14:54:57.019: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Running", Reason="", readiness=true. Elapsed: 2.006988192s
Jan 17 14:54:57.019: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8" satisfied condition "running"
STEP: deleting the pod gracefully 01/17/23 14:54:57.019
Jan 17 14:54:57.019: INFO: Deleting pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8" in namespace "var-expansion-7909"
Jan 17 14:54:57.033: INFO: Wait up to 5m0s for pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan 17 14:55:29.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7909" for this suite. 01/17/23 14:55:29.044
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","completed":7,"skipped":110,"failed":0}
------------------------------
• [SLOW TEST] [154.649 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 14:52:54.402
    Jan 17 14:52:54.402: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename var-expansion 01/17/23 14:52:54.403
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 14:52:54.438
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 14:52:54.44
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:224
    STEP: creating the pod with failed condition 01/17/23 14:52:54.442
    Jan 17 14:52:54.480: INFO: Waiting up to 2m0s for pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8" in namespace "var-expansion-7909" to be "running"
    Jan 17 14:52:54.485: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.257345ms
    Jan 17 14:52:56.490: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009825443s
    Jan 17 14:52:58.490: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00953972s
    Jan 17 14:53:00.490: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009759414s
    Jan 17 14:53:02.489: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.00862331s
    Jan 17 14:53:04.490: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 10.009374659s
    Jan 17 14:53:06.490: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 12.009608683s
    Jan 17 14:53:08.490: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 14.009591213s
    Jan 17 14:53:10.490: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 16.010001835s
    Jan 17 14:53:12.490: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 18.010082207s
    Jan 17 14:53:14.490: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 20.009589103s
    Jan 17 14:53:16.490: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 22.009755092s
    Jan 17 14:53:18.490: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 24.0093353s
    Jan 17 14:53:20.491: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 26.010388804s
    Jan 17 14:53:22.490: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 28.009650669s
    Jan 17 14:53:24.490: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 30.010119559s
    Jan 17 14:53:26.491: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 32.010481226s
    Jan 17 14:53:28.489: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 34.008483639s
    Jan 17 14:53:30.491: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 36.010411601s
    Jan 17 14:53:32.490: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 38.009981818s
    Jan 17 14:53:34.490: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 40.009491353s
    Jan 17 14:53:36.490: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 42.009743751s
    Jan 17 14:53:38.489: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 44.008777059s
    Jan 17 14:53:40.490: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 46.009784923s
    Jan 17 14:53:42.491: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 48.01067709s
    Jan 17 14:53:44.491: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 50.010334081s
    Jan 17 14:53:46.490: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 52.010236974s
    Jan 17 14:53:48.490: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 54.009889212s
    Jan 17 14:53:50.490: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 56.010099759s
    Jan 17 14:53:52.490: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 58.010183835s
    Jan 17 14:53:54.490: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.00939932s
    Jan 17 14:53:56.491: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.010278634s
    Jan 17 14:53:58.490: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.010088313s
    Jan 17 14:54:00.490: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.009266667s
    Jan 17 14:54:02.490: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.010203181s
    Jan 17 14:54:04.493: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.012600962s
    Jan 17 14:54:06.490: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.009684102s
    Jan 17 14:54:08.489: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.009007277s
    Jan 17 14:54:10.491: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.010491456s
    Jan 17 14:54:12.490: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.01022882s
    Jan 17 14:54:14.490: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.010120722s
    Jan 17 14:54:16.491: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.01027001s
    Jan 17 14:54:18.490: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.009576548s
    Jan 17 14:54:20.490: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.009747396s
    Jan 17 14:54:22.491: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.010456561s
    Jan 17 14:54:24.489: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.008902587s
    Jan 17 14:54:26.491: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.01078347s
    Jan 17 14:54:28.490: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.009709014s
    Jan 17 14:54:30.491: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.010541389s
    Jan 17 14:54:32.490: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.009800951s
    Jan 17 14:54:34.491: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.010608303s
    Jan 17 14:54:36.490: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.00997006s
    Jan 17 14:54:38.494: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.013969132s
    Jan 17 14:54:40.491: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.010627378s
    Jan 17 14:54:42.490: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.009957545s
    Jan 17 14:54:44.490: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.010140324s
    Jan 17 14:54:46.491: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.01059758s
    Jan 17 14:54:48.490: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.010001745s
    Jan 17 14:54:50.491: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.010703468s
    Jan 17 14:54:52.490: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.010063461s
    Jan 17 14:54:54.490: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.009839074s
    Jan 17 14:54:54.494: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.013340074s
    STEP: updating the pod 01/17/23 14:54:54.494
    Jan 17 14:54:55.012: INFO: Successfully updated pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8"
    STEP: waiting for pod running 01/17/23 14:54:55.012
    Jan 17 14:54:55.012: INFO: Waiting up to 2m0s for pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8" in namespace "var-expansion-7909" to be "running"
    Jan 17 14:54:55.015: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.277649ms
    Jan 17 14:54:57.019: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8": Phase="Running", Reason="", readiness=true. Elapsed: 2.006988192s
    Jan 17 14:54:57.019: INFO: Pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8" satisfied condition "running"
    STEP: deleting the pod gracefully 01/17/23 14:54:57.019
    Jan 17 14:54:57.019: INFO: Deleting pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8" in namespace "var-expansion-7909"
    Jan 17 14:54:57.033: INFO: Wait up to 5m0s for pod "var-expansion-59f7a67d-8d9f-4228-9649-63faf264e5b8" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan 17 14:55:29.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-7909" for this suite. 01/17/23 14:55:29.044
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:114
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 14:55:29.053
Jan 17 14:55:29.053: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename container-lifecycle-hook 01/17/23 14:55:29.054
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 14:55:29.087
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 14:55:29.09
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 01/17/23 14:55:29.099
Jan 17 14:55:29.122: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-1135" to be "running and ready"
Jan 17 14:55:29.142: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 19.557592ms
Jan 17 14:55:29.142: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 17 14:55:31.146: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023946321s
Jan 17 14:55:31.146: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 17 14:55:33.146: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.023924278s
Jan 17 14:55:33.146: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan 17 14:55:33.146: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:114
STEP: create the pod with lifecycle hook 01/17/23 14:55:33.15
Jan 17 14:55:33.163: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-1135" to be "running and ready"
Jan 17 14:55:33.166: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.001027ms
Jan 17 14:55:33.166: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 17 14:55:35.171: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.007319871s
Jan 17 14:55:35.171: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
Jan 17 14:55:35.171: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 01/17/23 14:55:35.174
Jan 17 14:55:35.181: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 17 14:55:35.186: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 17 14:55:37.187: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 17 14:55:37.191: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 17 14:55:39.186: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 17 14:55:39.190: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 01/17/23 14:55:39.19
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Jan 17 14:55:39.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1135" for this suite. 01/17/23 14:55:39.208
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","completed":8,"skipped":189,"failed":0}
------------------------------
• [SLOW TEST] [10.161 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:114

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 14:55:29.053
    Jan 17 14:55:29.053: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/17/23 14:55:29.054
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 14:55:29.087
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 14:55:29.09
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 01/17/23 14:55:29.099
    Jan 17 14:55:29.122: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-1135" to be "running and ready"
    Jan 17 14:55:29.142: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 19.557592ms
    Jan 17 14:55:29.142: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 14:55:31.146: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023946321s
    Jan 17 14:55:31.146: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 14:55:33.146: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.023924278s
    Jan 17 14:55:33.146: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan 17 14:55:33.146: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:114
    STEP: create the pod with lifecycle hook 01/17/23 14:55:33.15
    Jan 17 14:55:33.163: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-1135" to be "running and ready"
    Jan 17 14:55:33.166: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.001027ms
    Jan 17 14:55:33.166: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 14:55:35.171: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.007319871s
    Jan 17 14:55:35.171: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    Jan 17 14:55:35.171: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 01/17/23 14:55:35.174
    Jan 17 14:55:35.181: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jan 17 14:55:35.186: INFO: Pod pod-with-prestop-exec-hook still exists
    Jan 17 14:55:37.187: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jan 17 14:55:37.191: INFO: Pod pod-with-prestop-exec-hook still exists
    Jan 17 14:55:39.186: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jan 17 14:55:39.190: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 01/17/23 14:55:39.19
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Jan 17 14:55:39.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-1135" for this suite. 01/17/23 14:55:39.208
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 14:55:39.214
Jan 17 14:55:39.215: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename dns 01/17/23 14:55:39.215
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 14:55:39.244
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 14:55:39.248
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 01/17/23 14:55:39.259
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9923 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9923;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9923 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9923;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9923.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9923.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9923.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9923.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9923.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9923.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9923.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9923.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9923.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9923.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9923.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9923.svc;check="$$(dig +notcp +noall +answer +search 156.128.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.128.156_udp@PTR;check="$$(dig +tcp +noall +answer +search 156.128.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.128.156_tcp@PTR;sleep 1; done
 01/17/23 14:55:39.287
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9923 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9923;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9923 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9923;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9923.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9923.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9923.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9923.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9923.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9923.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9923.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9923.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9923.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9923.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9923.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9923.svc;check="$$(dig +notcp +noall +answer +search 156.128.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.128.156_udp@PTR;check="$$(dig +tcp +noall +answer +search 156.128.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.128.156_tcp@PTR;sleep 1; done
 01/17/23 14:55:39.287
STEP: creating a pod to probe DNS 01/17/23 14:55:39.287
STEP: submitting the pod to kubernetes 01/17/23 14:55:39.287
Jan 17 14:55:39.347: INFO: Waiting up to 15m0s for pod "dns-test-5850af5a-ab0a-4ab3-8034-87ef52453b6c" in namespace "dns-9923" to be "running"
Jan 17 14:55:39.351: INFO: Pod "dns-test-5850af5a-ab0a-4ab3-8034-87ef52453b6c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.617152ms
Jan 17 14:55:41.356: INFO: Pod "dns-test-5850af5a-ab0a-4ab3-8034-87ef52453b6c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009236483s
Jan 17 14:55:43.356: INFO: Pod "dns-test-5850af5a-ab0a-4ab3-8034-87ef52453b6c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009592261s
Jan 17 14:55:45.355: INFO: Pod "dns-test-5850af5a-ab0a-4ab3-8034-87ef52453b6c": Phase="Running", Reason="", readiness=true. Elapsed: 6.008441805s
Jan 17 14:55:45.355: INFO: Pod "dns-test-5850af5a-ab0a-4ab3-8034-87ef52453b6c" satisfied condition "running"
STEP: retrieving the pod 01/17/23 14:55:45.355
STEP: looking for the results for each expected name from probers 01/17/23 14:55:45.359
Jan 17 14:55:45.364: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9923/dns-test-5850af5a-ab0a-4ab3-8034-87ef52453b6c: the server could not find the requested resource (get pods dns-test-5850af5a-ab0a-4ab3-8034-87ef52453b6c)
Jan 17 14:55:45.369: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9923/dns-test-5850af5a-ab0a-4ab3-8034-87ef52453b6c: the server could not find the requested resource (get pods dns-test-5850af5a-ab0a-4ab3-8034-87ef52453b6c)
Jan 17 14:55:45.374: INFO: Unable to read wheezy_udp@dns-test-service.dns-9923 from pod dns-9923/dns-test-5850af5a-ab0a-4ab3-8034-87ef52453b6c: the server could not find the requested resource (get pods dns-test-5850af5a-ab0a-4ab3-8034-87ef52453b6c)
Jan 17 14:55:45.377: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9923 from pod dns-9923/dns-test-5850af5a-ab0a-4ab3-8034-87ef52453b6c: the server could not find the requested resource (get pods dns-test-5850af5a-ab0a-4ab3-8034-87ef52453b6c)
Jan 17 14:55:45.381: INFO: Unable to read wheezy_udp@dns-test-service.dns-9923.svc from pod dns-9923/dns-test-5850af5a-ab0a-4ab3-8034-87ef52453b6c: the server could not find the requested resource (get pods dns-test-5850af5a-ab0a-4ab3-8034-87ef52453b6c)
Jan 17 14:55:45.389: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9923.svc from pod dns-9923/dns-test-5850af5a-ab0a-4ab3-8034-87ef52453b6c: the server could not find the requested resource (get pods dns-test-5850af5a-ab0a-4ab3-8034-87ef52453b6c)
Jan 17 14:55:45.392: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9923.svc from pod dns-9923/dns-test-5850af5a-ab0a-4ab3-8034-87ef52453b6c: the server could not find the requested resource (get pods dns-test-5850af5a-ab0a-4ab3-8034-87ef52453b6c)
Jan 17 14:55:45.411: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9923/dns-test-5850af5a-ab0a-4ab3-8034-87ef52453b6c: the server could not find the requested resource (get pods dns-test-5850af5a-ab0a-4ab3-8034-87ef52453b6c)
Jan 17 14:55:45.415: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9923/dns-test-5850af5a-ab0a-4ab3-8034-87ef52453b6c: the server could not find the requested resource (get pods dns-test-5850af5a-ab0a-4ab3-8034-87ef52453b6c)
Jan 17 14:55:45.418: INFO: Unable to read jessie_udp@dns-test-service.dns-9923 from pod dns-9923/dns-test-5850af5a-ab0a-4ab3-8034-87ef52453b6c: the server could not find the requested resource (get pods dns-test-5850af5a-ab0a-4ab3-8034-87ef52453b6c)
Jan 17 14:55:45.421: INFO: Unable to read jessie_tcp@dns-test-service.dns-9923 from pod dns-9923/dns-test-5850af5a-ab0a-4ab3-8034-87ef52453b6c: the server could not find the requested resource (get pods dns-test-5850af5a-ab0a-4ab3-8034-87ef52453b6c)
Jan 17 14:55:45.425: INFO: Unable to read jessie_udp@dns-test-service.dns-9923.svc from pod dns-9923/dns-test-5850af5a-ab0a-4ab3-8034-87ef52453b6c: the server could not find the requested resource (get pods dns-test-5850af5a-ab0a-4ab3-8034-87ef52453b6c)
Jan 17 14:55:45.429: INFO: Unable to read jessie_tcp@dns-test-service.dns-9923.svc from pod dns-9923/dns-test-5850af5a-ab0a-4ab3-8034-87ef52453b6c: the server could not find the requested resource (get pods dns-test-5850af5a-ab0a-4ab3-8034-87ef52453b6c)
Jan 17 14:55:45.432: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9923.svc from pod dns-9923/dns-test-5850af5a-ab0a-4ab3-8034-87ef52453b6c: the server could not find the requested resource (get pods dns-test-5850af5a-ab0a-4ab3-8034-87ef52453b6c)
Jan 17 14:55:45.435: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9923.svc from pod dns-9923/dns-test-5850af5a-ab0a-4ab3-8034-87ef52453b6c: the server could not find the requested resource (get pods dns-test-5850af5a-ab0a-4ab3-8034-87ef52453b6c)
Jan 17 14:55:45.452: INFO: Lookups using dns-9923/dns-test-5850af5a-ab0a-4ab3-8034-87ef52453b6c failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9923 wheezy_tcp@dns-test-service.dns-9923 wheezy_udp@dns-test-service.dns-9923.svc wheezy_udp@_http._tcp.dns-test-service.dns-9923.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9923.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9923 jessie_tcp@dns-test-service.dns-9923 jessie_udp@dns-test-service.dns-9923.svc jessie_tcp@dns-test-service.dns-9923.svc jessie_udp@_http._tcp.dns-test-service.dns-9923.svc jessie_tcp@_http._tcp.dns-test-service.dns-9923.svc]

Jan 17 14:55:50.533: INFO: DNS probes using dns-9923/dns-test-5850af5a-ab0a-4ab3-8034-87ef52453b6c succeeded

STEP: deleting the pod 01/17/23 14:55:50.533
STEP: deleting the test service 01/17/23 14:55:50.549
STEP: deleting the test headless service 01/17/23 14:55:50.583
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan 17 14:55:50.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9923" for this suite. 01/17/23 14:55:50.627
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","completed":9,"skipped":199,"failed":0}
------------------------------
• [SLOW TEST] [11.426 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 14:55:39.214
    Jan 17 14:55:39.215: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename dns 01/17/23 14:55:39.215
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 14:55:39.244
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 14:55:39.248
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 01/17/23 14:55:39.259
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9923 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9923;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9923 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9923;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9923.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9923.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9923.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9923.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9923.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9923.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9923.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9923.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9923.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9923.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9923.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9923.svc;check="$$(dig +notcp +noall +answer +search 156.128.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.128.156_udp@PTR;check="$$(dig +tcp +noall +answer +search 156.128.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.128.156_tcp@PTR;sleep 1; done
     01/17/23 14:55:39.287
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9923 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9923;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9923 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9923;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9923.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9923.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9923.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9923.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9923.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9923.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9923.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9923.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9923.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9923.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9923.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9923.svc;check="$$(dig +notcp +noall +answer +search 156.128.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.128.156_udp@PTR;check="$$(dig +tcp +noall +answer +search 156.128.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.128.156_tcp@PTR;sleep 1; done
     01/17/23 14:55:39.287
    STEP: creating a pod to probe DNS 01/17/23 14:55:39.287
    STEP: submitting the pod to kubernetes 01/17/23 14:55:39.287
    Jan 17 14:55:39.347: INFO: Waiting up to 15m0s for pod "dns-test-5850af5a-ab0a-4ab3-8034-87ef52453b6c" in namespace "dns-9923" to be "running"
    Jan 17 14:55:39.351: INFO: Pod "dns-test-5850af5a-ab0a-4ab3-8034-87ef52453b6c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.617152ms
    Jan 17 14:55:41.356: INFO: Pod "dns-test-5850af5a-ab0a-4ab3-8034-87ef52453b6c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009236483s
    Jan 17 14:55:43.356: INFO: Pod "dns-test-5850af5a-ab0a-4ab3-8034-87ef52453b6c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009592261s
    Jan 17 14:55:45.355: INFO: Pod "dns-test-5850af5a-ab0a-4ab3-8034-87ef52453b6c": Phase="Running", Reason="", readiness=true. Elapsed: 6.008441805s
    Jan 17 14:55:45.355: INFO: Pod "dns-test-5850af5a-ab0a-4ab3-8034-87ef52453b6c" satisfied condition "running"
    STEP: retrieving the pod 01/17/23 14:55:45.355
    STEP: looking for the results for each expected name from probers 01/17/23 14:55:45.359
    Jan 17 14:55:45.364: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9923/dns-test-5850af5a-ab0a-4ab3-8034-87ef52453b6c: the server could not find the requested resource (get pods dns-test-5850af5a-ab0a-4ab3-8034-87ef52453b6c)
    Jan 17 14:55:45.369: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9923/dns-test-5850af5a-ab0a-4ab3-8034-87ef52453b6c: the server could not find the requested resource (get pods dns-test-5850af5a-ab0a-4ab3-8034-87ef52453b6c)
    Jan 17 14:55:45.374: INFO: Unable to read wheezy_udp@dns-test-service.dns-9923 from pod dns-9923/dns-test-5850af5a-ab0a-4ab3-8034-87ef52453b6c: the server could not find the requested resource (get pods dns-test-5850af5a-ab0a-4ab3-8034-87ef52453b6c)
    Jan 17 14:55:45.377: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9923 from pod dns-9923/dns-test-5850af5a-ab0a-4ab3-8034-87ef52453b6c: the server could not find the requested resource (get pods dns-test-5850af5a-ab0a-4ab3-8034-87ef52453b6c)
    Jan 17 14:55:45.381: INFO: Unable to read wheezy_udp@dns-test-service.dns-9923.svc from pod dns-9923/dns-test-5850af5a-ab0a-4ab3-8034-87ef52453b6c: the server could not find the requested resource (get pods dns-test-5850af5a-ab0a-4ab3-8034-87ef52453b6c)
    Jan 17 14:55:45.389: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9923.svc from pod dns-9923/dns-test-5850af5a-ab0a-4ab3-8034-87ef52453b6c: the server could not find the requested resource (get pods dns-test-5850af5a-ab0a-4ab3-8034-87ef52453b6c)
    Jan 17 14:55:45.392: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9923.svc from pod dns-9923/dns-test-5850af5a-ab0a-4ab3-8034-87ef52453b6c: the server could not find the requested resource (get pods dns-test-5850af5a-ab0a-4ab3-8034-87ef52453b6c)
    Jan 17 14:55:45.411: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9923/dns-test-5850af5a-ab0a-4ab3-8034-87ef52453b6c: the server could not find the requested resource (get pods dns-test-5850af5a-ab0a-4ab3-8034-87ef52453b6c)
    Jan 17 14:55:45.415: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9923/dns-test-5850af5a-ab0a-4ab3-8034-87ef52453b6c: the server could not find the requested resource (get pods dns-test-5850af5a-ab0a-4ab3-8034-87ef52453b6c)
    Jan 17 14:55:45.418: INFO: Unable to read jessie_udp@dns-test-service.dns-9923 from pod dns-9923/dns-test-5850af5a-ab0a-4ab3-8034-87ef52453b6c: the server could not find the requested resource (get pods dns-test-5850af5a-ab0a-4ab3-8034-87ef52453b6c)
    Jan 17 14:55:45.421: INFO: Unable to read jessie_tcp@dns-test-service.dns-9923 from pod dns-9923/dns-test-5850af5a-ab0a-4ab3-8034-87ef52453b6c: the server could not find the requested resource (get pods dns-test-5850af5a-ab0a-4ab3-8034-87ef52453b6c)
    Jan 17 14:55:45.425: INFO: Unable to read jessie_udp@dns-test-service.dns-9923.svc from pod dns-9923/dns-test-5850af5a-ab0a-4ab3-8034-87ef52453b6c: the server could not find the requested resource (get pods dns-test-5850af5a-ab0a-4ab3-8034-87ef52453b6c)
    Jan 17 14:55:45.429: INFO: Unable to read jessie_tcp@dns-test-service.dns-9923.svc from pod dns-9923/dns-test-5850af5a-ab0a-4ab3-8034-87ef52453b6c: the server could not find the requested resource (get pods dns-test-5850af5a-ab0a-4ab3-8034-87ef52453b6c)
    Jan 17 14:55:45.432: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9923.svc from pod dns-9923/dns-test-5850af5a-ab0a-4ab3-8034-87ef52453b6c: the server could not find the requested resource (get pods dns-test-5850af5a-ab0a-4ab3-8034-87ef52453b6c)
    Jan 17 14:55:45.435: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9923.svc from pod dns-9923/dns-test-5850af5a-ab0a-4ab3-8034-87ef52453b6c: the server could not find the requested resource (get pods dns-test-5850af5a-ab0a-4ab3-8034-87ef52453b6c)
    Jan 17 14:55:45.452: INFO: Lookups using dns-9923/dns-test-5850af5a-ab0a-4ab3-8034-87ef52453b6c failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9923 wheezy_tcp@dns-test-service.dns-9923 wheezy_udp@dns-test-service.dns-9923.svc wheezy_udp@_http._tcp.dns-test-service.dns-9923.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9923.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9923 jessie_tcp@dns-test-service.dns-9923 jessie_udp@dns-test-service.dns-9923.svc jessie_tcp@dns-test-service.dns-9923.svc jessie_udp@_http._tcp.dns-test-service.dns-9923.svc jessie_tcp@_http._tcp.dns-test-service.dns-9923.svc]

    Jan 17 14:55:50.533: INFO: DNS probes using dns-9923/dns-test-5850af5a-ab0a-4ab3-8034-87ef52453b6c succeeded

    STEP: deleting the pod 01/17/23 14:55:50.533
    STEP: deleting the test service 01/17/23 14:55:50.549
    STEP: deleting the test headless service 01/17/23 14:55:50.583
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan 17 14:55:50.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-9923" for this suite. 01/17/23 14:55:50.627
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 14:55:50.642
Jan 17 14:55:50.642: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename kubelet-test 01/17/23 14:55:50.643
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 14:55:50.672
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 14:55:50.675
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 01/17/23 14:55:50.706
Jan 17 14:55:50.706: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases81db81dc-6e3d-4491-bde0-ce98c90205fb" in namespace "kubelet-test-7455" to be "completed"
Jan 17 14:55:50.710: INFO: Pod "agnhost-host-aliases81db81dc-6e3d-4491-bde0-ce98c90205fb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.464135ms
Jan 17 14:55:52.714: INFO: Pod "agnhost-host-aliases81db81dc-6e3d-4491-bde0-ce98c90205fb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007319573s
Jan 17 14:55:54.714: INFO: Pod "agnhost-host-aliases81db81dc-6e3d-4491-bde0-ce98c90205fb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007960195s
Jan 17 14:55:54.714: INFO: Pod "agnhost-host-aliases81db81dc-6e3d-4491-bde0-ce98c90205fb" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Jan 17 14:55:54.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7455" for this suite. 01/17/23 14:55:54.726
{"msg":"PASSED [sig-node] Kubelet when scheduling an agnhost Pod with hostAliases should write entries to /etc/hosts [NodeConformance] [Conformance]","completed":10,"skipped":267,"failed":0}
------------------------------
• [4.090 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 14:55:50.642
    Jan 17 14:55:50.642: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename kubelet-test 01/17/23 14:55:50.643
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 14:55:50.672
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 14:55:50.675
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 01/17/23 14:55:50.706
    Jan 17 14:55:50.706: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases81db81dc-6e3d-4491-bde0-ce98c90205fb" in namespace "kubelet-test-7455" to be "completed"
    Jan 17 14:55:50.710: INFO: Pod "agnhost-host-aliases81db81dc-6e3d-4491-bde0-ce98c90205fb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.464135ms
    Jan 17 14:55:52.714: INFO: Pod "agnhost-host-aliases81db81dc-6e3d-4491-bde0-ce98c90205fb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007319573s
    Jan 17 14:55:54.714: INFO: Pod "agnhost-host-aliases81db81dc-6e3d-4491-bde0-ce98c90205fb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007960195s
    Jan 17 14:55:54.714: INFO: Pod "agnhost-host-aliases81db81dc-6e3d-4491-bde0-ce98c90205fb" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Jan 17 14:55:54.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-7455" for this suite. 01/17/23 14:55:54.726
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 14:55:54.732
Jan 17 14:55:54.732: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename watch 01/17/23 14:55:54.733
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 14:55:54.77
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 14:55:54.772
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 01/17/23 14:55:54.774
STEP: creating a watch on configmaps with label B 01/17/23 14:55:54.775
STEP: creating a watch on configmaps with label A or B 01/17/23 14:55:54.777
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 01/17/23 14:55:54.778
Jan 17 14:55:54.782: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8912  33bf74fe-02e9-4dc0-8175-ae81bb7f5a0f 69671 0 2023-01-17 14:55:54 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-17 14:55:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 17 14:55:54.782: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8912  33bf74fe-02e9-4dc0-8175-ae81bb7f5a0f 69671 0 2023-01-17 14:55:54 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-17 14:55:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 01/17/23 14:55:54.782
Jan 17 14:55:54.795: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8912  33bf74fe-02e9-4dc0-8175-ae81bb7f5a0f 69672 0 2023-01-17 14:55:54 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-17 14:55:54 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 17 14:55:54.795: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8912  33bf74fe-02e9-4dc0-8175-ae81bb7f5a0f 69672 0 2023-01-17 14:55:54 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-17 14:55:54 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 01/17/23 14:55:54.795
Jan 17 14:55:54.805: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8912  33bf74fe-02e9-4dc0-8175-ae81bb7f5a0f 69675 0 2023-01-17 14:55:54 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-17 14:55:54 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 17 14:55:54.805: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8912  33bf74fe-02e9-4dc0-8175-ae81bb7f5a0f 69675 0 2023-01-17 14:55:54 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-17 14:55:54 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 01/17/23 14:55:54.805
Jan 17 14:55:54.821: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8912  33bf74fe-02e9-4dc0-8175-ae81bb7f5a0f 69680 0 2023-01-17 14:55:54 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-17 14:55:54 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 17 14:55:54.821: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8912  33bf74fe-02e9-4dc0-8175-ae81bb7f5a0f 69680 0 2023-01-17 14:55:54 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-17 14:55:54 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 01/17/23 14:55:54.821
Jan 17 14:55:54.830: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8912  554af5a1-8844-498c-9ec0-e6588edc788c 69683 0 2023-01-17 14:55:54 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-17 14:55:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 17 14:55:54.830: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8912  554af5a1-8844-498c-9ec0-e6588edc788c 69683 0 2023-01-17 14:55:54 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-17 14:55:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 01/17/23 14:56:04.831
Jan 17 14:56:04.838: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8912  554af5a1-8844-498c-9ec0-e6588edc788c 69815 0 2023-01-17 14:55:54 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-17 14:55:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 17 14:56:04.838: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8912  554af5a1-8844-498c-9ec0-e6588edc788c 69815 0 2023-01-17 14:55:54 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-17 14:55:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Jan 17 14:56:14.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8912" for this suite. 01/17/23 14:56:14.844
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","completed":11,"skipped":279,"failed":0}
------------------------------
• [SLOW TEST] [20.118 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 14:55:54.732
    Jan 17 14:55:54.732: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename watch 01/17/23 14:55:54.733
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 14:55:54.77
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 14:55:54.772
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 01/17/23 14:55:54.774
    STEP: creating a watch on configmaps with label B 01/17/23 14:55:54.775
    STEP: creating a watch on configmaps with label A or B 01/17/23 14:55:54.777
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 01/17/23 14:55:54.778
    Jan 17 14:55:54.782: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8912  33bf74fe-02e9-4dc0-8175-ae81bb7f5a0f 69671 0 2023-01-17 14:55:54 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-17 14:55:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 17 14:55:54.782: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8912  33bf74fe-02e9-4dc0-8175-ae81bb7f5a0f 69671 0 2023-01-17 14:55:54 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-17 14:55:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 01/17/23 14:55:54.782
    Jan 17 14:55:54.795: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8912  33bf74fe-02e9-4dc0-8175-ae81bb7f5a0f 69672 0 2023-01-17 14:55:54 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-17 14:55:54 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 17 14:55:54.795: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8912  33bf74fe-02e9-4dc0-8175-ae81bb7f5a0f 69672 0 2023-01-17 14:55:54 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-17 14:55:54 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 01/17/23 14:55:54.795
    Jan 17 14:55:54.805: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8912  33bf74fe-02e9-4dc0-8175-ae81bb7f5a0f 69675 0 2023-01-17 14:55:54 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-17 14:55:54 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 17 14:55:54.805: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8912  33bf74fe-02e9-4dc0-8175-ae81bb7f5a0f 69675 0 2023-01-17 14:55:54 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-17 14:55:54 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 01/17/23 14:55:54.805
    Jan 17 14:55:54.821: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8912  33bf74fe-02e9-4dc0-8175-ae81bb7f5a0f 69680 0 2023-01-17 14:55:54 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-17 14:55:54 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 17 14:55:54.821: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8912  33bf74fe-02e9-4dc0-8175-ae81bb7f5a0f 69680 0 2023-01-17 14:55:54 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-17 14:55:54 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 01/17/23 14:55:54.821
    Jan 17 14:55:54.830: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8912  554af5a1-8844-498c-9ec0-e6588edc788c 69683 0 2023-01-17 14:55:54 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-17 14:55:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 17 14:55:54.830: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8912  554af5a1-8844-498c-9ec0-e6588edc788c 69683 0 2023-01-17 14:55:54 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-17 14:55:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 01/17/23 14:56:04.831
    Jan 17 14:56:04.838: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8912  554af5a1-8844-498c-9ec0-e6588edc788c 69815 0 2023-01-17 14:55:54 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-17 14:55:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 17 14:56:04.838: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8912  554af5a1-8844-498c-9ec0-e6588edc788c 69815 0 2023-01-17 14:55:54 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-17 14:55:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Jan 17 14:56:14.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-8912" for this suite. 01/17/23 14:56:14.844
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 14:56:14.85
Jan 17 14:56:14.850: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename podtemplate 01/17/23 14:56:14.851
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 14:56:14.877
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 14:56:14.88
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 01/17/23 14:56:14.882
Jan 17 14:56:14.894: INFO: created test-podtemplate-1
Jan 17 14:56:14.899: INFO: created test-podtemplate-2
Jan 17 14:56:14.912: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 01/17/23 14:56:14.912
STEP: delete collection of pod templates 01/17/23 14:56:14.916
Jan 17 14:56:14.916: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 01/17/23 14:56:14.944
Jan 17 14:56:14.944: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Jan 17 14:56:14.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-5330" for this suite. 01/17/23 14:56:14.951
{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","completed":12,"skipped":279,"failed":0}
------------------------------
• [0.106 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 14:56:14.85
    Jan 17 14:56:14.850: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename podtemplate 01/17/23 14:56:14.851
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 14:56:14.877
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 14:56:14.88
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 01/17/23 14:56:14.882
    Jan 17 14:56:14.894: INFO: created test-podtemplate-1
    Jan 17 14:56:14.899: INFO: created test-podtemplate-2
    Jan 17 14:56:14.912: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 01/17/23 14:56:14.912
    STEP: delete collection of pod templates 01/17/23 14:56:14.916
    Jan 17 14:56:14.916: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 01/17/23 14:56:14.944
    Jan 17 14:56:14.944: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Jan 17 14:56:14.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-5330" for this suite. 01/17/23 14:56:14.951
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 14:56:14.957
Jan 17 14:56:14.957: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename configmap 01/17/23 14:56:14.958
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 14:56:14.984
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 14:56:14.986
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44
STEP: Creating configMap configmap-8367/configmap-test-dc0549c5-7bbd-4f9c-921a-7698853b1dc8 01/17/23 14:56:14.989
STEP: Creating a pod to test consume configMaps 01/17/23 14:56:15.001
Jan 17 14:56:15.038: INFO: Waiting up to 5m0s for pod "pod-configmaps-7a868f98-1559-4492-9a4d-9284af031050" in namespace "configmap-8367" to be "Succeeded or Failed"
Jan 17 14:56:15.049: INFO: Pod "pod-configmaps-7a868f98-1559-4492-9a4d-9284af031050": Phase="Pending", Reason="", readiness=false. Elapsed: 11.477799ms
Jan 17 14:56:17.054: INFO: Pod "pod-configmaps-7a868f98-1559-4492-9a4d-9284af031050": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016260001s
Jan 17 14:56:19.053: INFO: Pod "pod-configmaps-7a868f98-1559-4492-9a4d-9284af031050": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014930329s
STEP: Saw pod success 01/17/23 14:56:19.053
Jan 17 14:56:19.053: INFO: Pod "pod-configmaps-7a868f98-1559-4492-9a4d-9284af031050" satisfied condition "Succeeded or Failed"
Jan 17 14:56:19.056: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-configmaps-7a868f98-1559-4492-9a4d-9284af031050 container env-test: <nil>
STEP: delete the pod 01/17/23 14:56:19.067
Jan 17 14:56:19.082: INFO: Waiting for pod pod-configmaps-7a868f98-1559-4492-9a4d-9284af031050 to disappear
Jan 17 14:56:19.085: INFO: Pod pod-configmaps-7a868f98-1559-4492-9a4d-9284af031050 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Jan 17 14:56:19.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8367" for this suite. 01/17/23 14:56:19.089
{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","completed":13,"skipped":284,"failed":0}
------------------------------
• [4.137 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 14:56:14.957
    Jan 17 14:56:14.957: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename configmap 01/17/23 14:56:14.958
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 14:56:14.984
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 14:56:14.986
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:44
    STEP: Creating configMap configmap-8367/configmap-test-dc0549c5-7bbd-4f9c-921a-7698853b1dc8 01/17/23 14:56:14.989
    STEP: Creating a pod to test consume configMaps 01/17/23 14:56:15.001
    Jan 17 14:56:15.038: INFO: Waiting up to 5m0s for pod "pod-configmaps-7a868f98-1559-4492-9a4d-9284af031050" in namespace "configmap-8367" to be "Succeeded or Failed"
    Jan 17 14:56:15.049: INFO: Pod "pod-configmaps-7a868f98-1559-4492-9a4d-9284af031050": Phase="Pending", Reason="", readiness=false. Elapsed: 11.477799ms
    Jan 17 14:56:17.054: INFO: Pod "pod-configmaps-7a868f98-1559-4492-9a4d-9284af031050": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016260001s
    Jan 17 14:56:19.053: INFO: Pod "pod-configmaps-7a868f98-1559-4492-9a4d-9284af031050": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014930329s
    STEP: Saw pod success 01/17/23 14:56:19.053
    Jan 17 14:56:19.053: INFO: Pod "pod-configmaps-7a868f98-1559-4492-9a4d-9284af031050" satisfied condition "Succeeded or Failed"
    Jan 17 14:56:19.056: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-configmaps-7a868f98-1559-4492-9a4d-9284af031050 container env-test: <nil>
    STEP: delete the pod 01/17/23 14:56:19.067
    Jan 17 14:56:19.082: INFO: Waiting for pod pod-configmaps-7a868f98-1559-4492-9a4d-9284af031050 to disappear
    Jan 17 14:56:19.085: INFO: Pod pod-configmaps-7a868f98-1559-4492-9a4d-9284af031050 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 17 14:56:19.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-8367" for this suite. 01/17/23 14:56:19.089
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:194
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 14:56:19.095
Jan 17 14:56:19.095: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename container-runtime 01/17/23 14:56:19.096
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 14:56:19.116
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 14:56:19.118
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:194
STEP: create the container 01/17/23 14:56:19.12
STEP: wait for the container to reach Succeeded 01/17/23 14:56:19.142
STEP: get the container status 01/17/23 14:56:23.165
STEP: the container should be terminated 01/17/23 14:56:23.168
STEP: the termination message should be set 01/17/23 14:56:23.168
Jan 17 14:56:23.168: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 01/17/23 14:56:23.168
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Jan 17 14:56:23.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1826" for this suite. 01/17/23 14:56:23.189
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","completed":14,"skipped":306,"failed":0}
------------------------------
• [4.100 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 14:56:19.095
    Jan 17 14:56:19.095: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename container-runtime 01/17/23 14:56:19.096
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 14:56:19.116
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 14:56:19.118
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:194
    STEP: create the container 01/17/23 14:56:19.12
    STEP: wait for the container to reach Succeeded 01/17/23 14:56:19.142
    STEP: get the container status 01/17/23 14:56:23.165
    STEP: the container should be terminated 01/17/23 14:56:23.168
    STEP: the termination message should be set 01/17/23 14:56:23.168
    Jan 17 14:56:23.168: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 01/17/23 14:56:23.168
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Jan 17 14:56:23.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-1826" for this suite. 01/17/23 14:56:23.189
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 14:56:23.195
Jan 17 14:56:23.195: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename disruption 01/17/23 14:56:23.196
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 14:56:23.217
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 14:56:23.219
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107
STEP: creating the pdb 01/17/23 14:56:23.221
STEP: Waiting for the pdb to be processed 01/17/23 14:56:23.231
STEP: updating the pdb 01/17/23 14:56:25.24
STEP: Waiting for the pdb to be processed 01/17/23 14:56:25.247
STEP: patching the pdb 01/17/23 14:56:27.254
STEP: Waiting for the pdb to be processed 01/17/23 14:56:27.261
STEP: Waiting for the pdb to be deleted 01/17/23 14:56:29.274
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Jan 17 14:56:29.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-3699" for this suite. 01/17/23 14:56:29.28
{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","completed":15,"skipped":314,"failed":0}
------------------------------
• [SLOW TEST] [6.091 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 14:56:23.195
    Jan 17 14:56:23.195: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename disruption 01/17/23 14:56:23.196
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 14:56:23.217
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 14:56:23.219
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:107
    STEP: creating the pdb 01/17/23 14:56:23.221
    STEP: Waiting for the pdb to be processed 01/17/23 14:56:23.231
    STEP: updating the pdb 01/17/23 14:56:25.24
    STEP: Waiting for the pdb to be processed 01/17/23 14:56:25.247
    STEP: patching the pdb 01/17/23 14:56:27.254
    STEP: Waiting for the pdb to be processed 01/17/23 14:56:27.261
    STEP: Waiting for the pdb to be deleted 01/17/23 14:56:29.274
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Jan 17 14:56:29.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-3699" for this suite. 01/17/23 14:56:29.28
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 14:56:29.287
Jan 17 14:56:29.287: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename deployment 01/17/23 14:56:29.287
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 14:56:29.31
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 14:56:29.313
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
Jan 17 14:56:29.315: INFO: Creating deployment "test-recreate-deployment"
W0117 14:56:29.322019      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 17 14:56:29.322: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jan 17 14:56:29.333: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
Jan 17 14:56:31.341: INFO: Waiting deployment "test-recreate-deployment" to complete
Jan 17 14:56:31.344: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jan 17 14:56:31.353: INFO: Updating deployment test-recreate-deployment
Jan 17 14:56:31.353: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 17 14:56:31.432: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-9465  d0b2b800-6a05-43e9-91d7-3aa854328cc8 70314 2 2023-01-17 14:56:29 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-17 14:56:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 14:56:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000cae768 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-17 14:56:31 +0000 UTC,LastTransitionTime:2023-01-17 14:56:31 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-9d58999df" is progressing.,LastUpdateTime:2023-01-17 14:56:31 +0000 UTC,LastTransitionTime:2023-01-17 14:56:29 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Jan 17 14:56:31.435: INFO: New ReplicaSet "test-recreate-deployment-9d58999df" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-9d58999df  deployment-9465  71895a75-9780-4b98-bb09-dd3c7cb657b9 70312 1 2023-01-17 14:56:31 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment d0b2b800-6a05-43e9-91d7-3aa854328cc8 0xc000caecb0 0xc000caecb1}] [] [{kube-controller-manager Update apps/v1 2023-01-17 14:56:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d0b2b800-6a05-43e9-91d7-3aa854328cc8\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 14:56:31 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 9d58999df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000caed48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 17 14:56:31.435: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jan 17 14:56:31.435: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d8b6f647f  deployment-9465  a6ff6ae7-5e69-47f7-a706-68c01bb0da28 70303 2 2023-01-17 14:56:29 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment d0b2b800-6a05-43e9-91d7-3aa854328cc8 0xc000caeb87 0xc000caeb88}] [] [{kube-controller-manager Update apps/v1 2023-01-17 14:56:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d0b2b800-6a05-43e9-91d7-3aa854328cc8\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 14:56:31 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d8b6f647f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000caec48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 17 14:56:31.440: INFO: Pod "test-recreate-deployment-9d58999df-m5v85" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-9d58999df-m5v85 test-recreate-deployment-9d58999df- deployment-9465  69185ccd-a59d-41ac-ade8-ad90b9d98c93 70316 0 2023-01-17 14:56:31 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.131.0.33/23"],"mac_address":"0a:58:0a:83:00:21","gateway_ips":["10.131.0.1"],"ip_address":"10.131.0.33/23","gateway_ip":"10.131.0.1"}} openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-recreate-deployment-9d58999df 71895a75-9780-4b98-bb09-dd3c7cb657b9 0xc000caf277 0xc000caf278}] [] [{ip-10-0-135-246 Update v1 2023-01-17 14:56:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 14:56:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"71895a75-9780-4b98-bb09-dd3c7cb657b9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 14:56:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-x8dp2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-x8dp2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-151-22.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c29,c9,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-xkvqw,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 14:56:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 14:56:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 14:56:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 14:56:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.151.22,PodIP:,StartTime:2023-01-17 14:56:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan 17 14:56:31.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9465" for this suite. 01/17/23 14:56:31.445
{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","completed":16,"skipped":324,"failed":0}
------------------------------
• [2.165 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 14:56:29.287
    Jan 17 14:56:29.287: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename deployment 01/17/23 14:56:29.287
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 14:56:29.31
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 14:56:29.313
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    Jan 17 14:56:29.315: INFO: Creating deployment "test-recreate-deployment"
    W0117 14:56:29.322019      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jan 17 14:56:29.322: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    Jan 17 14:56:29.333: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
    Jan 17 14:56:31.341: INFO: Waiting deployment "test-recreate-deployment" to complete
    Jan 17 14:56:31.344: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    Jan 17 14:56:31.353: INFO: Updating deployment test-recreate-deployment
    Jan 17 14:56:31.353: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 17 14:56:31.432: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-9465  d0b2b800-6a05-43e9-91d7-3aa854328cc8 70314 2 2023-01-17 14:56:29 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-17 14:56:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 14:56:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000cae768 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-17 14:56:31 +0000 UTC,LastTransitionTime:2023-01-17 14:56:31 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-9d58999df" is progressing.,LastUpdateTime:2023-01-17 14:56:31 +0000 UTC,LastTransitionTime:2023-01-17 14:56:29 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Jan 17 14:56:31.435: INFO: New ReplicaSet "test-recreate-deployment-9d58999df" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-9d58999df  deployment-9465  71895a75-9780-4b98-bb09-dd3c7cb657b9 70312 1 2023-01-17 14:56:31 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment d0b2b800-6a05-43e9-91d7-3aa854328cc8 0xc000caecb0 0xc000caecb1}] [] [{kube-controller-manager Update apps/v1 2023-01-17 14:56:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d0b2b800-6a05-43e9-91d7-3aa854328cc8\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 14:56:31 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 9d58999df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000caed48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 17 14:56:31.435: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    Jan 17 14:56:31.435: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d8b6f647f  deployment-9465  a6ff6ae7-5e69-47f7-a706-68c01bb0da28 70303 2 2023-01-17 14:56:29 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment d0b2b800-6a05-43e9-91d7-3aa854328cc8 0xc000caeb87 0xc000caeb88}] [] [{kube-controller-manager Update apps/v1 2023-01-17 14:56:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d0b2b800-6a05-43e9-91d7-3aa854328cc8\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 14:56:31 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d8b6f647f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000caec48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 17 14:56:31.440: INFO: Pod "test-recreate-deployment-9d58999df-m5v85" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-9d58999df-m5v85 test-recreate-deployment-9d58999df- deployment-9465  69185ccd-a59d-41ac-ade8-ad90b9d98c93 70316 0 2023-01-17 14:56:31 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.131.0.33/23"],"mac_address":"0a:58:0a:83:00:21","gateway_ips":["10.131.0.1"],"ip_address":"10.131.0.33/23","gateway_ip":"10.131.0.1"}} openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-recreate-deployment-9d58999df 71895a75-9780-4b98-bb09-dd3c7cb657b9 0xc000caf277 0xc000caf278}] [] [{ip-10-0-135-246 Update v1 2023-01-17 14:56:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 14:56:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"71895a75-9780-4b98-bb09-dd3c7cb657b9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 14:56:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-x8dp2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-x8dp2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-151-22.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c29,c9,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-xkvqw,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 14:56:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 14:56:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 14:56:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 14:56:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.151.22,PodIP:,StartTime:2023-01-17 14:56:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan 17 14:56:31.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-9465" for this suite. 01/17/23 14:56:31.445
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 14:56:31.453
Jan 17 14:56:31.453: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename job 01/17/23 14:56:31.454
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 14:56:31.475
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 14:56:31.478
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194
STEP: Creating Indexed job 01/17/23 14:56:31.48
W0117 14:56:31.490166      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring job reaches completions 01/17/23 14:56:31.49
STEP: Ensuring pods with index for job exist 01/17/23 14:56:41.494
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jan 17 14:56:41.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1079" for this suite. 01/17/23 14:56:41.502
{"msg":"PASSED [sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]","completed":17,"skipped":347,"failed":0}
------------------------------
• [SLOW TEST] [10.055 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 14:56:31.453
    Jan 17 14:56:31.453: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename job 01/17/23 14:56:31.454
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 14:56:31.475
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 14:56:31.478
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:194
    STEP: Creating Indexed job 01/17/23 14:56:31.48
    W0117 14:56:31.490166      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Ensuring job reaches completions 01/17/23 14:56:31.49
    STEP: Ensuring pods with index for job exist 01/17/23 14:56:41.494
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jan 17 14:56:41.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-1079" for this suite. 01/17/23 14:56:41.502
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 14:56:41.508
Jan 17 14:56:41.508: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename replication-controller 01/17/23 14:56:41.509
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 14:56:41.533
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 14:56:41.536
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82
Jan 17 14:56:41.538: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 01/17/23 14:56:42.555
STEP: Checking rc "condition-test" has the desired failure condition set 01/17/23 14:56:42.561
STEP: Scaling down rc "condition-test" to satisfy pod quota 01/17/23 14:56:43.568
Jan 17 14:56:43.577: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 01/17/23 14:56:43.577
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Jan 17 14:56:44.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7921" for this suite. 01/17/23 14:56:44.588
{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","completed":18,"skipped":347,"failed":0}
------------------------------
• [3.085 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 14:56:41.508
    Jan 17 14:56:41.508: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename replication-controller 01/17/23 14:56:41.509
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 14:56:41.533
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 14:56:41.536
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:82
    Jan 17 14:56:41.538: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 01/17/23 14:56:42.555
    STEP: Checking rc "condition-test" has the desired failure condition set 01/17/23 14:56:42.561
    STEP: Scaling down rc "condition-test" to satisfy pod quota 01/17/23 14:56:43.568
    Jan 17 14:56:43.577: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 01/17/23 14:56:43.577
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Jan 17 14:56:44.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-7921" for this suite. 01/17/23 14:56:44.588
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 14:56:44.593
Jan 17 14:56:44.593: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename secrets 01/17/23 14:56:44.594
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 14:56:44.626
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 14:56:44.628
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139
STEP: Creating projection with secret that has name secret-emptykey-test-66d38d75-8bf6-4d43-9e29-3180cb72c435 01/17/23 14:56:44.63
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Jan 17 14:56:44.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3451" for this suite. 01/17/23 14:56:44.641
{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","completed":19,"skipped":349,"failed":0}
------------------------------
• [0.054 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 14:56:44.593
    Jan 17 14:56:44.593: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename secrets 01/17/23 14:56:44.594
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 14:56:44.626
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 14:56:44.628
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:139
    STEP: Creating projection with secret that has name secret-emptykey-test-66d38d75-8bf6-4d43-9e29-3180cb72c435 01/17/23 14:56:44.63
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Jan 17 14:56:44.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-3451" for this suite. 01/17/23 14:56:44.641
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 14:56:44.648
Jan 17 14:56:44.648: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename subpath 01/17/23 14:56:44.649
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 14:56:44.686
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 14:56:44.689
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/17/23 14:56:44.691
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-bsvv 01/17/23 14:56:44.706
STEP: Creating a pod to test atomic-volume-subpath 01/17/23 14:56:44.706
Jan 17 14:56:44.729: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-bsvv" in namespace "subpath-6642" to be "Succeeded or Failed"
Jan 17 14:56:44.733: INFO: Pod "pod-subpath-test-configmap-bsvv": Phase="Pending", Reason="", readiness=false. Elapsed: 4.259718ms
Jan 17 14:56:46.737: INFO: Pod "pod-subpath-test-configmap-bsvv": Phase="Running", Reason="", readiness=true. Elapsed: 2.00879897s
Jan 17 14:56:48.737: INFO: Pod "pod-subpath-test-configmap-bsvv": Phase="Running", Reason="", readiness=true. Elapsed: 4.008027827s
Jan 17 14:56:50.738: INFO: Pod "pod-subpath-test-configmap-bsvv": Phase="Running", Reason="", readiness=true. Elapsed: 6.008974805s
Jan 17 14:56:52.737: INFO: Pod "pod-subpath-test-configmap-bsvv": Phase="Running", Reason="", readiness=true. Elapsed: 8.008295555s
Jan 17 14:56:54.737: INFO: Pod "pod-subpath-test-configmap-bsvv": Phase="Running", Reason="", readiness=true. Elapsed: 10.008225677s
Jan 17 14:56:56.738: INFO: Pod "pod-subpath-test-configmap-bsvv": Phase="Running", Reason="", readiness=true. Elapsed: 12.009341821s
Jan 17 14:56:58.737: INFO: Pod "pod-subpath-test-configmap-bsvv": Phase="Running", Reason="", readiness=true. Elapsed: 14.008109533s
Jan 17 14:57:00.738: INFO: Pod "pod-subpath-test-configmap-bsvv": Phase="Running", Reason="", readiness=true. Elapsed: 16.009564542s
Jan 17 14:57:02.736: INFO: Pod "pod-subpath-test-configmap-bsvv": Phase="Running", Reason="", readiness=true. Elapsed: 18.007643475s
Jan 17 14:57:04.737: INFO: Pod "pod-subpath-test-configmap-bsvv": Phase="Running", Reason="", readiness=true. Elapsed: 20.008125967s
Jan 17 14:57:06.737: INFO: Pod "pod-subpath-test-configmap-bsvv": Phase="Running", Reason="", readiness=false. Elapsed: 22.008066173s
Jan 17 14:57:08.737: INFO: Pod "pod-subpath-test-configmap-bsvv": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.008038699s
STEP: Saw pod success 01/17/23 14:57:08.737
Jan 17 14:57:08.737: INFO: Pod "pod-subpath-test-configmap-bsvv" satisfied condition "Succeeded or Failed"
Jan 17 14:57:08.740: INFO: Trying to get logs from node ip-10-0-139-213.ec2.internal pod pod-subpath-test-configmap-bsvv container test-container-subpath-configmap-bsvv: <nil>
STEP: delete the pod 01/17/23 14:57:08.746
Jan 17 14:57:08.759: INFO: Waiting for pod pod-subpath-test-configmap-bsvv to disappear
Jan 17 14:57:08.761: INFO: Pod pod-subpath-test-configmap-bsvv no longer exists
STEP: Deleting pod pod-subpath-test-configmap-bsvv 01/17/23 14:57:08.761
Jan 17 14:57:08.761: INFO: Deleting pod "pod-subpath-test-configmap-bsvv" in namespace "subpath-6642"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Jan 17 14:57:08.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6642" for this suite. 01/17/23 14:57:08.767
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]","completed":20,"skipped":364,"failed":0}
------------------------------
• [SLOW TEST] [24.125 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 14:56:44.648
    Jan 17 14:56:44.648: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename subpath 01/17/23 14:56:44.649
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 14:56:44.686
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 14:56:44.689
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/17/23 14:56:44.691
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-bsvv 01/17/23 14:56:44.706
    STEP: Creating a pod to test atomic-volume-subpath 01/17/23 14:56:44.706
    Jan 17 14:56:44.729: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-bsvv" in namespace "subpath-6642" to be "Succeeded or Failed"
    Jan 17 14:56:44.733: INFO: Pod "pod-subpath-test-configmap-bsvv": Phase="Pending", Reason="", readiness=false. Elapsed: 4.259718ms
    Jan 17 14:56:46.737: INFO: Pod "pod-subpath-test-configmap-bsvv": Phase="Running", Reason="", readiness=true. Elapsed: 2.00879897s
    Jan 17 14:56:48.737: INFO: Pod "pod-subpath-test-configmap-bsvv": Phase="Running", Reason="", readiness=true. Elapsed: 4.008027827s
    Jan 17 14:56:50.738: INFO: Pod "pod-subpath-test-configmap-bsvv": Phase="Running", Reason="", readiness=true. Elapsed: 6.008974805s
    Jan 17 14:56:52.737: INFO: Pod "pod-subpath-test-configmap-bsvv": Phase="Running", Reason="", readiness=true. Elapsed: 8.008295555s
    Jan 17 14:56:54.737: INFO: Pod "pod-subpath-test-configmap-bsvv": Phase="Running", Reason="", readiness=true. Elapsed: 10.008225677s
    Jan 17 14:56:56.738: INFO: Pod "pod-subpath-test-configmap-bsvv": Phase="Running", Reason="", readiness=true. Elapsed: 12.009341821s
    Jan 17 14:56:58.737: INFO: Pod "pod-subpath-test-configmap-bsvv": Phase="Running", Reason="", readiness=true. Elapsed: 14.008109533s
    Jan 17 14:57:00.738: INFO: Pod "pod-subpath-test-configmap-bsvv": Phase="Running", Reason="", readiness=true. Elapsed: 16.009564542s
    Jan 17 14:57:02.736: INFO: Pod "pod-subpath-test-configmap-bsvv": Phase="Running", Reason="", readiness=true. Elapsed: 18.007643475s
    Jan 17 14:57:04.737: INFO: Pod "pod-subpath-test-configmap-bsvv": Phase="Running", Reason="", readiness=true. Elapsed: 20.008125967s
    Jan 17 14:57:06.737: INFO: Pod "pod-subpath-test-configmap-bsvv": Phase="Running", Reason="", readiness=false. Elapsed: 22.008066173s
    Jan 17 14:57:08.737: INFO: Pod "pod-subpath-test-configmap-bsvv": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.008038699s
    STEP: Saw pod success 01/17/23 14:57:08.737
    Jan 17 14:57:08.737: INFO: Pod "pod-subpath-test-configmap-bsvv" satisfied condition "Succeeded or Failed"
    Jan 17 14:57:08.740: INFO: Trying to get logs from node ip-10-0-139-213.ec2.internal pod pod-subpath-test-configmap-bsvv container test-container-subpath-configmap-bsvv: <nil>
    STEP: delete the pod 01/17/23 14:57:08.746
    Jan 17 14:57:08.759: INFO: Waiting for pod pod-subpath-test-configmap-bsvv to disappear
    Jan 17 14:57:08.761: INFO: Pod pod-subpath-test-configmap-bsvv no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-bsvv 01/17/23 14:57:08.761
    Jan 17 14:57:08.761: INFO: Deleting pod "pod-subpath-test-configmap-bsvv" in namespace "subpath-6642"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Jan 17 14:57:08.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-6642" for this suite. 01/17/23 14:57:08.767
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 14:57:08.774
Jan 17 14:57:08.774: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename pods 01/17/23 14:57:08.775
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 14:57:08.795
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 14:57:08.797
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225
STEP: creating the pod 01/17/23 14:57:08.799
STEP: setting up watch 01/17/23 14:57:08.799
STEP: submitting the pod to kubernetes 01/17/23 14:57:08.909
STEP: verifying the pod is in kubernetes 01/17/23 14:57:08.931
STEP: verifying pod creation was observed 01/17/23 14:57:08.935
Jan 17 14:57:08.935: INFO: Waiting up to 5m0s for pod "pod-submit-remove-2fcf16dc-1efd-4dc6-87ca-ca1c1c0031b9" in namespace "pods-6188" to be "running"
Jan 17 14:57:08.940: INFO: Pod "pod-submit-remove-2fcf16dc-1efd-4dc6-87ca-ca1c1c0031b9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.936528ms
Jan 17 14:57:10.944: INFO: Pod "pod-submit-remove-2fcf16dc-1efd-4dc6-87ca-ca1c1c0031b9": Phase="Running", Reason="", readiness=true. Elapsed: 2.009080447s
Jan 17 14:57:10.944: INFO: Pod "pod-submit-remove-2fcf16dc-1efd-4dc6-87ca-ca1c1c0031b9" satisfied condition "running"
STEP: deleting the pod gracefully 01/17/23 14:57:10.947
STEP: verifying pod deletion was observed 01/17/23 14:57:10.953
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 17 14:57:13.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6188" for this suite. 01/17/23 14:57:13.543
{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","completed":21,"skipped":369,"failed":0}
------------------------------
• [4.775 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 14:57:08.774
    Jan 17 14:57:08.774: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename pods 01/17/23 14:57:08.775
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 14:57:08.795
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 14:57:08.797
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:225
    STEP: creating the pod 01/17/23 14:57:08.799
    STEP: setting up watch 01/17/23 14:57:08.799
    STEP: submitting the pod to kubernetes 01/17/23 14:57:08.909
    STEP: verifying the pod is in kubernetes 01/17/23 14:57:08.931
    STEP: verifying pod creation was observed 01/17/23 14:57:08.935
    Jan 17 14:57:08.935: INFO: Waiting up to 5m0s for pod "pod-submit-remove-2fcf16dc-1efd-4dc6-87ca-ca1c1c0031b9" in namespace "pods-6188" to be "running"
    Jan 17 14:57:08.940: INFO: Pod "pod-submit-remove-2fcf16dc-1efd-4dc6-87ca-ca1c1c0031b9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.936528ms
    Jan 17 14:57:10.944: INFO: Pod "pod-submit-remove-2fcf16dc-1efd-4dc6-87ca-ca1c1c0031b9": Phase="Running", Reason="", readiness=true. Elapsed: 2.009080447s
    Jan 17 14:57:10.944: INFO: Pod "pod-submit-remove-2fcf16dc-1efd-4dc6-87ca-ca1c1c0031b9" satisfied condition "running"
    STEP: deleting the pod gracefully 01/17/23 14:57:10.947
    STEP: verifying pod deletion was observed 01/17/23 14:57:10.953
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 17 14:57:13.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-6188" for this suite. 01/17/23 14:57:13.543
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 14:57:13.549
Jan 17 14:57:13.549: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename resourcequota 01/17/23 14:57:13.55
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 14:57:13.577
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 14:57:13.579
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316
STEP: Counting existing ResourceQuota 01/17/23 14:57:30.584
STEP: Creating a ResourceQuota 01/17/23 14:57:35.588
STEP: Ensuring resource quota status is calculated 01/17/23 14:57:35.593
STEP: Creating a ConfigMap 01/17/23 14:57:37.597
STEP: Ensuring resource quota status captures configMap creation 01/17/23 14:57:37.608
STEP: Deleting a ConfigMap 01/17/23 14:57:39.612
STEP: Ensuring resource quota status released usage 01/17/23 14:57:39.618
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 17 14:57:41.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8116" for this suite. 01/17/23 14:57:41.626
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","completed":22,"skipped":372,"failed":0}
------------------------------
• [SLOW TEST] [28.083 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 14:57:13.549
    Jan 17 14:57:13.549: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename resourcequota 01/17/23 14:57:13.55
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 14:57:13.577
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 14:57:13.579
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:316
    STEP: Counting existing ResourceQuota 01/17/23 14:57:30.584
    STEP: Creating a ResourceQuota 01/17/23 14:57:35.588
    STEP: Ensuring resource quota status is calculated 01/17/23 14:57:35.593
    STEP: Creating a ConfigMap 01/17/23 14:57:37.597
    STEP: Ensuring resource quota status captures configMap creation 01/17/23 14:57:37.608
    STEP: Deleting a ConfigMap 01/17/23 14:57:39.612
    STEP: Ensuring resource quota status released usage 01/17/23 14:57:39.618
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 17 14:57:41.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-8116" for this suite. 01/17/23 14:57:41.626
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 14:57:41.634
Jan 17 14:57:41.634: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename dns 01/17/23 14:57:41.634
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 14:57:41.661
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 14:57:41.663
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 01/17/23 14:57:41.665
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6385.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6385.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6385.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6385.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6385.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6385.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6385.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6385.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6385.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6385.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6385.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6385.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 140.109.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.109.140_udp@PTR;check="$$(dig +tcp +noall +answer +search 140.109.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.109.140_tcp@PTR;sleep 1; done
 01/17/23 14:57:41.7
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6385.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6385.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6385.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6385.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6385.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6385.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6385.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6385.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6385.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6385.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6385.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6385.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 140.109.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.109.140_udp@PTR;check="$$(dig +tcp +noall +answer +search 140.109.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.109.140_tcp@PTR;sleep 1; done
 01/17/23 14:57:41.7
STEP: creating a pod to probe DNS 01/17/23 14:57:41.7
STEP: submitting the pod to kubernetes 01/17/23 14:57:41.7
Jan 17 14:57:41.759: INFO: Waiting up to 15m0s for pod "dns-test-1a2be099-f9bf-4d90-8960-2b9bd29e4455" in namespace "dns-6385" to be "running"
Jan 17 14:57:41.764: INFO: Pod "dns-test-1a2be099-f9bf-4d90-8960-2b9bd29e4455": Phase="Pending", Reason="", readiness=false. Elapsed: 4.726953ms
Jan 17 14:57:43.773: INFO: Pod "dns-test-1a2be099-f9bf-4d90-8960-2b9bd29e4455": Phase="Running", Reason="", readiness=true. Elapsed: 2.0134423s
Jan 17 14:57:43.773: INFO: Pod "dns-test-1a2be099-f9bf-4d90-8960-2b9bd29e4455" satisfied condition "running"
STEP: retrieving the pod 01/17/23 14:57:43.773
STEP: looking for the results for each expected name from probers 01/17/23 14:57:43.776
Jan 17 14:57:43.784: INFO: Unable to read wheezy_udp@dns-test-service.dns-6385.svc.cluster.local from pod dns-6385/dns-test-1a2be099-f9bf-4d90-8960-2b9bd29e4455: the server could not find the requested resource (get pods dns-test-1a2be099-f9bf-4d90-8960-2b9bd29e4455)
Jan 17 14:57:43.787: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6385.svc.cluster.local from pod dns-6385/dns-test-1a2be099-f9bf-4d90-8960-2b9bd29e4455: the server could not find the requested resource (get pods dns-test-1a2be099-f9bf-4d90-8960-2b9bd29e4455)
Jan 17 14:57:43.790: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6385.svc.cluster.local from pod dns-6385/dns-test-1a2be099-f9bf-4d90-8960-2b9bd29e4455: the server could not find the requested resource (get pods dns-test-1a2be099-f9bf-4d90-8960-2b9bd29e4455)
Jan 17 14:57:43.794: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6385.svc.cluster.local from pod dns-6385/dns-test-1a2be099-f9bf-4d90-8960-2b9bd29e4455: the server could not find the requested resource (get pods dns-test-1a2be099-f9bf-4d90-8960-2b9bd29e4455)
Jan 17 14:57:43.810: INFO: Unable to read jessie_udp@dns-test-service.dns-6385.svc.cluster.local from pod dns-6385/dns-test-1a2be099-f9bf-4d90-8960-2b9bd29e4455: the server could not find the requested resource (get pods dns-test-1a2be099-f9bf-4d90-8960-2b9bd29e4455)
Jan 17 14:57:43.813: INFO: Unable to read jessie_tcp@dns-test-service.dns-6385.svc.cluster.local from pod dns-6385/dns-test-1a2be099-f9bf-4d90-8960-2b9bd29e4455: the server could not find the requested resource (get pods dns-test-1a2be099-f9bf-4d90-8960-2b9bd29e4455)
Jan 17 14:57:43.816: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6385.svc.cluster.local from pod dns-6385/dns-test-1a2be099-f9bf-4d90-8960-2b9bd29e4455: the server could not find the requested resource (get pods dns-test-1a2be099-f9bf-4d90-8960-2b9bd29e4455)
Jan 17 14:57:43.819: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6385.svc.cluster.local from pod dns-6385/dns-test-1a2be099-f9bf-4d90-8960-2b9bd29e4455: the server could not find the requested resource (get pods dns-test-1a2be099-f9bf-4d90-8960-2b9bd29e4455)
Jan 17 14:57:43.831: INFO: Lookups using dns-6385/dns-test-1a2be099-f9bf-4d90-8960-2b9bd29e4455 failed for: [wheezy_udp@dns-test-service.dns-6385.svc.cluster.local wheezy_tcp@dns-test-service.dns-6385.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6385.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6385.svc.cluster.local jessie_udp@dns-test-service.dns-6385.svc.cluster.local jessie_tcp@dns-test-service.dns-6385.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6385.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6385.svc.cluster.local]

Jan 17 14:57:48.893: INFO: DNS probes using dns-6385/dns-test-1a2be099-f9bf-4d90-8960-2b9bd29e4455 succeeded

STEP: deleting the pod 01/17/23 14:57:48.893
STEP: deleting the test service 01/17/23 14:57:48.91
STEP: deleting the test headless service 01/17/23 14:57:48.965
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan 17 14:57:48.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6385" for this suite. 01/17/23 14:57:49.012
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","completed":23,"skipped":407,"failed":0}
------------------------------
• [SLOW TEST] [7.395 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 14:57:41.634
    Jan 17 14:57:41.634: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename dns 01/17/23 14:57:41.634
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 14:57:41.661
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 14:57:41.663
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 01/17/23 14:57:41.665
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6385.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6385.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6385.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6385.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6385.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6385.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6385.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6385.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6385.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6385.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6385.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6385.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 140.109.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.109.140_udp@PTR;check="$$(dig +tcp +noall +answer +search 140.109.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.109.140_tcp@PTR;sleep 1; done
     01/17/23 14:57:41.7
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6385.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6385.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6385.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6385.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6385.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6385.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6385.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6385.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6385.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6385.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6385.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6385.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 140.109.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.109.140_udp@PTR;check="$$(dig +tcp +noall +answer +search 140.109.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.109.140_tcp@PTR;sleep 1; done
     01/17/23 14:57:41.7
    STEP: creating a pod to probe DNS 01/17/23 14:57:41.7
    STEP: submitting the pod to kubernetes 01/17/23 14:57:41.7
    Jan 17 14:57:41.759: INFO: Waiting up to 15m0s for pod "dns-test-1a2be099-f9bf-4d90-8960-2b9bd29e4455" in namespace "dns-6385" to be "running"
    Jan 17 14:57:41.764: INFO: Pod "dns-test-1a2be099-f9bf-4d90-8960-2b9bd29e4455": Phase="Pending", Reason="", readiness=false. Elapsed: 4.726953ms
    Jan 17 14:57:43.773: INFO: Pod "dns-test-1a2be099-f9bf-4d90-8960-2b9bd29e4455": Phase="Running", Reason="", readiness=true. Elapsed: 2.0134423s
    Jan 17 14:57:43.773: INFO: Pod "dns-test-1a2be099-f9bf-4d90-8960-2b9bd29e4455" satisfied condition "running"
    STEP: retrieving the pod 01/17/23 14:57:43.773
    STEP: looking for the results for each expected name from probers 01/17/23 14:57:43.776
    Jan 17 14:57:43.784: INFO: Unable to read wheezy_udp@dns-test-service.dns-6385.svc.cluster.local from pod dns-6385/dns-test-1a2be099-f9bf-4d90-8960-2b9bd29e4455: the server could not find the requested resource (get pods dns-test-1a2be099-f9bf-4d90-8960-2b9bd29e4455)
    Jan 17 14:57:43.787: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6385.svc.cluster.local from pod dns-6385/dns-test-1a2be099-f9bf-4d90-8960-2b9bd29e4455: the server could not find the requested resource (get pods dns-test-1a2be099-f9bf-4d90-8960-2b9bd29e4455)
    Jan 17 14:57:43.790: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6385.svc.cluster.local from pod dns-6385/dns-test-1a2be099-f9bf-4d90-8960-2b9bd29e4455: the server could not find the requested resource (get pods dns-test-1a2be099-f9bf-4d90-8960-2b9bd29e4455)
    Jan 17 14:57:43.794: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6385.svc.cluster.local from pod dns-6385/dns-test-1a2be099-f9bf-4d90-8960-2b9bd29e4455: the server could not find the requested resource (get pods dns-test-1a2be099-f9bf-4d90-8960-2b9bd29e4455)
    Jan 17 14:57:43.810: INFO: Unable to read jessie_udp@dns-test-service.dns-6385.svc.cluster.local from pod dns-6385/dns-test-1a2be099-f9bf-4d90-8960-2b9bd29e4455: the server could not find the requested resource (get pods dns-test-1a2be099-f9bf-4d90-8960-2b9bd29e4455)
    Jan 17 14:57:43.813: INFO: Unable to read jessie_tcp@dns-test-service.dns-6385.svc.cluster.local from pod dns-6385/dns-test-1a2be099-f9bf-4d90-8960-2b9bd29e4455: the server could not find the requested resource (get pods dns-test-1a2be099-f9bf-4d90-8960-2b9bd29e4455)
    Jan 17 14:57:43.816: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6385.svc.cluster.local from pod dns-6385/dns-test-1a2be099-f9bf-4d90-8960-2b9bd29e4455: the server could not find the requested resource (get pods dns-test-1a2be099-f9bf-4d90-8960-2b9bd29e4455)
    Jan 17 14:57:43.819: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6385.svc.cluster.local from pod dns-6385/dns-test-1a2be099-f9bf-4d90-8960-2b9bd29e4455: the server could not find the requested resource (get pods dns-test-1a2be099-f9bf-4d90-8960-2b9bd29e4455)
    Jan 17 14:57:43.831: INFO: Lookups using dns-6385/dns-test-1a2be099-f9bf-4d90-8960-2b9bd29e4455 failed for: [wheezy_udp@dns-test-service.dns-6385.svc.cluster.local wheezy_tcp@dns-test-service.dns-6385.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6385.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6385.svc.cluster.local jessie_udp@dns-test-service.dns-6385.svc.cluster.local jessie_tcp@dns-test-service.dns-6385.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6385.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6385.svc.cluster.local]

    Jan 17 14:57:48.893: INFO: DNS probes using dns-6385/dns-test-1a2be099-f9bf-4d90-8960-2b9bd29e4455 succeeded

    STEP: deleting the pod 01/17/23 14:57:48.893
    STEP: deleting the test service 01/17/23 14:57:48.91
    STEP: deleting the test headless service 01/17/23 14:57:48.965
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan 17 14:57:48.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-6385" for this suite. 01/17/23 14:57:49.012
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 14:57:49.029
Jan 17 14:57:49.029: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename services 01/17/23 14:57:49.03
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 14:57:49.129
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 14:57:49.134
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237
STEP: creating service in namespace services-5740 01/17/23 14:57:49.137
STEP: creating service affinity-nodeport-transition in namespace services-5740 01/17/23 14:57:49.137
STEP: creating replication controller affinity-nodeport-transition in namespace services-5740 01/17/23 14:57:49.193
I0117 14:57:49.202327      22 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-5740, replica count: 3
I0117 14:57:52.253360      22 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 17 14:57:52.266: INFO: Creating new exec pod
Jan 17 14:57:52.282: INFO: Waiting up to 5m0s for pod "execpod-affinity5xmwz" in namespace "services-5740" to be "running"
Jan 17 14:57:52.286: INFO: Pod "execpod-affinity5xmwz": Phase="Pending", Reason="", readiness=false. Elapsed: 4.526041ms
Jan 17 14:57:54.292: INFO: Pod "execpod-affinity5xmwz": Phase="Running", Reason="", readiness=true. Elapsed: 2.010550532s
Jan 17 14:57:54.292: INFO: Pod "execpod-affinity5xmwz" satisfied condition "running"
Jan 17 14:57:55.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-5740 exec execpod-affinity5xmwz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
Jan 17 14:57:56.495: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Jan 17 14:57:56.495: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 17 14:57:56.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-5740 exec execpod-affinity5xmwz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.145.207 80'
Jan 17 14:57:56.604: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.145.207 80\nConnection to 172.30.145.207 80 port [tcp/http] succeeded!\n"
Jan 17 14:57:56.604: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 17 14:57:56.604: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-5740 exec execpod-affinity5xmwz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.151.22 32704'
Jan 17 14:57:57.774: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.151.22 32704\nConnection to 10.0.151.22 32704 port [tcp/*] succeeded!\n"
Jan 17 14:57:57.774: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 17 14:57:57.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-5740 exec execpod-affinity5xmwz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.139.213 32704'
Jan 17 14:57:58.929: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.139.213 32704\nConnection to 10.0.139.213 32704 port [tcp/*] succeeded!\n"
Jan 17 14:57:58.929: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 17 14:57:58.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-5740 exec execpod-affinity5xmwz -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.139.213:32704/ ; done'
Jan 17 14:57:59.129: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32704/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32704/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32704/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32704/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32704/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32704/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32704/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32704/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32704/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32704/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32704/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32704/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32704/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32704/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32704/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32704/\n"
Jan 17 14:57:59.129: INFO: stdout: "\naffinity-nodeport-transition-jkw56\naffinity-nodeport-transition-slhbb\naffinity-nodeport-transition-qvbj5\naffinity-nodeport-transition-qvbj5\naffinity-nodeport-transition-qvbj5\naffinity-nodeport-transition-slhbb\naffinity-nodeport-transition-slhbb\naffinity-nodeport-transition-qvbj5\naffinity-nodeport-transition-slhbb\naffinity-nodeport-transition-slhbb\naffinity-nodeport-transition-jkw56\naffinity-nodeport-transition-qvbj5\naffinity-nodeport-transition-slhbb\naffinity-nodeport-transition-slhbb\naffinity-nodeport-transition-slhbb\naffinity-nodeport-transition-slhbb"
Jan 17 14:57:59.129: INFO: Received response from host: affinity-nodeport-transition-jkw56
Jan 17 14:57:59.129: INFO: Received response from host: affinity-nodeport-transition-slhbb
Jan 17 14:57:59.129: INFO: Received response from host: affinity-nodeport-transition-qvbj5
Jan 17 14:57:59.129: INFO: Received response from host: affinity-nodeport-transition-qvbj5
Jan 17 14:57:59.129: INFO: Received response from host: affinity-nodeport-transition-qvbj5
Jan 17 14:57:59.129: INFO: Received response from host: affinity-nodeport-transition-slhbb
Jan 17 14:57:59.129: INFO: Received response from host: affinity-nodeport-transition-slhbb
Jan 17 14:57:59.129: INFO: Received response from host: affinity-nodeport-transition-qvbj5
Jan 17 14:57:59.129: INFO: Received response from host: affinity-nodeport-transition-slhbb
Jan 17 14:57:59.129: INFO: Received response from host: affinity-nodeport-transition-slhbb
Jan 17 14:57:59.129: INFO: Received response from host: affinity-nodeport-transition-jkw56
Jan 17 14:57:59.129: INFO: Received response from host: affinity-nodeport-transition-qvbj5
Jan 17 14:57:59.129: INFO: Received response from host: affinity-nodeport-transition-slhbb
Jan 17 14:57:59.129: INFO: Received response from host: affinity-nodeport-transition-slhbb
Jan 17 14:57:59.129: INFO: Received response from host: affinity-nodeport-transition-slhbb
Jan 17 14:57:59.129: INFO: Received response from host: affinity-nodeport-transition-slhbb
Jan 17 14:57:59.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-5740 exec execpod-affinity5xmwz -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.139.213:32704/ ; done'
Jan 17 14:57:59.333: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32704/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32704/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32704/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32704/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32704/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32704/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32704/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32704/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32704/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32704/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32704/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32704/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32704/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32704/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32704/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32704/\n"
Jan 17 14:57:59.333: INFO: stdout: "\naffinity-nodeport-transition-slhbb\naffinity-nodeport-transition-slhbb\naffinity-nodeport-transition-slhbb\naffinity-nodeport-transition-slhbb\naffinity-nodeport-transition-slhbb\naffinity-nodeport-transition-slhbb\naffinity-nodeport-transition-slhbb\naffinity-nodeport-transition-slhbb\naffinity-nodeport-transition-slhbb\naffinity-nodeport-transition-slhbb\naffinity-nodeport-transition-slhbb\naffinity-nodeport-transition-slhbb\naffinity-nodeport-transition-slhbb\naffinity-nodeport-transition-slhbb\naffinity-nodeport-transition-slhbb\naffinity-nodeport-transition-slhbb"
Jan 17 14:57:59.333: INFO: Received response from host: affinity-nodeport-transition-slhbb
Jan 17 14:57:59.333: INFO: Received response from host: affinity-nodeport-transition-slhbb
Jan 17 14:57:59.333: INFO: Received response from host: affinity-nodeport-transition-slhbb
Jan 17 14:57:59.333: INFO: Received response from host: affinity-nodeport-transition-slhbb
Jan 17 14:57:59.333: INFO: Received response from host: affinity-nodeport-transition-slhbb
Jan 17 14:57:59.333: INFO: Received response from host: affinity-nodeport-transition-slhbb
Jan 17 14:57:59.333: INFO: Received response from host: affinity-nodeport-transition-slhbb
Jan 17 14:57:59.333: INFO: Received response from host: affinity-nodeport-transition-slhbb
Jan 17 14:57:59.333: INFO: Received response from host: affinity-nodeport-transition-slhbb
Jan 17 14:57:59.333: INFO: Received response from host: affinity-nodeport-transition-slhbb
Jan 17 14:57:59.333: INFO: Received response from host: affinity-nodeport-transition-slhbb
Jan 17 14:57:59.333: INFO: Received response from host: affinity-nodeport-transition-slhbb
Jan 17 14:57:59.333: INFO: Received response from host: affinity-nodeport-transition-slhbb
Jan 17 14:57:59.333: INFO: Received response from host: affinity-nodeport-transition-slhbb
Jan 17 14:57:59.333: INFO: Received response from host: affinity-nodeport-transition-slhbb
Jan 17 14:57:59.333: INFO: Received response from host: affinity-nodeport-transition-slhbb
Jan 17 14:57:59.333: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-5740, will wait for the garbage collector to delete the pods 01/17/23 14:57:59.346
Jan 17 14:57:59.407: INFO: Deleting ReplicationController affinity-nodeport-transition took: 6.532715ms
Jan 17 14:57:59.507: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.852007ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 17 14:58:01.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5740" for this suite. 01/17/23 14:58:01.78
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","completed":24,"skipped":426,"failed":0}
------------------------------
• [SLOW TEST] [12.760 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 14:57:49.029
    Jan 17 14:57:49.029: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename services 01/17/23 14:57:49.03
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 14:57:49.129
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 14:57:49.134
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2237
    STEP: creating service in namespace services-5740 01/17/23 14:57:49.137
    STEP: creating service affinity-nodeport-transition in namespace services-5740 01/17/23 14:57:49.137
    STEP: creating replication controller affinity-nodeport-transition in namespace services-5740 01/17/23 14:57:49.193
    I0117 14:57:49.202327      22 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-5740, replica count: 3
    I0117 14:57:52.253360      22 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 17 14:57:52.266: INFO: Creating new exec pod
    Jan 17 14:57:52.282: INFO: Waiting up to 5m0s for pod "execpod-affinity5xmwz" in namespace "services-5740" to be "running"
    Jan 17 14:57:52.286: INFO: Pod "execpod-affinity5xmwz": Phase="Pending", Reason="", readiness=false. Elapsed: 4.526041ms
    Jan 17 14:57:54.292: INFO: Pod "execpod-affinity5xmwz": Phase="Running", Reason="", readiness=true. Elapsed: 2.010550532s
    Jan 17 14:57:54.292: INFO: Pod "execpod-affinity5xmwz" satisfied condition "running"
    Jan 17 14:57:55.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-5740 exec execpod-affinity5xmwz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
    Jan 17 14:57:56.495: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    Jan 17 14:57:56.495: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 17 14:57:56.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-5740 exec execpod-affinity5xmwz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.145.207 80'
    Jan 17 14:57:56.604: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.145.207 80\nConnection to 172.30.145.207 80 port [tcp/http] succeeded!\n"
    Jan 17 14:57:56.604: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 17 14:57:56.604: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-5740 exec execpod-affinity5xmwz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.151.22 32704'
    Jan 17 14:57:57.774: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.151.22 32704\nConnection to 10.0.151.22 32704 port [tcp/*] succeeded!\n"
    Jan 17 14:57:57.774: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 17 14:57:57.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-5740 exec execpod-affinity5xmwz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.139.213 32704'
    Jan 17 14:57:58.929: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.139.213 32704\nConnection to 10.0.139.213 32704 port [tcp/*] succeeded!\n"
    Jan 17 14:57:58.929: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 17 14:57:58.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-5740 exec execpod-affinity5xmwz -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.139.213:32704/ ; done'
    Jan 17 14:57:59.129: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32704/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32704/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32704/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32704/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32704/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32704/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32704/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32704/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32704/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32704/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32704/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32704/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32704/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32704/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32704/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32704/\n"
    Jan 17 14:57:59.129: INFO: stdout: "\naffinity-nodeport-transition-jkw56\naffinity-nodeport-transition-slhbb\naffinity-nodeport-transition-qvbj5\naffinity-nodeport-transition-qvbj5\naffinity-nodeport-transition-qvbj5\naffinity-nodeport-transition-slhbb\naffinity-nodeport-transition-slhbb\naffinity-nodeport-transition-qvbj5\naffinity-nodeport-transition-slhbb\naffinity-nodeport-transition-slhbb\naffinity-nodeport-transition-jkw56\naffinity-nodeport-transition-qvbj5\naffinity-nodeport-transition-slhbb\naffinity-nodeport-transition-slhbb\naffinity-nodeport-transition-slhbb\naffinity-nodeport-transition-slhbb"
    Jan 17 14:57:59.129: INFO: Received response from host: affinity-nodeport-transition-jkw56
    Jan 17 14:57:59.129: INFO: Received response from host: affinity-nodeport-transition-slhbb
    Jan 17 14:57:59.129: INFO: Received response from host: affinity-nodeport-transition-qvbj5
    Jan 17 14:57:59.129: INFO: Received response from host: affinity-nodeport-transition-qvbj5
    Jan 17 14:57:59.129: INFO: Received response from host: affinity-nodeport-transition-qvbj5
    Jan 17 14:57:59.129: INFO: Received response from host: affinity-nodeport-transition-slhbb
    Jan 17 14:57:59.129: INFO: Received response from host: affinity-nodeport-transition-slhbb
    Jan 17 14:57:59.129: INFO: Received response from host: affinity-nodeport-transition-qvbj5
    Jan 17 14:57:59.129: INFO: Received response from host: affinity-nodeport-transition-slhbb
    Jan 17 14:57:59.129: INFO: Received response from host: affinity-nodeport-transition-slhbb
    Jan 17 14:57:59.129: INFO: Received response from host: affinity-nodeport-transition-jkw56
    Jan 17 14:57:59.129: INFO: Received response from host: affinity-nodeport-transition-qvbj5
    Jan 17 14:57:59.129: INFO: Received response from host: affinity-nodeport-transition-slhbb
    Jan 17 14:57:59.129: INFO: Received response from host: affinity-nodeport-transition-slhbb
    Jan 17 14:57:59.129: INFO: Received response from host: affinity-nodeport-transition-slhbb
    Jan 17 14:57:59.129: INFO: Received response from host: affinity-nodeport-transition-slhbb
    Jan 17 14:57:59.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-5740 exec execpod-affinity5xmwz -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.139.213:32704/ ; done'
    Jan 17 14:57:59.333: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32704/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32704/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32704/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32704/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32704/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32704/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32704/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32704/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32704/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32704/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32704/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32704/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32704/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32704/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32704/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32704/\n"
    Jan 17 14:57:59.333: INFO: stdout: "\naffinity-nodeport-transition-slhbb\naffinity-nodeport-transition-slhbb\naffinity-nodeport-transition-slhbb\naffinity-nodeport-transition-slhbb\naffinity-nodeport-transition-slhbb\naffinity-nodeport-transition-slhbb\naffinity-nodeport-transition-slhbb\naffinity-nodeport-transition-slhbb\naffinity-nodeport-transition-slhbb\naffinity-nodeport-transition-slhbb\naffinity-nodeport-transition-slhbb\naffinity-nodeport-transition-slhbb\naffinity-nodeport-transition-slhbb\naffinity-nodeport-transition-slhbb\naffinity-nodeport-transition-slhbb\naffinity-nodeport-transition-slhbb"
    Jan 17 14:57:59.333: INFO: Received response from host: affinity-nodeport-transition-slhbb
    Jan 17 14:57:59.333: INFO: Received response from host: affinity-nodeport-transition-slhbb
    Jan 17 14:57:59.333: INFO: Received response from host: affinity-nodeport-transition-slhbb
    Jan 17 14:57:59.333: INFO: Received response from host: affinity-nodeport-transition-slhbb
    Jan 17 14:57:59.333: INFO: Received response from host: affinity-nodeport-transition-slhbb
    Jan 17 14:57:59.333: INFO: Received response from host: affinity-nodeport-transition-slhbb
    Jan 17 14:57:59.333: INFO: Received response from host: affinity-nodeport-transition-slhbb
    Jan 17 14:57:59.333: INFO: Received response from host: affinity-nodeport-transition-slhbb
    Jan 17 14:57:59.333: INFO: Received response from host: affinity-nodeport-transition-slhbb
    Jan 17 14:57:59.333: INFO: Received response from host: affinity-nodeport-transition-slhbb
    Jan 17 14:57:59.333: INFO: Received response from host: affinity-nodeport-transition-slhbb
    Jan 17 14:57:59.333: INFO: Received response from host: affinity-nodeport-transition-slhbb
    Jan 17 14:57:59.333: INFO: Received response from host: affinity-nodeport-transition-slhbb
    Jan 17 14:57:59.333: INFO: Received response from host: affinity-nodeport-transition-slhbb
    Jan 17 14:57:59.333: INFO: Received response from host: affinity-nodeport-transition-slhbb
    Jan 17 14:57:59.333: INFO: Received response from host: affinity-nodeport-transition-slhbb
    Jan 17 14:57:59.333: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-5740, will wait for the garbage collector to delete the pods 01/17/23 14:57:59.346
    Jan 17 14:57:59.407: INFO: Deleting ReplicationController affinity-nodeport-transition took: 6.532715ms
    Jan 17 14:57:59.507: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.852007ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 17 14:58:01.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-5740" for this suite. 01/17/23 14:58:01.78
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 14:58:01.789
Jan 17 14:58:01.789: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename sched-pred 01/17/23 14:58:01.79
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 14:58:01.83
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 14:58:01.835
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jan 17 14:58:01.842: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 17 14:58:01.858: INFO: Waiting for terminating namespaces to be deleted...
Jan 17 14:58:01.864: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-139-213.ec2.internal before test
Jan 17 14:58:01.907: INFO: aws-ebs-csi-driver-node-5tmvr from openshift-cluster-csi-drivers started at 2023-01-17 12:55:50 +0000 UTC (3 container statuses recorded)
Jan 17 14:58:01.907: INFO: 	Container csi-driver ready: true, restart count 0
Jan 17 14:58:01.907: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Jan 17 14:58:01.907: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
Jan 17 14:58:01.907: INFO: tuned-jzlnj from openshift-cluster-node-tuning-operator started at 2023-01-17 12:55:50 +0000 UTC (1 container statuses recorded)
Jan 17 14:58:01.907: INFO: 	Container tuned ready: true, restart count 0
Jan 17 14:58:01.907: INFO: downloads-8d695cd69-ltk55 from openshift-console started at 2023-01-17 12:57:39 +0000 UTC (1 container statuses recorded)
Jan 17 14:58:01.907: INFO: 	Container download-server ready: true, restart count 0
Jan 17 14:58:01.907: INFO: dns-default-jgzr9 from openshift-dns started at 2023-01-17 12:56:45 +0000 UTC (2 container statuses recorded)
Jan 17 14:58:01.907: INFO: 	Container dns ready: true, restart count 0
Jan 17 14:58:01.907: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 17 14:58:01.907: INFO: node-resolver-gkxnp from openshift-dns started at 2023-01-17 12:55:50 +0000 UTC (1 container statuses recorded)
Jan 17 14:58:01.907: INFO: 	Container dns-node-resolver ready: true, restart count 0
Jan 17 14:58:01.907: INFO: image-registry-6d685bd45d-mk7wb from openshift-image-registry started at 2023-01-17 12:56:45 +0000 UTC (1 container statuses recorded)
Jan 17 14:58:01.907: INFO: 	Container registry ready: true, restart count 0
Jan 17 14:58:01.907: INFO: node-ca-4l49x from openshift-image-registry started at 2023-01-17 12:55:50 +0000 UTC (1 container statuses recorded)
Jan 17 14:58:01.907: INFO: 	Container node-ca ready: true, restart count 0
Jan 17 14:58:01.907: INFO: ingress-canary-xqzwm from openshift-ingress-canary started at 2023-01-17 12:56:45 +0000 UTC (1 container statuses recorded)
Jan 17 14:58:01.907: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Jan 17 14:58:01.907: INFO: router-default-c95cc587f-9clvn from openshift-ingress started at 2023-01-17 12:56:45 +0000 UTC (1 container statuses recorded)
Jan 17 14:58:01.907: INFO: 	Container router ready: true, restart count 0
Jan 17 14:58:01.907: INFO: machine-config-daemon-6hjqr from openshift-machine-config-operator started at 2023-01-17 12:55:50 +0000 UTC (2 container statuses recorded)
Jan 17 14:58:01.907: INFO: 	Container machine-config-daemon ready: true, restart count 0
Jan 17 14:58:01.907: INFO: 	Container oauth-proxy ready: true, restart count 0
Jan 17 14:58:01.907: INFO: alertmanager-main-0 from openshift-monitoring started at 2023-01-17 12:59:05 +0000 UTC (6 container statuses recorded)
Jan 17 14:58:01.907: INFO: 	Container alertmanager ready: true, restart count 1
Jan 17 14:58:01.907: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jan 17 14:58:01.907: INFO: 	Container config-reloader ready: true, restart count 0
Jan 17 14:58:01.907: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 17 14:58:01.907: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Jan 17 14:58:01.907: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jan 17 14:58:01.907: INFO: node-exporter-lwbc8 from openshift-monitoring started at 2023-01-17 12:56:55 +0000 UTC (2 container statuses recorded)
Jan 17 14:58:01.907: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 17 14:58:01.907: INFO: 	Container node-exporter ready: true, restart count 0
Jan 17 14:58:01.907: INFO: prometheus-adapter-cd9bc68fc-dstxm from openshift-monitoring started at 2023-01-17 12:57:45 +0000 UTC (1 container statuses recorded)
Jan 17 14:58:01.907: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jan 17 14:58:01.907: INFO: prometheus-k8s-0 from openshift-monitoring started at 2023-01-17 12:59:05 +0000 UTC (6 container statuses recorded)
Jan 17 14:58:01.907: INFO: 	Container config-reloader ready: true, restart count 0
Jan 17 14:58:01.907: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 17 14:58:01.907: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Jan 17 14:58:01.907: INFO: 	Container prometheus ready: true, restart count 0
Jan 17 14:58:01.907: INFO: 	Container prometheus-proxy ready: true, restart count 0
Jan 17 14:58:01.907: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jan 17 14:58:01.907: INFO: thanos-querier-5fccfc4877-prbhx from openshift-monitoring started at 2023-01-17 12:57:02 +0000 UTC (6 container statuses recorded)
Jan 17 14:58:01.907: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 17 14:58:01.907: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Jan 17 14:58:01.907: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Jan 17 14:58:01.907: INFO: 	Container oauth-proxy ready: true, restart count 0
Jan 17 14:58:01.907: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jan 17 14:58:01.907: INFO: 	Container thanos-query ready: true, restart count 0
Jan 17 14:58:01.907: INFO: multus-additional-cni-plugins-bpc6w from openshift-multus started at 2023-01-17 12:55:50 +0000 UTC (1 container statuses recorded)
Jan 17 14:58:01.907: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Jan 17 14:58:01.907: INFO: multus-tt5fs from openshift-multus started at 2023-01-17 12:55:50 +0000 UTC (1 container statuses recorded)
Jan 17 14:58:01.907: INFO: 	Container kube-multus ready: true, restart count 0
Jan 17 14:58:01.907: INFO: network-metrics-daemon-22p8j from openshift-multus started at 2023-01-17 12:55:50 +0000 UTC (2 container statuses recorded)
Jan 17 14:58:01.907: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 17 14:58:01.907: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Jan 17 14:58:01.907: INFO: network-check-target-k685r from openshift-network-diagnostics started at 2023-01-17 12:55:50 +0000 UTC (1 container statuses recorded)
Jan 17 14:58:01.907: INFO: 	Container network-check-target-container ready: true, restart count 0
Jan 17 14:58:01.907: INFO: ovnkube-node-7n8jx from openshift-ovn-kubernetes started at 2023-01-17 12:55:50 +0000 UTC (5 container statuses recorded)
Jan 17 14:58:01.907: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 17 14:58:01.907: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 0
Jan 17 14:58:01.907: INFO: 	Container ovn-acl-logging ready: true, restart count 0
Jan 17 14:58:01.907: INFO: 	Container ovn-controller ready: true, restart count 0
Jan 17 14:58:01.907: INFO: 	Container ovnkube-node ready: true, restart count 0
Jan 17 14:58:01.907: INFO: sonobuoy-systemd-logs-daemon-set-329d7d7181d04e86-mhx5d from sonobuoy started at 2023-01-17 14:47:25 +0000 UTC (2 container statuses recorded)
Jan 17 14:58:01.907: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 17 14:58:01.907: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 17 14:58:01.907: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-151-22.ec2.internal before test
Jan 17 14:58:01.955: INFO: aws-ebs-csi-driver-node-mx922 from openshift-cluster-csi-drivers started at 2023-01-17 12:51:50 +0000 UTC (3 container statuses recorded)
Jan 17 14:58:01.955: INFO: 	Container csi-driver ready: true, restart count 0
Jan 17 14:58:01.955: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Jan 17 14:58:01.955: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Jan 17 14:58:01.955: INFO: tuned-9hhcs from openshift-cluster-node-tuning-operator started at 2023-01-17 12:51:50 +0000 UTC (1 container statuses recorded)
Jan 17 14:58:01.955: INFO: 	Container tuned ready: true, restart count 0
Jan 17 14:58:01.955: INFO: downloads-8d695cd69-w4jfq from openshift-console started at 2023-01-17 12:57:39 +0000 UTC (1 container statuses recorded)
Jan 17 14:58:01.955: INFO: 	Container download-server ready: true, restart count 0
Jan 17 14:58:01.955: INFO: dns-default-d977v from openshift-dns started at 2023-01-17 12:52:21 +0000 UTC (2 container statuses recorded)
Jan 17 14:58:01.955: INFO: 	Container dns ready: true, restart count 0
Jan 17 14:58:01.955: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 17 14:58:01.955: INFO: node-resolver-fxksv from openshift-dns started at 2023-01-17 12:51:50 +0000 UTC (1 container statuses recorded)
Jan 17 14:58:01.955: INFO: 	Container dns-node-resolver ready: true, restart count 0
Jan 17 14:58:01.955: INFO: node-ca-tpp6r from openshift-image-registry started at 2023-01-17 12:54:40 +0000 UTC (1 container statuses recorded)
Jan 17 14:58:01.955: INFO: 	Container node-ca ready: true, restart count 0
Jan 17 14:58:01.955: INFO: ingress-canary-d45j9 from openshift-ingress-canary started at 2023-01-17 12:54:09 +0000 UTC (1 container statuses recorded)
Jan 17 14:58:01.955: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Jan 17 14:58:01.955: INFO: router-default-c95cc587f-24fmv from openshift-ingress started at 2023-01-17 12:52:21 +0000 UTC (1 container statuses recorded)
Jan 17 14:58:01.955: INFO: 	Container router ready: true, restart count 1
Jan 17 14:58:01.955: INFO: machine-config-daemon-bgrjb from openshift-machine-config-operator started at 2023-01-17 12:51:50 +0000 UTC (2 container statuses recorded)
Jan 17 14:58:01.955: INFO: 	Container machine-config-daemon ready: true, restart count 0
Jan 17 14:58:01.955: INFO: 	Container oauth-proxy ready: true, restart count 0
Jan 17 14:58:01.955: INFO: alertmanager-main-1 from openshift-monitoring started at 2023-01-17 12:58:32 +0000 UTC (6 container statuses recorded)
Jan 17 14:58:01.955: INFO: 	Container alertmanager ready: true, restart count 1
Jan 17 14:58:01.955: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jan 17 14:58:01.955: INFO: 	Container config-reloader ready: true, restart count 0
Jan 17 14:58:01.955: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 17 14:58:01.955: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Jan 17 14:58:01.955: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jan 17 14:58:01.955: INFO: node-exporter-snggg from openshift-monitoring started at 2023-01-17 12:56:55 +0000 UTC (2 container statuses recorded)
Jan 17 14:58:01.955: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 17 14:58:01.955: INFO: 	Container node-exporter ready: true, restart count 0
Jan 17 14:58:01.955: INFO: prometheus-adapter-cd9bc68fc-fr52q from openshift-monitoring started at 2023-01-17 12:57:45 +0000 UTC (1 container statuses recorded)
Jan 17 14:58:01.955: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jan 17 14:58:01.955: INFO: prometheus-operator-admission-webhook-7d4759d465-skshz from openshift-monitoring started at 2023-01-17 12:52:21 +0000 UTC (1 container statuses recorded)
Jan 17 14:58:01.955: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Jan 17 14:58:01.955: INFO: multus-additional-cni-plugins-grzml from openshift-multus started at 2023-01-17 12:51:50 +0000 UTC (1 container statuses recorded)
Jan 17 14:58:01.955: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Jan 17 14:58:01.955: INFO: multus-qspfd from openshift-multus started at 2023-01-17 12:51:50 +0000 UTC (1 container statuses recorded)
Jan 17 14:58:01.955: INFO: 	Container kube-multus ready: true, restart count 0
Jan 17 14:58:01.955: INFO: network-metrics-daemon-x4zrh from openshift-multus started at 2023-01-17 12:51:50 +0000 UTC (2 container statuses recorded)
Jan 17 14:58:01.955: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 17 14:58:01.955: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Jan 17 14:58:01.955: INFO: network-check-source-746dd6c885-ndrxh from openshift-network-diagnostics started at 2023-01-17 12:52:21 +0000 UTC (1 container statuses recorded)
Jan 17 14:58:01.955: INFO: 	Container check-endpoints ready: true, restart count 0
Jan 17 14:58:01.955: INFO: network-check-target-f794b from openshift-network-diagnostics started at 2023-01-17 12:51:50 +0000 UTC (1 container statuses recorded)
Jan 17 14:58:01.955: INFO: 	Container network-check-target-container ready: true, restart count 0
Jan 17 14:58:01.955: INFO: collect-profiles-27899415-c7474 from openshift-operator-lifecycle-manager started at 2023-01-17 14:15:00 +0000 UTC (1 container statuses recorded)
Jan 17 14:58:01.955: INFO: 	Container collect-profiles ready: false, restart count 0
Jan 17 14:58:01.955: INFO: collect-profiles-27899430-lm84n from openshift-operator-lifecycle-manager started at 2023-01-17 14:30:00 +0000 UTC (1 container statuses recorded)
Jan 17 14:58:01.955: INFO: 	Container collect-profiles ready: false, restart count 0
Jan 17 14:58:01.955: INFO: collect-profiles-27899445-5rxw6 from openshift-operator-lifecycle-manager started at 2023-01-17 14:45:00 +0000 UTC (1 container statuses recorded)
Jan 17 14:58:01.955: INFO: 	Container collect-profiles ready: false, restart count 0
Jan 17 14:58:01.955: INFO: ovnkube-node-bks72 from openshift-ovn-kubernetes started at 2023-01-17 12:51:50 +0000 UTC (5 container statuses recorded)
Jan 17 14:58:01.955: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 17 14:58:01.955: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 0
Jan 17 14:58:01.955: INFO: 	Container ovn-acl-logging ready: true, restart count 0
Jan 17 14:58:01.955: INFO: 	Container ovn-controller ready: true, restart count 0
Jan 17 14:58:01.955: INFO: 	Container ovnkube-node ready: true, restart count 0
Jan 17 14:58:01.955: INFO: sonobuoy from sonobuoy started at 2023-01-17 14:47:22 +0000 UTC (1 container statuses recorded)
Jan 17 14:58:01.955: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 17 14:58:01.955: INFO: sonobuoy-e2e-job-0a9b408aa2f34bad from sonobuoy started at 2023-01-17 14:47:25 +0000 UTC (2 container statuses recorded)
Jan 17 14:58:01.955: INFO: 	Container e2e ready: true, restart count 0
Jan 17 14:58:01.955: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 17 14:58:01.955: INFO: sonobuoy-systemd-logs-daemon-set-329d7d7181d04e86-8knzd from sonobuoy started at 2023-01-17 14:47:25 +0000 UTC (2 container statuses recorded)
Jan 17 14:58:01.955: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 17 14:58:01.955: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 17 14:58:01.955: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-165-14.ec2.internal before test
Jan 17 14:58:01.977: INFO: aws-ebs-csi-driver-node-x9fxp from openshift-cluster-csi-drivers started at 2023-01-17 12:55:53 +0000 UTC (3 container statuses recorded)
Jan 17 14:58:01.977: INFO: 	Container csi-driver ready: true, restart count 0
Jan 17 14:58:01.977: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Jan 17 14:58:01.977: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Jan 17 14:58:01.977: INFO: tuned-cgskr from openshift-cluster-node-tuning-operator started at 2023-01-17 12:55:53 +0000 UTC (1 container statuses recorded)
Jan 17 14:58:01.977: INFO: 	Container tuned ready: true, restart count 0
Jan 17 14:58:01.977: INFO: dns-default-8q4m7 from openshift-dns started at 2023-01-17 12:56:45 +0000 UTC (2 container statuses recorded)
Jan 17 14:58:01.977: INFO: 	Container dns ready: true, restart count 0
Jan 17 14:58:01.977: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 17 14:58:01.977: INFO: node-resolver-dvw44 from openshift-dns started at 2023-01-17 12:55:53 +0000 UTC (1 container statuses recorded)
Jan 17 14:58:01.977: INFO: 	Container dns-node-resolver ready: true, restart count 0
Jan 17 14:58:01.977: INFO: image-registry-6d685bd45d-g2mxt from openshift-image-registry started at 2023-01-17 12:56:45 +0000 UTC (1 container statuses recorded)
Jan 17 14:58:01.977: INFO: 	Container registry ready: true, restart count 0
Jan 17 14:58:01.977: INFO: node-ca-45nfl from openshift-image-registry started at 2023-01-17 12:55:53 +0000 UTC (1 container statuses recorded)
Jan 17 14:58:01.977: INFO: 	Container node-ca ready: true, restart count 0
Jan 17 14:58:01.977: INFO: ingress-canary-q6p6t from openshift-ingress-canary started at 2023-01-17 12:56:45 +0000 UTC (1 container statuses recorded)
Jan 17 14:58:01.977: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Jan 17 14:58:01.977: INFO: machine-config-daemon-v7v4j from openshift-machine-config-operator started at 2023-01-17 12:55:53 +0000 UTC (2 container statuses recorded)
Jan 17 14:58:01.977: INFO: 	Container machine-config-daemon ready: true, restart count 0
Jan 17 14:58:01.977: INFO: 	Container oauth-proxy ready: true, restart count 0
Jan 17 14:58:01.977: INFO: kube-state-metrics-75455b796c-rgbdl from openshift-monitoring started at 2023-01-17 12:56:55 +0000 UTC (3 container statuses recorded)
Jan 17 14:58:01.977: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jan 17 14:58:01.977: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jan 17 14:58:01.977: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jan 17 14:58:01.977: INFO: node-exporter-pdxfc from openshift-monitoring started at 2023-01-17 12:56:55 +0000 UTC (2 container statuses recorded)
Jan 17 14:58:01.977: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 17 14:58:01.977: INFO: 	Container node-exporter ready: true, restart count 0
Jan 17 14:58:01.977: INFO: openshift-state-metrics-5ff95d844f-dl27q from openshift-monitoring started at 2023-01-17 12:56:55 +0000 UTC (3 container statuses recorded)
Jan 17 14:58:01.977: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jan 17 14:58:01.977: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jan 17 14:58:01.977: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Jan 17 14:58:01.977: INFO: prometheus-k8s-1 from openshift-monitoring started at 2023-01-17 12:58:37 +0000 UTC (6 container statuses recorded)
Jan 17 14:58:01.977: INFO: 	Container config-reloader ready: true, restart count 0
Jan 17 14:58:01.977: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 17 14:58:01.977: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Jan 17 14:58:01.977: INFO: 	Container prometheus ready: true, restart count 0
Jan 17 14:58:01.977: INFO: 	Container prometheus-proxy ready: true, restart count 0
Jan 17 14:58:01.977: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jan 17 14:58:01.977: INFO: prometheus-operator-admission-webhook-7d4759d465-bw8tr from openshift-monitoring started at 2023-01-17 12:56:45 +0000 UTC (1 container statuses recorded)
Jan 17 14:58:01.977: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Jan 17 14:58:01.977: INFO: telemeter-client-585b5cf5bf-zlltk from openshift-monitoring started at 2023-01-17 12:57:00 +0000 UTC (3 container statuses recorded)
Jan 17 14:58:01.977: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 17 14:58:01.977: INFO: 	Container reload ready: true, restart count 0
Jan 17 14:58:01.977: INFO: 	Container telemeter-client ready: true, restart count 0
Jan 17 14:58:01.977: INFO: thanos-querier-5fccfc4877-lc5nk from openshift-monitoring started at 2023-01-17 12:57:02 +0000 UTC (6 container statuses recorded)
Jan 17 14:58:01.977: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 17 14:58:01.977: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Jan 17 14:58:01.977: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Jan 17 14:58:01.977: INFO: 	Container oauth-proxy ready: true, restart count 0
Jan 17 14:58:01.977: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jan 17 14:58:01.977: INFO: 	Container thanos-query ready: true, restart count 0
Jan 17 14:58:01.977: INFO: multus-additional-cni-plugins-zg84j from openshift-multus started at 2023-01-17 12:55:53 +0000 UTC (1 container statuses recorded)
Jan 17 14:58:01.977: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Jan 17 14:58:01.977: INFO: multus-jq6qc from openshift-multus started at 2023-01-17 12:55:53 +0000 UTC (1 container statuses recorded)
Jan 17 14:58:01.977: INFO: 	Container kube-multus ready: true, restart count 0
Jan 17 14:58:01.977: INFO: network-metrics-daemon-gngg5 from openshift-multus started at 2023-01-17 12:55:53 +0000 UTC (2 container statuses recorded)
Jan 17 14:58:01.977: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 17 14:58:01.977: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Jan 17 14:58:01.977: INFO: network-check-target-v9g9x from openshift-network-diagnostics started at 2023-01-17 12:55:53 +0000 UTC (1 container statuses recorded)
Jan 17 14:58:01.977: INFO: 	Container network-check-target-container ready: true, restart count 0
Jan 17 14:58:01.977: INFO: ovnkube-node-q54mr from openshift-ovn-kubernetes started at 2023-01-17 12:55:53 +0000 UTC (5 container statuses recorded)
Jan 17 14:58:01.977: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 17 14:58:01.977: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 0
Jan 17 14:58:01.977: INFO: 	Container ovn-acl-logging ready: true, restart count 0
Jan 17 14:58:01.977: INFO: 	Container ovn-controller ready: true, restart count 0
Jan 17 14:58:01.977: INFO: 	Container ovnkube-node ready: true, restart count 0
Jan 17 14:58:01.977: INFO: sonobuoy-systemd-logs-daemon-set-329d7d7181d04e86-qcrqh from sonobuoy started at 2023-01-17 14:47:25 +0000 UTC (2 container statuses recorded)
Jan 17 14:58:01.977: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 17 14:58:01.977: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326
STEP: verifying the node has the label node ip-10-0-139-213.ec2.internal 01/17/23 14:58:02.038
STEP: verifying the node has the label node ip-10-0-151-22.ec2.internal 01/17/23 14:58:02.055
STEP: verifying the node has the label node ip-10-0-165-14.ec2.internal 01/17/23 14:58:02.078
Jan 17 14:58:02.133: INFO: Pod aws-ebs-csi-driver-node-5tmvr requesting resource cpu=30m on Node ip-10-0-139-213.ec2.internal
Jan 17 14:58:02.133: INFO: Pod aws-ebs-csi-driver-node-mx922 requesting resource cpu=30m on Node ip-10-0-151-22.ec2.internal
Jan 17 14:58:02.133: INFO: Pod aws-ebs-csi-driver-node-x9fxp requesting resource cpu=30m on Node ip-10-0-165-14.ec2.internal
Jan 17 14:58:02.133: INFO: Pod tuned-9hhcs requesting resource cpu=10m on Node ip-10-0-151-22.ec2.internal
Jan 17 14:58:02.133: INFO: Pod tuned-cgskr requesting resource cpu=10m on Node ip-10-0-165-14.ec2.internal
Jan 17 14:58:02.133: INFO: Pod tuned-jzlnj requesting resource cpu=10m on Node ip-10-0-139-213.ec2.internal
Jan 17 14:58:02.133: INFO: Pod downloads-8d695cd69-ltk55 requesting resource cpu=10m on Node ip-10-0-139-213.ec2.internal
Jan 17 14:58:02.133: INFO: Pod downloads-8d695cd69-w4jfq requesting resource cpu=10m on Node ip-10-0-151-22.ec2.internal
Jan 17 14:58:02.133: INFO: Pod dns-default-8q4m7 requesting resource cpu=60m on Node ip-10-0-165-14.ec2.internal
Jan 17 14:58:02.133: INFO: Pod dns-default-d977v requesting resource cpu=60m on Node ip-10-0-151-22.ec2.internal
Jan 17 14:58:02.133: INFO: Pod dns-default-jgzr9 requesting resource cpu=60m on Node ip-10-0-139-213.ec2.internal
Jan 17 14:58:02.133: INFO: Pod node-resolver-dvw44 requesting resource cpu=5m on Node ip-10-0-165-14.ec2.internal
Jan 17 14:58:02.133: INFO: Pod node-resolver-fxksv requesting resource cpu=5m on Node ip-10-0-151-22.ec2.internal
Jan 17 14:58:02.133: INFO: Pod node-resolver-gkxnp requesting resource cpu=5m on Node ip-10-0-139-213.ec2.internal
Jan 17 14:58:02.133: INFO: Pod image-registry-6d685bd45d-g2mxt requesting resource cpu=100m on Node ip-10-0-165-14.ec2.internal
Jan 17 14:58:02.133: INFO: Pod image-registry-6d685bd45d-mk7wb requesting resource cpu=100m on Node ip-10-0-139-213.ec2.internal
Jan 17 14:58:02.133: INFO: Pod node-ca-45nfl requesting resource cpu=10m on Node ip-10-0-165-14.ec2.internal
Jan 17 14:58:02.133: INFO: Pod node-ca-4l49x requesting resource cpu=10m on Node ip-10-0-139-213.ec2.internal
Jan 17 14:58:02.133: INFO: Pod node-ca-tpp6r requesting resource cpu=10m on Node ip-10-0-151-22.ec2.internal
Jan 17 14:58:02.133: INFO: Pod ingress-canary-d45j9 requesting resource cpu=10m on Node ip-10-0-151-22.ec2.internal
Jan 17 14:58:02.133: INFO: Pod ingress-canary-q6p6t requesting resource cpu=10m on Node ip-10-0-165-14.ec2.internal
Jan 17 14:58:02.133: INFO: Pod ingress-canary-xqzwm requesting resource cpu=10m on Node ip-10-0-139-213.ec2.internal
Jan 17 14:58:02.133: INFO: Pod router-default-c95cc587f-24fmv requesting resource cpu=100m on Node ip-10-0-151-22.ec2.internal
Jan 17 14:58:02.133: INFO: Pod router-default-c95cc587f-9clvn requesting resource cpu=100m on Node ip-10-0-139-213.ec2.internal
Jan 17 14:58:02.133: INFO: Pod machine-config-daemon-6hjqr requesting resource cpu=40m on Node ip-10-0-139-213.ec2.internal
Jan 17 14:58:02.133: INFO: Pod machine-config-daemon-bgrjb requesting resource cpu=40m on Node ip-10-0-151-22.ec2.internal
Jan 17 14:58:02.133: INFO: Pod machine-config-daemon-v7v4j requesting resource cpu=40m on Node ip-10-0-165-14.ec2.internal
Jan 17 14:58:02.133: INFO: Pod alertmanager-main-0 requesting resource cpu=9m on Node ip-10-0-139-213.ec2.internal
Jan 17 14:58:02.133: INFO: Pod alertmanager-main-1 requesting resource cpu=9m on Node ip-10-0-151-22.ec2.internal
Jan 17 14:58:02.133: INFO: Pod kube-state-metrics-75455b796c-rgbdl requesting resource cpu=4m on Node ip-10-0-165-14.ec2.internal
Jan 17 14:58:02.133: INFO: Pod node-exporter-lwbc8 requesting resource cpu=9m on Node ip-10-0-139-213.ec2.internal
Jan 17 14:58:02.133: INFO: Pod node-exporter-pdxfc requesting resource cpu=9m on Node ip-10-0-165-14.ec2.internal
Jan 17 14:58:02.133: INFO: Pod node-exporter-snggg requesting resource cpu=9m on Node ip-10-0-151-22.ec2.internal
Jan 17 14:58:02.133: INFO: Pod openshift-state-metrics-5ff95d844f-dl27q requesting resource cpu=3m on Node ip-10-0-165-14.ec2.internal
Jan 17 14:58:02.133: INFO: Pod prometheus-adapter-cd9bc68fc-dstxm requesting resource cpu=1m on Node ip-10-0-139-213.ec2.internal
Jan 17 14:58:02.133: INFO: Pod prometheus-adapter-cd9bc68fc-fr52q requesting resource cpu=1m on Node ip-10-0-151-22.ec2.internal
Jan 17 14:58:02.133: INFO: Pod prometheus-k8s-0 requesting resource cpu=75m on Node ip-10-0-139-213.ec2.internal
Jan 17 14:58:02.133: INFO: Pod prometheus-k8s-1 requesting resource cpu=75m on Node ip-10-0-165-14.ec2.internal
Jan 17 14:58:02.133: INFO: Pod prometheus-operator-admission-webhook-7d4759d465-bw8tr requesting resource cpu=5m on Node ip-10-0-165-14.ec2.internal
Jan 17 14:58:02.133: INFO: Pod prometheus-operator-admission-webhook-7d4759d465-skshz requesting resource cpu=5m on Node ip-10-0-151-22.ec2.internal
Jan 17 14:58:02.133: INFO: Pod telemeter-client-585b5cf5bf-zlltk requesting resource cpu=3m on Node ip-10-0-165-14.ec2.internal
Jan 17 14:58:02.133: INFO: Pod thanos-querier-5fccfc4877-lc5nk requesting resource cpu=15m on Node ip-10-0-165-14.ec2.internal
Jan 17 14:58:02.133: INFO: Pod thanos-querier-5fccfc4877-prbhx requesting resource cpu=15m on Node ip-10-0-139-213.ec2.internal
Jan 17 14:58:02.133: INFO: Pod multus-additional-cni-plugins-bpc6w requesting resource cpu=10m on Node ip-10-0-139-213.ec2.internal
Jan 17 14:58:02.133: INFO: Pod multus-additional-cni-plugins-grzml requesting resource cpu=10m on Node ip-10-0-151-22.ec2.internal
Jan 17 14:58:02.133: INFO: Pod multus-additional-cni-plugins-zg84j requesting resource cpu=10m on Node ip-10-0-165-14.ec2.internal
Jan 17 14:58:02.133: INFO: Pod multus-jq6qc requesting resource cpu=10m on Node ip-10-0-165-14.ec2.internal
Jan 17 14:58:02.133: INFO: Pod multus-qspfd requesting resource cpu=10m on Node ip-10-0-151-22.ec2.internal
Jan 17 14:58:02.133: INFO: Pod multus-tt5fs requesting resource cpu=10m on Node ip-10-0-139-213.ec2.internal
Jan 17 14:58:02.133: INFO: Pod network-metrics-daemon-22p8j requesting resource cpu=20m on Node ip-10-0-139-213.ec2.internal
Jan 17 14:58:02.133: INFO: Pod network-metrics-daemon-gngg5 requesting resource cpu=20m on Node ip-10-0-165-14.ec2.internal
Jan 17 14:58:02.133: INFO: Pod network-metrics-daemon-x4zrh requesting resource cpu=20m on Node ip-10-0-151-22.ec2.internal
Jan 17 14:58:02.133: INFO: Pod network-check-source-746dd6c885-ndrxh requesting resource cpu=10m on Node ip-10-0-151-22.ec2.internal
Jan 17 14:58:02.133: INFO: Pod network-check-target-f794b requesting resource cpu=10m on Node ip-10-0-151-22.ec2.internal
Jan 17 14:58:02.133: INFO: Pod network-check-target-k685r requesting resource cpu=10m on Node ip-10-0-139-213.ec2.internal
Jan 17 14:58:02.133: INFO: Pod network-check-target-v9g9x requesting resource cpu=10m on Node ip-10-0-165-14.ec2.internal
Jan 17 14:58:02.133: INFO: Pod ovnkube-node-7n8jx requesting resource cpu=50m on Node ip-10-0-139-213.ec2.internal
Jan 17 14:58:02.133: INFO: Pod ovnkube-node-bks72 requesting resource cpu=50m on Node ip-10-0-151-22.ec2.internal
Jan 17 14:58:02.133: INFO: Pod ovnkube-node-q54mr requesting resource cpu=50m on Node ip-10-0-165-14.ec2.internal
Jan 17 14:58:02.133: INFO: Pod sonobuoy requesting resource cpu=0m on Node ip-10-0-151-22.ec2.internal
Jan 17 14:58:02.133: INFO: Pod sonobuoy-e2e-job-0a9b408aa2f34bad requesting resource cpu=0m on Node ip-10-0-151-22.ec2.internal
Jan 17 14:58:02.133: INFO: Pod sonobuoy-systemd-logs-daemon-set-329d7d7181d04e86-8knzd requesting resource cpu=0m on Node ip-10-0-151-22.ec2.internal
Jan 17 14:58:02.133: INFO: Pod sonobuoy-systemd-logs-daemon-set-329d7d7181d04e86-mhx5d requesting resource cpu=0m on Node ip-10-0-139-213.ec2.internal
Jan 17 14:58:02.133: INFO: Pod sonobuoy-systemd-logs-daemon-set-329d7d7181d04e86-qcrqh requesting resource cpu=0m on Node ip-10-0-165-14.ec2.internal
STEP: Starting Pods to consume most of the cluster CPU. 01/17/23 14:58:02.133
Jan 17 14:58:02.133: INFO: Creating a pod which consumes cpu=2041m on Node ip-10-0-139-213.ec2.internal
Jan 17 14:58:02.151: INFO: Creating a pod which consumes cpu=2163m on Node ip-10-0-151-22.ec2.internal
Jan 17 14:58:02.165: INFO: Creating a pod which consumes cpu=2114m on Node ip-10-0-165-14.ec2.internal
Jan 17 14:58:02.181: INFO: Waiting up to 5m0s for pod "filler-pod-716a485a-8cfa-42db-9f7c-8ac4af000258" in namespace "sched-pred-2480" to be "running"
Jan 17 14:58:02.185: INFO: Pod "filler-pod-716a485a-8cfa-42db-9f7c-8ac4af000258": Phase="Pending", Reason="", readiness=false. Elapsed: 4.251633ms
Jan 17 14:58:04.189: INFO: Pod "filler-pod-716a485a-8cfa-42db-9f7c-8ac4af000258": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008346823s
Jan 17 14:58:06.191: INFO: Pod "filler-pod-716a485a-8cfa-42db-9f7c-8ac4af000258": Phase="Running", Reason="", readiness=true. Elapsed: 4.009990167s
Jan 17 14:58:06.191: INFO: Pod "filler-pod-716a485a-8cfa-42db-9f7c-8ac4af000258" satisfied condition "running"
Jan 17 14:58:06.191: INFO: Waiting up to 5m0s for pod "filler-pod-0f6ce53f-24e0-4fd8-aeab-b421701942fe" in namespace "sched-pred-2480" to be "running"
Jan 17 14:58:06.194: INFO: Pod "filler-pod-0f6ce53f-24e0-4fd8-aeab-b421701942fe": Phase="Running", Reason="", readiness=true. Elapsed: 2.951116ms
Jan 17 14:58:06.194: INFO: Pod "filler-pod-0f6ce53f-24e0-4fd8-aeab-b421701942fe" satisfied condition "running"
Jan 17 14:58:06.194: INFO: Waiting up to 5m0s for pod "filler-pod-f49ca2ed-bdc7-496e-8f20-3ac530582756" in namespace "sched-pred-2480" to be "running"
Jan 17 14:58:06.197: INFO: Pod "filler-pod-f49ca2ed-bdc7-496e-8f20-3ac530582756": Phase="Running", Reason="", readiness=true. Elapsed: 2.562587ms
Jan 17 14:58:06.197: INFO: Pod "filler-pod-f49ca2ed-bdc7-496e-8f20-3ac530582756" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 01/17/23 14:58:06.197
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0f6ce53f-24e0-4fd8-aeab-b421701942fe.173b208e545cf704], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2480/filler-pod-0f6ce53f-24e0-4fd8-aeab-b421701942fe to ip-10-0-151-22.ec2.internal] 01/17/23 14:58:06.199
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0f6ce53f-24e0-4fd8-aeab-b421701942fe.173b208e7382a5dd], Reason = [AddedInterface], Message = [Add eth0 [10.131.0.39/23] from ovn-kubernetes] 01/17/23 14:58:06.2
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0f6ce53f-24e0-4fd8-aeab-b421701942fe.173b208e74e5fb39], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 01/17/23 14:58:06.2
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0f6ce53f-24e0-4fd8-aeab-b421701942fe.173b208e7cc51ea6], Reason = [Created], Message = [Created container filler-pod-0f6ce53f-24e0-4fd8-aeab-b421701942fe] 01/17/23 14:58:06.2
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0f6ce53f-24e0-4fd8-aeab-b421701942fe.173b208e7e7227bd], Reason = [Started], Message = [Started container filler-pod-0f6ce53f-24e0-4fd8-aeab-b421701942fe] 01/17/23 14:58:06.2
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-716a485a-8cfa-42db-9f7c-8ac4af000258.173b208e53c53fb1], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2480/filler-pod-716a485a-8cfa-42db-9f7c-8ac4af000258 to ip-10-0-139-213.ec2.internal] 01/17/23 14:58:06.2
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-716a485a-8cfa-42db-9f7c-8ac4af000258.173b208e84cbd091], Reason = [AddedInterface], Message = [Add eth0 [10.129.2.23/23] from ovn-kubernetes] 01/17/23 14:58:06.2
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-716a485a-8cfa-42db-9f7c-8ac4af000258.173b208e85f43744], Reason = [Pulling], Message = [Pulling image "registry.k8s.io/pause:3.8"] 01/17/23 14:58:06.2
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-716a485a-8cfa-42db-9f7c-8ac4af000258.173b208eaa26a8d5], Reason = [Pulled], Message = [Successfully pulled image "registry.k8s.io/pause:3.8" in 607.258136ms] 01/17/23 14:58:06.2
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-716a485a-8cfa-42db-9f7c-8ac4af000258.173b208eb292a335], Reason = [Created], Message = [Created container filler-pod-716a485a-8cfa-42db-9f7c-8ac4af000258] 01/17/23 14:58:06.2
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-716a485a-8cfa-42db-9f7c-8ac4af000258.173b208eb44a9dcf], Reason = [Started], Message = [Started container filler-pod-716a485a-8cfa-42db-9f7c-8ac4af000258] 01/17/23 14:58:06.2
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f49ca2ed-bdc7-496e-8f20-3ac530582756.173b208e557c20b7], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2480/filler-pod-f49ca2ed-bdc7-496e-8f20-3ac530582756 to ip-10-0-165-14.ec2.internal] 01/17/23 14:58:06.2
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f49ca2ed-bdc7-496e-8f20-3ac530582756.173b208e736fa9e4], Reason = [AddedInterface], Message = [Add eth0 [10.128.2.24/23] from ovn-kubernetes] 01/17/23 14:58:06.2
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f49ca2ed-bdc7-496e-8f20-3ac530582756.173b208e74abc213], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 01/17/23 14:58:06.2
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f49ca2ed-bdc7-496e-8f20-3ac530582756.173b208e7c90c618], Reason = [Created], Message = [Created container filler-pod-f49ca2ed-bdc7-496e-8f20-3ac530582756] 01/17/23 14:58:06.2
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f49ca2ed-bdc7-496e-8f20-3ac530582756.173b208e7d721c16], Reason = [Started], Message = [Started container filler-pod-f49ca2ed-bdc7-496e-8f20-3ac530582756] 01/17/23 14:58:06.2
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.173b208f4556f81e], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 6 Insufficient cpu. preemption: 0/6 nodes are available: 3 No preemption victims found for incoming pod, 3 Preemption is not helpful for scheduling.] 01/17/23 14:58:06.22
STEP: removing the label node off the node ip-10-0-139-213.ec2.internal 01/17/23 14:58:07.218
STEP: verifying the node doesn't have the label node 01/17/23 14:58:07.229
STEP: removing the label node off the node ip-10-0-151-22.ec2.internal 01/17/23 14:58:07.24
STEP: verifying the node doesn't have the label node 01/17/23 14:58:07.265
STEP: removing the label node off the node ip-10-0-165-14.ec2.internal 01/17/23 14:58:07.268
STEP: verifying the node doesn't have the label node 01/17/23 14:58:07.286
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Jan 17 14:58:07.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2480" for this suite. 01/17/23 14:58:07.299
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","completed":25,"skipped":445,"failed":0}
------------------------------
• [SLOW TEST] [5.521 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 14:58:01.789
    Jan 17 14:58:01.789: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename sched-pred 01/17/23 14:58:01.79
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 14:58:01.83
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 14:58:01.835
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Jan 17 14:58:01.842: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan 17 14:58:01.858: INFO: Waiting for terminating namespaces to be deleted...
    Jan 17 14:58:01.864: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-139-213.ec2.internal before test
    Jan 17 14:58:01.907: INFO: aws-ebs-csi-driver-node-5tmvr from openshift-cluster-csi-drivers started at 2023-01-17 12:55:50 +0000 UTC (3 container statuses recorded)
    Jan 17 14:58:01.907: INFO: 	Container csi-driver ready: true, restart count 0
    Jan 17 14:58:01.907: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    Jan 17 14:58:01.907: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
    Jan 17 14:58:01.907: INFO: tuned-jzlnj from openshift-cluster-node-tuning-operator started at 2023-01-17 12:55:50 +0000 UTC (1 container statuses recorded)
    Jan 17 14:58:01.907: INFO: 	Container tuned ready: true, restart count 0
    Jan 17 14:58:01.907: INFO: downloads-8d695cd69-ltk55 from openshift-console started at 2023-01-17 12:57:39 +0000 UTC (1 container statuses recorded)
    Jan 17 14:58:01.907: INFO: 	Container download-server ready: true, restart count 0
    Jan 17 14:58:01.907: INFO: dns-default-jgzr9 from openshift-dns started at 2023-01-17 12:56:45 +0000 UTC (2 container statuses recorded)
    Jan 17 14:58:01.907: INFO: 	Container dns ready: true, restart count 0
    Jan 17 14:58:01.907: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 17 14:58:01.907: INFO: node-resolver-gkxnp from openshift-dns started at 2023-01-17 12:55:50 +0000 UTC (1 container statuses recorded)
    Jan 17 14:58:01.907: INFO: 	Container dns-node-resolver ready: true, restart count 0
    Jan 17 14:58:01.907: INFO: image-registry-6d685bd45d-mk7wb from openshift-image-registry started at 2023-01-17 12:56:45 +0000 UTC (1 container statuses recorded)
    Jan 17 14:58:01.907: INFO: 	Container registry ready: true, restart count 0
    Jan 17 14:58:01.907: INFO: node-ca-4l49x from openshift-image-registry started at 2023-01-17 12:55:50 +0000 UTC (1 container statuses recorded)
    Jan 17 14:58:01.907: INFO: 	Container node-ca ready: true, restart count 0
    Jan 17 14:58:01.907: INFO: ingress-canary-xqzwm from openshift-ingress-canary started at 2023-01-17 12:56:45 +0000 UTC (1 container statuses recorded)
    Jan 17 14:58:01.907: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    Jan 17 14:58:01.907: INFO: router-default-c95cc587f-9clvn from openshift-ingress started at 2023-01-17 12:56:45 +0000 UTC (1 container statuses recorded)
    Jan 17 14:58:01.907: INFO: 	Container router ready: true, restart count 0
    Jan 17 14:58:01.907: INFO: machine-config-daemon-6hjqr from openshift-machine-config-operator started at 2023-01-17 12:55:50 +0000 UTC (2 container statuses recorded)
    Jan 17 14:58:01.907: INFO: 	Container machine-config-daemon ready: true, restart count 0
    Jan 17 14:58:01.907: INFO: 	Container oauth-proxy ready: true, restart count 0
    Jan 17 14:58:01.907: INFO: alertmanager-main-0 from openshift-monitoring started at 2023-01-17 12:59:05 +0000 UTC (6 container statuses recorded)
    Jan 17 14:58:01.907: INFO: 	Container alertmanager ready: true, restart count 1
    Jan 17 14:58:01.907: INFO: 	Container alertmanager-proxy ready: true, restart count 0
    Jan 17 14:58:01.907: INFO: 	Container config-reloader ready: true, restart count 0
    Jan 17 14:58:01.907: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 17 14:58:01.907: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
    Jan 17 14:58:01.907: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Jan 17 14:58:01.907: INFO: node-exporter-lwbc8 from openshift-monitoring started at 2023-01-17 12:56:55 +0000 UTC (2 container statuses recorded)
    Jan 17 14:58:01.907: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 17 14:58:01.907: INFO: 	Container node-exporter ready: true, restart count 0
    Jan 17 14:58:01.907: INFO: prometheus-adapter-cd9bc68fc-dstxm from openshift-monitoring started at 2023-01-17 12:57:45 +0000 UTC (1 container statuses recorded)
    Jan 17 14:58:01.907: INFO: 	Container prometheus-adapter ready: true, restart count 0
    Jan 17 14:58:01.907: INFO: prometheus-k8s-0 from openshift-monitoring started at 2023-01-17 12:59:05 +0000 UTC (6 container statuses recorded)
    Jan 17 14:58:01.907: INFO: 	Container config-reloader ready: true, restart count 0
    Jan 17 14:58:01.907: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 17 14:58:01.907: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
    Jan 17 14:58:01.907: INFO: 	Container prometheus ready: true, restart count 0
    Jan 17 14:58:01.907: INFO: 	Container prometheus-proxy ready: true, restart count 0
    Jan 17 14:58:01.907: INFO: 	Container thanos-sidecar ready: true, restart count 0
    Jan 17 14:58:01.907: INFO: thanos-querier-5fccfc4877-prbhx from openshift-monitoring started at 2023-01-17 12:57:02 +0000 UTC (6 container statuses recorded)
    Jan 17 14:58:01.907: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 17 14:58:01.907: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
    Jan 17 14:58:01.907: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
    Jan 17 14:58:01.907: INFO: 	Container oauth-proxy ready: true, restart count 0
    Jan 17 14:58:01.907: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Jan 17 14:58:01.907: INFO: 	Container thanos-query ready: true, restart count 0
    Jan 17 14:58:01.907: INFO: multus-additional-cni-plugins-bpc6w from openshift-multus started at 2023-01-17 12:55:50 +0000 UTC (1 container statuses recorded)
    Jan 17 14:58:01.907: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    Jan 17 14:58:01.907: INFO: multus-tt5fs from openshift-multus started at 2023-01-17 12:55:50 +0000 UTC (1 container statuses recorded)
    Jan 17 14:58:01.907: INFO: 	Container kube-multus ready: true, restart count 0
    Jan 17 14:58:01.907: INFO: network-metrics-daemon-22p8j from openshift-multus started at 2023-01-17 12:55:50 +0000 UTC (2 container statuses recorded)
    Jan 17 14:58:01.907: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 17 14:58:01.907: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    Jan 17 14:58:01.907: INFO: network-check-target-k685r from openshift-network-diagnostics started at 2023-01-17 12:55:50 +0000 UTC (1 container statuses recorded)
    Jan 17 14:58:01.907: INFO: 	Container network-check-target-container ready: true, restart count 0
    Jan 17 14:58:01.907: INFO: ovnkube-node-7n8jx from openshift-ovn-kubernetes started at 2023-01-17 12:55:50 +0000 UTC (5 container statuses recorded)
    Jan 17 14:58:01.907: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 17 14:58:01.907: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 0
    Jan 17 14:58:01.907: INFO: 	Container ovn-acl-logging ready: true, restart count 0
    Jan 17 14:58:01.907: INFO: 	Container ovn-controller ready: true, restart count 0
    Jan 17 14:58:01.907: INFO: 	Container ovnkube-node ready: true, restart count 0
    Jan 17 14:58:01.907: INFO: sonobuoy-systemd-logs-daemon-set-329d7d7181d04e86-mhx5d from sonobuoy started at 2023-01-17 14:47:25 +0000 UTC (2 container statuses recorded)
    Jan 17 14:58:01.907: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 17 14:58:01.907: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 17 14:58:01.907: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-151-22.ec2.internal before test
    Jan 17 14:58:01.955: INFO: aws-ebs-csi-driver-node-mx922 from openshift-cluster-csi-drivers started at 2023-01-17 12:51:50 +0000 UTC (3 container statuses recorded)
    Jan 17 14:58:01.955: INFO: 	Container csi-driver ready: true, restart count 0
    Jan 17 14:58:01.955: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    Jan 17 14:58:01.955: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Jan 17 14:58:01.955: INFO: tuned-9hhcs from openshift-cluster-node-tuning-operator started at 2023-01-17 12:51:50 +0000 UTC (1 container statuses recorded)
    Jan 17 14:58:01.955: INFO: 	Container tuned ready: true, restart count 0
    Jan 17 14:58:01.955: INFO: downloads-8d695cd69-w4jfq from openshift-console started at 2023-01-17 12:57:39 +0000 UTC (1 container statuses recorded)
    Jan 17 14:58:01.955: INFO: 	Container download-server ready: true, restart count 0
    Jan 17 14:58:01.955: INFO: dns-default-d977v from openshift-dns started at 2023-01-17 12:52:21 +0000 UTC (2 container statuses recorded)
    Jan 17 14:58:01.955: INFO: 	Container dns ready: true, restart count 0
    Jan 17 14:58:01.955: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 17 14:58:01.955: INFO: node-resolver-fxksv from openshift-dns started at 2023-01-17 12:51:50 +0000 UTC (1 container statuses recorded)
    Jan 17 14:58:01.955: INFO: 	Container dns-node-resolver ready: true, restart count 0
    Jan 17 14:58:01.955: INFO: node-ca-tpp6r from openshift-image-registry started at 2023-01-17 12:54:40 +0000 UTC (1 container statuses recorded)
    Jan 17 14:58:01.955: INFO: 	Container node-ca ready: true, restart count 0
    Jan 17 14:58:01.955: INFO: ingress-canary-d45j9 from openshift-ingress-canary started at 2023-01-17 12:54:09 +0000 UTC (1 container statuses recorded)
    Jan 17 14:58:01.955: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    Jan 17 14:58:01.955: INFO: router-default-c95cc587f-24fmv from openshift-ingress started at 2023-01-17 12:52:21 +0000 UTC (1 container statuses recorded)
    Jan 17 14:58:01.955: INFO: 	Container router ready: true, restart count 1
    Jan 17 14:58:01.955: INFO: machine-config-daemon-bgrjb from openshift-machine-config-operator started at 2023-01-17 12:51:50 +0000 UTC (2 container statuses recorded)
    Jan 17 14:58:01.955: INFO: 	Container machine-config-daemon ready: true, restart count 0
    Jan 17 14:58:01.955: INFO: 	Container oauth-proxy ready: true, restart count 0
    Jan 17 14:58:01.955: INFO: alertmanager-main-1 from openshift-monitoring started at 2023-01-17 12:58:32 +0000 UTC (6 container statuses recorded)
    Jan 17 14:58:01.955: INFO: 	Container alertmanager ready: true, restart count 1
    Jan 17 14:58:01.955: INFO: 	Container alertmanager-proxy ready: true, restart count 0
    Jan 17 14:58:01.955: INFO: 	Container config-reloader ready: true, restart count 0
    Jan 17 14:58:01.955: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 17 14:58:01.955: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
    Jan 17 14:58:01.955: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Jan 17 14:58:01.955: INFO: node-exporter-snggg from openshift-monitoring started at 2023-01-17 12:56:55 +0000 UTC (2 container statuses recorded)
    Jan 17 14:58:01.955: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 17 14:58:01.955: INFO: 	Container node-exporter ready: true, restart count 0
    Jan 17 14:58:01.955: INFO: prometheus-adapter-cd9bc68fc-fr52q from openshift-monitoring started at 2023-01-17 12:57:45 +0000 UTC (1 container statuses recorded)
    Jan 17 14:58:01.955: INFO: 	Container prometheus-adapter ready: true, restart count 0
    Jan 17 14:58:01.955: INFO: prometheus-operator-admission-webhook-7d4759d465-skshz from openshift-monitoring started at 2023-01-17 12:52:21 +0000 UTC (1 container statuses recorded)
    Jan 17 14:58:01.955: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
    Jan 17 14:58:01.955: INFO: multus-additional-cni-plugins-grzml from openshift-multus started at 2023-01-17 12:51:50 +0000 UTC (1 container statuses recorded)
    Jan 17 14:58:01.955: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    Jan 17 14:58:01.955: INFO: multus-qspfd from openshift-multus started at 2023-01-17 12:51:50 +0000 UTC (1 container statuses recorded)
    Jan 17 14:58:01.955: INFO: 	Container kube-multus ready: true, restart count 0
    Jan 17 14:58:01.955: INFO: network-metrics-daemon-x4zrh from openshift-multus started at 2023-01-17 12:51:50 +0000 UTC (2 container statuses recorded)
    Jan 17 14:58:01.955: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 17 14:58:01.955: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    Jan 17 14:58:01.955: INFO: network-check-source-746dd6c885-ndrxh from openshift-network-diagnostics started at 2023-01-17 12:52:21 +0000 UTC (1 container statuses recorded)
    Jan 17 14:58:01.955: INFO: 	Container check-endpoints ready: true, restart count 0
    Jan 17 14:58:01.955: INFO: network-check-target-f794b from openshift-network-diagnostics started at 2023-01-17 12:51:50 +0000 UTC (1 container statuses recorded)
    Jan 17 14:58:01.955: INFO: 	Container network-check-target-container ready: true, restart count 0
    Jan 17 14:58:01.955: INFO: collect-profiles-27899415-c7474 from openshift-operator-lifecycle-manager started at 2023-01-17 14:15:00 +0000 UTC (1 container statuses recorded)
    Jan 17 14:58:01.955: INFO: 	Container collect-profiles ready: false, restart count 0
    Jan 17 14:58:01.955: INFO: collect-profiles-27899430-lm84n from openshift-operator-lifecycle-manager started at 2023-01-17 14:30:00 +0000 UTC (1 container statuses recorded)
    Jan 17 14:58:01.955: INFO: 	Container collect-profiles ready: false, restart count 0
    Jan 17 14:58:01.955: INFO: collect-profiles-27899445-5rxw6 from openshift-operator-lifecycle-manager started at 2023-01-17 14:45:00 +0000 UTC (1 container statuses recorded)
    Jan 17 14:58:01.955: INFO: 	Container collect-profiles ready: false, restart count 0
    Jan 17 14:58:01.955: INFO: ovnkube-node-bks72 from openshift-ovn-kubernetes started at 2023-01-17 12:51:50 +0000 UTC (5 container statuses recorded)
    Jan 17 14:58:01.955: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 17 14:58:01.955: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 0
    Jan 17 14:58:01.955: INFO: 	Container ovn-acl-logging ready: true, restart count 0
    Jan 17 14:58:01.955: INFO: 	Container ovn-controller ready: true, restart count 0
    Jan 17 14:58:01.955: INFO: 	Container ovnkube-node ready: true, restart count 0
    Jan 17 14:58:01.955: INFO: sonobuoy from sonobuoy started at 2023-01-17 14:47:22 +0000 UTC (1 container statuses recorded)
    Jan 17 14:58:01.955: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan 17 14:58:01.955: INFO: sonobuoy-e2e-job-0a9b408aa2f34bad from sonobuoy started at 2023-01-17 14:47:25 +0000 UTC (2 container statuses recorded)
    Jan 17 14:58:01.955: INFO: 	Container e2e ready: true, restart count 0
    Jan 17 14:58:01.955: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 17 14:58:01.955: INFO: sonobuoy-systemd-logs-daemon-set-329d7d7181d04e86-8knzd from sonobuoy started at 2023-01-17 14:47:25 +0000 UTC (2 container statuses recorded)
    Jan 17 14:58:01.955: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 17 14:58:01.955: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 17 14:58:01.955: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-165-14.ec2.internal before test
    Jan 17 14:58:01.977: INFO: aws-ebs-csi-driver-node-x9fxp from openshift-cluster-csi-drivers started at 2023-01-17 12:55:53 +0000 UTC (3 container statuses recorded)
    Jan 17 14:58:01.977: INFO: 	Container csi-driver ready: true, restart count 0
    Jan 17 14:58:01.977: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    Jan 17 14:58:01.977: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Jan 17 14:58:01.977: INFO: tuned-cgskr from openshift-cluster-node-tuning-operator started at 2023-01-17 12:55:53 +0000 UTC (1 container statuses recorded)
    Jan 17 14:58:01.977: INFO: 	Container tuned ready: true, restart count 0
    Jan 17 14:58:01.977: INFO: dns-default-8q4m7 from openshift-dns started at 2023-01-17 12:56:45 +0000 UTC (2 container statuses recorded)
    Jan 17 14:58:01.977: INFO: 	Container dns ready: true, restart count 0
    Jan 17 14:58:01.977: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 17 14:58:01.977: INFO: node-resolver-dvw44 from openshift-dns started at 2023-01-17 12:55:53 +0000 UTC (1 container statuses recorded)
    Jan 17 14:58:01.977: INFO: 	Container dns-node-resolver ready: true, restart count 0
    Jan 17 14:58:01.977: INFO: image-registry-6d685bd45d-g2mxt from openshift-image-registry started at 2023-01-17 12:56:45 +0000 UTC (1 container statuses recorded)
    Jan 17 14:58:01.977: INFO: 	Container registry ready: true, restart count 0
    Jan 17 14:58:01.977: INFO: node-ca-45nfl from openshift-image-registry started at 2023-01-17 12:55:53 +0000 UTC (1 container statuses recorded)
    Jan 17 14:58:01.977: INFO: 	Container node-ca ready: true, restart count 0
    Jan 17 14:58:01.977: INFO: ingress-canary-q6p6t from openshift-ingress-canary started at 2023-01-17 12:56:45 +0000 UTC (1 container statuses recorded)
    Jan 17 14:58:01.977: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    Jan 17 14:58:01.977: INFO: machine-config-daemon-v7v4j from openshift-machine-config-operator started at 2023-01-17 12:55:53 +0000 UTC (2 container statuses recorded)
    Jan 17 14:58:01.977: INFO: 	Container machine-config-daemon ready: true, restart count 0
    Jan 17 14:58:01.977: INFO: 	Container oauth-proxy ready: true, restart count 0
    Jan 17 14:58:01.977: INFO: kube-state-metrics-75455b796c-rgbdl from openshift-monitoring started at 2023-01-17 12:56:55 +0000 UTC (3 container statuses recorded)
    Jan 17 14:58:01.977: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
    Jan 17 14:58:01.977: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
    Jan 17 14:58:01.977: INFO: 	Container kube-state-metrics ready: true, restart count 0
    Jan 17 14:58:01.977: INFO: node-exporter-pdxfc from openshift-monitoring started at 2023-01-17 12:56:55 +0000 UTC (2 container statuses recorded)
    Jan 17 14:58:01.977: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 17 14:58:01.977: INFO: 	Container node-exporter ready: true, restart count 0
    Jan 17 14:58:01.977: INFO: openshift-state-metrics-5ff95d844f-dl27q from openshift-monitoring started at 2023-01-17 12:56:55 +0000 UTC (3 container statuses recorded)
    Jan 17 14:58:01.977: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
    Jan 17 14:58:01.977: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
    Jan 17 14:58:01.977: INFO: 	Container openshift-state-metrics ready: true, restart count 0
    Jan 17 14:58:01.977: INFO: prometheus-k8s-1 from openshift-monitoring started at 2023-01-17 12:58:37 +0000 UTC (6 container statuses recorded)
    Jan 17 14:58:01.977: INFO: 	Container config-reloader ready: true, restart count 0
    Jan 17 14:58:01.977: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 17 14:58:01.977: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
    Jan 17 14:58:01.977: INFO: 	Container prometheus ready: true, restart count 0
    Jan 17 14:58:01.977: INFO: 	Container prometheus-proxy ready: true, restart count 0
    Jan 17 14:58:01.977: INFO: 	Container thanos-sidecar ready: true, restart count 0
    Jan 17 14:58:01.977: INFO: prometheus-operator-admission-webhook-7d4759d465-bw8tr from openshift-monitoring started at 2023-01-17 12:56:45 +0000 UTC (1 container statuses recorded)
    Jan 17 14:58:01.977: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
    Jan 17 14:58:01.977: INFO: telemeter-client-585b5cf5bf-zlltk from openshift-monitoring started at 2023-01-17 12:57:00 +0000 UTC (3 container statuses recorded)
    Jan 17 14:58:01.977: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 17 14:58:01.977: INFO: 	Container reload ready: true, restart count 0
    Jan 17 14:58:01.977: INFO: 	Container telemeter-client ready: true, restart count 0
    Jan 17 14:58:01.977: INFO: thanos-querier-5fccfc4877-lc5nk from openshift-monitoring started at 2023-01-17 12:57:02 +0000 UTC (6 container statuses recorded)
    Jan 17 14:58:01.977: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 17 14:58:01.977: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
    Jan 17 14:58:01.977: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
    Jan 17 14:58:01.977: INFO: 	Container oauth-proxy ready: true, restart count 0
    Jan 17 14:58:01.977: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Jan 17 14:58:01.977: INFO: 	Container thanos-query ready: true, restart count 0
    Jan 17 14:58:01.977: INFO: multus-additional-cni-plugins-zg84j from openshift-multus started at 2023-01-17 12:55:53 +0000 UTC (1 container statuses recorded)
    Jan 17 14:58:01.977: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    Jan 17 14:58:01.977: INFO: multus-jq6qc from openshift-multus started at 2023-01-17 12:55:53 +0000 UTC (1 container statuses recorded)
    Jan 17 14:58:01.977: INFO: 	Container kube-multus ready: true, restart count 0
    Jan 17 14:58:01.977: INFO: network-metrics-daemon-gngg5 from openshift-multus started at 2023-01-17 12:55:53 +0000 UTC (2 container statuses recorded)
    Jan 17 14:58:01.977: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 17 14:58:01.977: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    Jan 17 14:58:01.977: INFO: network-check-target-v9g9x from openshift-network-diagnostics started at 2023-01-17 12:55:53 +0000 UTC (1 container statuses recorded)
    Jan 17 14:58:01.977: INFO: 	Container network-check-target-container ready: true, restart count 0
    Jan 17 14:58:01.977: INFO: ovnkube-node-q54mr from openshift-ovn-kubernetes started at 2023-01-17 12:55:53 +0000 UTC (5 container statuses recorded)
    Jan 17 14:58:01.977: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 17 14:58:01.977: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 0
    Jan 17 14:58:01.977: INFO: 	Container ovn-acl-logging ready: true, restart count 0
    Jan 17 14:58:01.977: INFO: 	Container ovn-controller ready: true, restart count 0
    Jan 17 14:58:01.977: INFO: 	Container ovnkube-node ready: true, restart count 0
    Jan 17 14:58:01.977: INFO: sonobuoy-systemd-logs-daemon-set-329d7d7181d04e86-qcrqh from sonobuoy started at 2023-01-17 14:47:25 +0000 UTC (2 container statuses recorded)
    Jan 17 14:58:01.977: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 17 14:58:01.977: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:326
    STEP: verifying the node has the label node ip-10-0-139-213.ec2.internal 01/17/23 14:58:02.038
    STEP: verifying the node has the label node ip-10-0-151-22.ec2.internal 01/17/23 14:58:02.055
    STEP: verifying the node has the label node ip-10-0-165-14.ec2.internal 01/17/23 14:58:02.078
    Jan 17 14:58:02.133: INFO: Pod aws-ebs-csi-driver-node-5tmvr requesting resource cpu=30m on Node ip-10-0-139-213.ec2.internal
    Jan 17 14:58:02.133: INFO: Pod aws-ebs-csi-driver-node-mx922 requesting resource cpu=30m on Node ip-10-0-151-22.ec2.internal
    Jan 17 14:58:02.133: INFO: Pod aws-ebs-csi-driver-node-x9fxp requesting resource cpu=30m on Node ip-10-0-165-14.ec2.internal
    Jan 17 14:58:02.133: INFO: Pod tuned-9hhcs requesting resource cpu=10m on Node ip-10-0-151-22.ec2.internal
    Jan 17 14:58:02.133: INFO: Pod tuned-cgskr requesting resource cpu=10m on Node ip-10-0-165-14.ec2.internal
    Jan 17 14:58:02.133: INFO: Pod tuned-jzlnj requesting resource cpu=10m on Node ip-10-0-139-213.ec2.internal
    Jan 17 14:58:02.133: INFO: Pod downloads-8d695cd69-ltk55 requesting resource cpu=10m on Node ip-10-0-139-213.ec2.internal
    Jan 17 14:58:02.133: INFO: Pod downloads-8d695cd69-w4jfq requesting resource cpu=10m on Node ip-10-0-151-22.ec2.internal
    Jan 17 14:58:02.133: INFO: Pod dns-default-8q4m7 requesting resource cpu=60m on Node ip-10-0-165-14.ec2.internal
    Jan 17 14:58:02.133: INFO: Pod dns-default-d977v requesting resource cpu=60m on Node ip-10-0-151-22.ec2.internal
    Jan 17 14:58:02.133: INFO: Pod dns-default-jgzr9 requesting resource cpu=60m on Node ip-10-0-139-213.ec2.internal
    Jan 17 14:58:02.133: INFO: Pod node-resolver-dvw44 requesting resource cpu=5m on Node ip-10-0-165-14.ec2.internal
    Jan 17 14:58:02.133: INFO: Pod node-resolver-fxksv requesting resource cpu=5m on Node ip-10-0-151-22.ec2.internal
    Jan 17 14:58:02.133: INFO: Pod node-resolver-gkxnp requesting resource cpu=5m on Node ip-10-0-139-213.ec2.internal
    Jan 17 14:58:02.133: INFO: Pod image-registry-6d685bd45d-g2mxt requesting resource cpu=100m on Node ip-10-0-165-14.ec2.internal
    Jan 17 14:58:02.133: INFO: Pod image-registry-6d685bd45d-mk7wb requesting resource cpu=100m on Node ip-10-0-139-213.ec2.internal
    Jan 17 14:58:02.133: INFO: Pod node-ca-45nfl requesting resource cpu=10m on Node ip-10-0-165-14.ec2.internal
    Jan 17 14:58:02.133: INFO: Pod node-ca-4l49x requesting resource cpu=10m on Node ip-10-0-139-213.ec2.internal
    Jan 17 14:58:02.133: INFO: Pod node-ca-tpp6r requesting resource cpu=10m on Node ip-10-0-151-22.ec2.internal
    Jan 17 14:58:02.133: INFO: Pod ingress-canary-d45j9 requesting resource cpu=10m on Node ip-10-0-151-22.ec2.internal
    Jan 17 14:58:02.133: INFO: Pod ingress-canary-q6p6t requesting resource cpu=10m on Node ip-10-0-165-14.ec2.internal
    Jan 17 14:58:02.133: INFO: Pod ingress-canary-xqzwm requesting resource cpu=10m on Node ip-10-0-139-213.ec2.internal
    Jan 17 14:58:02.133: INFO: Pod router-default-c95cc587f-24fmv requesting resource cpu=100m on Node ip-10-0-151-22.ec2.internal
    Jan 17 14:58:02.133: INFO: Pod router-default-c95cc587f-9clvn requesting resource cpu=100m on Node ip-10-0-139-213.ec2.internal
    Jan 17 14:58:02.133: INFO: Pod machine-config-daemon-6hjqr requesting resource cpu=40m on Node ip-10-0-139-213.ec2.internal
    Jan 17 14:58:02.133: INFO: Pod machine-config-daemon-bgrjb requesting resource cpu=40m on Node ip-10-0-151-22.ec2.internal
    Jan 17 14:58:02.133: INFO: Pod machine-config-daemon-v7v4j requesting resource cpu=40m on Node ip-10-0-165-14.ec2.internal
    Jan 17 14:58:02.133: INFO: Pod alertmanager-main-0 requesting resource cpu=9m on Node ip-10-0-139-213.ec2.internal
    Jan 17 14:58:02.133: INFO: Pod alertmanager-main-1 requesting resource cpu=9m on Node ip-10-0-151-22.ec2.internal
    Jan 17 14:58:02.133: INFO: Pod kube-state-metrics-75455b796c-rgbdl requesting resource cpu=4m on Node ip-10-0-165-14.ec2.internal
    Jan 17 14:58:02.133: INFO: Pod node-exporter-lwbc8 requesting resource cpu=9m on Node ip-10-0-139-213.ec2.internal
    Jan 17 14:58:02.133: INFO: Pod node-exporter-pdxfc requesting resource cpu=9m on Node ip-10-0-165-14.ec2.internal
    Jan 17 14:58:02.133: INFO: Pod node-exporter-snggg requesting resource cpu=9m on Node ip-10-0-151-22.ec2.internal
    Jan 17 14:58:02.133: INFO: Pod openshift-state-metrics-5ff95d844f-dl27q requesting resource cpu=3m on Node ip-10-0-165-14.ec2.internal
    Jan 17 14:58:02.133: INFO: Pod prometheus-adapter-cd9bc68fc-dstxm requesting resource cpu=1m on Node ip-10-0-139-213.ec2.internal
    Jan 17 14:58:02.133: INFO: Pod prometheus-adapter-cd9bc68fc-fr52q requesting resource cpu=1m on Node ip-10-0-151-22.ec2.internal
    Jan 17 14:58:02.133: INFO: Pod prometheus-k8s-0 requesting resource cpu=75m on Node ip-10-0-139-213.ec2.internal
    Jan 17 14:58:02.133: INFO: Pod prometheus-k8s-1 requesting resource cpu=75m on Node ip-10-0-165-14.ec2.internal
    Jan 17 14:58:02.133: INFO: Pod prometheus-operator-admission-webhook-7d4759d465-bw8tr requesting resource cpu=5m on Node ip-10-0-165-14.ec2.internal
    Jan 17 14:58:02.133: INFO: Pod prometheus-operator-admission-webhook-7d4759d465-skshz requesting resource cpu=5m on Node ip-10-0-151-22.ec2.internal
    Jan 17 14:58:02.133: INFO: Pod telemeter-client-585b5cf5bf-zlltk requesting resource cpu=3m on Node ip-10-0-165-14.ec2.internal
    Jan 17 14:58:02.133: INFO: Pod thanos-querier-5fccfc4877-lc5nk requesting resource cpu=15m on Node ip-10-0-165-14.ec2.internal
    Jan 17 14:58:02.133: INFO: Pod thanos-querier-5fccfc4877-prbhx requesting resource cpu=15m on Node ip-10-0-139-213.ec2.internal
    Jan 17 14:58:02.133: INFO: Pod multus-additional-cni-plugins-bpc6w requesting resource cpu=10m on Node ip-10-0-139-213.ec2.internal
    Jan 17 14:58:02.133: INFO: Pod multus-additional-cni-plugins-grzml requesting resource cpu=10m on Node ip-10-0-151-22.ec2.internal
    Jan 17 14:58:02.133: INFO: Pod multus-additional-cni-plugins-zg84j requesting resource cpu=10m on Node ip-10-0-165-14.ec2.internal
    Jan 17 14:58:02.133: INFO: Pod multus-jq6qc requesting resource cpu=10m on Node ip-10-0-165-14.ec2.internal
    Jan 17 14:58:02.133: INFO: Pod multus-qspfd requesting resource cpu=10m on Node ip-10-0-151-22.ec2.internal
    Jan 17 14:58:02.133: INFO: Pod multus-tt5fs requesting resource cpu=10m on Node ip-10-0-139-213.ec2.internal
    Jan 17 14:58:02.133: INFO: Pod network-metrics-daemon-22p8j requesting resource cpu=20m on Node ip-10-0-139-213.ec2.internal
    Jan 17 14:58:02.133: INFO: Pod network-metrics-daemon-gngg5 requesting resource cpu=20m on Node ip-10-0-165-14.ec2.internal
    Jan 17 14:58:02.133: INFO: Pod network-metrics-daemon-x4zrh requesting resource cpu=20m on Node ip-10-0-151-22.ec2.internal
    Jan 17 14:58:02.133: INFO: Pod network-check-source-746dd6c885-ndrxh requesting resource cpu=10m on Node ip-10-0-151-22.ec2.internal
    Jan 17 14:58:02.133: INFO: Pod network-check-target-f794b requesting resource cpu=10m on Node ip-10-0-151-22.ec2.internal
    Jan 17 14:58:02.133: INFO: Pod network-check-target-k685r requesting resource cpu=10m on Node ip-10-0-139-213.ec2.internal
    Jan 17 14:58:02.133: INFO: Pod network-check-target-v9g9x requesting resource cpu=10m on Node ip-10-0-165-14.ec2.internal
    Jan 17 14:58:02.133: INFO: Pod ovnkube-node-7n8jx requesting resource cpu=50m on Node ip-10-0-139-213.ec2.internal
    Jan 17 14:58:02.133: INFO: Pod ovnkube-node-bks72 requesting resource cpu=50m on Node ip-10-0-151-22.ec2.internal
    Jan 17 14:58:02.133: INFO: Pod ovnkube-node-q54mr requesting resource cpu=50m on Node ip-10-0-165-14.ec2.internal
    Jan 17 14:58:02.133: INFO: Pod sonobuoy requesting resource cpu=0m on Node ip-10-0-151-22.ec2.internal
    Jan 17 14:58:02.133: INFO: Pod sonobuoy-e2e-job-0a9b408aa2f34bad requesting resource cpu=0m on Node ip-10-0-151-22.ec2.internal
    Jan 17 14:58:02.133: INFO: Pod sonobuoy-systemd-logs-daemon-set-329d7d7181d04e86-8knzd requesting resource cpu=0m on Node ip-10-0-151-22.ec2.internal
    Jan 17 14:58:02.133: INFO: Pod sonobuoy-systemd-logs-daemon-set-329d7d7181d04e86-mhx5d requesting resource cpu=0m on Node ip-10-0-139-213.ec2.internal
    Jan 17 14:58:02.133: INFO: Pod sonobuoy-systemd-logs-daemon-set-329d7d7181d04e86-qcrqh requesting resource cpu=0m on Node ip-10-0-165-14.ec2.internal
    STEP: Starting Pods to consume most of the cluster CPU. 01/17/23 14:58:02.133
    Jan 17 14:58:02.133: INFO: Creating a pod which consumes cpu=2041m on Node ip-10-0-139-213.ec2.internal
    Jan 17 14:58:02.151: INFO: Creating a pod which consumes cpu=2163m on Node ip-10-0-151-22.ec2.internal
    Jan 17 14:58:02.165: INFO: Creating a pod which consumes cpu=2114m on Node ip-10-0-165-14.ec2.internal
    Jan 17 14:58:02.181: INFO: Waiting up to 5m0s for pod "filler-pod-716a485a-8cfa-42db-9f7c-8ac4af000258" in namespace "sched-pred-2480" to be "running"
    Jan 17 14:58:02.185: INFO: Pod "filler-pod-716a485a-8cfa-42db-9f7c-8ac4af000258": Phase="Pending", Reason="", readiness=false. Elapsed: 4.251633ms
    Jan 17 14:58:04.189: INFO: Pod "filler-pod-716a485a-8cfa-42db-9f7c-8ac4af000258": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008346823s
    Jan 17 14:58:06.191: INFO: Pod "filler-pod-716a485a-8cfa-42db-9f7c-8ac4af000258": Phase="Running", Reason="", readiness=true. Elapsed: 4.009990167s
    Jan 17 14:58:06.191: INFO: Pod "filler-pod-716a485a-8cfa-42db-9f7c-8ac4af000258" satisfied condition "running"
    Jan 17 14:58:06.191: INFO: Waiting up to 5m0s for pod "filler-pod-0f6ce53f-24e0-4fd8-aeab-b421701942fe" in namespace "sched-pred-2480" to be "running"
    Jan 17 14:58:06.194: INFO: Pod "filler-pod-0f6ce53f-24e0-4fd8-aeab-b421701942fe": Phase="Running", Reason="", readiness=true. Elapsed: 2.951116ms
    Jan 17 14:58:06.194: INFO: Pod "filler-pod-0f6ce53f-24e0-4fd8-aeab-b421701942fe" satisfied condition "running"
    Jan 17 14:58:06.194: INFO: Waiting up to 5m0s for pod "filler-pod-f49ca2ed-bdc7-496e-8f20-3ac530582756" in namespace "sched-pred-2480" to be "running"
    Jan 17 14:58:06.197: INFO: Pod "filler-pod-f49ca2ed-bdc7-496e-8f20-3ac530582756": Phase="Running", Reason="", readiness=true. Elapsed: 2.562587ms
    Jan 17 14:58:06.197: INFO: Pod "filler-pod-f49ca2ed-bdc7-496e-8f20-3ac530582756" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 01/17/23 14:58:06.197
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-0f6ce53f-24e0-4fd8-aeab-b421701942fe.173b208e545cf704], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2480/filler-pod-0f6ce53f-24e0-4fd8-aeab-b421701942fe to ip-10-0-151-22.ec2.internal] 01/17/23 14:58:06.199
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-0f6ce53f-24e0-4fd8-aeab-b421701942fe.173b208e7382a5dd], Reason = [AddedInterface], Message = [Add eth0 [10.131.0.39/23] from ovn-kubernetes] 01/17/23 14:58:06.2
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-0f6ce53f-24e0-4fd8-aeab-b421701942fe.173b208e74e5fb39], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 01/17/23 14:58:06.2
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-0f6ce53f-24e0-4fd8-aeab-b421701942fe.173b208e7cc51ea6], Reason = [Created], Message = [Created container filler-pod-0f6ce53f-24e0-4fd8-aeab-b421701942fe] 01/17/23 14:58:06.2
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-0f6ce53f-24e0-4fd8-aeab-b421701942fe.173b208e7e7227bd], Reason = [Started], Message = [Started container filler-pod-0f6ce53f-24e0-4fd8-aeab-b421701942fe] 01/17/23 14:58:06.2
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-716a485a-8cfa-42db-9f7c-8ac4af000258.173b208e53c53fb1], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2480/filler-pod-716a485a-8cfa-42db-9f7c-8ac4af000258 to ip-10-0-139-213.ec2.internal] 01/17/23 14:58:06.2
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-716a485a-8cfa-42db-9f7c-8ac4af000258.173b208e84cbd091], Reason = [AddedInterface], Message = [Add eth0 [10.129.2.23/23] from ovn-kubernetes] 01/17/23 14:58:06.2
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-716a485a-8cfa-42db-9f7c-8ac4af000258.173b208e85f43744], Reason = [Pulling], Message = [Pulling image "registry.k8s.io/pause:3.8"] 01/17/23 14:58:06.2
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-716a485a-8cfa-42db-9f7c-8ac4af000258.173b208eaa26a8d5], Reason = [Pulled], Message = [Successfully pulled image "registry.k8s.io/pause:3.8" in 607.258136ms] 01/17/23 14:58:06.2
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-716a485a-8cfa-42db-9f7c-8ac4af000258.173b208eb292a335], Reason = [Created], Message = [Created container filler-pod-716a485a-8cfa-42db-9f7c-8ac4af000258] 01/17/23 14:58:06.2
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-716a485a-8cfa-42db-9f7c-8ac4af000258.173b208eb44a9dcf], Reason = [Started], Message = [Started container filler-pod-716a485a-8cfa-42db-9f7c-8ac4af000258] 01/17/23 14:58:06.2
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-f49ca2ed-bdc7-496e-8f20-3ac530582756.173b208e557c20b7], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2480/filler-pod-f49ca2ed-bdc7-496e-8f20-3ac530582756 to ip-10-0-165-14.ec2.internal] 01/17/23 14:58:06.2
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-f49ca2ed-bdc7-496e-8f20-3ac530582756.173b208e736fa9e4], Reason = [AddedInterface], Message = [Add eth0 [10.128.2.24/23] from ovn-kubernetes] 01/17/23 14:58:06.2
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-f49ca2ed-bdc7-496e-8f20-3ac530582756.173b208e74abc213], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 01/17/23 14:58:06.2
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-f49ca2ed-bdc7-496e-8f20-3ac530582756.173b208e7c90c618], Reason = [Created], Message = [Created container filler-pod-f49ca2ed-bdc7-496e-8f20-3ac530582756] 01/17/23 14:58:06.2
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-f49ca2ed-bdc7-496e-8f20-3ac530582756.173b208e7d721c16], Reason = [Started], Message = [Started container filler-pod-f49ca2ed-bdc7-496e-8f20-3ac530582756] 01/17/23 14:58:06.2
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.173b208f4556f81e], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 6 Insufficient cpu. preemption: 0/6 nodes are available: 3 No preemption victims found for incoming pod, 3 Preemption is not helpful for scheduling.] 01/17/23 14:58:06.22
    STEP: removing the label node off the node ip-10-0-139-213.ec2.internal 01/17/23 14:58:07.218
    STEP: verifying the node doesn't have the label node 01/17/23 14:58:07.229
    STEP: removing the label node off the node ip-10-0-151-22.ec2.internal 01/17/23 14:58:07.24
    STEP: verifying the node doesn't have the label node 01/17/23 14:58:07.265
    STEP: removing the label node off the node ip-10-0-165-14.ec2.internal 01/17/23 14:58:07.268
    STEP: verifying the node doesn't have the label node 01/17/23 14:58:07.286
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Jan 17 14:58:07.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-2480" for this suite. 01/17/23 14:58:07.299
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1785
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 14:58:07.312
Jan 17 14:58:07.312: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename kubectl 01/17/23 14:58:07.313
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 14:58:07.344
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 14:58:07.347
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1785
STEP: starting the proxy server 01/17/23 14:58:07.349
Jan 17 14:58:07.349: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-5317 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 01/17/23 14:58:07.378
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 17 14:58:07.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5317" for this suite. 01/17/23 14:58:07.389
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","completed":26,"skipped":480,"failed":0}
------------------------------
• [0.092 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1778
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1785

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 14:58:07.312
    Jan 17 14:58:07.312: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename kubectl 01/17/23 14:58:07.313
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 14:58:07.344
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 14:58:07.347
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1785
    STEP: starting the proxy server 01/17/23 14:58:07.349
    Jan 17 14:58:07.349: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-5317 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 01/17/23 14:58:07.378
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 17 14:58:07.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-5317" for this suite. 01/17/23 14:58:07.389
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 14:58:07.406
Jan 17 14:58:07.406: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename webhook 01/17/23 14:58:07.406
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 14:58:07.447
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 14:58:07.45
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/17/23 14:58:07.526
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/17/23 14:58:07.655
STEP: Deploying the webhook pod 01/17/23 14:58:07.702
STEP: Wait for the deployment to be ready 01/17/23 14:58:07.713
Jan 17 14:58:07.721: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/17/23 14:58:09.732
STEP: Verifying the service has paired with the endpoint 01/17/23 14:58:09.743
Jan 17 14:58:10.743: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208
STEP: Registering the webhook via the AdmissionRegistration API 01/17/23 14:58:10.747
STEP: create a pod 01/17/23 14:58:10.76
Jan 17 14:58:10.773: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-2673" to be "running"
Jan 17 14:58:10.776: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.843012ms
Jan 17 14:58:12.780: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.00678048s
Jan 17 14:58:12.780: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 01/17/23 14:58:12.78
Jan 17 14:58:12.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=webhook-2673 attach --namespace=webhook-2673 to-be-attached-pod -i -c=container1'
Jan 17 14:58:12.845: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 17 14:58:12.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2673" for this suite. 01/17/23 14:58:12.855
STEP: Destroying namespace "webhook-2673-markers" for this suite. 01/17/23 14:58:12.862
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","completed":27,"skipped":536,"failed":0}
------------------------------
• [SLOW TEST] [5.543 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 14:58:07.406
    Jan 17 14:58:07.406: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename webhook 01/17/23 14:58:07.406
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 14:58:07.447
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 14:58:07.45
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/17/23 14:58:07.526
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/17/23 14:58:07.655
    STEP: Deploying the webhook pod 01/17/23 14:58:07.702
    STEP: Wait for the deployment to be ready 01/17/23 14:58:07.713
    Jan 17 14:58:07.721: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/17/23 14:58:09.732
    STEP: Verifying the service has paired with the endpoint 01/17/23 14:58:09.743
    Jan 17 14:58:10.743: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:208
    STEP: Registering the webhook via the AdmissionRegistration API 01/17/23 14:58:10.747
    STEP: create a pod 01/17/23 14:58:10.76
    Jan 17 14:58:10.773: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-2673" to be "running"
    Jan 17 14:58:10.776: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.843012ms
    Jan 17 14:58:12.780: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.00678048s
    Jan 17 14:58:12.780: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 01/17/23 14:58:12.78
    Jan 17 14:58:12.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=webhook-2673 attach --namespace=webhook-2673 to-be-attached-pod -i -c=container1'
    Jan 17 14:58:12.845: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 17 14:58:12.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-2673" for this suite. 01/17/23 14:58:12.855
    STEP: Destroying namespace "webhook-2673-markers" for this suite. 01/17/23 14:58:12.862
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 14:58:12.949
Jan 17 14:58:12.949: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename watch 01/17/23 14:58:12.949
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 14:58:12.989
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 14:58:12.992
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 01/17/23 14:58:12.994
STEP: modifying the configmap once 01/17/23 14:58:13.003
STEP: modifying the configmap a second time 01/17/23 14:58:13.017
STEP: deleting the configmap 01/17/23 14:58:13.031
STEP: creating a watch on configmaps from the resource version returned by the first update 01/17/23 14:58:13.039
STEP: Expecting to observe notifications for all changes to the configmap after the first update 01/17/23 14:58:13.04
Jan 17 14:58:13.041: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-6101  fcbf0c57-7799-4ea7-9136-441a86d8a72e 72044 0 2023-01-17 14:58:12 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-01-17 14:58:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 17 14:58:13.041: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-6101  fcbf0c57-7799-4ea7-9136-441a86d8a72e 72047 0 2023-01-17 14:58:12 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-01-17 14:58:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Jan 17 14:58:13.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6101" for this suite. 01/17/23 14:58:13.045
{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","completed":28,"skipped":539,"failed":0}
------------------------------
• [0.105 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 14:58:12.949
    Jan 17 14:58:12.949: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename watch 01/17/23 14:58:12.949
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 14:58:12.989
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 14:58:12.992
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 01/17/23 14:58:12.994
    STEP: modifying the configmap once 01/17/23 14:58:13.003
    STEP: modifying the configmap a second time 01/17/23 14:58:13.017
    STEP: deleting the configmap 01/17/23 14:58:13.031
    STEP: creating a watch on configmaps from the resource version returned by the first update 01/17/23 14:58:13.039
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 01/17/23 14:58:13.04
    Jan 17 14:58:13.041: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-6101  fcbf0c57-7799-4ea7-9136-441a86d8a72e 72044 0 2023-01-17 14:58:12 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-01-17 14:58:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 17 14:58:13.041: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-6101  fcbf0c57-7799-4ea7-9136-441a86d8a72e 72047 0 2023-01-17 14:58:12 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-01-17 14:58:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Jan 17 14:58:13.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-6101" for this suite. 01/17/23 14:58:13.045
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 14:58:13.056
Jan 17 14:58:13.056: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename dns 01/17/23 14:58:13.056
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 14:58:13.088
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 14:58:13.091
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 01/17/23 14:58:13.093
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 01/17/23 14:58:13.093
STEP: creating a pod to probe DNS 01/17/23 14:58:13.093
STEP: submitting the pod to kubernetes 01/17/23 14:58:13.093
Jan 17 14:58:13.143: INFO: Waiting up to 15m0s for pod "dns-test-685f7f79-a072-472a-885c-c6f0395adee9" in namespace "dns-4683" to be "running"
Jan 17 14:58:13.156: INFO: Pod "dns-test-685f7f79-a072-472a-885c-c6f0395adee9": Phase="Pending", Reason="", readiness=false. Elapsed: 12.982262ms
Jan 17 14:58:15.160: INFO: Pod "dns-test-685f7f79-a072-472a-885c-c6f0395adee9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016660007s
Jan 17 14:58:17.161: INFO: Pod "dns-test-685f7f79-a072-472a-885c-c6f0395adee9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017704954s
Jan 17 14:58:19.161: INFO: Pod "dns-test-685f7f79-a072-472a-885c-c6f0395adee9": Phase="Running", Reason="", readiness=true. Elapsed: 6.017587549s
Jan 17 14:58:19.161: INFO: Pod "dns-test-685f7f79-a072-472a-885c-c6f0395adee9" satisfied condition "running"
STEP: retrieving the pod 01/17/23 14:58:19.161
STEP: looking for the results for each expected name from probers 01/17/23 14:58:19.164
Jan 17 14:58:19.181: INFO: DNS probes using dns-4683/dns-test-685f7f79-a072-472a-885c-c6f0395adee9 succeeded

STEP: deleting the pod 01/17/23 14:58:19.181
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan 17 14:58:19.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4683" for this suite. 01/17/23 14:58:19.199
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","completed":29,"skipped":623,"failed":0}
------------------------------
• [SLOW TEST] [6.150 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 14:58:13.056
    Jan 17 14:58:13.056: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename dns 01/17/23 14:58:13.056
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 14:58:13.088
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 14:58:13.091
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     01/17/23 14:58:13.093
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     01/17/23 14:58:13.093
    STEP: creating a pod to probe DNS 01/17/23 14:58:13.093
    STEP: submitting the pod to kubernetes 01/17/23 14:58:13.093
    Jan 17 14:58:13.143: INFO: Waiting up to 15m0s for pod "dns-test-685f7f79-a072-472a-885c-c6f0395adee9" in namespace "dns-4683" to be "running"
    Jan 17 14:58:13.156: INFO: Pod "dns-test-685f7f79-a072-472a-885c-c6f0395adee9": Phase="Pending", Reason="", readiness=false. Elapsed: 12.982262ms
    Jan 17 14:58:15.160: INFO: Pod "dns-test-685f7f79-a072-472a-885c-c6f0395adee9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016660007s
    Jan 17 14:58:17.161: INFO: Pod "dns-test-685f7f79-a072-472a-885c-c6f0395adee9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017704954s
    Jan 17 14:58:19.161: INFO: Pod "dns-test-685f7f79-a072-472a-885c-c6f0395adee9": Phase="Running", Reason="", readiness=true. Elapsed: 6.017587549s
    Jan 17 14:58:19.161: INFO: Pod "dns-test-685f7f79-a072-472a-885c-c6f0395adee9" satisfied condition "running"
    STEP: retrieving the pod 01/17/23 14:58:19.161
    STEP: looking for the results for each expected name from probers 01/17/23 14:58:19.164
    Jan 17 14:58:19.181: INFO: DNS probes using dns-4683/dns-test-685f7f79-a072-472a-885c-c6f0395adee9 succeeded

    STEP: deleting the pod 01/17/23 14:58:19.181
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan 17 14:58:19.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-4683" for this suite. 01/17/23 14:58:19.199
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 14:58:19.206
Jan 17 14:58:19.206: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename sched-preemption 01/17/23 14:58:19.206
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 14:58:19.307
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 14:58:19.309
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jan 17 14:58:19.430: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 17 14:59:19.546: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125
STEP: Create pods that use 4/5 of node resources. 01/17/23 14:59:19.551
Jan 17 14:59:19.588: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jan 17 14:59:19.608: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jan 17 14:59:19.644: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jan 17 14:59:19.664: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Jan 17 14:59:19.697: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Jan 17 14:59:19.710: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 01/17/23 14:59:19.71
Jan 17 14:59:19.710: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-608" to be "running"
Jan 17 14:59:19.713: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.756515ms
Jan 17 14:59:21.721: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010761792s
Jan 17 14:59:23.717: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006626047s
Jan 17 14:59:25.719: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.008464681s
Jan 17 14:59:27.719: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.008402969s
Jan 17 14:59:27.719: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Jan 17 14:59:27.719: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-608" to be "running"
Jan 17 14:59:27.722: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.222629ms
Jan 17 14:59:27.722: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Jan 17 14:59:27.722: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-608" to be "running"
Jan 17 14:59:27.725: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.806534ms
Jan 17 14:59:27.725: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Jan 17 14:59:27.725: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-608" to be "running"
Jan 17 14:59:27.728: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.978652ms
Jan 17 14:59:27.728: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Jan 17 14:59:27.728: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-608" to be "running"
Jan 17 14:59:27.731: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.981393ms
Jan 17 14:59:29.735: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.007210975s
Jan 17 14:59:29.735: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Jan 17 14:59:29.735: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-608" to be "running"
Jan 17 14:59:29.738: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.109129ms
Jan 17 14:59:29.738: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 01/17/23 14:59:29.738
Jan 17 14:59:29.751: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-608" to be "running"
Jan 17 14:59:29.754: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.046852ms
Jan 17 14:59:31.765: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013984261s
Jan 17 14:59:33.758: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.007266443s
Jan 17 14:59:33.758: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Jan 17 14:59:33.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-608" for this suite. 01/17/23 14:59:33.783
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","completed":30,"skipped":628,"failed":0}
------------------------------
• [SLOW TEST] [74.630 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 14:58:19.206
    Jan 17 14:58:19.206: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename sched-preemption 01/17/23 14:58:19.206
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 14:58:19.307
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 14:58:19.309
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Jan 17 14:58:19.430: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 17 14:59:19.546: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:125
    STEP: Create pods that use 4/5 of node resources. 01/17/23 14:59:19.551
    Jan 17 14:59:19.588: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Jan 17 14:59:19.608: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Jan 17 14:59:19.644: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Jan 17 14:59:19.664: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Jan 17 14:59:19.697: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Jan 17 14:59:19.710: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 01/17/23 14:59:19.71
    Jan 17 14:59:19.710: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-608" to be "running"
    Jan 17 14:59:19.713: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.756515ms
    Jan 17 14:59:21.721: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010761792s
    Jan 17 14:59:23.717: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006626047s
    Jan 17 14:59:25.719: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.008464681s
    Jan 17 14:59:27.719: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.008402969s
    Jan 17 14:59:27.719: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Jan 17 14:59:27.719: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-608" to be "running"
    Jan 17 14:59:27.722: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.222629ms
    Jan 17 14:59:27.722: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Jan 17 14:59:27.722: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-608" to be "running"
    Jan 17 14:59:27.725: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.806534ms
    Jan 17 14:59:27.725: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Jan 17 14:59:27.725: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-608" to be "running"
    Jan 17 14:59:27.728: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.978652ms
    Jan 17 14:59:27.728: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Jan 17 14:59:27.728: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-608" to be "running"
    Jan 17 14:59:27.731: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.981393ms
    Jan 17 14:59:29.735: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.007210975s
    Jan 17 14:59:29.735: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Jan 17 14:59:29.735: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-608" to be "running"
    Jan 17 14:59:29.738: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.109129ms
    Jan 17 14:59:29.738: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 01/17/23 14:59:29.738
    Jan 17 14:59:29.751: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-608" to be "running"
    Jan 17 14:59:29.754: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.046852ms
    Jan 17 14:59:31.765: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013984261s
    Jan 17 14:59:33.758: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.007266443s
    Jan 17 14:59:33.758: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Jan 17 14:59:33.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-608" for this suite. 01/17/23 14:59:33.783
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 14:59:33.836
Jan 17 14:59:33.836: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename podtemplate 01/17/23 14:59:33.837
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 14:59:33.858
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 14:59:33.86
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Jan 17 14:59:33.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-1078" for this suite. 01/17/23 14:59:33.913
{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","completed":31,"skipped":630,"failed":0}
------------------------------
• [0.091 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 14:59:33.836
    Jan 17 14:59:33.836: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename podtemplate 01/17/23 14:59:33.837
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 14:59:33.858
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 14:59:33.86
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Jan 17 14:59:33.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-1078" for this suite. 01/17/23 14:59:33.913
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 14:59:33.928
Jan 17 14:59:33.928: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename resourcequota 01/17/23 14:59:33.929
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 14:59:33.955
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 14:59:33.957
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793
STEP: Creating a ResourceQuota with best effort scope 01/17/23 14:59:33.959
STEP: Ensuring ResourceQuota status is calculated 01/17/23 14:59:33.968
STEP: Creating a ResourceQuota with not best effort scope 01/17/23 14:59:35.972
STEP: Ensuring ResourceQuota status is calculated 01/17/23 14:59:35.978
STEP: Creating a best-effort pod 01/17/23 14:59:37.982
STEP: Ensuring resource quota with best effort scope captures the pod usage 01/17/23 14:59:38.001
STEP: Ensuring resource quota with not best effort ignored the pod usage 01/17/23 14:59:40.005
STEP: Deleting the pod 01/17/23 14:59:42.01
STEP: Ensuring resource quota status released the pod usage 01/17/23 14:59:42.024
STEP: Creating a not best-effort pod 01/17/23 14:59:44.028
STEP: Ensuring resource quota with not best effort scope captures the pod usage 01/17/23 14:59:44.044
STEP: Ensuring resource quota with best effort scope ignored the pod usage 01/17/23 14:59:46.048
STEP: Deleting the pod 01/17/23 14:59:48.053
STEP: Ensuring resource quota status released the pod usage 01/17/23 14:59:48.068
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 17 14:59:50.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9259" for this suite. 01/17/23 14:59:50.075
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","completed":32,"skipped":638,"failed":0}
------------------------------
• [SLOW TEST] [16.154 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 14:59:33.928
    Jan 17 14:59:33.928: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename resourcequota 01/17/23 14:59:33.929
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 14:59:33.955
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 14:59:33.957
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:793
    STEP: Creating a ResourceQuota with best effort scope 01/17/23 14:59:33.959
    STEP: Ensuring ResourceQuota status is calculated 01/17/23 14:59:33.968
    STEP: Creating a ResourceQuota with not best effort scope 01/17/23 14:59:35.972
    STEP: Ensuring ResourceQuota status is calculated 01/17/23 14:59:35.978
    STEP: Creating a best-effort pod 01/17/23 14:59:37.982
    STEP: Ensuring resource quota with best effort scope captures the pod usage 01/17/23 14:59:38.001
    STEP: Ensuring resource quota with not best effort ignored the pod usage 01/17/23 14:59:40.005
    STEP: Deleting the pod 01/17/23 14:59:42.01
    STEP: Ensuring resource quota status released the pod usage 01/17/23 14:59:42.024
    STEP: Creating a not best-effort pod 01/17/23 14:59:44.028
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 01/17/23 14:59:44.044
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 01/17/23 14:59:46.048
    STEP: Deleting the pod 01/17/23 14:59:48.053
    STEP: Ensuring resource quota status released the pod usage 01/17/23 14:59:48.068
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 17 14:59:50.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-9259" for this suite. 01/17/23 14:59:50.075
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 14:59:50.082
Jan 17 14:59:50.082: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename ingress 01/17/23 14:59:50.083
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 14:59:50.105
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 14:59:50.108
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 01/17/23 14:59:50.11
STEP: getting /apis/networking.k8s.io 01/17/23 14:59:50.111
STEP: getting /apis/networking.k8s.iov1 01/17/23 14:59:50.112
STEP: creating 01/17/23 14:59:50.112
STEP: getting 01/17/23 14:59:50.147
STEP: listing 01/17/23 14:59:50.161
STEP: watching 01/17/23 14:59:50.168
Jan 17 14:59:50.168: INFO: starting watch
STEP: cluster-wide listing 01/17/23 14:59:50.169
STEP: cluster-wide watching 01/17/23 14:59:50.175
Jan 17 14:59:50.176: INFO: starting watch
STEP: patching 01/17/23 14:59:50.176
STEP: updating 01/17/23 14:59:50.181
Jan 17 14:59:50.189: INFO: waiting for watch events with expected annotations
Jan 17 14:59:50.189: INFO: saw patched and updated annotations
STEP: patching /status 01/17/23 14:59:50.189
STEP: updating /status 01/17/23 14:59:50.195
STEP: get /status 01/17/23 14:59:50.202
STEP: deleting 01/17/23 14:59:50.205
STEP: deleting a collection 01/17/23 14:59:50.216
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:187
Jan 17 14:59:50.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-6163" for this suite. 01/17/23 14:59:50.235
{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","completed":33,"skipped":638,"failed":0}
------------------------------
• [0.160 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 14:59:50.082
    Jan 17 14:59:50.082: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename ingress 01/17/23 14:59:50.083
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 14:59:50.105
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 14:59:50.108
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 01/17/23 14:59:50.11
    STEP: getting /apis/networking.k8s.io 01/17/23 14:59:50.111
    STEP: getting /apis/networking.k8s.iov1 01/17/23 14:59:50.112
    STEP: creating 01/17/23 14:59:50.112
    STEP: getting 01/17/23 14:59:50.147
    STEP: listing 01/17/23 14:59:50.161
    STEP: watching 01/17/23 14:59:50.168
    Jan 17 14:59:50.168: INFO: starting watch
    STEP: cluster-wide listing 01/17/23 14:59:50.169
    STEP: cluster-wide watching 01/17/23 14:59:50.175
    Jan 17 14:59:50.176: INFO: starting watch
    STEP: patching 01/17/23 14:59:50.176
    STEP: updating 01/17/23 14:59:50.181
    Jan 17 14:59:50.189: INFO: waiting for watch events with expected annotations
    Jan 17 14:59:50.189: INFO: saw patched and updated annotations
    STEP: patching /status 01/17/23 14:59:50.189
    STEP: updating /status 01/17/23 14:59:50.195
    STEP: get /status 01/17/23 14:59:50.202
    STEP: deleting 01/17/23 14:59:50.205
    STEP: deleting a collection 01/17/23 14:59:50.216
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/framework.go:187
    Jan 17 14:59:50.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ingress-6163" for this suite. 01/17/23 14:59:50.235
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 14:59:50.243
Jan 17 14:59:50.243: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename projected 01/17/23 14:59:50.244
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 14:59:50.273
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 14:59:50.276
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214
Jan 17 14:59:50.288: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-f2015374-b561-4d8e-b24e-7639d515ec08 01/17/23 14:59:50.288
STEP: Creating secret with name s-test-opt-upd-3dfc1d90-aec3-4066-bfe3-a589c56b416d 01/17/23 14:59:50.293
STEP: Creating the pod 01/17/23 14:59:50.298
Jan 17 14:59:50.325: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e9a69fce-c3ba-4e0a-9f73-017e938d9089" in namespace "projected-5765" to be "running and ready"
Jan 17 14:59:50.330: INFO: Pod "pod-projected-secrets-e9a69fce-c3ba-4e0a-9f73-017e938d9089": Phase="Pending", Reason="", readiness=false. Elapsed: 5.338484ms
Jan 17 14:59:50.330: INFO: The phase of Pod pod-projected-secrets-e9a69fce-c3ba-4e0a-9f73-017e938d9089 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 14:59:52.334: INFO: Pod "pod-projected-secrets-e9a69fce-c3ba-4e0a-9f73-017e938d9089": Phase="Running", Reason="", readiness=true. Elapsed: 2.009048462s
Jan 17 14:59:52.334: INFO: The phase of Pod pod-projected-secrets-e9a69fce-c3ba-4e0a-9f73-017e938d9089 is Running (Ready = true)
Jan 17 14:59:52.334: INFO: Pod "pod-projected-secrets-e9a69fce-c3ba-4e0a-9f73-017e938d9089" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-f2015374-b561-4d8e-b24e-7639d515ec08 01/17/23 14:59:52.358
STEP: Updating secret s-test-opt-upd-3dfc1d90-aec3-4066-bfe3-a589c56b416d 01/17/23 14:59:52.363
STEP: Creating secret with name s-test-opt-create-170aee0d-f849-4658-a8bb-d3d0af960f95 01/17/23 14:59:52.368
STEP: waiting to observe update in volume 01/17/23 14:59:52.372
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan 17 14:59:54.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5765" for this suite. 01/17/23 14:59:54.4
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":34,"skipped":689,"failed":0}
------------------------------
• [4.162 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 14:59:50.243
    Jan 17 14:59:50.243: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename projected 01/17/23 14:59:50.244
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 14:59:50.273
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 14:59:50.276
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:214
    Jan 17 14:59:50.288: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
    STEP: Creating secret with name s-test-opt-del-f2015374-b561-4d8e-b24e-7639d515ec08 01/17/23 14:59:50.288
    STEP: Creating secret with name s-test-opt-upd-3dfc1d90-aec3-4066-bfe3-a589c56b416d 01/17/23 14:59:50.293
    STEP: Creating the pod 01/17/23 14:59:50.298
    Jan 17 14:59:50.325: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e9a69fce-c3ba-4e0a-9f73-017e938d9089" in namespace "projected-5765" to be "running and ready"
    Jan 17 14:59:50.330: INFO: Pod "pod-projected-secrets-e9a69fce-c3ba-4e0a-9f73-017e938d9089": Phase="Pending", Reason="", readiness=false. Elapsed: 5.338484ms
    Jan 17 14:59:50.330: INFO: The phase of Pod pod-projected-secrets-e9a69fce-c3ba-4e0a-9f73-017e938d9089 is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 14:59:52.334: INFO: Pod "pod-projected-secrets-e9a69fce-c3ba-4e0a-9f73-017e938d9089": Phase="Running", Reason="", readiness=true. Elapsed: 2.009048462s
    Jan 17 14:59:52.334: INFO: The phase of Pod pod-projected-secrets-e9a69fce-c3ba-4e0a-9f73-017e938d9089 is Running (Ready = true)
    Jan 17 14:59:52.334: INFO: Pod "pod-projected-secrets-e9a69fce-c3ba-4e0a-9f73-017e938d9089" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-f2015374-b561-4d8e-b24e-7639d515ec08 01/17/23 14:59:52.358
    STEP: Updating secret s-test-opt-upd-3dfc1d90-aec3-4066-bfe3-a589c56b416d 01/17/23 14:59:52.363
    STEP: Creating secret with name s-test-opt-create-170aee0d-f849-4658-a8bb-d3d0af960f95 01/17/23 14:59:52.368
    STEP: waiting to observe update in volume 01/17/23 14:59:52.372
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan 17 14:59:54.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5765" for this suite. 01/17/23 14:59:54.4
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 14:59:54.405
Jan 17 14:59:54.406: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename projected 01/17/23 14:59:54.406
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 14:59:54.428
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 14:59:54.43
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260
STEP: Creating a pod to test downward API volume plugin 01/17/23 14:59:54.432
Jan 17 14:59:54.457: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ab150766-bba1-4336-892f-d4a0076711e2" in namespace "projected-9995" to be "Succeeded or Failed"
Jan 17 14:59:54.463: INFO: Pod "downwardapi-volume-ab150766-bba1-4336-892f-d4a0076711e2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.274027ms
Jan 17 14:59:56.467: INFO: Pod "downwardapi-volume-ab150766-bba1-4336-892f-d4a0076711e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010217952s
Jan 17 14:59:58.468: INFO: Pod "downwardapi-volume-ab150766-bba1-4336-892f-d4a0076711e2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01129492s
STEP: Saw pod success 01/17/23 14:59:58.468
Jan 17 14:59:58.468: INFO: Pod "downwardapi-volume-ab150766-bba1-4336-892f-d4a0076711e2" satisfied condition "Succeeded or Failed"
Jan 17 14:59:58.472: INFO: Trying to get logs from node ip-10-0-165-14.ec2.internal pod downwardapi-volume-ab150766-bba1-4336-892f-d4a0076711e2 container client-container: <nil>
STEP: delete the pod 01/17/23 14:59:58.482
Jan 17 14:59:58.496: INFO: Waiting for pod downwardapi-volume-ab150766-bba1-4336-892f-d4a0076711e2 to disappear
Jan 17 14:59:58.498: INFO: Pod downwardapi-volume-ab150766-bba1-4336-892f-d4a0076711e2 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 17 14:59:58.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9995" for this suite. 01/17/23 14:59:58.502
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","completed":35,"skipped":692,"failed":0}
------------------------------
• [4.119 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 14:59:54.405
    Jan 17 14:59:54.406: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename projected 01/17/23 14:59:54.406
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 14:59:54.428
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 14:59:54.43
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:260
    STEP: Creating a pod to test downward API volume plugin 01/17/23 14:59:54.432
    Jan 17 14:59:54.457: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ab150766-bba1-4336-892f-d4a0076711e2" in namespace "projected-9995" to be "Succeeded or Failed"
    Jan 17 14:59:54.463: INFO: Pod "downwardapi-volume-ab150766-bba1-4336-892f-d4a0076711e2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.274027ms
    Jan 17 14:59:56.467: INFO: Pod "downwardapi-volume-ab150766-bba1-4336-892f-d4a0076711e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010217952s
    Jan 17 14:59:58.468: INFO: Pod "downwardapi-volume-ab150766-bba1-4336-892f-d4a0076711e2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01129492s
    STEP: Saw pod success 01/17/23 14:59:58.468
    Jan 17 14:59:58.468: INFO: Pod "downwardapi-volume-ab150766-bba1-4336-892f-d4a0076711e2" satisfied condition "Succeeded or Failed"
    Jan 17 14:59:58.472: INFO: Trying to get logs from node ip-10-0-165-14.ec2.internal pod downwardapi-volume-ab150766-bba1-4336-892f-d4a0076711e2 container client-container: <nil>
    STEP: delete the pod 01/17/23 14:59:58.482
    Jan 17 14:59:58.496: INFO: Waiting for pod downwardapi-volume-ab150766-bba1-4336-892f-d4a0076711e2 to disappear
    Jan 17 14:59:58.498: INFO: Pod downwardapi-volume-ab150766-bba1-4336-892f-d4a0076711e2 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 17 14:59:58.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9995" for this suite. 01/17/23 14:59:58.502
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 14:59:58.525
Jan 17 14:59:58.525: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename svc-latency 01/17/23 14:59:58.525
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 14:59:58.55
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 14:59:58.553
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
Jan 17 14:59:58.555: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: creating replication controller svc-latency-rc in namespace svc-latency-9800 01/17/23 14:59:58.555
W0117 14:59:58.565498      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "svc-latency-rc" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "svc-latency-rc" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "svc-latency-rc" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "svc-latency-rc" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
I0117 14:59:58.565681      22 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-9800, replica count: 1
I0117 14:59:59.616485      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 17 14:59:59.727: INFO: Created: latency-svc-nsjn9
Jan 17 14:59:59.734: INFO: Got endpoints: latency-svc-nsjn9 [17.568315ms]
Jan 17 14:59:59.769: INFO: Created: latency-svc-zgxrg
Jan 17 14:59:59.780: INFO: Got endpoints: latency-svc-zgxrg [28.664762ms]
Jan 17 14:59:59.785: INFO: Created: latency-svc-f75n4
Jan 17 14:59:59.792: INFO: Got endpoints: latency-svc-f75n4 [40.166849ms]
Jan 17 14:59:59.794: INFO: Created: latency-svc-24vh6
Jan 17 14:59:59.804: INFO: Got endpoints: latency-svc-24vh6 [52.275503ms]
Jan 17 14:59:59.805: INFO: Created: latency-svc-lfh7f
Jan 17 14:59:59.812: INFO: Got endpoints: latency-svc-lfh7f [59.88329ms]
Jan 17 14:59:59.827: INFO: Created: latency-svc-mrmnx
Jan 17 14:59:59.834: INFO: Got endpoints: latency-svc-mrmnx [81.516505ms]
Jan 17 14:59:59.861: INFO: Created: latency-svc-rlrhc
Jan 17 14:59:59.870: INFO: Got endpoints: latency-svc-rlrhc [118.064121ms]
Jan 17 14:59:59.881: INFO: Created: latency-svc-mc6qs
Jan 17 14:59:59.913: INFO: Got endpoints: latency-svc-mc6qs [160.8402ms]
Jan 17 14:59:59.914: INFO: Created: latency-svc-wwpv6
Jan 17 14:59:59.934: INFO: Got endpoints: latency-svc-wwpv6 [181.70928ms]
Jan 17 14:59:59.944: INFO: Created: latency-svc-mdpxh
Jan 17 14:59:59.955: INFO: Got endpoints: latency-svc-mdpxh [202.609942ms]
Jan 17 14:59:59.968: INFO: Created: latency-svc-2jcn2
Jan 17 14:59:59.973: INFO: Created: latency-svc-jljhj
Jan 17 14:59:59.982: INFO: Got endpoints: latency-svc-2jcn2 [229.597833ms]
Jan 17 14:59:59.990: INFO: Got endpoints: latency-svc-jljhj [255.281074ms]
Jan 17 14:59:59.990: INFO: Created: latency-svc-4sxx5
Jan 17 14:59:59.993: INFO: Got endpoints: latency-svc-4sxx5 [241.279458ms]
Jan 17 15:00:00.010: INFO: Created: latency-svc-cd5z7
Jan 17 15:00:00.024: INFO: Got endpoints: latency-svc-cd5z7 [270.696617ms]
Jan 17 15:00:00.024: INFO: Created: latency-svc-dzgcd
Jan 17 15:00:00.034: INFO: Got endpoints: latency-svc-dzgcd [281.47344ms]
Jan 17 15:00:00.037: INFO: Created: latency-svc-g8xld
Jan 17 15:00:00.050: INFO: Got endpoints: latency-svc-g8xld [297.399056ms]
Jan 17 15:00:00.058: INFO: Created: latency-svc-tfbkc
Jan 17 15:00:00.058: INFO: Got endpoints: latency-svc-tfbkc [278.275805ms]
Jan 17 15:00:00.066: INFO: Created: latency-svc-pbxdl
Jan 17 15:00:00.070: INFO: Created: latency-svc-qm9lv
Jan 17 15:00:00.074: INFO: Got endpoints: latency-svc-pbxdl [282.545474ms]
Jan 17 15:00:00.078: INFO: Got endpoints: latency-svc-qm9lv [274.117685ms]
Jan 17 15:00:00.083: INFO: Created: latency-svc-w6gg2
Jan 17 15:00:00.106: INFO: Got endpoints: latency-svc-w6gg2 [293.603825ms]
Jan 17 15:00:00.107: INFO: Created: latency-svc-6zmks
Jan 17 15:00:00.117: INFO: Got endpoints: latency-svc-6zmks [283.261686ms]
Jan 17 15:00:00.126: INFO: Created: latency-svc-4z92j
Jan 17 15:00:00.135: INFO: Created: latency-svc-lcbnr
Jan 17 15:00:00.136: INFO: Got endpoints: latency-svc-4z92j [265.649975ms]
Jan 17 15:00:00.148: INFO: Got endpoints: latency-svc-lcbnr [234.731695ms]
Jan 17 15:00:00.148: INFO: Created: latency-svc-nvllp
Jan 17 15:00:00.156: INFO: Got endpoints: latency-svc-nvllp [222.079252ms]
Jan 17 15:00:00.165: INFO: Created: latency-svc-vqv9s
Jan 17 15:00:00.179: INFO: Got endpoints: latency-svc-vqv9s [223.777537ms]
Jan 17 15:00:00.180: INFO: Created: latency-svc-wczk9
Jan 17 15:00:00.190: INFO: Got endpoints: latency-svc-wczk9 [207.732438ms]
Jan 17 15:00:00.200: INFO: Created: latency-svc-jn8r2
Jan 17 15:00:00.212: INFO: Created: latency-svc-tl9bz
Jan 17 15:00:00.215: INFO: Got endpoints: latency-svc-jn8r2 [225.215769ms]
Jan 17 15:00:00.222: INFO: Got endpoints: latency-svc-tl9bz [228.553719ms]
Jan 17 15:00:00.223: INFO: Created: latency-svc-47czc
Jan 17 15:00:00.231: INFO: Got endpoints: latency-svc-47czc [206.984692ms]
Jan 17 15:00:00.243: INFO: Created: latency-svc-npg5n
Jan 17 15:00:00.263: INFO: Got endpoints: latency-svc-npg5n [228.936138ms]
Jan 17 15:00:00.267: INFO: Created: latency-svc-s55cd
Jan 17 15:00:00.276: INFO: Got endpoints: latency-svc-s55cd [226.112552ms]
Jan 17 15:00:00.277: INFO: Created: latency-svc-bft9h
Jan 17 15:00:00.291: INFO: Got endpoints: latency-svc-bft9h [232.87776ms]
Jan 17 15:00:00.292: INFO: Created: latency-svc-clp8g
Jan 17 15:00:00.302: INFO: Got endpoints: latency-svc-clp8g [227.399302ms]
Jan 17 15:00:00.314: INFO: Created: latency-svc-d9skh
Jan 17 15:00:00.320: INFO: Created: latency-svc-2r4w2
Jan 17 15:00:00.326: INFO: Got endpoints: latency-svc-d9skh [247.314105ms]
Jan 17 15:00:00.330: INFO: Got endpoints: latency-svc-2r4w2 [224.688393ms]
Jan 17 15:00:00.337: INFO: Created: latency-svc-qhrct
Jan 17 15:00:00.346: INFO: Got endpoints: latency-svc-qhrct [228.841541ms]
Jan 17 15:00:00.354: INFO: Created: latency-svc-nd5q8
Jan 17 15:00:00.371: INFO: Created: latency-svc-qn7wt
Jan 17 15:00:00.372: INFO: Got endpoints: latency-svc-nd5q8 [236.035497ms]
Jan 17 15:00:00.378: INFO: Got endpoints: latency-svc-qn7wt [229.802776ms]
Jan 17 15:00:00.387: INFO: Created: latency-svc-2bm5z
Jan 17 15:00:00.394: INFO: Got endpoints: latency-svc-2bm5z [238.216822ms]
Jan 17 15:00:00.402: INFO: Created: latency-svc-jdr2k
Jan 17 15:00:00.416: INFO: Got endpoints: latency-svc-jdr2k [236.633736ms]
Jan 17 15:00:00.425: INFO: Created: latency-svc-rjvps
Jan 17 15:00:00.440: INFO: Got endpoints: latency-svc-rjvps [249.810417ms]
Jan 17 15:00:00.440: INFO: Created: latency-svc-2p5b4
Jan 17 15:00:00.449: INFO: Got endpoints: latency-svc-2p5b4 [234.185323ms]
Jan 17 15:00:00.455: INFO: Created: latency-svc-dmm5j
Jan 17 15:00:00.465: INFO: Created: latency-svc-ndr8t
Jan 17 15:00:00.466: INFO: Got endpoints: latency-svc-dmm5j [244.055449ms]
Jan 17 15:00:00.478: INFO: Got endpoints: latency-svc-ndr8t [246.89893ms]
Jan 17 15:00:00.491: INFO: Created: latency-svc-c9x4k
Jan 17 15:00:00.498: INFO: Got endpoints: latency-svc-c9x4k [235.052989ms]
Jan 17 15:00:00.498: INFO: Created: latency-svc-8hwqx
Jan 17 15:00:00.507: INFO: Got endpoints: latency-svc-8hwqx [230.299235ms]
Jan 17 15:00:00.511: INFO: Created: latency-svc-q2rsb
Jan 17 15:00:00.519: INFO: Got endpoints: latency-svc-q2rsb [227.933618ms]
Jan 17 15:00:00.523: INFO: Created: latency-svc-p5x74
Jan 17 15:00:00.529: INFO: Created: latency-svc-45bhq
Jan 17 15:00:00.532: INFO: Got endpoints: latency-svc-p5x74 [229.798854ms]
Jan 17 15:00:00.542: INFO: Got endpoints: latency-svc-45bhq [215.95143ms]
Jan 17 15:00:00.544: INFO: Created: latency-svc-pkqr8
Jan 17 15:00:00.551: INFO: Got endpoints: latency-svc-pkqr8 [220.100501ms]
Jan 17 15:00:00.555: INFO: Created: latency-svc-f4fdr
Jan 17 15:00:00.570: INFO: Got endpoints: latency-svc-f4fdr [223.519276ms]
Jan 17 15:00:00.582: INFO: Created: latency-svc-d8rhl
Jan 17 15:00:00.589: INFO: Got endpoints: latency-svc-d8rhl [216.771866ms]
Jan 17 15:00:00.598: INFO: Created: latency-svc-vqk7h
Jan 17 15:00:00.610: INFO: Got endpoints: latency-svc-vqk7h [232.248842ms]
Jan 17 15:00:00.610: INFO: Created: latency-svc-dzq72
Jan 17 15:00:00.627: INFO: Got endpoints: latency-svc-dzq72 [232.658485ms]
Jan 17 15:00:00.629: INFO: Created: latency-svc-rsbj5
Jan 17 15:00:00.640: INFO: Got endpoints: latency-svc-rsbj5 [224.011999ms]
Jan 17 15:00:00.646: INFO: Created: latency-svc-lh6bs
Jan 17 15:00:00.669: INFO: Got endpoints: latency-svc-lh6bs [228.783446ms]
Jan 17 15:00:00.669: INFO: Created: latency-svc-5qrw5
Jan 17 15:00:00.678: INFO: Got endpoints: latency-svc-5qrw5 [228.255499ms]
Jan 17 15:00:00.687: INFO: Created: latency-svc-wggf9
Jan 17 15:00:00.687: INFO: Got endpoints: latency-svc-wggf9 [221.185032ms]
Jan 17 15:00:00.690: INFO: Created: latency-svc-6xlf2
Jan 17 15:00:00.698: INFO: Got endpoints: latency-svc-6xlf2 [219.843545ms]
Jan 17 15:00:00.702: INFO: Created: latency-svc-mwcnb
Jan 17 15:00:00.710: INFO: Got endpoints: latency-svc-mwcnb [212.103678ms]
Jan 17 15:00:00.714: INFO: Created: latency-svc-p92hd
Jan 17 15:00:00.720: INFO: Got endpoints: latency-svc-p92hd [213.639415ms]
Jan 17 15:00:00.742: INFO: Created: latency-svc-8zm4d
Jan 17 15:00:00.742: INFO: Created: latency-svc-kdlf6
Jan 17 15:00:00.742: INFO: Got endpoints: latency-svc-kdlf6 [210.118319ms]
Jan 17 15:00:00.755: INFO: Created: latency-svc-64mg2
Jan 17 15:00:00.760: INFO: Got endpoints: latency-svc-8zm4d [240.883611ms]
Jan 17 15:00:00.761: INFO: Created: latency-svc-pmg4j
Jan 17 15:00:00.786: INFO: Created: latency-svc-tdr7f
Jan 17 15:00:00.786: INFO: Got endpoints: latency-svc-pmg4j [235.47687ms]
Jan 17 15:00:00.786: INFO: Got endpoints: latency-svc-64mg2 [244.697118ms]
Jan 17 15:00:00.792: INFO: Got endpoints: latency-svc-tdr7f [222.247709ms]
Jan 17 15:00:00.796: INFO: Created: latency-svc-jgwnx
Jan 17 15:00:00.804: INFO: Created: latency-svc-wbrkb
Jan 17 15:00:00.807: INFO: Got endpoints: latency-svc-jgwnx [217.708199ms]
Jan 17 15:00:00.815: INFO: Created: latency-svc-k49dc
Jan 17 15:00:00.816: INFO: Got endpoints: latency-svc-wbrkb [205.891825ms]
Jan 17 15:00:00.824: INFO: Got endpoints: latency-svc-k49dc [197.321832ms]
Jan 17 15:00:00.830: INFO: Created: latency-svc-ltpm9
Jan 17 15:00:00.842: INFO: Got endpoints: latency-svc-ltpm9 [202.443467ms]
Jan 17 15:00:00.843: INFO: Created: latency-svc-nb7f4
Jan 17 15:00:00.853: INFO: Created: latency-svc-5wvjl
Jan 17 15:00:00.865: INFO: Got endpoints: latency-svc-5wvjl [187.169584ms]
Jan 17 15:00:00.865: INFO: Got endpoints: latency-svc-nb7f4 [196.114594ms]
Jan 17 15:00:00.878: INFO: Created: latency-svc-5sdqg
Jan 17 15:00:00.886: INFO: Got endpoints: latency-svc-5sdqg [198.834643ms]
Jan 17 15:00:00.887: INFO: Created: latency-svc-5pd8f
Jan 17 15:00:00.904: INFO: Got endpoints: latency-svc-5pd8f [205.897257ms]
Jan 17 15:00:00.906: INFO: Created: latency-svc-kd429
Jan 17 15:00:00.917: INFO: Got endpoints: latency-svc-kd429 [206.214792ms]
Jan 17 15:00:00.917: INFO: Created: latency-svc-qjpw7
Jan 17 15:00:00.929: INFO: Got endpoints: latency-svc-qjpw7 [208.277788ms]
Jan 17 15:00:00.954: INFO: Created: latency-svc-cjr2z
Jan 17 15:00:00.965: INFO: Got endpoints: latency-svc-cjr2z [204.819828ms]
Jan 17 15:00:00.966: INFO: Created: latency-svc-mlhst
Jan 17 15:00:00.977: INFO: Got endpoints: latency-svc-mlhst [191.17166ms]
Jan 17 15:00:00.981: INFO: Created: latency-svc-g4285
Jan 17 15:00:00.999: INFO: Got endpoints: latency-svc-g4285 [212.417413ms]
Jan 17 15:00:00.999: INFO: Created: latency-svc-fpd6m
Jan 17 15:00:01.005: INFO: Got endpoints: latency-svc-fpd6m [213.446285ms]
Jan 17 15:00:01.012: INFO: Created: latency-svc-4gl7r
Jan 17 15:00:01.019: INFO: Got endpoints: latency-svc-4gl7r [212.138228ms]
Jan 17 15:00:01.021: INFO: Created: latency-svc-vflqz
Jan 17 15:00:01.038: INFO: Got endpoints: latency-svc-vflqz [221.93003ms]
Jan 17 15:00:01.038: INFO: Created: latency-svc-rw8m6
Jan 17 15:00:01.046: INFO: Created: latency-svc-82jvg
Jan 17 15:00:01.056: INFO: Got endpoints: latency-svc-rw8m6 [231.700817ms]
Jan 17 15:00:01.067: INFO: Got endpoints: latency-svc-82jvg [224.48486ms]
Jan 17 15:00:01.067: INFO: Created: latency-svc-t2pxf
Jan 17 15:00:01.070: INFO: Got endpoints: latency-svc-t2pxf [205.599951ms]
Jan 17 15:00:01.086: INFO: Created: latency-svc-trd49
Jan 17 15:00:01.091: INFO: Got endpoints: latency-svc-trd49 [225.880157ms]
Jan 17 15:00:01.092: INFO: Created: latency-svc-brc98
Jan 17 15:00:01.106: INFO: Got endpoints: latency-svc-brc98 [220.225168ms]
Jan 17 15:00:01.120: INFO: Created: latency-svc-pw6q9
Jan 17 15:00:01.122: INFO: Got endpoints: latency-svc-pw6q9 [218.883205ms]
Jan 17 15:00:01.127: INFO: Created: latency-svc-wr8wp
Jan 17 15:00:01.136: INFO: Created: latency-svc-fwf9k
Jan 17 15:00:01.142: INFO: Got endpoints: latency-svc-wr8wp [399.793652ms]
Jan 17 15:00:01.148: INFO: Got endpoints: latency-svc-fwf9k [230.877713ms]
Jan 17 15:00:01.153: INFO: Created: latency-svc-bld6v
Jan 17 15:00:01.166: INFO: Got endpoints: latency-svc-bld6v [237.427906ms]
Jan 17 15:00:01.168: INFO: Created: latency-svc-mwp9s
Jan 17 15:00:01.177: INFO: Got endpoints: latency-svc-mwp9s [211.68945ms]
Jan 17 15:00:01.180: INFO: Created: latency-svc-dwwmt
Jan 17 15:00:01.194: INFO: Got endpoints: latency-svc-dwwmt [216.764653ms]
Jan 17 15:00:01.194: INFO: Created: latency-svc-zhcv4
Jan 17 15:00:01.198: INFO: Got endpoints: latency-svc-zhcv4 [198.926036ms]
Jan 17 15:00:01.203: INFO: Created: latency-svc-b8k72
Jan 17 15:00:01.215: INFO: Got endpoints: latency-svc-b8k72 [209.831106ms]
Jan 17 15:00:01.216: INFO: Created: latency-svc-wxhfq
Jan 17 15:00:01.229: INFO: Got endpoints: latency-svc-wxhfq [210.243198ms]
Jan 17 15:00:01.234: INFO: Created: latency-svc-jnxdg
Jan 17 15:00:01.239: INFO: Got endpoints: latency-svc-jnxdg [201.665931ms]
Jan 17 15:00:01.250: INFO: Created: latency-svc-9t9vs
Jan 17 15:00:01.258: INFO: Got endpoints: latency-svc-9t9vs [201.875505ms]
Jan 17 15:00:01.266: INFO: Created: latency-svc-bz6p7
Jan 17 15:00:01.287: INFO: Created: latency-svc-t7rmq
Jan 17 15:00:01.289: INFO: Got endpoints: latency-svc-bz6p7 [222.875156ms]
Jan 17 15:00:01.298: INFO: Got endpoints: latency-svc-t7rmq [227.994481ms]
Jan 17 15:00:01.312: INFO: Created: latency-svc-n22lb
Jan 17 15:00:01.317: INFO: Created: latency-svc-qljb9
Jan 17 15:00:01.321: INFO: Got endpoints: latency-svc-n22lb [230.506656ms]
Jan 17 15:00:01.328: INFO: Got endpoints: latency-svc-qljb9 [221.4219ms]
Jan 17 15:00:01.335: INFO: Created: latency-svc-7wt7l
Jan 17 15:00:01.342: INFO: Created: latency-svc-rdldt
Jan 17 15:00:01.355: INFO: Got endpoints: latency-svc-rdldt [213.027449ms]
Jan 17 15:00:01.355: INFO: Got endpoints: latency-svc-7wt7l [232.307327ms]
Jan 17 15:00:01.355: INFO: Created: latency-svc-w69rs
Jan 17 15:00:01.375: INFO: Got endpoints: latency-svc-w69rs [226.997081ms]
Jan 17 15:00:01.388: INFO: Created: latency-svc-4q87f
Jan 17 15:00:01.404: INFO: Created: latency-svc-lsms7
Jan 17 15:00:01.417: INFO: Got endpoints: latency-svc-4q87f [250.774861ms]
Jan 17 15:00:01.417: INFO: Got endpoints: latency-svc-lsms7 [240.136398ms]
Jan 17 15:00:01.427: INFO: Created: latency-svc-g4h7k
Jan 17 15:00:01.439: INFO: Got endpoints: latency-svc-g4h7k [244.668356ms]
Jan 17 15:00:01.443: INFO: Created: latency-svc-tssj2
Jan 17 15:00:01.473: INFO: Got endpoints: latency-svc-tssj2 [274.985877ms]
Jan 17 15:00:01.500: INFO: Created: latency-svc-whwxf
Jan 17 15:00:01.512: INFO: Got endpoints: latency-svc-whwxf [297.186627ms]
Jan 17 15:00:01.512: INFO: Created: latency-svc-q4j5x
Jan 17 15:00:01.524: INFO: Created: latency-svc-r97tg
Jan 17 15:00:01.536: INFO: Got endpoints: latency-svc-q4j5x [306.70661ms]
Jan 17 15:00:01.536: INFO: Created: latency-svc-nf548
Jan 17 15:00:01.542: INFO: Got endpoints: latency-svc-r97tg [302.57542ms]
Jan 17 15:00:01.545: INFO: Got endpoints: latency-svc-nf548 [287.010524ms]
Jan 17 15:00:01.549: INFO: Created: latency-svc-z56vk
Jan 17 15:00:01.560: INFO: Created: latency-svc-fxmgp
Jan 17 15:00:01.560: INFO: Got endpoints: latency-svc-z56vk [270.480463ms]
Jan 17 15:00:01.584: INFO: Got endpoints: latency-svc-fxmgp [285.530473ms]
Jan 17 15:00:01.591: INFO: Created: latency-svc-nj6w7
Jan 17 15:00:01.602: INFO: Got endpoints: latency-svc-nj6w7 [280.387224ms]
Jan 17 15:00:01.604: INFO: Created: latency-svc-5v85q
Jan 17 15:00:01.615: INFO: Created: latency-svc-str4m
Jan 17 15:00:01.616: INFO: Got endpoints: latency-svc-5v85q [288.488215ms]
Jan 17 15:00:01.629: INFO: Got endpoints: latency-svc-str4m [274.12741ms]
Jan 17 15:00:01.633: INFO: Created: latency-svc-f8gcw
Jan 17 15:00:01.640: INFO: Got endpoints: latency-svc-f8gcw [285.058391ms]
Jan 17 15:00:01.640: INFO: Created: latency-svc-t76tm
Jan 17 15:00:01.647: INFO: Got endpoints: latency-svc-t76tm [272.098378ms]
Jan 17 15:00:01.647: INFO: Created: latency-svc-8sswx
Jan 17 15:00:01.653: INFO: Got endpoints: latency-svc-8sswx [236.39995ms]
Jan 17 15:00:01.763: INFO: Created: latency-svc-kcx7m
Jan 17 15:00:01.764: INFO: Created: latency-svc-nj7gv
Jan 17 15:00:01.765: INFO: Created: latency-svc-72vkt
Jan 17 15:00:01.767: INFO: Created: latency-svc-9pv9s
Jan 17 15:00:01.768: INFO: Created: latency-svc-rpkzl
Jan 17 15:00:01.768: INFO: Created: latency-svc-qvzbr
Jan 17 15:00:01.769: INFO: Created: latency-svc-j2jb2
Jan 17 15:00:01.769: INFO: Created: latency-svc-vwz4x
Jan 17 15:00:01.769: INFO: Created: latency-svc-h4cgr
Jan 17 15:00:01.769: INFO: Created: latency-svc-jjt44
Jan 17 15:00:01.773: INFO: Created: latency-svc-lvp42
Jan 17 15:00:01.774: INFO: Created: latency-svc-j24s9
Jan 17 15:00:01.774: INFO: Created: latency-svc-w75fv
Jan 17 15:00:01.776: INFO: Created: latency-svc-mt8zc
Jan 17 15:00:01.776: INFO: Created: latency-svc-w8p7f
Jan 17 15:00:01.778: INFO: Got endpoints: latency-svc-kcx7m [232.775371ms]
Jan 17 15:00:01.779: INFO: Got endpoints: latency-svc-nj7gv [305.805497ms]
Jan 17 15:00:01.781: INFO: Got endpoints: latency-svc-h4cgr [140.851597ms]
Jan 17 15:00:01.781: INFO: Got endpoints: latency-svc-72vkt [127.537866ms]
Jan 17 15:00:01.783: INFO: Got endpoints: latency-svc-jjt44 [166.950617ms]
Jan 17 15:00:01.791: INFO: Got endpoints: latency-svc-9pv9s [373.818199ms]
Jan 17 15:00:01.802: INFO: Got endpoints: latency-svc-vwz4x [363.174332ms]
Jan 17 15:00:01.802: INFO: Got endpoints: latency-svc-qvzbr [155.289377ms]
Jan 17 15:00:01.802: INFO: Got endpoints: latency-svc-rpkzl [289.963537ms]
Jan 17 15:00:01.805: INFO: Created: latency-svc-52prk
Jan 17 15:00:01.814: INFO: Got endpoints: latency-svc-w75fv [211.890801ms]
Jan 17 15:00:01.814: INFO: Got endpoints: latency-svc-j2jb2 [271.596158ms]
Jan 17 15:00:01.821: INFO: Got endpoints: latency-svc-j24s9 [237.24022ms]
Jan 17 15:00:01.831: INFO: Got endpoints: latency-svc-52prk [52.936967ms]
Jan 17 15:00:01.831: INFO: Got endpoints: latency-svc-lvp42 [270.855162ms]
Jan 17 15:00:01.831: INFO: Got endpoints: latency-svc-w8p7f [295.010732ms]
Jan 17 15:00:01.831: INFO: Got endpoints: latency-svc-mt8zc [202.109189ms]
Jan 17 15:00:01.835: INFO: Created: latency-svc-j44nn
Jan 17 15:00:01.851: INFO: Got endpoints: latency-svc-j44nn [71.933914ms]
Jan 17 15:00:01.856: INFO: Created: latency-svc-krt58
Jan 17 15:00:01.869: INFO: Got endpoints: latency-svc-krt58 [88.126072ms]
Jan 17 15:00:01.869: INFO: Created: latency-svc-tcqpf
Jan 17 15:00:01.876: INFO: Got endpoints: latency-svc-tcqpf [95.414273ms]
Jan 17 15:00:01.883: INFO: Created: latency-svc-k4v7m
Jan 17 15:00:01.896: INFO: Created: latency-svc-thh5z
Jan 17 15:00:01.896: INFO: Got endpoints: latency-svc-k4v7m [112.709662ms]
Jan 17 15:00:01.907: INFO: Got endpoints: latency-svc-thh5z [116.086934ms]
Jan 17 15:00:01.910: INFO: Created: latency-svc-mgszx
Jan 17 15:00:01.922: INFO: Got endpoints: latency-svc-mgszx [119.733053ms]
Jan 17 15:00:01.922: INFO: Created: latency-svc-d4fjd
Jan 17 15:00:01.942: INFO: Got endpoints: latency-svc-d4fjd [139.64654ms]
Jan 17 15:00:01.942: INFO: Created: latency-svc-wg56b
Jan 17 15:00:01.945: INFO: Got endpoints: latency-svc-wg56b [143.086138ms]
Jan 17 15:00:01.961: INFO: Created: latency-svc-cq88f
Jan 17 15:00:01.965: INFO: Got endpoints: latency-svc-cq88f [151.227434ms]
Jan 17 15:00:01.965: INFO: Created: latency-svc-rwvzv
Jan 17 15:00:01.978: INFO: Created: latency-svc-lhj47
Jan 17 15:00:01.987: INFO: Got endpoints: latency-svc-rwvzv [173.105983ms]
Jan 17 15:00:01.993: INFO: Got endpoints: latency-svc-lhj47 [171.584277ms]
Jan 17 15:00:01.996: INFO: Created: latency-svc-mmxx7
Jan 17 15:00:02.004: INFO: Got endpoints: latency-svc-mmxx7 [173.553553ms]
Jan 17 15:00:02.021: INFO: Created: latency-svc-7hdbw
Jan 17 15:00:02.028: INFO: Created: latency-svc-mvgzk
Jan 17 15:00:02.030: INFO: Got endpoints: latency-svc-7hdbw [199.0174ms]
Jan 17 15:00:02.041: INFO: Got endpoints: latency-svc-mvgzk [210.165146ms]
Jan 17 15:00:02.047: INFO: Created: latency-svc-6lg9h
Jan 17 15:00:02.060: INFO: Created: latency-svc-cdjtn
Jan 17 15:00:02.060: INFO: Got endpoints: latency-svc-6lg9h [228.982278ms]
Jan 17 15:00:02.068: INFO: Got endpoints: latency-svc-cdjtn [217.152977ms]
Jan 17 15:00:02.082: INFO: Created: latency-svc-9hkll
Jan 17 15:00:02.082: INFO: Created: latency-svc-68nm7
Jan 17 15:00:02.085: INFO: Got endpoints: latency-svc-9hkll [216.021129ms]
Jan 17 15:00:02.090: INFO: Created: latency-svc-sz9tz
Jan 17 15:00:02.102: INFO: Got endpoints: latency-svc-68nm7 [225.841612ms]
Jan 17 15:00:02.102: INFO: Got endpoints: latency-svc-sz9tz [206.347357ms]
Jan 17 15:00:02.113: INFO: Created: latency-svc-jcbn5
Jan 17 15:00:02.124: INFO: Got endpoints: latency-svc-jcbn5 [217.231319ms]
Jan 17 15:00:02.126: INFO: Created: latency-svc-lqccf
Jan 17 15:00:02.136: INFO: Got endpoints: latency-svc-lqccf [214.503709ms]
Jan 17 15:00:02.143: INFO: Created: latency-svc-w6xzt
Jan 17 15:00:02.149: INFO: Created: latency-svc-2hzvg
Jan 17 15:00:02.154: INFO: Got endpoints: latency-svc-w6xzt [212.17244ms]
Jan 17 15:00:02.159: INFO: Created: latency-svc-k4kkw
Jan 17 15:00:02.161: INFO: Got endpoints: latency-svc-2hzvg [215.39277ms]
Jan 17 15:00:02.166: INFO: Got endpoints: latency-svc-k4kkw [201.367473ms]
Jan 17 15:00:02.171: INFO: Created: latency-svc-v6tl6
Jan 17 15:00:02.179: INFO: Created: latency-svc-wr2d4
Jan 17 15:00:02.186: INFO: Got endpoints: latency-svc-v6tl6 [198.943518ms]
Jan 17 15:00:02.187: INFO: Got endpoints: latency-svc-wr2d4 [194.087ms]
Jan 17 15:00:02.190: INFO: Created: latency-svc-m5jc2
Jan 17 15:00:02.199: INFO: Created: latency-svc-b8wnf
Jan 17 15:00:02.203: INFO: Got endpoints: latency-svc-m5jc2 [198.657574ms]
Jan 17 15:00:02.222: INFO: Got endpoints: latency-svc-b8wnf [192.271935ms]
Jan 17 15:00:02.223: INFO: Created: latency-svc-fdbdk
Jan 17 15:00:02.229: INFO: Created: latency-svc-cbbzw
Jan 17 15:00:02.244: INFO: Got endpoints: latency-svc-fdbdk [202.691616ms]
Jan 17 15:00:02.244: INFO: Got endpoints: latency-svc-cbbzw [184.331898ms]
Jan 17 15:00:02.257: INFO: Created: latency-svc-w49sh
Jan 17 15:00:02.266: INFO: Got endpoints: latency-svc-w49sh [198.051731ms]
Jan 17 15:00:02.269: INFO: Created: latency-svc-lwpwq
Jan 17 15:00:02.278: INFO: Got endpoints: latency-svc-lwpwq [192.806088ms]
Jan 17 15:00:02.279: INFO: Created: latency-svc-zmwn6
Jan 17 15:00:02.289: INFO: Got endpoints: latency-svc-zmwn6 [186.46429ms]
Jan 17 15:00:02.293: INFO: Created: latency-svc-mcckg
Jan 17 15:00:02.303: INFO: Got endpoints: latency-svc-mcckg [201.002847ms]
Jan 17 15:00:02.304: INFO: Created: latency-svc-wp5bp
Jan 17 15:00:02.310: INFO: Got endpoints: latency-svc-wp5bp [185.338156ms]
Jan 17 15:00:02.320: INFO: Created: latency-svc-xjscf
Jan 17 15:00:02.330: INFO: Got endpoints: latency-svc-xjscf [193.408775ms]
Jan 17 15:00:02.332: INFO: Created: latency-svc-rmlx7
Jan 17 15:00:02.361: INFO: Got endpoints: latency-svc-rmlx7 [206.612343ms]
Jan 17 15:00:02.361: INFO: Created: latency-svc-jzvgd
Jan 17 15:00:02.372: INFO: Got endpoints: latency-svc-jzvgd [210.809037ms]
Jan 17 15:00:02.372: INFO: Created: latency-svc-6npm6
Jan 17 15:00:02.383: INFO: Created: latency-svc-jlw99
Jan 17 15:00:02.383: INFO: Got endpoints: latency-svc-6npm6 [216.539128ms]
Jan 17 15:00:02.390: INFO: Got endpoints: latency-svc-jlw99 [203.663846ms]
Jan 17 15:00:02.402: INFO: Created: latency-svc-952wn
Jan 17 15:00:02.408: INFO: Created: latency-svc-n2snp
Jan 17 15:00:02.410: INFO: Got endpoints: latency-svc-952wn [223.007434ms]
Jan 17 15:00:02.415: INFO: Got endpoints: latency-svc-n2snp [212.143777ms]
Jan 17 15:00:02.430: INFO: Created: latency-svc-48d46
Jan 17 15:00:02.452: INFO: Got endpoints: latency-svc-48d46 [207.747299ms]
Jan 17 15:00:02.452: INFO: Created: latency-svc-2bbv9
Jan 17 15:00:02.457: INFO: Created: latency-svc-ntl4c
Jan 17 15:00:02.459: INFO: Got endpoints: latency-svc-2bbv9 [214.897013ms]
Jan 17 15:00:02.473: INFO: Got endpoints: latency-svc-ntl4c [206.853965ms]
Jan 17 15:00:02.473: INFO: Created: latency-svc-tbzjl
Jan 17 15:00:02.490: INFO: Got endpoints: latency-svc-tbzjl [211.942458ms]
Jan 17 15:00:02.490: INFO: Created: latency-svc-8zstw
Jan 17 15:00:02.499: INFO: Got endpoints: latency-svc-8zstw [210.094995ms]
Jan 17 15:00:02.499: INFO: Created: latency-svc-8q5ws
Jan 17 15:00:02.518: INFO: Created: latency-svc-xf9cp
Jan 17 15:00:02.518: INFO: Got endpoints: latency-svc-xf9cp [208.464658ms]
Jan 17 15:00:02.518: INFO: Got endpoints: latency-svc-8q5ws [214.947272ms]
Jan 17 15:00:02.520: INFO: Created: latency-svc-8c9ch
Jan 17 15:00:02.533: INFO: Created: latency-svc-d9g7g
Jan 17 15:00:02.533: INFO: Got endpoints: latency-svc-8c9ch [203.680958ms]
Jan 17 15:00:02.559: INFO: Got endpoints: latency-svc-d9g7g [198.480837ms]
Jan 17 15:00:02.560: INFO: Created: latency-svc-99r72
Jan 17 15:00:02.567: INFO: Got endpoints: latency-svc-99r72 [195.230474ms]
Jan 17 15:00:02.571: INFO: Created: latency-svc-7w4dk
Jan 17 15:00:02.580: INFO: Got endpoints: latency-svc-7w4dk [197.041453ms]
Jan 17 15:00:02.595: INFO: Created: latency-svc-jpshh
Jan 17 15:00:02.595: INFO: Got endpoints: latency-svc-jpshh [205.548168ms]
Jan 17 15:00:02.600: INFO: Created: latency-svc-9c4cs
Jan 17 15:00:02.601: INFO: Created: latency-svc-mnv6s
Jan 17 15:00:02.611: INFO: Got endpoints: latency-svc-mnv6s [389.054956ms]
Jan 17 15:00:02.611: INFO: Got endpoints: latency-svc-9c4cs [201.430476ms]
Jan 17 15:00:02.614: INFO: Created: latency-svc-plxs5
Jan 17 15:00:02.620: INFO: Got endpoints: latency-svc-plxs5 [204.579648ms]
Jan 17 15:00:02.623: INFO: Created: latency-svc-lr5pn
Jan 17 15:00:02.631: INFO: Created: latency-svc-wmpdd
Jan 17 15:00:02.632: INFO: Got endpoints: latency-svc-lr5pn [179.876171ms]
Jan 17 15:00:02.648: INFO: Got endpoints: latency-svc-wmpdd [189.066973ms]
Jan 17 15:00:02.657: INFO: Created: latency-svc-9pn2f
Jan 17 15:00:02.669: INFO: Got endpoints: latency-svc-9pn2f [196.539843ms]
Jan 17 15:00:02.675: INFO: Created: latency-svc-tzhvq
Jan 17 15:00:02.677: INFO: Created: latency-svc-27jlm
Jan 17 15:00:02.685: INFO: Got endpoints: latency-svc-tzhvq [186.296035ms]
Jan 17 15:00:02.687: INFO: Got endpoints: latency-svc-27jlm [196.885803ms]
Jan 17 15:00:02.704: INFO: Created: latency-svc-8ql7n
Jan 17 15:00:02.710: INFO: Got endpoints: latency-svc-8ql7n [191.839664ms]
Jan 17 15:00:02.714: INFO: Created: latency-svc-pws8b
Jan 17 15:00:02.723: INFO: Got endpoints: latency-svc-pws8b [204.254117ms]
Jan 17 15:00:02.723: INFO: Latencies: [28.664762ms 40.166849ms 52.275503ms 52.936967ms 59.88329ms 71.933914ms 81.516505ms 88.126072ms 95.414273ms 112.709662ms 116.086934ms 118.064121ms 119.733053ms 127.537866ms 139.64654ms 140.851597ms 143.086138ms 151.227434ms 155.289377ms 160.8402ms 166.950617ms 171.584277ms 173.105983ms 173.553553ms 179.876171ms 181.70928ms 184.331898ms 185.338156ms 186.296035ms 186.46429ms 187.169584ms 189.066973ms 191.17166ms 191.839664ms 192.271935ms 192.806088ms 193.408775ms 194.087ms 195.230474ms 196.114594ms 196.539843ms 196.885803ms 197.041453ms 197.321832ms 198.051731ms 198.480837ms 198.657574ms 198.834643ms 198.926036ms 198.943518ms 199.0174ms 201.002847ms 201.367473ms 201.430476ms 201.665931ms 201.875505ms 202.109189ms 202.443467ms 202.609942ms 202.691616ms 203.663846ms 203.680958ms 204.254117ms 204.579648ms 204.819828ms 205.548168ms 205.599951ms 205.891825ms 205.897257ms 206.214792ms 206.347357ms 206.612343ms 206.853965ms 206.984692ms 207.732438ms 207.747299ms 208.277788ms 208.464658ms 209.831106ms 210.094995ms 210.118319ms 210.165146ms 210.243198ms 210.809037ms 211.68945ms 211.890801ms 211.942458ms 212.103678ms 212.138228ms 212.143777ms 212.17244ms 212.417413ms 213.027449ms 213.446285ms 213.639415ms 214.503709ms 214.897013ms 214.947272ms 215.39277ms 215.95143ms 216.021129ms 216.539128ms 216.764653ms 216.771866ms 217.152977ms 217.231319ms 217.708199ms 218.883205ms 219.843545ms 220.100501ms 220.225168ms 221.185032ms 221.4219ms 221.93003ms 222.079252ms 222.247709ms 222.875156ms 223.007434ms 223.519276ms 223.777537ms 224.011999ms 224.48486ms 224.688393ms 225.215769ms 225.841612ms 225.880157ms 226.112552ms 226.997081ms 227.399302ms 227.933618ms 227.994481ms 228.255499ms 228.553719ms 228.783446ms 228.841541ms 228.936138ms 228.982278ms 229.597833ms 229.798854ms 229.802776ms 230.299235ms 230.506656ms 230.877713ms 231.700817ms 232.248842ms 232.307327ms 232.658485ms 232.775371ms 232.87776ms 234.185323ms 234.731695ms 235.052989ms 235.47687ms 236.035497ms 236.39995ms 236.633736ms 237.24022ms 237.427906ms 238.216822ms 240.136398ms 240.883611ms 241.279458ms 244.055449ms 244.668356ms 244.697118ms 246.89893ms 247.314105ms 249.810417ms 250.774861ms 255.281074ms 265.649975ms 270.480463ms 270.696617ms 270.855162ms 271.596158ms 272.098378ms 274.117685ms 274.12741ms 274.985877ms 278.275805ms 280.387224ms 281.47344ms 282.545474ms 283.261686ms 285.058391ms 285.530473ms 287.010524ms 288.488215ms 289.963537ms 293.603825ms 295.010732ms 297.186627ms 297.399056ms 302.57542ms 305.805497ms 306.70661ms 363.174332ms 373.818199ms 389.054956ms 399.793652ms]
Jan 17 15:00:02.723: INFO: 50 %ile: 216.021129ms
Jan 17 15:00:02.723: INFO: 90 %ile: 280.387224ms
Jan 17 15:00:02.723: INFO: 99 %ile: 389.054956ms
Jan 17 15:00:02.723: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:187
Jan 17 15:00:02.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-9800" for this suite. 01/17/23 15:00:02.731
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","completed":36,"skipped":704,"failed":0}
------------------------------
• [4.220 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 14:59:58.525
    Jan 17 14:59:58.525: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename svc-latency 01/17/23 14:59:58.525
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 14:59:58.55
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 14:59:58.553
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    Jan 17 14:59:58.555: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-9800 01/17/23 14:59:58.555
    W0117 14:59:58.565498      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "svc-latency-rc" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "svc-latency-rc" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "svc-latency-rc" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "svc-latency-rc" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    I0117 14:59:58.565681      22 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-9800, replica count: 1
    I0117 14:59:59.616485      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 17 14:59:59.727: INFO: Created: latency-svc-nsjn9
    Jan 17 14:59:59.734: INFO: Got endpoints: latency-svc-nsjn9 [17.568315ms]
    Jan 17 14:59:59.769: INFO: Created: latency-svc-zgxrg
    Jan 17 14:59:59.780: INFO: Got endpoints: latency-svc-zgxrg [28.664762ms]
    Jan 17 14:59:59.785: INFO: Created: latency-svc-f75n4
    Jan 17 14:59:59.792: INFO: Got endpoints: latency-svc-f75n4 [40.166849ms]
    Jan 17 14:59:59.794: INFO: Created: latency-svc-24vh6
    Jan 17 14:59:59.804: INFO: Got endpoints: latency-svc-24vh6 [52.275503ms]
    Jan 17 14:59:59.805: INFO: Created: latency-svc-lfh7f
    Jan 17 14:59:59.812: INFO: Got endpoints: latency-svc-lfh7f [59.88329ms]
    Jan 17 14:59:59.827: INFO: Created: latency-svc-mrmnx
    Jan 17 14:59:59.834: INFO: Got endpoints: latency-svc-mrmnx [81.516505ms]
    Jan 17 14:59:59.861: INFO: Created: latency-svc-rlrhc
    Jan 17 14:59:59.870: INFO: Got endpoints: latency-svc-rlrhc [118.064121ms]
    Jan 17 14:59:59.881: INFO: Created: latency-svc-mc6qs
    Jan 17 14:59:59.913: INFO: Got endpoints: latency-svc-mc6qs [160.8402ms]
    Jan 17 14:59:59.914: INFO: Created: latency-svc-wwpv6
    Jan 17 14:59:59.934: INFO: Got endpoints: latency-svc-wwpv6 [181.70928ms]
    Jan 17 14:59:59.944: INFO: Created: latency-svc-mdpxh
    Jan 17 14:59:59.955: INFO: Got endpoints: latency-svc-mdpxh [202.609942ms]
    Jan 17 14:59:59.968: INFO: Created: latency-svc-2jcn2
    Jan 17 14:59:59.973: INFO: Created: latency-svc-jljhj
    Jan 17 14:59:59.982: INFO: Got endpoints: latency-svc-2jcn2 [229.597833ms]
    Jan 17 14:59:59.990: INFO: Got endpoints: latency-svc-jljhj [255.281074ms]
    Jan 17 14:59:59.990: INFO: Created: latency-svc-4sxx5
    Jan 17 14:59:59.993: INFO: Got endpoints: latency-svc-4sxx5 [241.279458ms]
    Jan 17 15:00:00.010: INFO: Created: latency-svc-cd5z7
    Jan 17 15:00:00.024: INFO: Got endpoints: latency-svc-cd5z7 [270.696617ms]
    Jan 17 15:00:00.024: INFO: Created: latency-svc-dzgcd
    Jan 17 15:00:00.034: INFO: Got endpoints: latency-svc-dzgcd [281.47344ms]
    Jan 17 15:00:00.037: INFO: Created: latency-svc-g8xld
    Jan 17 15:00:00.050: INFO: Got endpoints: latency-svc-g8xld [297.399056ms]
    Jan 17 15:00:00.058: INFO: Created: latency-svc-tfbkc
    Jan 17 15:00:00.058: INFO: Got endpoints: latency-svc-tfbkc [278.275805ms]
    Jan 17 15:00:00.066: INFO: Created: latency-svc-pbxdl
    Jan 17 15:00:00.070: INFO: Created: latency-svc-qm9lv
    Jan 17 15:00:00.074: INFO: Got endpoints: latency-svc-pbxdl [282.545474ms]
    Jan 17 15:00:00.078: INFO: Got endpoints: latency-svc-qm9lv [274.117685ms]
    Jan 17 15:00:00.083: INFO: Created: latency-svc-w6gg2
    Jan 17 15:00:00.106: INFO: Got endpoints: latency-svc-w6gg2 [293.603825ms]
    Jan 17 15:00:00.107: INFO: Created: latency-svc-6zmks
    Jan 17 15:00:00.117: INFO: Got endpoints: latency-svc-6zmks [283.261686ms]
    Jan 17 15:00:00.126: INFO: Created: latency-svc-4z92j
    Jan 17 15:00:00.135: INFO: Created: latency-svc-lcbnr
    Jan 17 15:00:00.136: INFO: Got endpoints: latency-svc-4z92j [265.649975ms]
    Jan 17 15:00:00.148: INFO: Got endpoints: latency-svc-lcbnr [234.731695ms]
    Jan 17 15:00:00.148: INFO: Created: latency-svc-nvllp
    Jan 17 15:00:00.156: INFO: Got endpoints: latency-svc-nvllp [222.079252ms]
    Jan 17 15:00:00.165: INFO: Created: latency-svc-vqv9s
    Jan 17 15:00:00.179: INFO: Got endpoints: latency-svc-vqv9s [223.777537ms]
    Jan 17 15:00:00.180: INFO: Created: latency-svc-wczk9
    Jan 17 15:00:00.190: INFO: Got endpoints: latency-svc-wczk9 [207.732438ms]
    Jan 17 15:00:00.200: INFO: Created: latency-svc-jn8r2
    Jan 17 15:00:00.212: INFO: Created: latency-svc-tl9bz
    Jan 17 15:00:00.215: INFO: Got endpoints: latency-svc-jn8r2 [225.215769ms]
    Jan 17 15:00:00.222: INFO: Got endpoints: latency-svc-tl9bz [228.553719ms]
    Jan 17 15:00:00.223: INFO: Created: latency-svc-47czc
    Jan 17 15:00:00.231: INFO: Got endpoints: latency-svc-47czc [206.984692ms]
    Jan 17 15:00:00.243: INFO: Created: latency-svc-npg5n
    Jan 17 15:00:00.263: INFO: Got endpoints: latency-svc-npg5n [228.936138ms]
    Jan 17 15:00:00.267: INFO: Created: latency-svc-s55cd
    Jan 17 15:00:00.276: INFO: Got endpoints: latency-svc-s55cd [226.112552ms]
    Jan 17 15:00:00.277: INFO: Created: latency-svc-bft9h
    Jan 17 15:00:00.291: INFO: Got endpoints: latency-svc-bft9h [232.87776ms]
    Jan 17 15:00:00.292: INFO: Created: latency-svc-clp8g
    Jan 17 15:00:00.302: INFO: Got endpoints: latency-svc-clp8g [227.399302ms]
    Jan 17 15:00:00.314: INFO: Created: latency-svc-d9skh
    Jan 17 15:00:00.320: INFO: Created: latency-svc-2r4w2
    Jan 17 15:00:00.326: INFO: Got endpoints: latency-svc-d9skh [247.314105ms]
    Jan 17 15:00:00.330: INFO: Got endpoints: latency-svc-2r4w2 [224.688393ms]
    Jan 17 15:00:00.337: INFO: Created: latency-svc-qhrct
    Jan 17 15:00:00.346: INFO: Got endpoints: latency-svc-qhrct [228.841541ms]
    Jan 17 15:00:00.354: INFO: Created: latency-svc-nd5q8
    Jan 17 15:00:00.371: INFO: Created: latency-svc-qn7wt
    Jan 17 15:00:00.372: INFO: Got endpoints: latency-svc-nd5q8 [236.035497ms]
    Jan 17 15:00:00.378: INFO: Got endpoints: latency-svc-qn7wt [229.802776ms]
    Jan 17 15:00:00.387: INFO: Created: latency-svc-2bm5z
    Jan 17 15:00:00.394: INFO: Got endpoints: latency-svc-2bm5z [238.216822ms]
    Jan 17 15:00:00.402: INFO: Created: latency-svc-jdr2k
    Jan 17 15:00:00.416: INFO: Got endpoints: latency-svc-jdr2k [236.633736ms]
    Jan 17 15:00:00.425: INFO: Created: latency-svc-rjvps
    Jan 17 15:00:00.440: INFO: Got endpoints: latency-svc-rjvps [249.810417ms]
    Jan 17 15:00:00.440: INFO: Created: latency-svc-2p5b4
    Jan 17 15:00:00.449: INFO: Got endpoints: latency-svc-2p5b4 [234.185323ms]
    Jan 17 15:00:00.455: INFO: Created: latency-svc-dmm5j
    Jan 17 15:00:00.465: INFO: Created: latency-svc-ndr8t
    Jan 17 15:00:00.466: INFO: Got endpoints: latency-svc-dmm5j [244.055449ms]
    Jan 17 15:00:00.478: INFO: Got endpoints: latency-svc-ndr8t [246.89893ms]
    Jan 17 15:00:00.491: INFO: Created: latency-svc-c9x4k
    Jan 17 15:00:00.498: INFO: Got endpoints: latency-svc-c9x4k [235.052989ms]
    Jan 17 15:00:00.498: INFO: Created: latency-svc-8hwqx
    Jan 17 15:00:00.507: INFO: Got endpoints: latency-svc-8hwqx [230.299235ms]
    Jan 17 15:00:00.511: INFO: Created: latency-svc-q2rsb
    Jan 17 15:00:00.519: INFO: Got endpoints: latency-svc-q2rsb [227.933618ms]
    Jan 17 15:00:00.523: INFO: Created: latency-svc-p5x74
    Jan 17 15:00:00.529: INFO: Created: latency-svc-45bhq
    Jan 17 15:00:00.532: INFO: Got endpoints: latency-svc-p5x74 [229.798854ms]
    Jan 17 15:00:00.542: INFO: Got endpoints: latency-svc-45bhq [215.95143ms]
    Jan 17 15:00:00.544: INFO: Created: latency-svc-pkqr8
    Jan 17 15:00:00.551: INFO: Got endpoints: latency-svc-pkqr8 [220.100501ms]
    Jan 17 15:00:00.555: INFO: Created: latency-svc-f4fdr
    Jan 17 15:00:00.570: INFO: Got endpoints: latency-svc-f4fdr [223.519276ms]
    Jan 17 15:00:00.582: INFO: Created: latency-svc-d8rhl
    Jan 17 15:00:00.589: INFO: Got endpoints: latency-svc-d8rhl [216.771866ms]
    Jan 17 15:00:00.598: INFO: Created: latency-svc-vqk7h
    Jan 17 15:00:00.610: INFO: Got endpoints: latency-svc-vqk7h [232.248842ms]
    Jan 17 15:00:00.610: INFO: Created: latency-svc-dzq72
    Jan 17 15:00:00.627: INFO: Got endpoints: latency-svc-dzq72 [232.658485ms]
    Jan 17 15:00:00.629: INFO: Created: latency-svc-rsbj5
    Jan 17 15:00:00.640: INFO: Got endpoints: latency-svc-rsbj5 [224.011999ms]
    Jan 17 15:00:00.646: INFO: Created: latency-svc-lh6bs
    Jan 17 15:00:00.669: INFO: Got endpoints: latency-svc-lh6bs [228.783446ms]
    Jan 17 15:00:00.669: INFO: Created: latency-svc-5qrw5
    Jan 17 15:00:00.678: INFO: Got endpoints: latency-svc-5qrw5 [228.255499ms]
    Jan 17 15:00:00.687: INFO: Created: latency-svc-wggf9
    Jan 17 15:00:00.687: INFO: Got endpoints: latency-svc-wggf9 [221.185032ms]
    Jan 17 15:00:00.690: INFO: Created: latency-svc-6xlf2
    Jan 17 15:00:00.698: INFO: Got endpoints: latency-svc-6xlf2 [219.843545ms]
    Jan 17 15:00:00.702: INFO: Created: latency-svc-mwcnb
    Jan 17 15:00:00.710: INFO: Got endpoints: latency-svc-mwcnb [212.103678ms]
    Jan 17 15:00:00.714: INFO: Created: latency-svc-p92hd
    Jan 17 15:00:00.720: INFO: Got endpoints: latency-svc-p92hd [213.639415ms]
    Jan 17 15:00:00.742: INFO: Created: latency-svc-8zm4d
    Jan 17 15:00:00.742: INFO: Created: latency-svc-kdlf6
    Jan 17 15:00:00.742: INFO: Got endpoints: latency-svc-kdlf6 [210.118319ms]
    Jan 17 15:00:00.755: INFO: Created: latency-svc-64mg2
    Jan 17 15:00:00.760: INFO: Got endpoints: latency-svc-8zm4d [240.883611ms]
    Jan 17 15:00:00.761: INFO: Created: latency-svc-pmg4j
    Jan 17 15:00:00.786: INFO: Created: latency-svc-tdr7f
    Jan 17 15:00:00.786: INFO: Got endpoints: latency-svc-pmg4j [235.47687ms]
    Jan 17 15:00:00.786: INFO: Got endpoints: latency-svc-64mg2 [244.697118ms]
    Jan 17 15:00:00.792: INFO: Got endpoints: latency-svc-tdr7f [222.247709ms]
    Jan 17 15:00:00.796: INFO: Created: latency-svc-jgwnx
    Jan 17 15:00:00.804: INFO: Created: latency-svc-wbrkb
    Jan 17 15:00:00.807: INFO: Got endpoints: latency-svc-jgwnx [217.708199ms]
    Jan 17 15:00:00.815: INFO: Created: latency-svc-k49dc
    Jan 17 15:00:00.816: INFO: Got endpoints: latency-svc-wbrkb [205.891825ms]
    Jan 17 15:00:00.824: INFO: Got endpoints: latency-svc-k49dc [197.321832ms]
    Jan 17 15:00:00.830: INFO: Created: latency-svc-ltpm9
    Jan 17 15:00:00.842: INFO: Got endpoints: latency-svc-ltpm9 [202.443467ms]
    Jan 17 15:00:00.843: INFO: Created: latency-svc-nb7f4
    Jan 17 15:00:00.853: INFO: Created: latency-svc-5wvjl
    Jan 17 15:00:00.865: INFO: Got endpoints: latency-svc-5wvjl [187.169584ms]
    Jan 17 15:00:00.865: INFO: Got endpoints: latency-svc-nb7f4 [196.114594ms]
    Jan 17 15:00:00.878: INFO: Created: latency-svc-5sdqg
    Jan 17 15:00:00.886: INFO: Got endpoints: latency-svc-5sdqg [198.834643ms]
    Jan 17 15:00:00.887: INFO: Created: latency-svc-5pd8f
    Jan 17 15:00:00.904: INFO: Got endpoints: latency-svc-5pd8f [205.897257ms]
    Jan 17 15:00:00.906: INFO: Created: latency-svc-kd429
    Jan 17 15:00:00.917: INFO: Got endpoints: latency-svc-kd429 [206.214792ms]
    Jan 17 15:00:00.917: INFO: Created: latency-svc-qjpw7
    Jan 17 15:00:00.929: INFO: Got endpoints: latency-svc-qjpw7 [208.277788ms]
    Jan 17 15:00:00.954: INFO: Created: latency-svc-cjr2z
    Jan 17 15:00:00.965: INFO: Got endpoints: latency-svc-cjr2z [204.819828ms]
    Jan 17 15:00:00.966: INFO: Created: latency-svc-mlhst
    Jan 17 15:00:00.977: INFO: Got endpoints: latency-svc-mlhst [191.17166ms]
    Jan 17 15:00:00.981: INFO: Created: latency-svc-g4285
    Jan 17 15:00:00.999: INFO: Got endpoints: latency-svc-g4285 [212.417413ms]
    Jan 17 15:00:00.999: INFO: Created: latency-svc-fpd6m
    Jan 17 15:00:01.005: INFO: Got endpoints: latency-svc-fpd6m [213.446285ms]
    Jan 17 15:00:01.012: INFO: Created: latency-svc-4gl7r
    Jan 17 15:00:01.019: INFO: Got endpoints: latency-svc-4gl7r [212.138228ms]
    Jan 17 15:00:01.021: INFO: Created: latency-svc-vflqz
    Jan 17 15:00:01.038: INFO: Got endpoints: latency-svc-vflqz [221.93003ms]
    Jan 17 15:00:01.038: INFO: Created: latency-svc-rw8m6
    Jan 17 15:00:01.046: INFO: Created: latency-svc-82jvg
    Jan 17 15:00:01.056: INFO: Got endpoints: latency-svc-rw8m6 [231.700817ms]
    Jan 17 15:00:01.067: INFO: Got endpoints: latency-svc-82jvg [224.48486ms]
    Jan 17 15:00:01.067: INFO: Created: latency-svc-t2pxf
    Jan 17 15:00:01.070: INFO: Got endpoints: latency-svc-t2pxf [205.599951ms]
    Jan 17 15:00:01.086: INFO: Created: latency-svc-trd49
    Jan 17 15:00:01.091: INFO: Got endpoints: latency-svc-trd49 [225.880157ms]
    Jan 17 15:00:01.092: INFO: Created: latency-svc-brc98
    Jan 17 15:00:01.106: INFO: Got endpoints: latency-svc-brc98 [220.225168ms]
    Jan 17 15:00:01.120: INFO: Created: latency-svc-pw6q9
    Jan 17 15:00:01.122: INFO: Got endpoints: latency-svc-pw6q9 [218.883205ms]
    Jan 17 15:00:01.127: INFO: Created: latency-svc-wr8wp
    Jan 17 15:00:01.136: INFO: Created: latency-svc-fwf9k
    Jan 17 15:00:01.142: INFO: Got endpoints: latency-svc-wr8wp [399.793652ms]
    Jan 17 15:00:01.148: INFO: Got endpoints: latency-svc-fwf9k [230.877713ms]
    Jan 17 15:00:01.153: INFO: Created: latency-svc-bld6v
    Jan 17 15:00:01.166: INFO: Got endpoints: latency-svc-bld6v [237.427906ms]
    Jan 17 15:00:01.168: INFO: Created: latency-svc-mwp9s
    Jan 17 15:00:01.177: INFO: Got endpoints: latency-svc-mwp9s [211.68945ms]
    Jan 17 15:00:01.180: INFO: Created: latency-svc-dwwmt
    Jan 17 15:00:01.194: INFO: Got endpoints: latency-svc-dwwmt [216.764653ms]
    Jan 17 15:00:01.194: INFO: Created: latency-svc-zhcv4
    Jan 17 15:00:01.198: INFO: Got endpoints: latency-svc-zhcv4 [198.926036ms]
    Jan 17 15:00:01.203: INFO: Created: latency-svc-b8k72
    Jan 17 15:00:01.215: INFO: Got endpoints: latency-svc-b8k72 [209.831106ms]
    Jan 17 15:00:01.216: INFO: Created: latency-svc-wxhfq
    Jan 17 15:00:01.229: INFO: Got endpoints: latency-svc-wxhfq [210.243198ms]
    Jan 17 15:00:01.234: INFO: Created: latency-svc-jnxdg
    Jan 17 15:00:01.239: INFO: Got endpoints: latency-svc-jnxdg [201.665931ms]
    Jan 17 15:00:01.250: INFO: Created: latency-svc-9t9vs
    Jan 17 15:00:01.258: INFO: Got endpoints: latency-svc-9t9vs [201.875505ms]
    Jan 17 15:00:01.266: INFO: Created: latency-svc-bz6p7
    Jan 17 15:00:01.287: INFO: Created: latency-svc-t7rmq
    Jan 17 15:00:01.289: INFO: Got endpoints: latency-svc-bz6p7 [222.875156ms]
    Jan 17 15:00:01.298: INFO: Got endpoints: latency-svc-t7rmq [227.994481ms]
    Jan 17 15:00:01.312: INFO: Created: latency-svc-n22lb
    Jan 17 15:00:01.317: INFO: Created: latency-svc-qljb9
    Jan 17 15:00:01.321: INFO: Got endpoints: latency-svc-n22lb [230.506656ms]
    Jan 17 15:00:01.328: INFO: Got endpoints: latency-svc-qljb9 [221.4219ms]
    Jan 17 15:00:01.335: INFO: Created: latency-svc-7wt7l
    Jan 17 15:00:01.342: INFO: Created: latency-svc-rdldt
    Jan 17 15:00:01.355: INFO: Got endpoints: latency-svc-rdldt [213.027449ms]
    Jan 17 15:00:01.355: INFO: Got endpoints: latency-svc-7wt7l [232.307327ms]
    Jan 17 15:00:01.355: INFO: Created: latency-svc-w69rs
    Jan 17 15:00:01.375: INFO: Got endpoints: latency-svc-w69rs [226.997081ms]
    Jan 17 15:00:01.388: INFO: Created: latency-svc-4q87f
    Jan 17 15:00:01.404: INFO: Created: latency-svc-lsms7
    Jan 17 15:00:01.417: INFO: Got endpoints: latency-svc-4q87f [250.774861ms]
    Jan 17 15:00:01.417: INFO: Got endpoints: latency-svc-lsms7 [240.136398ms]
    Jan 17 15:00:01.427: INFO: Created: latency-svc-g4h7k
    Jan 17 15:00:01.439: INFO: Got endpoints: latency-svc-g4h7k [244.668356ms]
    Jan 17 15:00:01.443: INFO: Created: latency-svc-tssj2
    Jan 17 15:00:01.473: INFO: Got endpoints: latency-svc-tssj2 [274.985877ms]
    Jan 17 15:00:01.500: INFO: Created: latency-svc-whwxf
    Jan 17 15:00:01.512: INFO: Got endpoints: latency-svc-whwxf [297.186627ms]
    Jan 17 15:00:01.512: INFO: Created: latency-svc-q4j5x
    Jan 17 15:00:01.524: INFO: Created: latency-svc-r97tg
    Jan 17 15:00:01.536: INFO: Got endpoints: latency-svc-q4j5x [306.70661ms]
    Jan 17 15:00:01.536: INFO: Created: latency-svc-nf548
    Jan 17 15:00:01.542: INFO: Got endpoints: latency-svc-r97tg [302.57542ms]
    Jan 17 15:00:01.545: INFO: Got endpoints: latency-svc-nf548 [287.010524ms]
    Jan 17 15:00:01.549: INFO: Created: latency-svc-z56vk
    Jan 17 15:00:01.560: INFO: Created: latency-svc-fxmgp
    Jan 17 15:00:01.560: INFO: Got endpoints: latency-svc-z56vk [270.480463ms]
    Jan 17 15:00:01.584: INFO: Got endpoints: latency-svc-fxmgp [285.530473ms]
    Jan 17 15:00:01.591: INFO: Created: latency-svc-nj6w7
    Jan 17 15:00:01.602: INFO: Got endpoints: latency-svc-nj6w7 [280.387224ms]
    Jan 17 15:00:01.604: INFO: Created: latency-svc-5v85q
    Jan 17 15:00:01.615: INFO: Created: latency-svc-str4m
    Jan 17 15:00:01.616: INFO: Got endpoints: latency-svc-5v85q [288.488215ms]
    Jan 17 15:00:01.629: INFO: Got endpoints: latency-svc-str4m [274.12741ms]
    Jan 17 15:00:01.633: INFO: Created: latency-svc-f8gcw
    Jan 17 15:00:01.640: INFO: Got endpoints: latency-svc-f8gcw [285.058391ms]
    Jan 17 15:00:01.640: INFO: Created: latency-svc-t76tm
    Jan 17 15:00:01.647: INFO: Got endpoints: latency-svc-t76tm [272.098378ms]
    Jan 17 15:00:01.647: INFO: Created: latency-svc-8sswx
    Jan 17 15:00:01.653: INFO: Got endpoints: latency-svc-8sswx [236.39995ms]
    Jan 17 15:00:01.763: INFO: Created: latency-svc-kcx7m
    Jan 17 15:00:01.764: INFO: Created: latency-svc-nj7gv
    Jan 17 15:00:01.765: INFO: Created: latency-svc-72vkt
    Jan 17 15:00:01.767: INFO: Created: latency-svc-9pv9s
    Jan 17 15:00:01.768: INFO: Created: latency-svc-rpkzl
    Jan 17 15:00:01.768: INFO: Created: latency-svc-qvzbr
    Jan 17 15:00:01.769: INFO: Created: latency-svc-j2jb2
    Jan 17 15:00:01.769: INFO: Created: latency-svc-vwz4x
    Jan 17 15:00:01.769: INFO: Created: latency-svc-h4cgr
    Jan 17 15:00:01.769: INFO: Created: latency-svc-jjt44
    Jan 17 15:00:01.773: INFO: Created: latency-svc-lvp42
    Jan 17 15:00:01.774: INFO: Created: latency-svc-j24s9
    Jan 17 15:00:01.774: INFO: Created: latency-svc-w75fv
    Jan 17 15:00:01.776: INFO: Created: latency-svc-mt8zc
    Jan 17 15:00:01.776: INFO: Created: latency-svc-w8p7f
    Jan 17 15:00:01.778: INFO: Got endpoints: latency-svc-kcx7m [232.775371ms]
    Jan 17 15:00:01.779: INFO: Got endpoints: latency-svc-nj7gv [305.805497ms]
    Jan 17 15:00:01.781: INFO: Got endpoints: latency-svc-h4cgr [140.851597ms]
    Jan 17 15:00:01.781: INFO: Got endpoints: latency-svc-72vkt [127.537866ms]
    Jan 17 15:00:01.783: INFO: Got endpoints: latency-svc-jjt44 [166.950617ms]
    Jan 17 15:00:01.791: INFO: Got endpoints: latency-svc-9pv9s [373.818199ms]
    Jan 17 15:00:01.802: INFO: Got endpoints: latency-svc-vwz4x [363.174332ms]
    Jan 17 15:00:01.802: INFO: Got endpoints: latency-svc-qvzbr [155.289377ms]
    Jan 17 15:00:01.802: INFO: Got endpoints: latency-svc-rpkzl [289.963537ms]
    Jan 17 15:00:01.805: INFO: Created: latency-svc-52prk
    Jan 17 15:00:01.814: INFO: Got endpoints: latency-svc-w75fv [211.890801ms]
    Jan 17 15:00:01.814: INFO: Got endpoints: latency-svc-j2jb2 [271.596158ms]
    Jan 17 15:00:01.821: INFO: Got endpoints: latency-svc-j24s9 [237.24022ms]
    Jan 17 15:00:01.831: INFO: Got endpoints: latency-svc-52prk [52.936967ms]
    Jan 17 15:00:01.831: INFO: Got endpoints: latency-svc-lvp42 [270.855162ms]
    Jan 17 15:00:01.831: INFO: Got endpoints: latency-svc-w8p7f [295.010732ms]
    Jan 17 15:00:01.831: INFO: Got endpoints: latency-svc-mt8zc [202.109189ms]
    Jan 17 15:00:01.835: INFO: Created: latency-svc-j44nn
    Jan 17 15:00:01.851: INFO: Got endpoints: latency-svc-j44nn [71.933914ms]
    Jan 17 15:00:01.856: INFO: Created: latency-svc-krt58
    Jan 17 15:00:01.869: INFO: Got endpoints: latency-svc-krt58 [88.126072ms]
    Jan 17 15:00:01.869: INFO: Created: latency-svc-tcqpf
    Jan 17 15:00:01.876: INFO: Got endpoints: latency-svc-tcqpf [95.414273ms]
    Jan 17 15:00:01.883: INFO: Created: latency-svc-k4v7m
    Jan 17 15:00:01.896: INFO: Created: latency-svc-thh5z
    Jan 17 15:00:01.896: INFO: Got endpoints: latency-svc-k4v7m [112.709662ms]
    Jan 17 15:00:01.907: INFO: Got endpoints: latency-svc-thh5z [116.086934ms]
    Jan 17 15:00:01.910: INFO: Created: latency-svc-mgszx
    Jan 17 15:00:01.922: INFO: Got endpoints: latency-svc-mgszx [119.733053ms]
    Jan 17 15:00:01.922: INFO: Created: latency-svc-d4fjd
    Jan 17 15:00:01.942: INFO: Got endpoints: latency-svc-d4fjd [139.64654ms]
    Jan 17 15:00:01.942: INFO: Created: latency-svc-wg56b
    Jan 17 15:00:01.945: INFO: Got endpoints: latency-svc-wg56b [143.086138ms]
    Jan 17 15:00:01.961: INFO: Created: latency-svc-cq88f
    Jan 17 15:00:01.965: INFO: Got endpoints: latency-svc-cq88f [151.227434ms]
    Jan 17 15:00:01.965: INFO: Created: latency-svc-rwvzv
    Jan 17 15:00:01.978: INFO: Created: latency-svc-lhj47
    Jan 17 15:00:01.987: INFO: Got endpoints: latency-svc-rwvzv [173.105983ms]
    Jan 17 15:00:01.993: INFO: Got endpoints: latency-svc-lhj47 [171.584277ms]
    Jan 17 15:00:01.996: INFO: Created: latency-svc-mmxx7
    Jan 17 15:00:02.004: INFO: Got endpoints: latency-svc-mmxx7 [173.553553ms]
    Jan 17 15:00:02.021: INFO: Created: latency-svc-7hdbw
    Jan 17 15:00:02.028: INFO: Created: latency-svc-mvgzk
    Jan 17 15:00:02.030: INFO: Got endpoints: latency-svc-7hdbw [199.0174ms]
    Jan 17 15:00:02.041: INFO: Got endpoints: latency-svc-mvgzk [210.165146ms]
    Jan 17 15:00:02.047: INFO: Created: latency-svc-6lg9h
    Jan 17 15:00:02.060: INFO: Created: latency-svc-cdjtn
    Jan 17 15:00:02.060: INFO: Got endpoints: latency-svc-6lg9h [228.982278ms]
    Jan 17 15:00:02.068: INFO: Got endpoints: latency-svc-cdjtn [217.152977ms]
    Jan 17 15:00:02.082: INFO: Created: latency-svc-9hkll
    Jan 17 15:00:02.082: INFO: Created: latency-svc-68nm7
    Jan 17 15:00:02.085: INFO: Got endpoints: latency-svc-9hkll [216.021129ms]
    Jan 17 15:00:02.090: INFO: Created: latency-svc-sz9tz
    Jan 17 15:00:02.102: INFO: Got endpoints: latency-svc-68nm7 [225.841612ms]
    Jan 17 15:00:02.102: INFO: Got endpoints: latency-svc-sz9tz [206.347357ms]
    Jan 17 15:00:02.113: INFO: Created: latency-svc-jcbn5
    Jan 17 15:00:02.124: INFO: Got endpoints: latency-svc-jcbn5 [217.231319ms]
    Jan 17 15:00:02.126: INFO: Created: latency-svc-lqccf
    Jan 17 15:00:02.136: INFO: Got endpoints: latency-svc-lqccf [214.503709ms]
    Jan 17 15:00:02.143: INFO: Created: latency-svc-w6xzt
    Jan 17 15:00:02.149: INFO: Created: latency-svc-2hzvg
    Jan 17 15:00:02.154: INFO: Got endpoints: latency-svc-w6xzt [212.17244ms]
    Jan 17 15:00:02.159: INFO: Created: latency-svc-k4kkw
    Jan 17 15:00:02.161: INFO: Got endpoints: latency-svc-2hzvg [215.39277ms]
    Jan 17 15:00:02.166: INFO: Got endpoints: latency-svc-k4kkw [201.367473ms]
    Jan 17 15:00:02.171: INFO: Created: latency-svc-v6tl6
    Jan 17 15:00:02.179: INFO: Created: latency-svc-wr2d4
    Jan 17 15:00:02.186: INFO: Got endpoints: latency-svc-v6tl6 [198.943518ms]
    Jan 17 15:00:02.187: INFO: Got endpoints: latency-svc-wr2d4 [194.087ms]
    Jan 17 15:00:02.190: INFO: Created: latency-svc-m5jc2
    Jan 17 15:00:02.199: INFO: Created: latency-svc-b8wnf
    Jan 17 15:00:02.203: INFO: Got endpoints: latency-svc-m5jc2 [198.657574ms]
    Jan 17 15:00:02.222: INFO: Got endpoints: latency-svc-b8wnf [192.271935ms]
    Jan 17 15:00:02.223: INFO: Created: latency-svc-fdbdk
    Jan 17 15:00:02.229: INFO: Created: latency-svc-cbbzw
    Jan 17 15:00:02.244: INFO: Got endpoints: latency-svc-fdbdk [202.691616ms]
    Jan 17 15:00:02.244: INFO: Got endpoints: latency-svc-cbbzw [184.331898ms]
    Jan 17 15:00:02.257: INFO: Created: latency-svc-w49sh
    Jan 17 15:00:02.266: INFO: Got endpoints: latency-svc-w49sh [198.051731ms]
    Jan 17 15:00:02.269: INFO: Created: latency-svc-lwpwq
    Jan 17 15:00:02.278: INFO: Got endpoints: latency-svc-lwpwq [192.806088ms]
    Jan 17 15:00:02.279: INFO: Created: latency-svc-zmwn6
    Jan 17 15:00:02.289: INFO: Got endpoints: latency-svc-zmwn6 [186.46429ms]
    Jan 17 15:00:02.293: INFO: Created: latency-svc-mcckg
    Jan 17 15:00:02.303: INFO: Got endpoints: latency-svc-mcckg [201.002847ms]
    Jan 17 15:00:02.304: INFO: Created: latency-svc-wp5bp
    Jan 17 15:00:02.310: INFO: Got endpoints: latency-svc-wp5bp [185.338156ms]
    Jan 17 15:00:02.320: INFO: Created: latency-svc-xjscf
    Jan 17 15:00:02.330: INFO: Got endpoints: latency-svc-xjscf [193.408775ms]
    Jan 17 15:00:02.332: INFO: Created: latency-svc-rmlx7
    Jan 17 15:00:02.361: INFO: Got endpoints: latency-svc-rmlx7 [206.612343ms]
    Jan 17 15:00:02.361: INFO: Created: latency-svc-jzvgd
    Jan 17 15:00:02.372: INFO: Got endpoints: latency-svc-jzvgd [210.809037ms]
    Jan 17 15:00:02.372: INFO: Created: latency-svc-6npm6
    Jan 17 15:00:02.383: INFO: Created: latency-svc-jlw99
    Jan 17 15:00:02.383: INFO: Got endpoints: latency-svc-6npm6 [216.539128ms]
    Jan 17 15:00:02.390: INFO: Got endpoints: latency-svc-jlw99 [203.663846ms]
    Jan 17 15:00:02.402: INFO: Created: latency-svc-952wn
    Jan 17 15:00:02.408: INFO: Created: latency-svc-n2snp
    Jan 17 15:00:02.410: INFO: Got endpoints: latency-svc-952wn [223.007434ms]
    Jan 17 15:00:02.415: INFO: Got endpoints: latency-svc-n2snp [212.143777ms]
    Jan 17 15:00:02.430: INFO: Created: latency-svc-48d46
    Jan 17 15:00:02.452: INFO: Got endpoints: latency-svc-48d46 [207.747299ms]
    Jan 17 15:00:02.452: INFO: Created: latency-svc-2bbv9
    Jan 17 15:00:02.457: INFO: Created: latency-svc-ntl4c
    Jan 17 15:00:02.459: INFO: Got endpoints: latency-svc-2bbv9 [214.897013ms]
    Jan 17 15:00:02.473: INFO: Got endpoints: latency-svc-ntl4c [206.853965ms]
    Jan 17 15:00:02.473: INFO: Created: latency-svc-tbzjl
    Jan 17 15:00:02.490: INFO: Got endpoints: latency-svc-tbzjl [211.942458ms]
    Jan 17 15:00:02.490: INFO: Created: latency-svc-8zstw
    Jan 17 15:00:02.499: INFO: Got endpoints: latency-svc-8zstw [210.094995ms]
    Jan 17 15:00:02.499: INFO: Created: latency-svc-8q5ws
    Jan 17 15:00:02.518: INFO: Created: latency-svc-xf9cp
    Jan 17 15:00:02.518: INFO: Got endpoints: latency-svc-xf9cp [208.464658ms]
    Jan 17 15:00:02.518: INFO: Got endpoints: latency-svc-8q5ws [214.947272ms]
    Jan 17 15:00:02.520: INFO: Created: latency-svc-8c9ch
    Jan 17 15:00:02.533: INFO: Created: latency-svc-d9g7g
    Jan 17 15:00:02.533: INFO: Got endpoints: latency-svc-8c9ch [203.680958ms]
    Jan 17 15:00:02.559: INFO: Got endpoints: latency-svc-d9g7g [198.480837ms]
    Jan 17 15:00:02.560: INFO: Created: latency-svc-99r72
    Jan 17 15:00:02.567: INFO: Got endpoints: latency-svc-99r72 [195.230474ms]
    Jan 17 15:00:02.571: INFO: Created: latency-svc-7w4dk
    Jan 17 15:00:02.580: INFO: Got endpoints: latency-svc-7w4dk [197.041453ms]
    Jan 17 15:00:02.595: INFO: Created: latency-svc-jpshh
    Jan 17 15:00:02.595: INFO: Got endpoints: latency-svc-jpshh [205.548168ms]
    Jan 17 15:00:02.600: INFO: Created: latency-svc-9c4cs
    Jan 17 15:00:02.601: INFO: Created: latency-svc-mnv6s
    Jan 17 15:00:02.611: INFO: Got endpoints: latency-svc-mnv6s [389.054956ms]
    Jan 17 15:00:02.611: INFO: Got endpoints: latency-svc-9c4cs [201.430476ms]
    Jan 17 15:00:02.614: INFO: Created: latency-svc-plxs5
    Jan 17 15:00:02.620: INFO: Got endpoints: latency-svc-plxs5 [204.579648ms]
    Jan 17 15:00:02.623: INFO: Created: latency-svc-lr5pn
    Jan 17 15:00:02.631: INFO: Created: latency-svc-wmpdd
    Jan 17 15:00:02.632: INFO: Got endpoints: latency-svc-lr5pn [179.876171ms]
    Jan 17 15:00:02.648: INFO: Got endpoints: latency-svc-wmpdd [189.066973ms]
    Jan 17 15:00:02.657: INFO: Created: latency-svc-9pn2f
    Jan 17 15:00:02.669: INFO: Got endpoints: latency-svc-9pn2f [196.539843ms]
    Jan 17 15:00:02.675: INFO: Created: latency-svc-tzhvq
    Jan 17 15:00:02.677: INFO: Created: latency-svc-27jlm
    Jan 17 15:00:02.685: INFO: Got endpoints: latency-svc-tzhvq [186.296035ms]
    Jan 17 15:00:02.687: INFO: Got endpoints: latency-svc-27jlm [196.885803ms]
    Jan 17 15:00:02.704: INFO: Created: latency-svc-8ql7n
    Jan 17 15:00:02.710: INFO: Got endpoints: latency-svc-8ql7n [191.839664ms]
    Jan 17 15:00:02.714: INFO: Created: latency-svc-pws8b
    Jan 17 15:00:02.723: INFO: Got endpoints: latency-svc-pws8b [204.254117ms]
    Jan 17 15:00:02.723: INFO: Latencies: [28.664762ms 40.166849ms 52.275503ms 52.936967ms 59.88329ms 71.933914ms 81.516505ms 88.126072ms 95.414273ms 112.709662ms 116.086934ms 118.064121ms 119.733053ms 127.537866ms 139.64654ms 140.851597ms 143.086138ms 151.227434ms 155.289377ms 160.8402ms 166.950617ms 171.584277ms 173.105983ms 173.553553ms 179.876171ms 181.70928ms 184.331898ms 185.338156ms 186.296035ms 186.46429ms 187.169584ms 189.066973ms 191.17166ms 191.839664ms 192.271935ms 192.806088ms 193.408775ms 194.087ms 195.230474ms 196.114594ms 196.539843ms 196.885803ms 197.041453ms 197.321832ms 198.051731ms 198.480837ms 198.657574ms 198.834643ms 198.926036ms 198.943518ms 199.0174ms 201.002847ms 201.367473ms 201.430476ms 201.665931ms 201.875505ms 202.109189ms 202.443467ms 202.609942ms 202.691616ms 203.663846ms 203.680958ms 204.254117ms 204.579648ms 204.819828ms 205.548168ms 205.599951ms 205.891825ms 205.897257ms 206.214792ms 206.347357ms 206.612343ms 206.853965ms 206.984692ms 207.732438ms 207.747299ms 208.277788ms 208.464658ms 209.831106ms 210.094995ms 210.118319ms 210.165146ms 210.243198ms 210.809037ms 211.68945ms 211.890801ms 211.942458ms 212.103678ms 212.138228ms 212.143777ms 212.17244ms 212.417413ms 213.027449ms 213.446285ms 213.639415ms 214.503709ms 214.897013ms 214.947272ms 215.39277ms 215.95143ms 216.021129ms 216.539128ms 216.764653ms 216.771866ms 217.152977ms 217.231319ms 217.708199ms 218.883205ms 219.843545ms 220.100501ms 220.225168ms 221.185032ms 221.4219ms 221.93003ms 222.079252ms 222.247709ms 222.875156ms 223.007434ms 223.519276ms 223.777537ms 224.011999ms 224.48486ms 224.688393ms 225.215769ms 225.841612ms 225.880157ms 226.112552ms 226.997081ms 227.399302ms 227.933618ms 227.994481ms 228.255499ms 228.553719ms 228.783446ms 228.841541ms 228.936138ms 228.982278ms 229.597833ms 229.798854ms 229.802776ms 230.299235ms 230.506656ms 230.877713ms 231.700817ms 232.248842ms 232.307327ms 232.658485ms 232.775371ms 232.87776ms 234.185323ms 234.731695ms 235.052989ms 235.47687ms 236.035497ms 236.39995ms 236.633736ms 237.24022ms 237.427906ms 238.216822ms 240.136398ms 240.883611ms 241.279458ms 244.055449ms 244.668356ms 244.697118ms 246.89893ms 247.314105ms 249.810417ms 250.774861ms 255.281074ms 265.649975ms 270.480463ms 270.696617ms 270.855162ms 271.596158ms 272.098378ms 274.117685ms 274.12741ms 274.985877ms 278.275805ms 280.387224ms 281.47344ms 282.545474ms 283.261686ms 285.058391ms 285.530473ms 287.010524ms 288.488215ms 289.963537ms 293.603825ms 295.010732ms 297.186627ms 297.399056ms 302.57542ms 305.805497ms 306.70661ms 363.174332ms 373.818199ms 389.054956ms 399.793652ms]
    Jan 17 15:00:02.723: INFO: 50 %ile: 216.021129ms
    Jan 17 15:00:02.723: INFO: 90 %ile: 280.387224ms
    Jan 17 15:00:02.723: INFO: 99 %ile: 389.054956ms
    Jan 17 15:00:02.723: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/framework.go:187
    Jan 17 15:00:02.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svc-latency-9800" for this suite. 01/17/23 15:00:02.731
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:929
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:00:02.745
Jan 17 15:00:02.745: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename kubectl 01/17/23 15:00:02.746
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:00:02.772
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:00:02.775
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:929
STEP: create deployment with httpd image 01/17/23 15:00:02.776
Jan 17 15:00:02.777: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-6622 create -f -'
Jan 17 15:00:04.646: INFO: stderr: ""
Jan 17 15:00:04.646: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 01/17/23 15:00:04.646
Jan 17 15:00:04.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-6622 diff -f -'
Jan 17 15:00:06.301: INFO: rc: 1
Jan 17 15:00:06.301: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-6622 delete -f -'
Jan 17 15:00:06.350: INFO: stderr: ""
Jan 17 15:00:06.350: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 17 15:00:06.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6622" for this suite. 01/17/23 15:00:06.354
{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","completed":37,"skipped":716,"failed":0}
------------------------------
• [3.616 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:923
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:929

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:00:02.745
    Jan 17 15:00:02.745: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename kubectl 01/17/23 15:00:02.746
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:00:02.772
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:00:02.775
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:929
    STEP: create deployment with httpd image 01/17/23 15:00:02.776
    Jan 17 15:00:02.777: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-6622 create -f -'
    Jan 17 15:00:04.646: INFO: stderr: ""
    Jan 17 15:00:04.646: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 01/17/23 15:00:04.646
    Jan 17 15:00:04.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-6622 diff -f -'
    Jan 17 15:00:06.301: INFO: rc: 1
    Jan 17 15:00:06.301: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-6622 delete -f -'
    Jan 17 15:00:06.350: INFO: stderr: ""
    Jan 17 15:00:06.350: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 17 15:00:06.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-6622" for this suite. 01/17/23 15:00:06.354
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:00:06.362
Jan 17 15:00:06.362: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename services 01/17/23 15:00:06.362
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:00:06.385
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:00:06.388
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231
STEP: creating an Endpoint 01/17/23 15:00:06.401
STEP: waiting for available Endpoint 01/17/23 15:00:06.406
STEP: listing all Endpoints 01/17/23 15:00:06.408
STEP: updating the Endpoint 01/17/23 15:00:06.416
STEP: fetching the Endpoint 01/17/23 15:00:06.426
STEP: patching the Endpoint 01/17/23 15:00:06.432
STEP: fetching the Endpoint 01/17/23 15:00:06.443
STEP: deleting the Endpoint by Collection 01/17/23 15:00:06.449
STEP: waiting for Endpoint deletion 01/17/23 15:00:06.457
STEP: fetching the Endpoint 01/17/23 15:00:06.458
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 17 15:00:06.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2356" for this suite. 01/17/23 15:00:06.467
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","completed":38,"skipped":747,"failed":0}
------------------------------
• [0.114 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:00:06.362
    Jan 17 15:00:06.362: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename services 01/17/23 15:00:06.362
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:00:06.385
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:00:06.388
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3231
    STEP: creating an Endpoint 01/17/23 15:00:06.401
    STEP: waiting for available Endpoint 01/17/23 15:00:06.406
    STEP: listing all Endpoints 01/17/23 15:00:06.408
    STEP: updating the Endpoint 01/17/23 15:00:06.416
    STEP: fetching the Endpoint 01/17/23 15:00:06.426
    STEP: patching the Endpoint 01/17/23 15:00:06.432
    STEP: fetching the Endpoint 01/17/23 15:00:06.443
    STEP: deleting the Endpoint by Collection 01/17/23 15:00:06.449
    STEP: waiting for Endpoint deletion 01/17/23 15:00:06.457
    STEP: fetching the Endpoint 01/17/23 15:00:06.458
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 17 15:00:06.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-2356" for this suite. 01/17/23 15:00:06.467
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:00:06.476
Jan 17 15:00:06.476: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename custom-resource-definition 01/17/23 15:00:06.477
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:00:06.605
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:00:06.607
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
Jan 17 15:00:06.609: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 17 15:00:13.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7382" for this suite. 01/17/23 15:00:13.955
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","completed":39,"skipped":758,"failed":0}
------------------------------
• [SLOW TEST] [7.515 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:00:06.476
    Jan 17 15:00:06.476: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename custom-resource-definition 01/17/23 15:00:06.477
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:00:06.605
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:00:06.607
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    Jan 17 15:00:06.609: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 17 15:00:13.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-7382" for this suite. 01/17/23 15:00:13.955
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:00:13.991
Jan 17 15:00:13.991: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename secrets 01/17/23 15:00:13.992
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:00:14.069
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:00:14.072
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94
STEP: creating secret secrets-2447/secret-test-03138cc8-3d5d-4fb1-be90-dd08cc48355f 01/17/23 15:00:14.075
STEP: Creating a pod to test consume secrets 01/17/23 15:00:14.118
Jan 17 15:00:14.173: INFO: Waiting up to 5m0s for pod "pod-configmaps-d16afee3-655e-4ca9-b818-8cfae2948ca1" in namespace "secrets-2447" to be "Succeeded or Failed"
Jan 17 15:00:14.195: INFO: Pod "pod-configmaps-d16afee3-655e-4ca9-b818-8cfae2948ca1": Phase="Pending", Reason="", readiness=false. Elapsed: 22.682735ms
Jan 17 15:00:16.203: INFO: Pod "pod-configmaps-d16afee3-655e-4ca9-b818-8cfae2948ca1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030401784s
Jan 17 15:00:18.202: INFO: Pod "pod-configmaps-d16afee3-655e-4ca9-b818-8cfae2948ca1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028844319s
STEP: Saw pod success 01/17/23 15:00:18.202
Jan 17 15:00:18.202: INFO: Pod "pod-configmaps-d16afee3-655e-4ca9-b818-8cfae2948ca1" satisfied condition "Succeeded or Failed"
Jan 17 15:00:18.212: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-configmaps-d16afee3-655e-4ca9-b818-8cfae2948ca1 container env-test: <nil>
STEP: delete the pod 01/17/23 15:00:18.221
Jan 17 15:00:18.245: INFO: Waiting for pod pod-configmaps-d16afee3-655e-4ca9-b818-8cfae2948ca1 to disappear
Jan 17 15:00:18.249: INFO: Pod pod-configmaps-d16afee3-655e-4ca9-b818-8cfae2948ca1 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Jan 17 15:00:18.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2447" for this suite. 01/17/23 15:00:18.257
{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","completed":40,"skipped":768,"failed":0}
------------------------------
• [4.281 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:00:13.991
    Jan 17 15:00:13.991: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename secrets 01/17/23 15:00:13.992
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:00:14.069
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:00:14.072
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:94
    STEP: creating secret secrets-2447/secret-test-03138cc8-3d5d-4fb1-be90-dd08cc48355f 01/17/23 15:00:14.075
    STEP: Creating a pod to test consume secrets 01/17/23 15:00:14.118
    Jan 17 15:00:14.173: INFO: Waiting up to 5m0s for pod "pod-configmaps-d16afee3-655e-4ca9-b818-8cfae2948ca1" in namespace "secrets-2447" to be "Succeeded or Failed"
    Jan 17 15:00:14.195: INFO: Pod "pod-configmaps-d16afee3-655e-4ca9-b818-8cfae2948ca1": Phase="Pending", Reason="", readiness=false. Elapsed: 22.682735ms
    Jan 17 15:00:16.203: INFO: Pod "pod-configmaps-d16afee3-655e-4ca9-b818-8cfae2948ca1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030401784s
    Jan 17 15:00:18.202: INFO: Pod "pod-configmaps-d16afee3-655e-4ca9-b818-8cfae2948ca1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028844319s
    STEP: Saw pod success 01/17/23 15:00:18.202
    Jan 17 15:00:18.202: INFO: Pod "pod-configmaps-d16afee3-655e-4ca9-b818-8cfae2948ca1" satisfied condition "Succeeded or Failed"
    Jan 17 15:00:18.212: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-configmaps-d16afee3-655e-4ca9-b818-8cfae2948ca1 container env-test: <nil>
    STEP: delete the pod 01/17/23 15:00:18.221
    Jan 17 15:00:18.245: INFO: Waiting for pod pod-configmaps-d16afee3-655e-4ca9-b818-8cfae2948ca1 to disappear
    Jan 17 15:00:18.249: INFO: Pod pod-configmaps-d16afee3-655e-4ca9-b818-8cfae2948ca1 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Jan 17 15:00:18.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-2447" for this suite. 01/17/23 15:00:18.257
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:00:18.272
Jan 17 15:00:18.272: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename dns 01/17/23 15:00:18.273
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:00:18.333
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:00:18.336
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 01/17/23 15:00:18.341
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-2929.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-2929.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 01/17/23 15:00:18.357
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-2929.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-2929.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 01/17/23 15:00:18.357
STEP: creating a pod to probe DNS 01/17/23 15:00:18.357
STEP: submitting the pod to kubernetes 01/17/23 15:00:18.357
Jan 17 15:00:18.414: INFO: Waiting up to 15m0s for pod "dns-test-83a86ebf-55f0-4dd3-8180-fd512484925e" in namespace "dns-2929" to be "running"
Jan 17 15:00:18.426: INFO: Pod "dns-test-83a86ebf-55f0-4dd3-8180-fd512484925e": Phase="Pending", Reason="", readiness=false. Elapsed: 11.504326ms
Jan 17 15:00:20.429: INFO: Pod "dns-test-83a86ebf-55f0-4dd3-8180-fd512484925e": Phase="Running", Reason="", readiness=true. Elapsed: 2.014691179s
Jan 17 15:00:20.429: INFO: Pod "dns-test-83a86ebf-55f0-4dd3-8180-fd512484925e" satisfied condition "running"
STEP: retrieving the pod 01/17/23 15:00:20.429
STEP: looking for the results for each expected name from probers 01/17/23 15:00:20.432
Jan 17 15:00:20.448: INFO: DNS probes using dns-2929/dns-test-83a86ebf-55f0-4dd3-8180-fd512484925e succeeded

STEP: deleting the pod 01/17/23 15:00:20.448
STEP: deleting the test headless service 01/17/23 15:00:20.462
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan 17 15:00:20.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2929" for this suite. 01/17/23 15:00:20.505
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [Conformance]","completed":41,"skipped":772,"failed":0}
------------------------------
• [2.242 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:00:18.272
    Jan 17 15:00:18.272: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename dns 01/17/23 15:00:18.273
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:00:18.333
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:00:18.336
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 01/17/23 15:00:18.341
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-2929.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-2929.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     01/17/23 15:00:18.357
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-2929.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-2929.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     01/17/23 15:00:18.357
    STEP: creating a pod to probe DNS 01/17/23 15:00:18.357
    STEP: submitting the pod to kubernetes 01/17/23 15:00:18.357
    Jan 17 15:00:18.414: INFO: Waiting up to 15m0s for pod "dns-test-83a86ebf-55f0-4dd3-8180-fd512484925e" in namespace "dns-2929" to be "running"
    Jan 17 15:00:18.426: INFO: Pod "dns-test-83a86ebf-55f0-4dd3-8180-fd512484925e": Phase="Pending", Reason="", readiness=false. Elapsed: 11.504326ms
    Jan 17 15:00:20.429: INFO: Pod "dns-test-83a86ebf-55f0-4dd3-8180-fd512484925e": Phase="Running", Reason="", readiness=true. Elapsed: 2.014691179s
    Jan 17 15:00:20.429: INFO: Pod "dns-test-83a86ebf-55f0-4dd3-8180-fd512484925e" satisfied condition "running"
    STEP: retrieving the pod 01/17/23 15:00:20.429
    STEP: looking for the results for each expected name from probers 01/17/23 15:00:20.432
    Jan 17 15:00:20.448: INFO: DNS probes using dns-2929/dns-test-83a86ebf-55f0-4dd3-8180-fd512484925e succeeded

    STEP: deleting the pod 01/17/23 15:00:20.448
    STEP: deleting the test headless service 01/17/23 15:00:20.462
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan 17 15:00:20.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-2929" for this suite. 01/17/23 15:00:20.505
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:00:20.514
Jan 17 15:00:20.514: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename webhook 01/17/23 15:00:20.515
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:00:20.568
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:00:20.571
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/17/23 15:00:20.591
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/17/23 15:00:20.841
STEP: Deploying the webhook pod 01/17/23 15:00:20.863
STEP: Wait for the deployment to be ready 01/17/23 15:00:20.877
Jan 17 15:00:20.895: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/17/23 15:00:22.908
STEP: Verifying the service has paired with the endpoint 01/17/23 15:00:22.92
Jan 17 15:00:23.920: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196
STEP: Registering the webhook via the AdmissionRegistration API 01/17/23 15:00:23.924
STEP: create a pod that should be denied by the webhook 01/17/23 15:00:23.938
STEP: create a pod that causes the webhook to hang 01/17/23 15:00:23.955
STEP: create a configmap that should be denied by the webhook 01/17/23 15:00:33.968
STEP: create a configmap that should be admitted by the webhook 01/17/23 15:00:33.985
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 01/17/23 15:00:33.993
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 01/17/23 15:00:34.004
STEP: create a namespace that bypass the webhook 01/17/23 15:00:34.009
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 01/17/23 15:00:34.022
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 17 15:00:34.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1593" for this suite. 01/17/23 15:00:34.063
STEP: Destroying namespace "webhook-1593-markers" for this suite. 01/17/23 15:00:34.074
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","completed":42,"skipped":784,"failed":0}
------------------------------
• [SLOW TEST] [13.636 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:00:20.514
    Jan 17 15:00:20.514: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename webhook 01/17/23 15:00:20.515
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:00:20.568
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:00:20.571
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/17/23 15:00:20.591
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/17/23 15:00:20.841
    STEP: Deploying the webhook pod 01/17/23 15:00:20.863
    STEP: Wait for the deployment to be ready 01/17/23 15:00:20.877
    Jan 17 15:00:20.895: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/17/23 15:00:22.908
    STEP: Verifying the service has paired with the endpoint 01/17/23 15:00:22.92
    Jan 17 15:00:23.920: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:196
    STEP: Registering the webhook via the AdmissionRegistration API 01/17/23 15:00:23.924
    STEP: create a pod that should be denied by the webhook 01/17/23 15:00:23.938
    STEP: create a pod that causes the webhook to hang 01/17/23 15:00:23.955
    STEP: create a configmap that should be denied by the webhook 01/17/23 15:00:33.968
    STEP: create a configmap that should be admitted by the webhook 01/17/23 15:00:33.985
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 01/17/23 15:00:33.993
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 01/17/23 15:00:34.004
    STEP: create a namespace that bypass the webhook 01/17/23 15:00:34.009
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 01/17/23 15:00:34.022
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 17 15:00:34.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-1593" for this suite. 01/17/23 15:00:34.063
    STEP: Destroying namespace "webhook-1593-markers" for this suite. 01/17/23 15:00:34.074
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:00:34.151
Jan 17 15:00:34.151: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename var-expansion 01/17/23 15:00:34.151
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:00:34.252
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:00:34.255
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43
STEP: Creating a pod to test env composition 01/17/23 15:00:34.257
Jan 17 15:00:34.283: INFO: Waiting up to 5m0s for pod "var-expansion-df320115-de1b-41ef-9602-00f0583d834e" in namespace "var-expansion-7927" to be "Succeeded or Failed"
Jan 17 15:00:34.288: INFO: Pod "var-expansion-df320115-de1b-41ef-9602-00f0583d834e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.52279ms
Jan 17 15:00:36.292: INFO: Pod "var-expansion-df320115-de1b-41ef-9602-00f0583d834e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008957673s
Jan 17 15:00:38.293: INFO: Pod "var-expansion-df320115-de1b-41ef-9602-00f0583d834e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010664099s
STEP: Saw pod success 01/17/23 15:00:38.293
Jan 17 15:00:38.293: INFO: Pod "var-expansion-df320115-de1b-41ef-9602-00f0583d834e" satisfied condition "Succeeded or Failed"
Jan 17 15:00:38.296: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod var-expansion-df320115-de1b-41ef-9602-00f0583d834e container dapi-container: <nil>
STEP: delete the pod 01/17/23 15:00:38.303
Jan 17 15:00:38.319: INFO: Waiting for pod var-expansion-df320115-de1b-41ef-9602-00f0583d834e to disappear
Jan 17 15:00:38.322: INFO: Pod var-expansion-df320115-de1b-41ef-9602-00f0583d834e no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan 17 15:00:38.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7927" for this suite. 01/17/23 15:00:38.326
{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","completed":43,"skipped":792,"failed":0}
------------------------------
• [4.182 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:00:34.151
    Jan 17 15:00:34.151: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename var-expansion 01/17/23 15:00:34.151
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:00:34.252
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:00:34.255
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:43
    STEP: Creating a pod to test env composition 01/17/23 15:00:34.257
    Jan 17 15:00:34.283: INFO: Waiting up to 5m0s for pod "var-expansion-df320115-de1b-41ef-9602-00f0583d834e" in namespace "var-expansion-7927" to be "Succeeded or Failed"
    Jan 17 15:00:34.288: INFO: Pod "var-expansion-df320115-de1b-41ef-9602-00f0583d834e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.52279ms
    Jan 17 15:00:36.292: INFO: Pod "var-expansion-df320115-de1b-41ef-9602-00f0583d834e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008957673s
    Jan 17 15:00:38.293: INFO: Pod "var-expansion-df320115-de1b-41ef-9602-00f0583d834e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010664099s
    STEP: Saw pod success 01/17/23 15:00:38.293
    Jan 17 15:00:38.293: INFO: Pod "var-expansion-df320115-de1b-41ef-9602-00f0583d834e" satisfied condition "Succeeded or Failed"
    Jan 17 15:00:38.296: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod var-expansion-df320115-de1b-41ef-9602-00f0583d834e container dapi-container: <nil>
    STEP: delete the pod 01/17/23 15:00:38.303
    Jan 17 15:00:38.319: INFO: Waiting for pod var-expansion-df320115-de1b-41ef-9602-00f0583d834e to disappear
    Jan 17 15:00:38.322: INFO: Pod var-expansion-df320115-de1b-41ef-9602-00f0583d834e no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan 17 15:00:38.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-7927" for this suite. 01/17/23 15:00:38.326
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:00:38.333
Jan 17 15:00:38.333: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename deployment 01/17/23 15:00:38.333
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:00:38.355
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:00:38.357
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
W0117 15:00:38.367608      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 17 15:00:38.374: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jan 17 15:00:43.379: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/17/23 15:00:43.379
Jan 17 15:00:43.379: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 01/17/23 15:00:43.389
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 17 15:00:43.397: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-4635  5f68a123-6931-4ed5-bf25-fba25a5f15d5 76066 1 2023-01-17 15:00:43 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-01-17 15:00:43 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003a3be58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Jan 17 15:00:43.401: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
Jan 17 15:00:43.401: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Jan 17 15:00:43.401: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-4635  54a94b24-f800-4ce7-947a-1529daaac321 76067 1 2023-01-17 15:00:38 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 5f68a123-6931-4ed5-bf25-fba25a5f15d5 0xc000dec2f7 0xc000dec2f8}] [] [{e2e.test Update apps/v1 2023-01-17 15:00:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 15:00:40 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-01-17 15:00:43 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"5f68a123-6931-4ed5-bf25-fba25a5f15d5\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc000dec3b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 17 15:00:43.404: INFO: Pod "test-cleanup-controller-569nm" is available:
&Pod{ObjectMeta:{test-cleanup-controller-569nm test-cleanup-controller- deployment-4635  90e25e15-3428-4e5b-9fab-84ece2db273d 76048 0 2023-01-17 15:00:38 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.131.0.49/23"],"mac_address":"0a:58:0a:83:00:31","gateway_ips":["10.131.0.1"],"ip_address":"10.131.0.49/23","gateway_ip":"10.131.0.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.131.0.49"
    ],
    "mac": "0a:58:0a:83:00:31",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.131.0.49"
    ],
    "mac": "0a:58:0a:83:00:31",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-cleanup-controller 54a94b24-f800-4ce7-947a-1529daaac321 0xc0027609c7 0xc0027609c8}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:00:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:00:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"54a94b24-f800-4ce7-947a-1529daaac321\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-17 15:00:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-17 15:00:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.0.49\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r498b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r498b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-151-22.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c34,c9,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:00:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:00:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:00:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:00:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.151.22,PodIP:10.131.0.49,StartTime:2023-01-17 15:00:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 15:00:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://c35e89f2f2a0998979abb05704a6e3bc1cbe6f5f37dfa3ba6fad5bb4a77dee0e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.131.0.49,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan 17 15:00:43.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4635" for this suite. 01/17/23 15:00:43.408
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","completed":44,"skipped":799,"failed":0}
------------------------------
• [SLOW TEST] [5.082 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:00:38.333
    Jan 17 15:00:38.333: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename deployment 01/17/23 15:00:38.333
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:00:38.355
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:00:38.357
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    W0117 15:00:38.367608      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jan 17 15:00:38.374: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    Jan 17 15:00:43.379: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/17/23 15:00:43.379
    Jan 17 15:00:43.379: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 01/17/23 15:00:43.389
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 17 15:00:43.397: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-4635  5f68a123-6931-4ed5-bf25-fba25a5f15d5 76066 1 2023-01-17 15:00:43 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-01-17 15:00:43 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003a3be58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

    Jan 17 15:00:43.401: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
    Jan 17 15:00:43.401: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
    Jan 17 15:00:43.401: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-4635  54a94b24-f800-4ce7-947a-1529daaac321 76067 1 2023-01-17 15:00:38 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 5f68a123-6931-4ed5-bf25-fba25a5f15d5 0xc000dec2f7 0xc000dec2f8}] [] [{e2e.test Update apps/v1 2023-01-17 15:00:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 15:00:40 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-01-17 15:00:43 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"5f68a123-6931-4ed5-bf25-fba25a5f15d5\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc000dec3b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 17 15:00:43.404: INFO: Pod "test-cleanup-controller-569nm" is available:
    &Pod{ObjectMeta:{test-cleanup-controller-569nm test-cleanup-controller- deployment-4635  90e25e15-3428-4e5b-9fab-84ece2db273d 76048 0 2023-01-17 15:00:38 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.131.0.49/23"],"mac_address":"0a:58:0a:83:00:31","gateway_ips":["10.131.0.1"],"ip_address":"10.131.0.49/23","gateway_ip":"10.131.0.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.131.0.49"
        ],
        "mac": "0a:58:0a:83:00:31",
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.131.0.49"
        ],
        "mac": "0a:58:0a:83:00:31",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-cleanup-controller 54a94b24-f800-4ce7-947a-1529daaac321 0xc0027609c7 0xc0027609c8}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:00:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:00:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"54a94b24-f800-4ce7-947a-1529daaac321\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-17 15:00:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-17 15:00:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.0.49\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r498b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r498b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-151-22.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c34,c9,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:00:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:00:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:00:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:00:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.151.22,PodIP:10.131.0.49,StartTime:2023-01-17 15:00:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 15:00:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://c35e89f2f2a0998979abb05704a6e3bc1cbe6f5f37dfa3ba6fad5bb4a77dee0e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.131.0.49,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan 17 15:00:43.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-4635" for this suite. 01/17/23 15:00:43.408
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:00:43.416
Jan 17 15:00:43.416: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename pods 01/17/23 15:00:43.416
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:00:43.44
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:00:43.443
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617
Jan 17 15:00:43.445: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: creating the pod 01/17/23 15:00:43.445
STEP: submitting the pod to kubernetes 01/17/23 15:00:43.445
Jan 17 15:00:43.483: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-6a64ebbd-03a7-4894-bb84-f5cc267c1dd6" in namespace "pods-5492" to be "running and ready"
Jan 17 15:00:43.487: INFO: Pod "pod-logs-websocket-6a64ebbd-03a7-4894-bb84-f5cc267c1dd6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.996067ms
Jan 17 15:00:43.487: INFO: The phase of Pod pod-logs-websocket-6a64ebbd-03a7-4894-bb84-f5cc267c1dd6 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 15:00:45.491: INFO: Pod "pod-logs-websocket-6a64ebbd-03a7-4894-bb84-f5cc267c1dd6": Phase="Running", Reason="", readiness=true. Elapsed: 2.007769857s
Jan 17 15:00:45.491: INFO: The phase of Pod pod-logs-websocket-6a64ebbd-03a7-4894-bb84-f5cc267c1dd6 is Running (Ready = true)
Jan 17 15:00:45.491: INFO: Pod "pod-logs-websocket-6a64ebbd-03a7-4894-bb84-f5cc267c1dd6" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 17 15:00:45.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5492" for this suite. 01/17/23 15:00:45.512
{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","completed":45,"skipped":805,"failed":0}
------------------------------
• [2.102 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:00:43.416
    Jan 17 15:00:43.416: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename pods 01/17/23 15:00:43.416
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:00:43.44
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:00:43.443
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:617
    Jan 17 15:00:43.445: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: creating the pod 01/17/23 15:00:43.445
    STEP: submitting the pod to kubernetes 01/17/23 15:00:43.445
    Jan 17 15:00:43.483: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-6a64ebbd-03a7-4894-bb84-f5cc267c1dd6" in namespace "pods-5492" to be "running and ready"
    Jan 17 15:00:43.487: INFO: Pod "pod-logs-websocket-6a64ebbd-03a7-4894-bb84-f5cc267c1dd6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.996067ms
    Jan 17 15:00:43.487: INFO: The phase of Pod pod-logs-websocket-6a64ebbd-03a7-4894-bb84-f5cc267c1dd6 is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 15:00:45.491: INFO: Pod "pod-logs-websocket-6a64ebbd-03a7-4894-bb84-f5cc267c1dd6": Phase="Running", Reason="", readiness=true. Elapsed: 2.007769857s
    Jan 17 15:00:45.491: INFO: The phase of Pod pod-logs-websocket-6a64ebbd-03a7-4894-bb84-f5cc267c1dd6 is Running (Ready = true)
    Jan 17 15:00:45.491: INFO: Pod "pod-logs-websocket-6a64ebbd-03a7-4894-bb84-f5cc267c1dd6" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 17 15:00:45.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-5492" for this suite. 01/17/23 15:00:45.512
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:00:45.518
Jan 17 15:00:45.518: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename services 01/17/23 15:00:45.519
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:00:45.541
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:00:45.544
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204
STEP: creating service in namespace services-2997 01/17/23 15:00:45.545
STEP: creating service affinity-nodeport in namespace services-2997 01/17/23 15:00:45.546
STEP: creating replication controller affinity-nodeport in namespace services-2997 01/17/23 15:00:45.569
I0117 15:00:45.577805      22 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-2997, replica count: 3
I0117 15:00:48.629203      22 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 17 15:00:48.643: INFO: Creating new exec pod
Jan 17 15:00:48.656: INFO: Waiting up to 5m0s for pod "execpod-affinity4ch4x" in namespace "services-2997" to be "running"
Jan 17 15:00:48.660: INFO: Pod "execpod-affinity4ch4x": Phase="Pending", Reason="", readiness=false. Elapsed: 4.162831ms
Jan 17 15:00:50.664: INFO: Pod "execpod-affinity4ch4x": Phase="Running", Reason="", readiness=true. Elapsed: 2.008447408s
Jan 17 15:00:50.664: INFO: Pod "execpod-affinity4ch4x" satisfied condition "running"
Jan 17 15:00:51.668: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-2997 exec execpod-affinity4ch4x -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
Jan 17 15:00:52.813: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Jan 17 15:00:52.813: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 17 15:00:52.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-2997 exec execpod-affinity4ch4x -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.21.233 80'
Jan 17 15:00:52.937: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.21.233 80\nConnection to 172.30.21.233 80 port [tcp/http] succeeded!\n"
Jan 17 15:00:52.937: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 17 15:00:52.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-2997 exec execpod-affinity4ch4x -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.139.213 32247'
Jan 17 15:00:54.097: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.139.213 32247\nConnection to 10.0.139.213 32247 port [tcp/*] succeeded!\n"
Jan 17 15:00:54.097: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 17 15:00:54.097: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-2997 exec execpod-affinity4ch4x -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.165.14 32247'
Jan 17 15:00:55.248: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.165.14 32247\nConnection to 10.0.165.14 32247 port [tcp/*] succeeded!\n"
Jan 17 15:00:55.248: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 17 15:00:55.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-2997 exec execpod-affinity4ch4x -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.139.213:32247/ ; done'
Jan 17 15:00:55.439: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32247/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32247/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32247/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32247/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32247/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32247/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32247/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32247/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32247/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32247/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32247/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32247/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32247/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32247/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32247/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32247/\n"
Jan 17 15:00:55.439: INFO: stdout: "\naffinity-nodeport-kcgrw\naffinity-nodeport-kcgrw\naffinity-nodeport-kcgrw\naffinity-nodeport-kcgrw\naffinity-nodeport-kcgrw\naffinity-nodeport-kcgrw\naffinity-nodeport-kcgrw\naffinity-nodeport-kcgrw\naffinity-nodeport-kcgrw\naffinity-nodeport-kcgrw\naffinity-nodeport-kcgrw\naffinity-nodeport-kcgrw\naffinity-nodeport-kcgrw\naffinity-nodeport-kcgrw\naffinity-nodeport-kcgrw\naffinity-nodeport-kcgrw"
Jan 17 15:00:55.439: INFO: Received response from host: affinity-nodeport-kcgrw
Jan 17 15:00:55.439: INFO: Received response from host: affinity-nodeport-kcgrw
Jan 17 15:00:55.439: INFO: Received response from host: affinity-nodeport-kcgrw
Jan 17 15:00:55.439: INFO: Received response from host: affinity-nodeport-kcgrw
Jan 17 15:00:55.439: INFO: Received response from host: affinity-nodeport-kcgrw
Jan 17 15:00:55.439: INFO: Received response from host: affinity-nodeport-kcgrw
Jan 17 15:00:55.439: INFO: Received response from host: affinity-nodeport-kcgrw
Jan 17 15:00:55.439: INFO: Received response from host: affinity-nodeport-kcgrw
Jan 17 15:00:55.439: INFO: Received response from host: affinity-nodeport-kcgrw
Jan 17 15:00:55.439: INFO: Received response from host: affinity-nodeport-kcgrw
Jan 17 15:00:55.439: INFO: Received response from host: affinity-nodeport-kcgrw
Jan 17 15:00:55.439: INFO: Received response from host: affinity-nodeport-kcgrw
Jan 17 15:00:55.439: INFO: Received response from host: affinity-nodeport-kcgrw
Jan 17 15:00:55.439: INFO: Received response from host: affinity-nodeport-kcgrw
Jan 17 15:00:55.439: INFO: Received response from host: affinity-nodeport-kcgrw
Jan 17 15:00:55.439: INFO: Received response from host: affinity-nodeport-kcgrw
Jan 17 15:00:55.439: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-2997, will wait for the garbage collector to delete the pods 01/17/23 15:00:55.45
Jan 17 15:00:55.510: INFO: Deleting ReplicationController affinity-nodeport took: 6.534987ms
Jan 17 15:00:55.610: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.351281ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 17 15:00:57.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2997" for this suite. 01/17/23 15:00:57.861
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","completed":46,"skipped":813,"failed":0}
------------------------------
• [SLOW TEST] [12.355 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:00:45.518
    Jan 17 15:00:45.518: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename services 01/17/23 15:00:45.519
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:00:45.541
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:00:45.544
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2204
    STEP: creating service in namespace services-2997 01/17/23 15:00:45.545
    STEP: creating service affinity-nodeport in namespace services-2997 01/17/23 15:00:45.546
    STEP: creating replication controller affinity-nodeport in namespace services-2997 01/17/23 15:00:45.569
    I0117 15:00:45.577805      22 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-2997, replica count: 3
    I0117 15:00:48.629203      22 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 17 15:00:48.643: INFO: Creating new exec pod
    Jan 17 15:00:48.656: INFO: Waiting up to 5m0s for pod "execpod-affinity4ch4x" in namespace "services-2997" to be "running"
    Jan 17 15:00:48.660: INFO: Pod "execpod-affinity4ch4x": Phase="Pending", Reason="", readiness=false. Elapsed: 4.162831ms
    Jan 17 15:00:50.664: INFO: Pod "execpod-affinity4ch4x": Phase="Running", Reason="", readiness=true. Elapsed: 2.008447408s
    Jan 17 15:00:50.664: INFO: Pod "execpod-affinity4ch4x" satisfied condition "running"
    Jan 17 15:00:51.668: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-2997 exec execpod-affinity4ch4x -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
    Jan 17 15:00:52.813: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    Jan 17 15:00:52.813: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 17 15:00:52.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-2997 exec execpod-affinity4ch4x -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.21.233 80'
    Jan 17 15:00:52.937: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.21.233 80\nConnection to 172.30.21.233 80 port [tcp/http] succeeded!\n"
    Jan 17 15:00:52.937: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 17 15:00:52.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-2997 exec execpod-affinity4ch4x -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.139.213 32247'
    Jan 17 15:00:54.097: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.139.213 32247\nConnection to 10.0.139.213 32247 port [tcp/*] succeeded!\n"
    Jan 17 15:00:54.097: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 17 15:00:54.097: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-2997 exec execpod-affinity4ch4x -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.165.14 32247'
    Jan 17 15:00:55.248: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.165.14 32247\nConnection to 10.0.165.14 32247 port [tcp/*] succeeded!\n"
    Jan 17 15:00:55.248: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 17 15:00:55.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-2997 exec execpod-affinity4ch4x -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.139.213:32247/ ; done'
    Jan 17 15:00:55.439: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32247/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32247/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32247/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32247/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32247/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32247/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32247/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32247/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32247/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32247/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32247/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32247/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32247/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32247/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32247/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:32247/\n"
    Jan 17 15:00:55.439: INFO: stdout: "\naffinity-nodeport-kcgrw\naffinity-nodeport-kcgrw\naffinity-nodeport-kcgrw\naffinity-nodeport-kcgrw\naffinity-nodeport-kcgrw\naffinity-nodeport-kcgrw\naffinity-nodeport-kcgrw\naffinity-nodeport-kcgrw\naffinity-nodeport-kcgrw\naffinity-nodeport-kcgrw\naffinity-nodeport-kcgrw\naffinity-nodeport-kcgrw\naffinity-nodeport-kcgrw\naffinity-nodeport-kcgrw\naffinity-nodeport-kcgrw\naffinity-nodeport-kcgrw"
    Jan 17 15:00:55.439: INFO: Received response from host: affinity-nodeport-kcgrw
    Jan 17 15:00:55.439: INFO: Received response from host: affinity-nodeport-kcgrw
    Jan 17 15:00:55.439: INFO: Received response from host: affinity-nodeport-kcgrw
    Jan 17 15:00:55.439: INFO: Received response from host: affinity-nodeport-kcgrw
    Jan 17 15:00:55.439: INFO: Received response from host: affinity-nodeport-kcgrw
    Jan 17 15:00:55.439: INFO: Received response from host: affinity-nodeport-kcgrw
    Jan 17 15:00:55.439: INFO: Received response from host: affinity-nodeport-kcgrw
    Jan 17 15:00:55.439: INFO: Received response from host: affinity-nodeport-kcgrw
    Jan 17 15:00:55.439: INFO: Received response from host: affinity-nodeport-kcgrw
    Jan 17 15:00:55.439: INFO: Received response from host: affinity-nodeport-kcgrw
    Jan 17 15:00:55.439: INFO: Received response from host: affinity-nodeport-kcgrw
    Jan 17 15:00:55.439: INFO: Received response from host: affinity-nodeport-kcgrw
    Jan 17 15:00:55.439: INFO: Received response from host: affinity-nodeport-kcgrw
    Jan 17 15:00:55.439: INFO: Received response from host: affinity-nodeport-kcgrw
    Jan 17 15:00:55.439: INFO: Received response from host: affinity-nodeport-kcgrw
    Jan 17 15:00:55.439: INFO: Received response from host: affinity-nodeport-kcgrw
    Jan 17 15:00:55.439: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-2997, will wait for the garbage collector to delete the pods 01/17/23 15:00:55.45
    Jan 17 15:00:55.510: INFO: Deleting ReplicationController affinity-nodeport took: 6.534987ms
    Jan 17 15:00:55.610: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.351281ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 17 15:00:57.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-2997" for this suite. 01/17/23 15:00:57.861
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:00:57.874
Jan 17 15:00:57.874: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename cronjob 01/17/23 15:00:57.875
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:00:57.917
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:00:57.919
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 01/17/23 15:00:57.921
W0117 15:00:57.934708      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring more than one job is running at a time 01/17/23 15:00:57.934
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 01/17/23 15:02:01.939
STEP: Removing cronjob 01/17/23 15:02:01.942
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Jan 17 15:02:01.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-7191" for this suite. 01/17/23 15:02:01.952
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","completed":47,"skipped":818,"failed":0}
------------------------------
• [SLOW TEST] [64.084 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:00:57.874
    Jan 17 15:00:57.874: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename cronjob 01/17/23 15:00:57.875
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:00:57.917
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:00:57.919
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 01/17/23 15:00:57.921
    W0117 15:00:57.934708      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Ensuring more than one job is running at a time 01/17/23 15:00:57.934
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 01/17/23 15:02:01.939
    STEP: Removing cronjob 01/17/23 15:02:01.942
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Jan 17 15:02:01.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-7191" for this suite. 01/17/23 15:02:01.952
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:02:01.959
Jan 17 15:02:01.959: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename daemonsets 01/17/23 15:02:01.96
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:02:01.991
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:02:01.993
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193
Jan 17 15:02:02.038: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 01/17/23 15:02:02.048
Jan 17 15:02:02.056: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 17 15:02:02.056: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 01/17/23 15:02:02.056
Jan 17 15:02:02.082: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 17 15:02:02.082: INFO: Node ip-10-0-165-14.ec2.internal is running 0 daemon pod, expected 1
Jan 17 15:02:03.086: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 17 15:02:03.086: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 01/17/23 15:02:03.089
Jan 17 15:02:03.103: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 17 15:02:03.103: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Jan 17 15:02:04.106: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 17 15:02:04.106: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 01/17/23 15:02:04.106
Jan 17 15:02:04.117: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 17 15:02:04.117: INFO: Node ip-10-0-165-14.ec2.internal is running 0 daemon pod, expected 1
Jan 17 15:02:05.120: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 17 15:02:05.120: INFO: Node ip-10-0-165-14.ec2.internal is running 0 daemon pod, expected 1
Jan 17 15:02:06.120: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 17 15:02:06.120: INFO: Node ip-10-0-165-14.ec2.internal is running 0 daemon pod, expected 1
Jan 17 15:02:07.120: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 17 15:02:07.120: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 01/17/23 15:02:07.127
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3700, will wait for the garbage collector to delete the pods 01/17/23 15:02:07.127
Jan 17 15:02:07.187: INFO: Deleting DaemonSet.extensions daemon-set took: 5.542016ms
Jan 17 15:02:07.287: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.472429ms
Jan 17 15:02:09.890: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 17 15:02:09.890: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 17 15:02:09.894: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"77085"},"items":null}

Jan 17 15:02:09.897: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"77085"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan 17 15:02:09.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3700" for this suite. 01/17/23 15:02:09.93
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","completed":48,"skipped":819,"failed":0}
------------------------------
• [SLOW TEST] [7.981 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:02:01.959
    Jan 17 15:02:01.959: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename daemonsets 01/17/23 15:02:01.96
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:02:01.991
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:02:01.993
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:193
    Jan 17 15:02:02.038: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 01/17/23 15:02:02.048
    Jan 17 15:02:02.056: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 17 15:02:02.056: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 01/17/23 15:02:02.056
    Jan 17 15:02:02.082: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 17 15:02:02.082: INFO: Node ip-10-0-165-14.ec2.internal is running 0 daemon pod, expected 1
    Jan 17 15:02:03.086: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 17 15:02:03.086: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 01/17/23 15:02:03.089
    Jan 17 15:02:03.103: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 17 15:02:03.103: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
    Jan 17 15:02:04.106: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 17 15:02:04.106: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 01/17/23 15:02:04.106
    Jan 17 15:02:04.117: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 17 15:02:04.117: INFO: Node ip-10-0-165-14.ec2.internal is running 0 daemon pod, expected 1
    Jan 17 15:02:05.120: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 17 15:02:05.120: INFO: Node ip-10-0-165-14.ec2.internal is running 0 daemon pod, expected 1
    Jan 17 15:02:06.120: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 17 15:02:06.120: INFO: Node ip-10-0-165-14.ec2.internal is running 0 daemon pod, expected 1
    Jan 17 15:02:07.120: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 17 15:02:07.120: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 01/17/23 15:02:07.127
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3700, will wait for the garbage collector to delete the pods 01/17/23 15:02:07.127
    Jan 17 15:02:07.187: INFO: Deleting DaemonSet.extensions daemon-set took: 5.542016ms
    Jan 17 15:02:07.287: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.472429ms
    Jan 17 15:02:09.890: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 17 15:02:09.890: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 17 15:02:09.894: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"77085"},"items":null}

    Jan 17 15:02:09.897: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"77085"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan 17 15:02:09.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-3700" for this suite. 01/17/23 15:02:09.93
  << End Captured GinkgoWriter Output
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:02:09.94
Jan 17 15:02:09.940: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename svcaccounts 01/17/23 15:02:09.941
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:02:09.971
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:02:09.981
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158
Jan 17 15:02:10.044: INFO: created pod pod-service-account-defaultsa
Jan 17 15:02:10.044: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jan 17 15:02:10.069: INFO: created pod pod-service-account-mountsa
Jan 17 15:02:10.069: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jan 17 15:02:10.098: INFO: created pod pod-service-account-nomountsa
Jan 17 15:02:10.098: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jan 17 15:02:10.125: INFO: created pod pod-service-account-defaultsa-mountspec
Jan 17 15:02:10.125: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jan 17 15:02:10.162: INFO: created pod pod-service-account-mountsa-mountspec
Jan 17 15:02:10.162: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jan 17 15:02:10.192: INFO: created pod pod-service-account-nomountsa-mountspec
Jan 17 15:02:10.192: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jan 17 15:02:10.206: INFO: created pod pod-service-account-defaultsa-nomountspec
Jan 17 15:02:10.206: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jan 17 15:02:10.226: INFO: created pod pod-service-account-mountsa-nomountspec
Jan 17 15:02:10.226: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jan 17 15:02:10.257: INFO: created pod pod-service-account-nomountsa-nomountspec
Jan 17 15:02:10.257: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jan 17 15:02:10.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-652" for this suite. 01/17/23 15:02:10.261
{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","completed":49,"skipped":819,"failed":0}
------------------------------
• [0.330 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:02:09.94
    Jan 17 15:02:09.940: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename svcaccounts 01/17/23 15:02:09.941
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:02:09.971
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:02:09.981
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:158
    Jan 17 15:02:10.044: INFO: created pod pod-service-account-defaultsa
    Jan 17 15:02:10.044: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    Jan 17 15:02:10.069: INFO: created pod pod-service-account-mountsa
    Jan 17 15:02:10.069: INFO: pod pod-service-account-mountsa service account token volume mount: true
    Jan 17 15:02:10.098: INFO: created pod pod-service-account-nomountsa
    Jan 17 15:02:10.098: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    Jan 17 15:02:10.125: INFO: created pod pod-service-account-defaultsa-mountspec
    Jan 17 15:02:10.125: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    Jan 17 15:02:10.162: INFO: created pod pod-service-account-mountsa-mountspec
    Jan 17 15:02:10.162: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    Jan 17 15:02:10.192: INFO: created pod pod-service-account-nomountsa-mountspec
    Jan 17 15:02:10.192: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    Jan 17 15:02:10.206: INFO: created pod pod-service-account-defaultsa-nomountspec
    Jan 17 15:02:10.206: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    Jan 17 15:02:10.226: INFO: created pod pod-service-account-mountsa-nomountspec
    Jan 17 15:02:10.226: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    Jan 17 15:02:10.257: INFO: created pod pod-service-account-nomountsa-nomountspec
    Jan 17 15:02:10.257: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jan 17 15:02:10.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-652" for this suite. 01/17/23 15:02:10.261
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:02:10.27
Jan 17 15:02:10.270: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename emptydir-wrapper 01/17/23 15:02:10.271
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:02:10.345
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:02:10.347
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 01/17/23 15:02:10.349
STEP: Creating RC which spawns configmap-volume pods 01/17/23 15:02:10.636
Jan 17 15:02:10.646: INFO: Pod name wrapped-volume-race-d92d1429-4404-47c0-a0f8-8ebf731f834e: Found 0 pods out of 5
Jan 17 15:02:15.660: INFO: Pod name wrapped-volume-race-d92d1429-4404-47c0-a0f8-8ebf731f834e: Found 5 pods out of 5
STEP: Ensuring each pod is running 01/17/23 15:02:15.66
Jan 17 15:02:15.660: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d92d1429-4404-47c0-a0f8-8ebf731f834e-5n699" in namespace "emptydir-wrapper-2428" to be "running"
Jan 17 15:02:15.664: INFO: Pod "wrapped-volume-race-d92d1429-4404-47c0-a0f8-8ebf731f834e-5n699": Phase="Running", Reason="", readiness=true. Elapsed: 3.842983ms
Jan 17 15:02:15.664: INFO: Pod "wrapped-volume-race-d92d1429-4404-47c0-a0f8-8ebf731f834e-5n699" satisfied condition "running"
Jan 17 15:02:15.664: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d92d1429-4404-47c0-a0f8-8ebf731f834e-5q8dh" in namespace "emptydir-wrapper-2428" to be "running"
Jan 17 15:02:15.667: INFO: Pod "wrapped-volume-race-d92d1429-4404-47c0-a0f8-8ebf731f834e-5q8dh": Phase="Running", Reason="", readiness=true. Elapsed: 2.843934ms
Jan 17 15:02:15.667: INFO: Pod "wrapped-volume-race-d92d1429-4404-47c0-a0f8-8ebf731f834e-5q8dh" satisfied condition "running"
Jan 17 15:02:15.667: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d92d1429-4404-47c0-a0f8-8ebf731f834e-ccx28" in namespace "emptydir-wrapper-2428" to be "running"
Jan 17 15:02:15.670: INFO: Pod "wrapped-volume-race-d92d1429-4404-47c0-a0f8-8ebf731f834e-ccx28": Phase="Running", Reason="", readiness=true. Elapsed: 2.683789ms
Jan 17 15:02:15.670: INFO: Pod "wrapped-volume-race-d92d1429-4404-47c0-a0f8-8ebf731f834e-ccx28" satisfied condition "running"
Jan 17 15:02:15.670: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d92d1429-4404-47c0-a0f8-8ebf731f834e-dcmgg" in namespace "emptydir-wrapper-2428" to be "running"
Jan 17 15:02:15.682: INFO: Pod "wrapped-volume-race-d92d1429-4404-47c0-a0f8-8ebf731f834e-dcmgg": Phase="Running", Reason="", readiness=true. Elapsed: 11.834067ms
Jan 17 15:02:15.682: INFO: Pod "wrapped-volume-race-d92d1429-4404-47c0-a0f8-8ebf731f834e-dcmgg" satisfied condition "running"
Jan 17 15:02:15.682: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d92d1429-4404-47c0-a0f8-8ebf731f834e-hzqbx" in namespace "emptydir-wrapper-2428" to be "running"
Jan 17 15:02:15.686: INFO: Pod "wrapped-volume-race-d92d1429-4404-47c0-a0f8-8ebf731f834e-hzqbx": Phase="Running", Reason="", readiness=true. Elapsed: 4.000982ms
Jan 17 15:02:15.686: INFO: Pod "wrapped-volume-race-d92d1429-4404-47c0-a0f8-8ebf731f834e-hzqbx" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-d92d1429-4404-47c0-a0f8-8ebf731f834e in namespace emptydir-wrapper-2428, will wait for the garbage collector to delete the pods 01/17/23 15:02:15.686
Jan 17 15:02:15.773: INFO: Deleting ReplicationController wrapped-volume-race-d92d1429-4404-47c0-a0f8-8ebf731f834e took: 33.449918ms
Jan 17 15:02:15.873: INFO: Terminating ReplicationController wrapped-volume-race-d92d1429-4404-47c0-a0f8-8ebf731f834e pods took: 100.307382ms
STEP: Creating RC which spawns configmap-volume pods 01/17/23 15:02:18.178
Jan 17 15:02:18.189: INFO: Pod name wrapped-volume-race-6c6c787b-bd42-4291-b543-b820446f99b8: Found 0 pods out of 5
Jan 17 15:02:23.193: INFO: Pod name wrapped-volume-race-6c6c787b-bd42-4291-b543-b820446f99b8: Found 5 pods out of 5
STEP: Ensuring each pod is running 01/17/23 15:02:23.193
Jan 17 15:02:23.193: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-6c6c787b-bd42-4291-b543-b820446f99b8-6bbb9" in namespace "emptydir-wrapper-2428" to be "running"
Jan 17 15:02:23.197: INFO: Pod "wrapped-volume-race-6c6c787b-bd42-4291-b543-b820446f99b8-6bbb9": Phase="Running", Reason="", readiness=true. Elapsed: 3.345164ms
Jan 17 15:02:23.197: INFO: Pod "wrapped-volume-race-6c6c787b-bd42-4291-b543-b820446f99b8-6bbb9" satisfied condition "running"
Jan 17 15:02:23.197: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-6c6c787b-bd42-4291-b543-b820446f99b8-d7tp8" in namespace "emptydir-wrapper-2428" to be "running"
Jan 17 15:02:23.199: INFO: Pod "wrapped-volume-race-6c6c787b-bd42-4291-b543-b820446f99b8-d7tp8": Phase="Running", Reason="", readiness=true. Elapsed: 2.728959ms
Jan 17 15:02:23.199: INFO: Pod "wrapped-volume-race-6c6c787b-bd42-4291-b543-b820446f99b8-d7tp8" satisfied condition "running"
Jan 17 15:02:23.199: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-6c6c787b-bd42-4291-b543-b820446f99b8-frptn" in namespace "emptydir-wrapper-2428" to be "running"
Jan 17 15:02:23.202: INFO: Pod "wrapped-volume-race-6c6c787b-bd42-4291-b543-b820446f99b8-frptn": Phase="Running", Reason="", readiness=true. Elapsed: 2.554093ms
Jan 17 15:02:23.202: INFO: Pod "wrapped-volume-race-6c6c787b-bd42-4291-b543-b820446f99b8-frptn" satisfied condition "running"
Jan 17 15:02:23.202: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-6c6c787b-bd42-4291-b543-b820446f99b8-hf2qq" in namespace "emptydir-wrapper-2428" to be "running"
Jan 17 15:02:23.205: INFO: Pod "wrapped-volume-race-6c6c787b-bd42-4291-b543-b820446f99b8-hf2qq": Phase="Running", Reason="", readiness=true. Elapsed: 2.692878ms
Jan 17 15:02:23.205: INFO: Pod "wrapped-volume-race-6c6c787b-bd42-4291-b543-b820446f99b8-hf2qq" satisfied condition "running"
Jan 17 15:02:23.205: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-6c6c787b-bd42-4291-b543-b820446f99b8-xhjdk" in namespace "emptydir-wrapper-2428" to be "running"
Jan 17 15:02:23.208: INFO: Pod "wrapped-volume-race-6c6c787b-bd42-4291-b543-b820446f99b8-xhjdk": Phase="Running", Reason="", readiness=true. Elapsed: 3.077309ms
Jan 17 15:02:23.208: INFO: Pod "wrapped-volume-race-6c6c787b-bd42-4291-b543-b820446f99b8-xhjdk" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-6c6c787b-bd42-4291-b543-b820446f99b8 in namespace emptydir-wrapper-2428, will wait for the garbage collector to delete the pods 01/17/23 15:02:23.208
Jan 17 15:02:23.268: INFO: Deleting ReplicationController wrapped-volume-race-6c6c787b-bd42-4291-b543-b820446f99b8 took: 7.169187ms
Jan 17 15:02:23.369: INFO: Terminating ReplicationController wrapped-volume-race-6c6c787b-bd42-4291-b543-b820446f99b8 pods took: 100.25237ms
STEP: Creating RC which spawns configmap-volume pods 01/17/23 15:02:25.174
Jan 17 15:02:25.185: INFO: Pod name wrapped-volume-race-b34676d3-39f1-4b51-9a6b-56b9b300cfc5: Found 0 pods out of 5
Jan 17 15:02:30.191: INFO: Pod name wrapped-volume-race-b34676d3-39f1-4b51-9a6b-56b9b300cfc5: Found 5 pods out of 5
STEP: Ensuring each pod is running 01/17/23 15:02:30.191
Jan 17 15:02:30.191: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b34676d3-39f1-4b51-9a6b-56b9b300cfc5-9jw8c" in namespace "emptydir-wrapper-2428" to be "running"
Jan 17 15:02:30.195: INFO: Pod "wrapped-volume-race-b34676d3-39f1-4b51-9a6b-56b9b300cfc5-9jw8c": Phase="Running", Reason="", readiness=true. Elapsed: 3.347593ms
Jan 17 15:02:30.195: INFO: Pod "wrapped-volume-race-b34676d3-39f1-4b51-9a6b-56b9b300cfc5-9jw8c" satisfied condition "running"
Jan 17 15:02:30.195: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b34676d3-39f1-4b51-9a6b-56b9b300cfc5-hb88p" in namespace "emptydir-wrapper-2428" to be "running"
Jan 17 15:02:30.198: INFO: Pod "wrapped-volume-race-b34676d3-39f1-4b51-9a6b-56b9b300cfc5-hb88p": Phase="Running", Reason="", readiness=true. Elapsed: 3.071209ms
Jan 17 15:02:30.198: INFO: Pod "wrapped-volume-race-b34676d3-39f1-4b51-9a6b-56b9b300cfc5-hb88p" satisfied condition "running"
Jan 17 15:02:30.198: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b34676d3-39f1-4b51-9a6b-56b9b300cfc5-r6mh2" in namespace "emptydir-wrapper-2428" to be "running"
Jan 17 15:02:30.201: INFO: Pod "wrapped-volume-race-b34676d3-39f1-4b51-9a6b-56b9b300cfc5-r6mh2": Phase="Running", Reason="", readiness=true. Elapsed: 2.840555ms
Jan 17 15:02:30.201: INFO: Pod "wrapped-volume-race-b34676d3-39f1-4b51-9a6b-56b9b300cfc5-r6mh2" satisfied condition "running"
Jan 17 15:02:30.201: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b34676d3-39f1-4b51-9a6b-56b9b300cfc5-t6sw8" in namespace "emptydir-wrapper-2428" to be "running"
Jan 17 15:02:30.204: INFO: Pod "wrapped-volume-race-b34676d3-39f1-4b51-9a6b-56b9b300cfc5-t6sw8": Phase="Running", Reason="", readiness=true. Elapsed: 3.07945ms
Jan 17 15:02:30.204: INFO: Pod "wrapped-volume-race-b34676d3-39f1-4b51-9a6b-56b9b300cfc5-t6sw8" satisfied condition "running"
Jan 17 15:02:30.204: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b34676d3-39f1-4b51-9a6b-56b9b300cfc5-x7xrm" in namespace "emptydir-wrapper-2428" to be "running"
Jan 17 15:02:30.207: INFO: Pod "wrapped-volume-race-b34676d3-39f1-4b51-9a6b-56b9b300cfc5-x7xrm": Phase="Running", Reason="", readiness=true. Elapsed: 3.164244ms
Jan 17 15:02:30.207: INFO: Pod "wrapped-volume-race-b34676d3-39f1-4b51-9a6b-56b9b300cfc5-x7xrm" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-b34676d3-39f1-4b51-9a6b-56b9b300cfc5 in namespace emptydir-wrapper-2428, will wait for the garbage collector to delete the pods 01/17/23 15:02:30.207
Jan 17 15:02:30.268: INFO: Deleting ReplicationController wrapped-volume-race-b34676d3-39f1-4b51-9a6b-56b9b300cfc5 took: 7.192203ms
Jan 17 15:02:30.369: INFO: Terminating ReplicationController wrapped-volume-race-b34676d3-39f1-4b51-9a6b-56b9b300cfc5 pods took: 101.1452ms
STEP: Cleaning up the configMaps 01/17/23 15:02:32.57
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
Jan 17 15:02:32.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-2428" for this suite. 01/17/23 15:02:32.898
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","completed":50,"skipped":824,"failed":0}
------------------------------
• [SLOW TEST] [22.634 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:02:10.27
    Jan 17 15:02:10.270: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename emptydir-wrapper 01/17/23 15:02:10.271
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:02:10.345
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:02:10.347
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 01/17/23 15:02:10.349
    STEP: Creating RC which spawns configmap-volume pods 01/17/23 15:02:10.636
    Jan 17 15:02:10.646: INFO: Pod name wrapped-volume-race-d92d1429-4404-47c0-a0f8-8ebf731f834e: Found 0 pods out of 5
    Jan 17 15:02:15.660: INFO: Pod name wrapped-volume-race-d92d1429-4404-47c0-a0f8-8ebf731f834e: Found 5 pods out of 5
    STEP: Ensuring each pod is running 01/17/23 15:02:15.66
    Jan 17 15:02:15.660: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d92d1429-4404-47c0-a0f8-8ebf731f834e-5n699" in namespace "emptydir-wrapper-2428" to be "running"
    Jan 17 15:02:15.664: INFO: Pod "wrapped-volume-race-d92d1429-4404-47c0-a0f8-8ebf731f834e-5n699": Phase="Running", Reason="", readiness=true. Elapsed: 3.842983ms
    Jan 17 15:02:15.664: INFO: Pod "wrapped-volume-race-d92d1429-4404-47c0-a0f8-8ebf731f834e-5n699" satisfied condition "running"
    Jan 17 15:02:15.664: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d92d1429-4404-47c0-a0f8-8ebf731f834e-5q8dh" in namespace "emptydir-wrapper-2428" to be "running"
    Jan 17 15:02:15.667: INFO: Pod "wrapped-volume-race-d92d1429-4404-47c0-a0f8-8ebf731f834e-5q8dh": Phase="Running", Reason="", readiness=true. Elapsed: 2.843934ms
    Jan 17 15:02:15.667: INFO: Pod "wrapped-volume-race-d92d1429-4404-47c0-a0f8-8ebf731f834e-5q8dh" satisfied condition "running"
    Jan 17 15:02:15.667: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d92d1429-4404-47c0-a0f8-8ebf731f834e-ccx28" in namespace "emptydir-wrapper-2428" to be "running"
    Jan 17 15:02:15.670: INFO: Pod "wrapped-volume-race-d92d1429-4404-47c0-a0f8-8ebf731f834e-ccx28": Phase="Running", Reason="", readiness=true. Elapsed: 2.683789ms
    Jan 17 15:02:15.670: INFO: Pod "wrapped-volume-race-d92d1429-4404-47c0-a0f8-8ebf731f834e-ccx28" satisfied condition "running"
    Jan 17 15:02:15.670: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d92d1429-4404-47c0-a0f8-8ebf731f834e-dcmgg" in namespace "emptydir-wrapper-2428" to be "running"
    Jan 17 15:02:15.682: INFO: Pod "wrapped-volume-race-d92d1429-4404-47c0-a0f8-8ebf731f834e-dcmgg": Phase="Running", Reason="", readiness=true. Elapsed: 11.834067ms
    Jan 17 15:02:15.682: INFO: Pod "wrapped-volume-race-d92d1429-4404-47c0-a0f8-8ebf731f834e-dcmgg" satisfied condition "running"
    Jan 17 15:02:15.682: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d92d1429-4404-47c0-a0f8-8ebf731f834e-hzqbx" in namespace "emptydir-wrapper-2428" to be "running"
    Jan 17 15:02:15.686: INFO: Pod "wrapped-volume-race-d92d1429-4404-47c0-a0f8-8ebf731f834e-hzqbx": Phase="Running", Reason="", readiness=true. Elapsed: 4.000982ms
    Jan 17 15:02:15.686: INFO: Pod "wrapped-volume-race-d92d1429-4404-47c0-a0f8-8ebf731f834e-hzqbx" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-d92d1429-4404-47c0-a0f8-8ebf731f834e in namespace emptydir-wrapper-2428, will wait for the garbage collector to delete the pods 01/17/23 15:02:15.686
    Jan 17 15:02:15.773: INFO: Deleting ReplicationController wrapped-volume-race-d92d1429-4404-47c0-a0f8-8ebf731f834e took: 33.449918ms
    Jan 17 15:02:15.873: INFO: Terminating ReplicationController wrapped-volume-race-d92d1429-4404-47c0-a0f8-8ebf731f834e pods took: 100.307382ms
    STEP: Creating RC which spawns configmap-volume pods 01/17/23 15:02:18.178
    Jan 17 15:02:18.189: INFO: Pod name wrapped-volume-race-6c6c787b-bd42-4291-b543-b820446f99b8: Found 0 pods out of 5
    Jan 17 15:02:23.193: INFO: Pod name wrapped-volume-race-6c6c787b-bd42-4291-b543-b820446f99b8: Found 5 pods out of 5
    STEP: Ensuring each pod is running 01/17/23 15:02:23.193
    Jan 17 15:02:23.193: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-6c6c787b-bd42-4291-b543-b820446f99b8-6bbb9" in namespace "emptydir-wrapper-2428" to be "running"
    Jan 17 15:02:23.197: INFO: Pod "wrapped-volume-race-6c6c787b-bd42-4291-b543-b820446f99b8-6bbb9": Phase="Running", Reason="", readiness=true. Elapsed: 3.345164ms
    Jan 17 15:02:23.197: INFO: Pod "wrapped-volume-race-6c6c787b-bd42-4291-b543-b820446f99b8-6bbb9" satisfied condition "running"
    Jan 17 15:02:23.197: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-6c6c787b-bd42-4291-b543-b820446f99b8-d7tp8" in namespace "emptydir-wrapper-2428" to be "running"
    Jan 17 15:02:23.199: INFO: Pod "wrapped-volume-race-6c6c787b-bd42-4291-b543-b820446f99b8-d7tp8": Phase="Running", Reason="", readiness=true. Elapsed: 2.728959ms
    Jan 17 15:02:23.199: INFO: Pod "wrapped-volume-race-6c6c787b-bd42-4291-b543-b820446f99b8-d7tp8" satisfied condition "running"
    Jan 17 15:02:23.199: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-6c6c787b-bd42-4291-b543-b820446f99b8-frptn" in namespace "emptydir-wrapper-2428" to be "running"
    Jan 17 15:02:23.202: INFO: Pod "wrapped-volume-race-6c6c787b-bd42-4291-b543-b820446f99b8-frptn": Phase="Running", Reason="", readiness=true. Elapsed: 2.554093ms
    Jan 17 15:02:23.202: INFO: Pod "wrapped-volume-race-6c6c787b-bd42-4291-b543-b820446f99b8-frptn" satisfied condition "running"
    Jan 17 15:02:23.202: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-6c6c787b-bd42-4291-b543-b820446f99b8-hf2qq" in namespace "emptydir-wrapper-2428" to be "running"
    Jan 17 15:02:23.205: INFO: Pod "wrapped-volume-race-6c6c787b-bd42-4291-b543-b820446f99b8-hf2qq": Phase="Running", Reason="", readiness=true. Elapsed: 2.692878ms
    Jan 17 15:02:23.205: INFO: Pod "wrapped-volume-race-6c6c787b-bd42-4291-b543-b820446f99b8-hf2qq" satisfied condition "running"
    Jan 17 15:02:23.205: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-6c6c787b-bd42-4291-b543-b820446f99b8-xhjdk" in namespace "emptydir-wrapper-2428" to be "running"
    Jan 17 15:02:23.208: INFO: Pod "wrapped-volume-race-6c6c787b-bd42-4291-b543-b820446f99b8-xhjdk": Phase="Running", Reason="", readiness=true. Elapsed: 3.077309ms
    Jan 17 15:02:23.208: INFO: Pod "wrapped-volume-race-6c6c787b-bd42-4291-b543-b820446f99b8-xhjdk" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-6c6c787b-bd42-4291-b543-b820446f99b8 in namespace emptydir-wrapper-2428, will wait for the garbage collector to delete the pods 01/17/23 15:02:23.208
    Jan 17 15:02:23.268: INFO: Deleting ReplicationController wrapped-volume-race-6c6c787b-bd42-4291-b543-b820446f99b8 took: 7.169187ms
    Jan 17 15:02:23.369: INFO: Terminating ReplicationController wrapped-volume-race-6c6c787b-bd42-4291-b543-b820446f99b8 pods took: 100.25237ms
    STEP: Creating RC which spawns configmap-volume pods 01/17/23 15:02:25.174
    Jan 17 15:02:25.185: INFO: Pod name wrapped-volume-race-b34676d3-39f1-4b51-9a6b-56b9b300cfc5: Found 0 pods out of 5
    Jan 17 15:02:30.191: INFO: Pod name wrapped-volume-race-b34676d3-39f1-4b51-9a6b-56b9b300cfc5: Found 5 pods out of 5
    STEP: Ensuring each pod is running 01/17/23 15:02:30.191
    Jan 17 15:02:30.191: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b34676d3-39f1-4b51-9a6b-56b9b300cfc5-9jw8c" in namespace "emptydir-wrapper-2428" to be "running"
    Jan 17 15:02:30.195: INFO: Pod "wrapped-volume-race-b34676d3-39f1-4b51-9a6b-56b9b300cfc5-9jw8c": Phase="Running", Reason="", readiness=true. Elapsed: 3.347593ms
    Jan 17 15:02:30.195: INFO: Pod "wrapped-volume-race-b34676d3-39f1-4b51-9a6b-56b9b300cfc5-9jw8c" satisfied condition "running"
    Jan 17 15:02:30.195: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b34676d3-39f1-4b51-9a6b-56b9b300cfc5-hb88p" in namespace "emptydir-wrapper-2428" to be "running"
    Jan 17 15:02:30.198: INFO: Pod "wrapped-volume-race-b34676d3-39f1-4b51-9a6b-56b9b300cfc5-hb88p": Phase="Running", Reason="", readiness=true. Elapsed: 3.071209ms
    Jan 17 15:02:30.198: INFO: Pod "wrapped-volume-race-b34676d3-39f1-4b51-9a6b-56b9b300cfc5-hb88p" satisfied condition "running"
    Jan 17 15:02:30.198: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b34676d3-39f1-4b51-9a6b-56b9b300cfc5-r6mh2" in namespace "emptydir-wrapper-2428" to be "running"
    Jan 17 15:02:30.201: INFO: Pod "wrapped-volume-race-b34676d3-39f1-4b51-9a6b-56b9b300cfc5-r6mh2": Phase="Running", Reason="", readiness=true. Elapsed: 2.840555ms
    Jan 17 15:02:30.201: INFO: Pod "wrapped-volume-race-b34676d3-39f1-4b51-9a6b-56b9b300cfc5-r6mh2" satisfied condition "running"
    Jan 17 15:02:30.201: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b34676d3-39f1-4b51-9a6b-56b9b300cfc5-t6sw8" in namespace "emptydir-wrapper-2428" to be "running"
    Jan 17 15:02:30.204: INFO: Pod "wrapped-volume-race-b34676d3-39f1-4b51-9a6b-56b9b300cfc5-t6sw8": Phase="Running", Reason="", readiness=true. Elapsed: 3.07945ms
    Jan 17 15:02:30.204: INFO: Pod "wrapped-volume-race-b34676d3-39f1-4b51-9a6b-56b9b300cfc5-t6sw8" satisfied condition "running"
    Jan 17 15:02:30.204: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b34676d3-39f1-4b51-9a6b-56b9b300cfc5-x7xrm" in namespace "emptydir-wrapper-2428" to be "running"
    Jan 17 15:02:30.207: INFO: Pod "wrapped-volume-race-b34676d3-39f1-4b51-9a6b-56b9b300cfc5-x7xrm": Phase="Running", Reason="", readiness=true. Elapsed: 3.164244ms
    Jan 17 15:02:30.207: INFO: Pod "wrapped-volume-race-b34676d3-39f1-4b51-9a6b-56b9b300cfc5-x7xrm" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-b34676d3-39f1-4b51-9a6b-56b9b300cfc5 in namespace emptydir-wrapper-2428, will wait for the garbage collector to delete the pods 01/17/23 15:02:30.207
    Jan 17 15:02:30.268: INFO: Deleting ReplicationController wrapped-volume-race-b34676d3-39f1-4b51-9a6b-56b9b300cfc5 took: 7.192203ms
    Jan 17 15:02:30.369: INFO: Terminating ReplicationController wrapped-volume-race-b34676d3-39f1-4b51-9a6b-56b9b300cfc5 pods took: 101.1452ms
    STEP: Cleaning up the configMaps 01/17/23 15:02:32.57
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:187
    Jan 17 15:02:32.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-wrapper-2428" for this suite. 01/17/23 15:02:32.898
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:02:32.905
Jan 17 15:02:32.905: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename watch 01/17/23 15:02:32.905
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:02:32.93
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:02:32.933
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 01/17/23 15:02:32.935
STEP: creating a new configmap 01/17/23 15:02:32.936
STEP: modifying the configmap once 01/17/23 15:02:32.944
STEP: closing the watch once it receives two notifications 01/17/23 15:02:32.955
Jan 17 15:02:32.956: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6650  417eefaa-12be-440d-9b7b-223fa068b0d5 77840 0 2023-01-17 15:02:32 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-17 15:02:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 17 15:02:32.956: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6650  417eefaa-12be-440d-9b7b-223fa068b0d5 77844 0 2023-01-17 15:02:32 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-17 15:02:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 01/17/23 15:02:32.956
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 01/17/23 15:02:32.979
STEP: deleting the configmap 01/17/23 15:02:32.98
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 01/17/23 15:02:32.993
Jan 17 15:02:32.993: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6650  417eefaa-12be-440d-9b7b-223fa068b0d5 77846 0 2023-01-17 15:02:32 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-17 15:02:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 17 15:02:32.994: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6650  417eefaa-12be-440d-9b7b-223fa068b0d5 77852 0 2023-01-17 15:02:32 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-17 15:02:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Jan 17 15:02:32.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6650" for this suite. 01/17/23 15:02:33.004
{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","completed":51,"skipped":838,"failed":0}
------------------------------
• [0.106 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:02:32.905
    Jan 17 15:02:32.905: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename watch 01/17/23 15:02:32.905
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:02:32.93
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:02:32.933
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 01/17/23 15:02:32.935
    STEP: creating a new configmap 01/17/23 15:02:32.936
    STEP: modifying the configmap once 01/17/23 15:02:32.944
    STEP: closing the watch once it receives two notifications 01/17/23 15:02:32.955
    Jan 17 15:02:32.956: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6650  417eefaa-12be-440d-9b7b-223fa068b0d5 77840 0 2023-01-17 15:02:32 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-17 15:02:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 17 15:02:32.956: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6650  417eefaa-12be-440d-9b7b-223fa068b0d5 77844 0 2023-01-17 15:02:32 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-17 15:02:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 01/17/23 15:02:32.956
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 01/17/23 15:02:32.979
    STEP: deleting the configmap 01/17/23 15:02:32.98
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 01/17/23 15:02:32.993
    Jan 17 15:02:32.993: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6650  417eefaa-12be-440d-9b7b-223fa068b0d5 77846 0 2023-01-17 15:02:32 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-17 15:02:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 17 15:02:32.994: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6650  417eefaa-12be-440d-9b7b-223fa068b0d5 77852 0 2023-01-17 15:02:32 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-17 15:02:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Jan 17 15:02:32.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-6650" for this suite. 01/17/23 15:02:33.004
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:02:33.012
Jan 17 15:02:33.012: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename namespaces 01/17/23 15:02:33.012
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:02:33.03
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:02:33.032
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298
STEP: Read namespace status 01/17/23 15:02:33.035
Jan 17 15:02:33.039: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 01/17/23 15:02:33.039
Jan 17 15:02:33.057: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 01/17/23 15:02:33.057
Jan 17 15:02:33.076: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Jan 17 15:02:33.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-1813" for this suite. 01/17/23 15:02:33.081
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should apply changes to a namespace status [Conformance]","completed":52,"skipped":865,"failed":0}
------------------------------
• [0.078 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:02:33.012
    Jan 17 15:02:33.012: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename namespaces 01/17/23 15:02:33.012
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:02:33.03
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:02:33.032
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:298
    STEP: Read namespace status 01/17/23 15:02:33.035
    Jan 17 15:02:33.039: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 01/17/23 15:02:33.039
    Jan 17 15:02:33.057: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 01/17/23 15:02:33.057
    Jan 17 15:02:33.076: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Jan 17 15:02:33.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-1813" for this suite. 01/17/23 15:02:33.081
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:02:33.09
Jan 17 15:02:33.090: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename emptydir 01/17/23 15:02:33.09
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:02:33.119
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:02:33.121
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136
STEP: Creating a pod to test emptydir 0666 on tmpfs 01/17/23 15:02:33.123
Jan 17 15:02:33.147: INFO: Waiting up to 5m0s for pod "pod-b66271f7-bb96-45e0-b48a-c109bc6b0fed" in namespace "emptydir-2539" to be "Succeeded or Failed"
Jan 17 15:02:33.150: INFO: Pod "pod-b66271f7-bb96-45e0-b48a-c109bc6b0fed": Phase="Pending", Reason="", readiness=false. Elapsed: 3.756287ms
Jan 17 15:02:35.155: INFO: Pod "pod-b66271f7-bb96-45e0-b48a-c109bc6b0fed": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008025165s
Jan 17 15:02:37.156: INFO: Pod "pod-b66271f7-bb96-45e0-b48a-c109bc6b0fed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009107653s
STEP: Saw pod success 01/17/23 15:02:37.156
Jan 17 15:02:37.156: INFO: Pod "pod-b66271f7-bb96-45e0-b48a-c109bc6b0fed" satisfied condition "Succeeded or Failed"
Jan 17 15:02:37.160: INFO: Trying to get logs from node ip-10-0-165-14.ec2.internal pod pod-b66271f7-bb96-45e0-b48a-c109bc6b0fed container test-container: <nil>
STEP: delete the pod 01/17/23 15:02:37.168
Jan 17 15:02:37.180: INFO: Waiting for pod pod-b66271f7-bb96-45e0-b48a-c109bc6b0fed to disappear
Jan 17 15:02:37.183: INFO: Pod pod-b66271f7-bb96-45e0-b48a-c109bc6b0fed no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 17 15:02:37.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2539" for this suite. 01/17/23 15:02:37.187
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":53,"skipped":865,"failed":0}
------------------------------
• [4.104 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:02:33.09
    Jan 17 15:02:33.090: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename emptydir 01/17/23 15:02:33.09
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:02:33.119
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:02:33.121
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:136
    STEP: Creating a pod to test emptydir 0666 on tmpfs 01/17/23 15:02:33.123
    Jan 17 15:02:33.147: INFO: Waiting up to 5m0s for pod "pod-b66271f7-bb96-45e0-b48a-c109bc6b0fed" in namespace "emptydir-2539" to be "Succeeded or Failed"
    Jan 17 15:02:33.150: INFO: Pod "pod-b66271f7-bb96-45e0-b48a-c109bc6b0fed": Phase="Pending", Reason="", readiness=false. Elapsed: 3.756287ms
    Jan 17 15:02:35.155: INFO: Pod "pod-b66271f7-bb96-45e0-b48a-c109bc6b0fed": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008025165s
    Jan 17 15:02:37.156: INFO: Pod "pod-b66271f7-bb96-45e0-b48a-c109bc6b0fed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009107653s
    STEP: Saw pod success 01/17/23 15:02:37.156
    Jan 17 15:02:37.156: INFO: Pod "pod-b66271f7-bb96-45e0-b48a-c109bc6b0fed" satisfied condition "Succeeded or Failed"
    Jan 17 15:02:37.160: INFO: Trying to get logs from node ip-10-0-165-14.ec2.internal pod pod-b66271f7-bb96-45e0-b48a-c109bc6b0fed container test-container: <nil>
    STEP: delete the pod 01/17/23 15:02:37.168
    Jan 17 15:02:37.180: INFO: Waiting for pod pod-b66271f7-bb96-45e0-b48a-c109bc6b0fed to disappear
    Jan 17 15:02:37.183: INFO: Pod pod-b66271f7-bb96-45e0-b48a-c109bc6b0fed no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 17 15:02:37.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-2539" for this suite. 01/17/23 15:02:37.187
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:02:37.194
Jan 17 15:02:37.194: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename projected 01/17/23 15:02:37.195
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:02:37.222
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:02:37.224
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52
STEP: Creating a pod to test downward API volume plugin 01/17/23 15:02:37.226
Jan 17 15:02:37.246: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e46292d1-5381-4582-8968-e7381e4da6e2" in namespace "projected-4163" to be "Succeeded or Failed"
Jan 17 15:02:37.249: INFO: Pod "downwardapi-volume-e46292d1-5381-4582-8968-e7381e4da6e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.904079ms
Jan 17 15:02:39.255: INFO: Pod "downwardapi-volume-e46292d1-5381-4582-8968-e7381e4da6e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009657762s
Jan 17 15:02:41.253: INFO: Pod "downwardapi-volume-e46292d1-5381-4582-8968-e7381e4da6e2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007830937s
STEP: Saw pod success 01/17/23 15:02:41.253
Jan 17 15:02:41.254: INFO: Pod "downwardapi-volume-e46292d1-5381-4582-8968-e7381e4da6e2" satisfied condition "Succeeded or Failed"
Jan 17 15:02:41.257: INFO: Trying to get logs from node ip-10-0-165-14.ec2.internal pod downwardapi-volume-e46292d1-5381-4582-8968-e7381e4da6e2 container client-container: <nil>
STEP: delete the pod 01/17/23 15:02:41.264
Jan 17 15:02:41.278: INFO: Waiting for pod downwardapi-volume-e46292d1-5381-4582-8968-e7381e4da6e2 to disappear
Jan 17 15:02:41.281: INFO: Pod downwardapi-volume-e46292d1-5381-4582-8968-e7381e4da6e2 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 17 15:02:41.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4163" for this suite. 01/17/23 15:02:41.285
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","completed":54,"skipped":875,"failed":0}
------------------------------
• [4.097 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:02:37.194
    Jan 17 15:02:37.194: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename projected 01/17/23 15:02:37.195
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:02:37.222
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:02:37.224
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:52
    STEP: Creating a pod to test downward API volume plugin 01/17/23 15:02:37.226
    Jan 17 15:02:37.246: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e46292d1-5381-4582-8968-e7381e4da6e2" in namespace "projected-4163" to be "Succeeded or Failed"
    Jan 17 15:02:37.249: INFO: Pod "downwardapi-volume-e46292d1-5381-4582-8968-e7381e4da6e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.904079ms
    Jan 17 15:02:39.255: INFO: Pod "downwardapi-volume-e46292d1-5381-4582-8968-e7381e4da6e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009657762s
    Jan 17 15:02:41.253: INFO: Pod "downwardapi-volume-e46292d1-5381-4582-8968-e7381e4da6e2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007830937s
    STEP: Saw pod success 01/17/23 15:02:41.253
    Jan 17 15:02:41.254: INFO: Pod "downwardapi-volume-e46292d1-5381-4582-8968-e7381e4da6e2" satisfied condition "Succeeded or Failed"
    Jan 17 15:02:41.257: INFO: Trying to get logs from node ip-10-0-165-14.ec2.internal pod downwardapi-volume-e46292d1-5381-4582-8968-e7381e4da6e2 container client-container: <nil>
    STEP: delete the pod 01/17/23 15:02:41.264
    Jan 17 15:02:41.278: INFO: Waiting for pod downwardapi-volume-e46292d1-5381-4582-8968-e7381e4da6e2 to disappear
    Jan 17 15:02:41.281: INFO: Pod downwardapi-volume-e46292d1-5381-4582-8968-e7381e4da6e2 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 17 15:02:41.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4163" for this suite. 01/17/23 15:02:41.285
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:02:41.292
Jan 17 15:02:41.292: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename projected 01/17/23 15:02:41.293
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:02:41.314
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:02:41.318
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220
STEP: Creating a pod to test downward API volume plugin 01/17/23 15:02:41.321
Jan 17 15:02:41.348: INFO: Waiting up to 5m0s for pod "downwardapi-volume-951f0903-4e63-48d3-b2e4-836a6884053d" in namespace "projected-2718" to be "Succeeded or Failed"
Jan 17 15:02:41.356: INFO: Pod "downwardapi-volume-951f0903-4e63-48d3-b2e4-836a6884053d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.607482ms
Jan 17 15:02:43.360: INFO: Pod "downwardapi-volume-951f0903-4e63-48d3-b2e4-836a6884053d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012301866s
Jan 17 15:02:45.361: INFO: Pod "downwardapi-volume-951f0903-4e63-48d3-b2e4-836a6884053d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012826409s
STEP: Saw pod success 01/17/23 15:02:45.361
Jan 17 15:02:45.361: INFO: Pod "downwardapi-volume-951f0903-4e63-48d3-b2e4-836a6884053d" satisfied condition "Succeeded or Failed"
Jan 17 15:02:45.368: INFO: Trying to get logs from node ip-10-0-165-14.ec2.internal pod downwardapi-volume-951f0903-4e63-48d3-b2e4-836a6884053d container client-container: <nil>
STEP: delete the pod 01/17/23 15:02:45.373
Jan 17 15:02:45.385: INFO: Waiting for pod downwardapi-volume-951f0903-4e63-48d3-b2e4-836a6884053d to disappear
Jan 17 15:02:45.387: INFO: Pod downwardapi-volume-951f0903-4e63-48d3-b2e4-836a6884053d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 17 15:02:45.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2718" for this suite. 01/17/23 15:02:45.393
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","completed":55,"skipped":911,"failed":0}
------------------------------
• [4.107 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:02:41.292
    Jan 17 15:02:41.292: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename projected 01/17/23 15:02:41.293
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:02:41.314
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:02:41.318
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:220
    STEP: Creating a pod to test downward API volume plugin 01/17/23 15:02:41.321
    Jan 17 15:02:41.348: INFO: Waiting up to 5m0s for pod "downwardapi-volume-951f0903-4e63-48d3-b2e4-836a6884053d" in namespace "projected-2718" to be "Succeeded or Failed"
    Jan 17 15:02:41.356: INFO: Pod "downwardapi-volume-951f0903-4e63-48d3-b2e4-836a6884053d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.607482ms
    Jan 17 15:02:43.360: INFO: Pod "downwardapi-volume-951f0903-4e63-48d3-b2e4-836a6884053d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012301866s
    Jan 17 15:02:45.361: INFO: Pod "downwardapi-volume-951f0903-4e63-48d3-b2e4-836a6884053d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012826409s
    STEP: Saw pod success 01/17/23 15:02:45.361
    Jan 17 15:02:45.361: INFO: Pod "downwardapi-volume-951f0903-4e63-48d3-b2e4-836a6884053d" satisfied condition "Succeeded or Failed"
    Jan 17 15:02:45.368: INFO: Trying to get logs from node ip-10-0-165-14.ec2.internal pod downwardapi-volume-951f0903-4e63-48d3-b2e4-836a6884053d container client-container: <nil>
    STEP: delete the pod 01/17/23 15:02:45.373
    Jan 17 15:02:45.385: INFO: Waiting for pod downwardapi-volume-951f0903-4e63-48d3-b2e4-836a6884053d to disappear
    Jan 17 15:02:45.387: INFO: Pod downwardapi-volume-951f0903-4e63-48d3-b2e4-836a6884053d no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 17 15:02:45.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2718" for this suite. 01/17/23 15:02:45.393
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:02:45.399
Jan 17 15:02:45.400: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename svcaccounts 01/17/23 15:02:45.4
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:02:45.432
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:02:45.434
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528
Jan 17 15:02:45.469: INFO: created pod
Jan 17 15:02:45.469: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-1449" to be "Succeeded or Failed"
Jan 17 15:02:45.481: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 11.887969ms
Jan 17 15:02:47.484: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015281471s
Jan 17 15:02:49.485: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015586975s
STEP: Saw pod success 01/17/23 15:02:49.485
Jan 17 15:02:49.485: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Jan 17 15:03:19.485: INFO: polling logs
Jan 17 15:03:19.492: INFO: Pod logs: 
I0117 15:02:46.160280       1 log.go:195] OK: Got token
I0117 15:02:46.160301       1 log.go:195] validating with in-cluster discovery
I0117 15:02:46.160581       1 log.go:195] OK: got issuer https://kubernetes.default.svc
I0117 15:02:46.160598       1 log.go:195] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-1449:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1673968365, NotBefore:1673967765, IssuedAt:1673967765, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1449", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"d7ed3f66-3805-4604-b64f-c31a6c2da081"}}}
I0117 15:02:46.169516       1 log.go:195] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc
I0117 15:02:46.181414       1 log.go:195] OK: Validated signature on JWT
I0117 15:02:46.181464       1 log.go:195] OK: Got valid claims from token!
I0117 15:02:46.181480       1 log.go:195] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-1449:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1673968365, NotBefore:1673967765, IssuedAt:1673967765, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1449", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"d7ed3f66-3805-4604-b64f-c31a6c2da081"}}}

Jan 17 15:03:19.492: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jan 17 15:03:19.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1449" for this suite. 01/17/23 15:03:19.502
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","completed":56,"skipped":947,"failed":0}
------------------------------
• [SLOW TEST] [34.110 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:02:45.399
    Jan 17 15:02:45.400: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename svcaccounts 01/17/23 15:02:45.4
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:02:45.432
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:02:45.434
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:528
    Jan 17 15:02:45.469: INFO: created pod
    Jan 17 15:02:45.469: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-1449" to be "Succeeded or Failed"
    Jan 17 15:02:45.481: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 11.887969ms
    Jan 17 15:02:47.484: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015281471s
    Jan 17 15:02:49.485: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015586975s
    STEP: Saw pod success 01/17/23 15:02:49.485
    Jan 17 15:02:49.485: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    Jan 17 15:03:19.485: INFO: polling logs
    Jan 17 15:03:19.492: INFO: Pod logs: 
    I0117 15:02:46.160280       1 log.go:195] OK: Got token
    I0117 15:02:46.160301       1 log.go:195] validating with in-cluster discovery
    I0117 15:02:46.160581       1 log.go:195] OK: got issuer https://kubernetes.default.svc
    I0117 15:02:46.160598       1 log.go:195] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-1449:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1673968365, NotBefore:1673967765, IssuedAt:1673967765, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1449", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"d7ed3f66-3805-4604-b64f-c31a6c2da081"}}}
    I0117 15:02:46.169516       1 log.go:195] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc
    I0117 15:02:46.181414       1 log.go:195] OK: Validated signature on JWT
    I0117 15:02:46.181464       1 log.go:195] OK: Got valid claims from token!
    I0117 15:02:46.181480       1 log.go:195] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-1449:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1673968365, NotBefore:1673967765, IssuedAt:1673967765, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1449", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"d7ed3f66-3805-4604-b64f-c31a6c2da081"}}}

    Jan 17 15:03:19.492: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jan 17 15:03:19.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-1449" for this suite. 01/17/23 15:03:19.502
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:03:19.51
Jan 17 15:03:19.511: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename pods 01/17/23 15:03:19.511
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:03:19.549
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:03:19.551
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203
STEP: creating pod 01/17/23 15:03:19.553
Jan 17 15:03:19.596: INFO: Waiting up to 5m0s for pod "pod-hostip-e13d11ff-5dd8-429c-9634-8deb759dc006" in namespace "pods-5014" to be "running and ready"
Jan 17 15:03:19.611: INFO: Pod "pod-hostip-e13d11ff-5dd8-429c-9634-8deb759dc006": Phase="Pending", Reason="", readiness=false. Elapsed: 14.970115ms
Jan 17 15:03:19.611: INFO: The phase of Pod pod-hostip-e13d11ff-5dd8-429c-9634-8deb759dc006 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 15:03:21.616: INFO: Pod "pod-hostip-e13d11ff-5dd8-429c-9634-8deb759dc006": Phase="Running", Reason="", readiness=true. Elapsed: 2.019694575s
Jan 17 15:03:21.616: INFO: The phase of Pod pod-hostip-e13d11ff-5dd8-429c-9634-8deb759dc006 is Running (Ready = true)
Jan 17 15:03:21.616: INFO: Pod "pod-hostip-e13d11ff-5dd8-429c-9634-8deb759dc006" satisfied condition "running and ready"
Jan 17 15:03:21.621: INFO: Pod pod-hostip-e13d11ff-5dd8-429c-9634-8deb759dc006 has hostIP: 10.0.165.14
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 17 15:03:21.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5014" for this suite. 01/17/23 15:03:21.625
{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","completed":57,"skipped":977,"failed":0}
------------------------------
• [2.122 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:03:19.51
    Jan 17 15:03:19.511: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename pods 01/17/23 15:03:19.511
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:03:19.549
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:03:19.551
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:203
    STEP: creating pod 01/17/23 15:03:19.553
    Jan 17 15:03:19.596: INFO: Waiting up to 5m0s for pod "pod-hostip-e13d11ff-5dd8-429c-9634-8deb759dc006" in namespace "pods-5014" to be "running and ready"
    Jan 17 15:03:19.611: INFO: Pod "pod-hostip-e13d11ff-5dd8-429c-9634-8deb759dc006": Phase="Pending", Reason="", readiness=false. Elapsed: 14.970115ms
    Jan 17 15:03:19.611: INFO: The phase of Pod pod-hostip-e13d11ff-5dd8-429c-9634-8deb759dc006 is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 15:03:21.616: INFO: Pod "pod-hostip-e13d11ff-5dd8-429c-9634-8deb759dc006": Phase="Running", Reason="", readiness=true. Elapsed: 2.019694575s
    Jan 17 15:03:21.616: INFO: The phase of Pod pod-hostip-e13d11ff-5dd8-429c-9634-8deb759dc006 is Running (Ready = true)
    Jan 17 15:03:21.616: INFO: Pod "pod-hostip-e13d11ff-5dd8-429c-9634-8deb759dc006" satisfied condition "running and ready"
    Jan 17 15:03:21.621: INFO: Pod pod-hostip-e13d11ff-5dd8-429c-9634-8deb759dc006 has hostIP: 10.0.165.14
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 17 15:03:21.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-5014" for this suite. 01/17/23 15:03:21.625
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:03:21.633
Jan 17 15:03:21.633: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename crd-watch 01/17/23 15:03:21.634
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:03:21.657
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:03:21.659
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
Jan 17 15:03:21.661: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Creating first CR  01/17/23 15:03:24.206
Jan 17 15:03:24.211: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-17T15:03:24Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-17T15:03:24Z]] name:name1 resourceVersion:78683 uid:4e011bcd-774a-4069-8803-b4c7bdd62ce3] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 01/17/23 15:03:34.211
Jan 17 15:03:34.217: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-17T15:03:34Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-17T15:03:34Z]] name:name2 resourceVersion:78798 uid:81b47be5-2ed5-45ed-8d7a-410626a6b731] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 01/17/23 15:03:44.218
Jan 17 15:03:44.224: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-17T15:03:24Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-17T15:03:44Z]] name:name1 resourceVersion:78877 uid:4e011bcd-774a-4069-8803-b4c7bdd62ce3] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 01/17/23 15:03:54.225
Jan 17 15:03:54.232: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-17T15:03:34Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-17T15:03:54Z]] name:name2 resourceVersion:78963 uid:81b47be5-2ed5-45ed-8d7a-410626a6b731] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 01/17/23 15:04:04.233
Jan 17 15:04:04.240: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-17T15:03:24Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-17T15:03:44Z]] name:name1 resourceVersion:79033 uid:4e011bcd-774a-4069-8803-b4c7bdd62ce3] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 01/17/23 15:04:14.24
Jan 17 15:04:14.248: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-17T15:03:34Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-17T15:03:54Z]] name:name2 resourceVersion:79092 uid:81b47be5-2ed5-45ed-8d7a-410626a6b731] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 17 15:04:24.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-7918" for this suite. 01/17/23 15:04:24.764
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","completed":58,"skipped":980,"failed":0}
------------------------------
• [SLOW TEST] [63.137 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:03:21.633
    Jan 17 15:03:21.633: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename crd-watch 01/17/23 15:03:21.634
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:03:21.657
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:03:21.659
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    Jan 17 15:03:21.661: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Creating first CR  01/17/23 15:03:24.206
    Jan 17 15:03:24.211: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-17T15:03:24Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-17T15:03:24Z]] name:name1 resourceVersion:78683 uid:4e011bcd-774a-4069-8803-b4c7bdd62ce3] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 01/17/23 15:03:34.211
    Jan 17 15:03:34.217: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-17T15:03:34Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-17T15:03:34Z]] name:name2 resourceVersion:78798 uid:81b47be5-2ed5-45ed-8d7a-410626a6b731] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 01/17/23 15:03:44.218
    Jan 17 15:03:44.224: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-17T15:03:24Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-17T15:03:44Z]] name:name1 resourceVersion:78877 uid:4e011bcd-774a-4069-8803-b4c7bdd62ce3] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 01/17/23 15:03:54.225
    Jan 17 15:03:54.232: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-17T15:03:34Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-17T15:03:54Z]] name:name2 resourceVersion:78963 uid:81b47be5-2ed5-45ed-8d7a-410626a6b731] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 01/17/23 15:04:04.233
    Jan 17 15:04:04.240: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-17T15:03:24Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-17T15:03:44Z]] name:name1 resourceVersion:79033 uid:4e011bcd-774a-4069-8803-b4c7bdd62ce3] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 01/17/23 15:04:14.24
    Jan 17 15:04:14.248: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-17T15:03:34Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-17T15:03:54Z]] name:name2 resourceVersion:79092 uid:81b47be5-2ed5-45ed-8d7a-410626a6b731] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 17 15:04:24.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-watch-7918" for this suite. 01/17/23 15:04:24.764
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:04:24.771
Jan 17 15:04:24.771: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename projected 01/17/23 15:04:24.772
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:04:24.802
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:04:24.805
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98
STEP: Creating configMap with name projected-configmap-test-volume-map-a4048c4e-3c94-431a-a127-807afb978ad4 01/17/23 15:04:24.807
STEP: Creating a pod to test consume configMaps 01/17/23 15:04:24.815
Jan 17 15:04:24.845: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ac1eb1d3-868f-4e24-9a15-fca19f77dbb5" in namespace "projected-4707" to be "Succeeded or Failed"
Jan 17 15:04:24.852: INFO: Pod "pod-projected-configmaps-ac1eb1d3-868f-4e24-9a15-fca19f77dbb5": Phase="Pending", Reason="", readiness=false. Elapsed: 7.115004ms
Jan 17 15:04:26.855: INFO: Pod "pod-projected-configmaps-ac1eb1d3-868f-4e24-9a15-fca19f77dbb5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010516399s
Jan 17 15:04:28.856: INFO: Pod "pod-projected-configmaps-ac1eb1d3-868f-4e24-9a15-fca19f77dbb5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011319853s
STEP: Saw pod success 01/17/23 15:04:28.856
Jan 17 15:04:28.856: INFO: Pod "pod-projected-configmaps-ac1eb1d3-868f-4e24-9a15-fca19f77dbb5" satisfied condition "Succeeded or Failed"
Jan 17 15:04:28.860: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-projected-configmaps-ac1eb1d3-868f-4e24-9a15-fca19f77dbb5 container agnhost-container: <nil>
STEP: delete the pod 01/17/23 15:04:28.873
Jan 17 15:04:28.890: INFO: Waiting for pod pod-projected-configmaps-ac1eb1d3-868f-4e24-9a15-fca19f77dbb5 to disappear
Jan 17 15:04:28.893: INFO: Pod pod-projected-configmaps-ac1eb1d3-868f-4e24-9a15-fca19f77dbb5 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan 17 15:04:28.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4707" for this suite. 01/17/23 15:04:28.899
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":59,"skipped":1005,"failed":0}
------------------------------
• [4.135 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:04:24.771
    Jan 17 15:04:24.771: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename projected 01/17/23 15:04:24.772
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:04:24.802
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:04:24.805
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:98
    STEP: Creating configMap with name projected-configmap-test-volume-map-a4048c4e-3c94-431a-a127-807afb978ad4 01/17/23 15:04:24.807
    STEP: Creating a pod to test consume configMaps 01/17/23 15:04:24.815
    Jan 17 15:04:24.845: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ac1eb1d3-868f-4e24-9a15-fca19f77dbb5" in namespace "projected-4707" to be "Succeeded or Failed"
    Jan 17 15:04:24.852: INFO: Pod "pod-projected-configmaps-ac1eb1d3-868f-4e24-9a15-fca19f77dbb5": Phase="Pending", Reason="", readiness=false. Elapsed: 7.115004ms
    Jan 17 15:04:26.855: INFO: Pod "pod-projected-configmaps-ac1eb1d3-868f-4e24-9a15-fca19f77dbb5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010516399s
    Jan 17 15:04:28.856: INFO: Pod "pod-projected-configmaps-ac1eb1d3-868f-4e24-9a15-fca19f77dbb5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011319853s
    STEP: Saw pod success 01/17/23 15:04:28.856
    Jan 17 15:04:28.856: INFO: Pod "pod-projected-configmaps-ac1eb1d3-868f-4e24-9a15-fca19f77dbb5" satisfied condition "Succeeded or Failed"
    Jan 17 15:04:28.860: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-projected-configmaps-ac1eb1d3-868f-4e24-9a15-fca19f77dbb5 container agnhost-container: <nil>
    STEP: delete the pod 01/17/23 15:04:28.873
    Jan 17 15:04:28.890: INFO: Waiting for pod pod-projected-configmaps-ac1eb1d3-868f-4e24-9a15-fca19f77dbb5 to disappear
    Jan 17 15:04:28.893: INFO: Pod pod-projected-configmaps-ac1eb1d3-868f-4e24-9a15-fca19f77dbb5 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan 17 15:04:28.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4707" for this suite. 01/17/23 15:04:28.899
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:04:28.91
Jan 17 15:04:28.910: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename projected 01/17/23 15:04:28.91
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:04:28.947
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:04:28.949
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87
STEP: Creating projection with secret that has name projected-secret-test-map-8508309d-6c6f-4ac3-a313-1403db728711 01/17/23 15:04:28.951
STEP: Creating a pod to test consume secrets 01/17/23 15:04:28.958
Jan 17 15:04:28.983: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e173bef1-f8da-4f50-8a77-e0ed58d1a916" in namespace "projected-5655" to be "Succeeded or Failed"
Jan 17 15:04:28.988: INFO: Pod "pod-projected-secrets-e173bef1-f8da-4f50-8a77-e0ed58d1a916": Phase="Pending", Reason="", readiness=false. Elapsed: 4.916905ms
Jan 17 15:04:30.993: INFO: Pod "pod-projected-secrets-e173bef1-f8da-4f50-8a77-e0ed58d1a916": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009494892s
Jan 17 15:04:32.992: INFO: Pod "pod-projected-secrets-e173bef1-f8da-4f50-8a77-e0ed58d1a916": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008762016s
STEP: Saw pod success 01/17/23 15:04:32.992
Jan 17 15:04:32.992: INFO: Pod "pod-projected-secrets-e173bef1-f8da-4f50-8a77-e0ed58d1a916" satisfied condition "Succeeded or Failed"
Jan 17 15:04:32.995: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-projected-secrets-e173bef1-f8da-4f50-8a77-e0ed58d1a916 container projected-secret-volume-test: <nil>
STEP: delete the pod 01/17/23 15:04:33.001
Jan 17 15:04:33.011: INFO: Waiting for pod pod-projected-secrets-e173bef1-f8da-4f50-8a77-e0ed58d1a916 to disappear
Jan 17 15:04:33.014: INFO: Pod pod-projected-secrets-e173bef1-f8da-4f50-8a77-e0ed58d1a916 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan 17 15:04:33.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5655" for this suite. 01/17/23 15:04:33.019
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":60,"skipped":1152,"failed":0}
------------------------------
• [4.116 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:04:28.91
    Jan 17 15:04:28.910: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename projected 01/17/23 15:04:28.91
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:04:28.947
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:04:28.949
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:87
    STEP: Creating projection with secret that has name projected-secret-test-map-8508309d-6c6f-4ac3-a313-1403db728711 01/17/23 15:04:28.951
    STEP: Creating a pod to test consume secrets 01/17/23 15:04:28.958
    Jan 17 15:04:28.983: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e173bef1-f8da-4f50-8a77-e0ed58d1a916" in namespace "projected-5655" to be "Succeeded or Failed"
    Jan 17 15:04:28.988: INFO: Pod "pod-projected-secrets-e173bef1-f8da-4f50-8a77-e0ed58d1a916": Phase="Pending", Reason="", readiness=false. Elapsed: 4.916905ms
    Jan 17 15:04:30.993: INFO: Pod "pod-projected-secrets-e173bef1-f8da-4f50-8a77-e0ed58d1a916": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009494892s
    Jan 17 15:04:32.992: INFO: Pod "pod-projected-secrets-e173bef1-f8da-4f50-8a77-e0ed58d1a916": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008762016s
    STEP: Saw pod success 01/17/23 15:04:32.992
    Jan 17 15:04:32.992: INFO: Pod "pod-projected-secrets-e173bef1-f8da-4f50-8a77-e0ed58d1a916" satisfied condition "Succeeded or Failed"
    Jan 17 15:04:32.995: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-projected-secrets-e173bef1-f8da-4f50-8a77-e0ed58d1a916 container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/17/23 15:04:33.001
    Jan 17 15:04:33.011: INFO: Waiting for pod pod-projected-secrets-e173bef1-f8da-4f50-8a77-e0ed58d1a916 to disappear
    Jan 17 15:04:33.014: INFO: Pod pod-projected-secrets-e173bef1-f8da-4f50-8a77-e0ed58d1a916 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan 17 15:04:33.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5655" for this suite. 01/17/23 15:04:33.019
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:04:33.026
Jan 17 15:04:33.026: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename secrets 01/17/23 15:04:33.027
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:04:33.05
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:04:33.052
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46
STEP: Creating secret with name secret-test-c2cde7b3-5c47-49d7-b0f1-1aeb63335fbf 01/17/23 15:04:33.054
STEP: Creating a pod to test consume secrets 01/17/23 15:04:33.061
Jan 17 15:04:33.097: INFO: Waiting up to 5m0s for pod "pod-secrets-4b42e8ad-b6dc-4e3d-b811-909b1aaf1551" in namespace "secrets-8961" to be "Succeeded or Failed"
Jan 17 15:04:33.100: INFO: Pod "pod-secrets-4b42e8ad-b6dc-4e3d-b811-909b1aaf1551": Phase="Pending", Reason="", readiness=false. Elapsed: 3.885879ms
Jan 17 15:04:35.106: INFO: Pod "pod-secrets-4b42e8ad-b6dc-4e3d-b811-909b1aaf1551": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00898517s
Jan 17 15:04:37.106: INFO: Pod "pod-secrets-4b42e8ad-b6dc-4e3d-b811-909b1aaf1551": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009591963s
STEP: Saw pod success 01/17/23 15:04:37.106
Jan 17 15:04:37.106: INFO: Pod "pod-secrets-4b42e8ad-b6dc-4e3d-b811-909b1aaf1551" satisfied condition "Succeeded or Failed"
Jan 17 15:04:37.109: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-secrets-4b42e8ad-b6dc-4e3d-b811-909b1aaf1551 container secret-volume-test: <nil>
STEP: delete the pod 01/17/23 15:04:37.114
Jan 17 15:04:37.127: INFO: Waiting for pod pod-secrets-4b42e8ad-b6dc-4e3d-b811-909b1aaf1551 to disappear
Jan 17 15:04:37.129: INFO: Pod pod-secrets-4b42e8ad-b6dc-4e3d-b811-909b1aaf1551 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan 17 15:04:37.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8961" for this suite. 01/17/23 15:04:37.133
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","completed":61,"skipped":1152,"failed":0}
------------------------------
• [4.114 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:04:33.026
    Jan 17 15:04:33.026: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename secrets 01/17/23 15:04:33.027
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:04:33.05
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:04:33.052
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:46
    STEP: Creating secret with name secret-test-c2cde7b3-5c47-49d7-b0f1-1aeb63335fbf 01/17/23 15:04:33.054
    STEP: Creating a pod to test consume secrets 01/17/23 15:04:33.061
    Jan 17 15:04:33.097: INFO: Waiting up to 5m0s for pod "pod-secrets-4b42e8ad-b6dc-4e3d-b811-909b1aaf1551" in namespace "secrets-8961" to be "Succeeded or Failed"
    Jan 17 15:04:33.100: INFO: Pod "pod-secrets-4b42e8ad-b6dc-4e3d-b811-909b1aaf1551": Phase="Pending", Reason="", readiness=false. Elapsed: 3.885879ms
    Jan 17 15:04:35.106: INFO: Pod "pod-secrets-4b42e8ad-b6dc-4e3d-b811-909b1aaf1551": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00898517s
    Jan 17 15:04:37.106: INFO: Pod "pod-secrets-4b42e8ad-b6dc-4e3d-b811-909b1aaf1551": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009591963s
    STEP: Saw pod success 01/17/23 15:04:37.106
    Jan 17 15:04:37.106: INFO: Pod "pod-secrets-4b42e8ad-b6dc-4e3d-b811-909b1aaf1551" satisfied condition "Succeeded or Failed"
    Jan 17 15:04:37.109: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-secrets-4b42e8ad-b6dc-4e3d-b811-909b1aaf1551 container secret-volume-test: <nil>
    STEP: delete the pod 01/17/23 15:04:37.114
    Jan 17 15:04:37.127: INFO: Waiting for pod pod-secrets-4b42e8ad-b6dc-4e3d-b811-909b1aaf1551 to disappear
    Jan 17 15:04:37.129: INFO: Pod pod-secrets-4b42e8ad-b6dc-4e3d-b811-909b1aaf1551 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan 17 15:04:37.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-8961" for this suite. 01/17/23 15:04:37.133
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:04:37.14
Jan 17 15:04:37.140: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename downward-api 01/17/23 15:04:37.14
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:04:37.165
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:04:37.168
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161
STEP: Creating the pod 01/17/23 15:04:37.17
Jan 17 15:04:37.196: INFO: Waiting up to 5m0s for pod "annotationupdate9a3b432c-69c5-4491-8ad6-e14a2d72c04f" in namespace "downward-api-305" to be "running and ready"
Jan 17 15:04:37.199: INFO: Pod "annotationupdate9a3b432c-69c5-4491-8ad6-e14a2d72c04f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.017451ms
Jan 17 15:04:37.199: INFO: The phase of Pod annotationupdate9a3b432c-69c5-4491-8ad6-e14a2d72c04f is Pending, waiting for it to be Running (with Ready = true)
Jan 17 15:04:39.203: INFO: Pod "annotationupdate9a3b432c-69c5-4491-8ad6-e14a2d72c04f": Phase="Running", Reason="", readiness=true. Elapsed: 2.006792406s
Jan 17 15:04:39.203: INFO: The phase of Pod annotationupdate9a3b432c-69c5-4491-8ad6-e14a2d72c04f is Running (Ready = true)
Jan 17 15:04:39.203: INFO: Pod "annotationupdate9a3b432c-69c5-4491-8ad6-e14a2d72c04f" satisfied condition "running and ready"
Jan 17 15:04:39.728: INFO: Successfully updated pod "annotationupdate9a3b432c-69c5-4491-8ad6-e14a2d72c04f"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 17 15:04:43.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-305" for this suite. 01/17/23 15:04:43.75
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","completed":62,"skipped":1160,"failed":0}
------------------------------
• [SLOW TEST] [6.617 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:04:37.14
    Jan 17 15:04:37.140: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename downward-api 01/17/23 15:04:37.14
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:04:37.165
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:04:37.168
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:161
    STEP: Creating the pod 01/17/23 15:04:37.17
    Jan 17 15:04:37.196: INFO: Waiting up to 5m0s for pod "annotationupdate9a3b432c-69c5-4491-8ad6-e14a2d72c04f" in namespace "downward-api-305" to be "running and ready"
    Jan 17 15:04:37.199: INFO: Pod "annotationupdate9a3b432c-69c5-4491-8ad6-e14a2d72c04f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.017451ms
    Jan 17 15:04:37.199: INFO: The phase of Pod annotationupdate9a3b432c-69c5-4491-8ad6-e14a2d72c04f is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 15:04:39.203: INFO: Pod "annotationupdate9a3b432c-69c5-4491-8ad6-e14a2d72c04f": Phase="Running", Reason="", readiness=true. Elapsed: 2.006792406s
    Jan 17 15:04:39.203: INFO: The phase of Pod annotationupdate9a3b432c-69c5-4491-8ad6-e14a2d72c04f is Running (Ready = true)
    Jan 17 15:04:39.203: INFO: Pod "annotationupdate9a3b432c-69c5-4491-8ad6-e14a2d72c04f" satisfied condition "running and ready"
    Jan 17 15:04:39.728: INFO: Successfully updated pod "annotationupdate9a3b432c-69c5-4491-8ad6-e14a2d72c04f"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 17 15:04:43.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-305" for this suite. 01/17/23 15:04:43.75
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:04:43.757
Jan 17 15:04:43.757: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename pods 01/17/23 15:04:43.757
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:04:43.78
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:04:43.782
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082
STEP: Create a pod 01/17/23 15:04:43.784
Jan 17 15:04:43.818: INFO: Waiting up to 5m0s for pod "pod-hw82g" in namespace "pods-506" to be "running"
Jan 17 15:04:43.822: INFO: Pod "pod-hw82g": Phase="Pending", Reason="", readiness=false. Elapsed: 3.448385ms
Jan 17 15:04:45.827: INFO: Pod "pod-hw82g": Phase="Running", Reason="", readiness=true. Elapsed: 2.008390026s
Jan 17 15:04:45.827: INFO: Pod "pod-hw82g" satisfied condition "running"
STEP: patching /status 01/17/23 15:04:45.827
Jan 17 15:04:45.834: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 17 15:04:45.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-506" for this suite. 01/17/23 15:04:45.84
{"msg":"PASSED [sig-node] Pods should patch a pod status [Conformance]","completed":63,"skipped":1161,"failed":0}
------------------------------
• [2.088 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:04:43.757
    Jan 17 15:04:43.757: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename pods 01/17/23 15:04:43.757
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:04:43.78
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:04:43.782
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1082
    STEP: Create a pod 01/17/23 15:04:43.784
    Jan 17 15:04:43.818: INFO: Waiting up to 5m0s for pod "pod-hw82g" in namespace "pods-506" to be "running"
    Jan 17 15:04:43.822: INFO: Pod "pod-hw82g": Phase="Pending", Reason="", readiness=false. Elapsed: 3.448385ms
    Jan 17 15:04:45.827: INFO: Pod "pod-hw82g": Phase="Running", Reason="", readiness=true. Elapsed: 2.008390026s
    Jan 17 15:04:45.827: INFO: Pod "pod-hw82g" satisfied condition "running"
    STEP: patching /status 01/17/23 15:04:45.827
    Jan 17 15:04:45.834: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 17 15:04:45.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-506" for this suite. 01/17/23 15:04:45.84
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:04:45.845
Jan 17 15:04:45.846: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename resourcequota 01/17/23 15:04:45.846
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:04:45.871
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:04:45.874
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65
STEP: Counting existing ResourceQuota 01/17/23 15:04:45.876
STEP: Creating a ResourceQuota 01/17/23 15:04:50.879
STEP: Ensuring resource quota status is calculated 01/17/23 15:04:50.884
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 17 15:04:52.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4999" for this suite. 01/17/23 15:04:52.893
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","completed":64,"skipped":1170,"failed":0}
------------------------------
• [SLOW TEST] [7.052 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:04:45.845
    Jan 17 15:04:45.846: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename resourcequota 01/17/23 15:04:45.846
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:04:45.871
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:04:45.874
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:65
    STEP: Counting existing ResourceQuota 01/17/23 15:04:45.876
    STEP: Creating a ResourceQuota 01/17/23 15:04:50.879
    STEP: Ensuring resource quota status is calculated 01/17/23 15:04:50.884
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 17 15:04:52.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-4999" for this suite. 01/17/23 15:04:52.893
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:04:52.899
Jan 17 15:04:52.899: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename resourcequota 01/17/23 15:04:52.899
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:04:52.923
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:04:52.925
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90
STEP: Counting existing ResourceQuota 01/17/23 15:04:52.927
STEP: Creating a ResourceQuota 01/17/23 15:04:57.931
STEP: Ensuring resource quota status is calculated 01/17/23 15:04:57.937
STEP: Creating a Service 01/17/23 15:04:59.941
STEP: Creating a NodePort Service 01/17/23 15:04:59.956
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 01/17/23 15:04:59.988
STEP: Ensuring resource quota status captures service creation 01/17/23 15:05:00.017
STEP: Deleting Services 01/17/23 15:05:02.021
STEP: Ensuring resource quota status released usage 01/17/23 15:05:02.077
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 17 15:05:04.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-380" for this suite. 01/17/23 15:05:04.085
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","completed":65,"skipped":1201,"failed":0}
------------------------------
• [SLOW TEST] [11.194 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:04:52.899
    Jan 17 15:04:52.899: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename resourcequota 01/17/23 15:04:52.899
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:04:52.923
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:04:52.925
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:90
    STEP: Counting existing ResourceQuota 01/17/23 15:04:52.927
    STEP: Creating a ResourceQuota 01/17/23 15:04:57.931
    STEP: Ensuring resource quota status is calculated 01/17/23 15:04:57.937
    STEP: Creating a Service 01/17/23 15:04:59.941
    STEP: Creating a NodePort Service 01/17/23 15:04:59.956
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 01/17/23 15:04:59.988
    STEP: Ensuring resource quota status captures service creation 01/17/23 15:05:00.017
    STEP: Deleting Services 01/17/23 15:05:02.021
    STEP: Ensuring resource quota status released usage 01/17/23 15:05:02.077
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 17 15:05:04.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-380" for this suite. 01/17/23 15:05:04.085
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:05:04.093
Jan 17 15:05:04.093: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename replication-controller 01/17/23 15:05:04.094
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:05:04.118
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:05:04.122
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91
STEP: Given a Pod with a 'name' label pod-adoption is created 01/17/23 15:05:04.125
Jan 17 15:05:04.159: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-3476" to be "running and ready"
Jan 17 15:05:04.169: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 9.142323ms
Jan 17 15:05:04.169: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Jan 17 15:05:06.174: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.014301354s
Jan 17 15:05:06.174: INFO: The phase of Pod pod-adoption is Running (Ready = true)
Jan 17 15:05:06.174: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 01/17/23 15:05:06.177
STEP: Then the orphan pod is adopted 01/17/23 15:05:06.182
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Jan 17 15:05:07.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3476" for this suite. 01/17/23 15:05:07.193
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","completed":66,"skipped":1208,"failed":0}
------------------------------
• [3.105 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:05:04.093
    Jan 17 15:05:04.093: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename replication-controller 01/17/23 15:05:04.094
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:05:04.118
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:05:04.122
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:91
    STEP: Given a Pod with a 'name' label pod-adoption is created 01/17/23 15:05:04.125
    Jan 17 15:05:04.159: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-3476" to be "running and ready"
    Jan 17 15:05:04.169: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 9.142323ms
    Jan 17 15:05:04.169: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 15:05:06.174: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.014301354s
    Jan 17 15:05:06.174: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    Jan 17 15:05:06.174: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 01/17/23 15:05:06.177
    STEP: Then the orphan pod is adopted 01/17/23 15:05:06.182
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Jan 17 15:05:07.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-3476" for this suite. 01/17/23 15:05:07.193
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:05:07.2
Jan 17 15:05:07.200: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename projected 01/17/23 15:05:07.201
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:05:07.231
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:05:07.234
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161
STEP: Creating the pod 01/17/23 15:05:07.236
Jan 17 15:05:07.297: INFO: Waiting up to 5m0s for pod "annotationupdate80682f9d-b53f-49ed-8a9f-2250d15ec9cf" in namespace "projected-8776" to be "running and ready"
Jan 17 15:05:07.303: INFO: Pod "annotationupdate80682f9d-b53f-49ed-8a9f-2250d15ec9cf": Phase="Pending", Reason="", readiness=false. Elapsed: 6.072369ms
Jan 17 15:05:07.303: INFO: The phase of Pod annotationupdate80682f9d-b53f-49ed-8a9f-2250d15ec9cf is Pending, waiting for it to be Running (with Ready = true)
Jan 17 15:05:09.307: INFO: Pod "annotationupdate80682f9d-b53f-49ed-8a9f-2250d15ec9cf": Phase="Running", Reason="", readiness=true. Elapsed: 2.010633623s
Jan 17 15:05:09.307: INFO: The phase of Pod annotationupdate80682f9d-b53f-49ed-8a9f-2250d15ec9cf is Running (Ready = true)
Jan 17 15:05:09.307: INFO: Pod "annotationupdate80682f9d-b53f-49ed-8a9f-2250d15ec9cf" satisfied condition "running and ready"
Jan 17 15:05:09.850: INFO: Successfully updated pod "annotationupdate80682f9d-b53f-49ed-8a9f-2250d15ec9cf"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 17 15:05:13.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8776" for this suite. 01/17/23 15:05:13.875
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","completed":67,"skipped":1281,"failed":0}
------------------------------
• [SLOW TEST] [6.680 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:05:07.2
    Jan 17 15:05:07.200: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename projected 01/17/23 15:05:07.201
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:05:07.231
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:05:07.234
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:161
    STEP: Creating the pod 01/17/23 15:05:07.236
    Jan 17 15:05:07.297: INFO: Waiting up to 5m0s for pod "annotationupdate80682f9d-b53f-49ed-8a9f-2250d15ec9cf" in namespace "projected-8776" to be "running and ready"
    Jan 17 15:05:07.303: INFO: Pod "annotationupdate80682f9d-b53f-49ed-8a9f-2250d15ec9cf": Phase="Pending", Reason="", readiness=false. Elapsed: 6.072369ms
    Jan 17 15:05:07.303: INFO: The phase of Pod annotationupdate80682f9d-b53f-49ed-8a9f-2250d15ec9cf is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 15:05:09.307: INFO: Pod "annotationupdate80682f9d-b53f-49ed-8a9f-2250d15ec9cf": Phase="Running", Reason="", readiness=true. Elapsed: 2.010633623s
    Jan 17 15:05:09.307: INFO: The phase of Pod annotationupdate80682f9d-b53f-49ed-8a9f-2250d15ec9cf is Running (Ready = true)
    Jan 17 15:05:09.307: INFO: Pod "annotationupdate80682f9d-b53f-49ed-8a9f-2250d15ec9cf" satisfied condition "running and ready"
    Jan 17 15:05:09.850: INFO: Successfully updated pod "annotationupdate80682f9d-b53f-49ed-8a9f-2250d15ec9cf"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 17 15:05:13.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8776" for this suite. 01/17/23 15:05:13.875
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:05:13.881
Jan 17 15:05:13.881: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename secrets 01/17/23 15:05:13.882
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:05:13.912
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:05:13.914
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan 17 15:05:13.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1229" for this suite. 01/17/23 15:05:14
{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","completed":68,"skipped":1285,"failed":0}
------------------------------
• [0.127 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:05:13.881
    Jan 17 15:05:13.881: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename secrets 01/17/23 15:05:13.882
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:05:13.912
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:05:13.914
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:385
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan 17 15:05:13.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-1229" for this suite. 01/17/23 15:05:14
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:05:14.008
Jan 17 15:05:14.008: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename webhook 01/17/23 15:05:14.009
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:05:14.037
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:05:14.039
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/17/23 15:05:14.06
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/17/23 15:05:14.342
STEP: Deploying the webhook pod 01/17/23 15:05:14.352
STEP: Wait for the deployment to be ready 01/17/23 15:05:14.368
Jan 17 15:05:14.374: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/17/23 15:05:16.384
STEP: Verifying the service has paired with the endpoint 01/17/23 15:05:16.394
Jan 17 15:05:17.395: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 01/17/23 15:05:17.398
STEP: create a pod that should be updated by the webhook 01/17/23 15:05:17.41
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 17 15:05:17.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6295" for this suite. 01/17/23 15:05:17.456
STEP: Destroying namespace "webhook-6295-markers" for this suite. 01/17/23 15:05:17.467
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","completed":69,"skipped":1304,"failed":0}
------------------------------
• [3.578 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:05:14.008
    Jan 17 15:05:14.008: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename webhook 01/17/23 15:05:14.009
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:05:14.037
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:05:14.039
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/17/23 15:05:14.06
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/17/23 15:05:14.342
    STEP: Deploying the webhook pod 01/17/23 15:05:14.352
    STEP: Wait for the deployment to be ready 01/17/23 15:05:14.368
    Jan 17 15:05:14.374: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/17/23 15:05:16.384
    STEP: Verifying the service has paired with the endpoint 01/17/23 15:05:16.394
    Jan 17 15:05:17.395: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:263
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 01/17/23 15:05:17.398
    STEP: create a pod that should be updated by the webhook 01/17/23 15:05:17.41
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 17 15:05:17.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-6295" for this suite. 01/17/23 15:05:17.456
    STEP: Destroying namespace "webhook-6295-markers" for this suite. 01/17/23 15:05:17.467
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:733
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:05:17.587
Jan 17 15:05:17.587: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename sched-preemption 01/17/23 15:05:17.588
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:05:17.624
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:05:17.626
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jan 17 15:05:17.646: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 17 15:06:17.765: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:06:17.769
Jan 17 15:06:17.769: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename sched-preemption-path 01/17/23 15:06:17.77
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:06:17.79
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:06:17.798
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:690
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:733
Jan 17 15:06:17.820: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Jan 17 15:06:17.825: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/framework.go:187
Jan 17 15:06:17.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-1828" for this suite. 01/17/23 15:06:17.859
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:706
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Jan 17 15:06:17.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-8975" for this suite. 01/17/23 15:06:17.884
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","completed":70,"skipped":1358,"failed":0}
------------------------------
• [SLOW TEST] [60.357 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:683
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:733

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:05:17.587
    Jan 17 15:05:17.587: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename sched-preemption 01/17/23 15:05:17.588
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:05:17.624
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:05:17.626
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Jan 17 15:05:17.646: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 17 15:06:17.765: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:06:17.769
    Jan 17 15:06:17.769: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename sched-preemption-path 01/17/23 15:06:17.77
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:06:17.79
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:06:17.798
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:690
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:733
    Jan 17 15:06:17.820: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Jan 17 15:06:17.825: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/framework.go:187
    Jan 17 15:06:17.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-path-1828" for this suite. 01/17/23 15:06:17.859
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:706
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Jan 17 15:06:17.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-8975" for this suite. 01/17/23 15:06:17.884
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:06:17.945
Jan 17 15:06:17.945: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename webhook 01/17/23 15:06:17.946
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:06:17.97
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:06:17.972
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/17/23 15:06:17.998
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/17/23 15:06:18.257
STEP: Deploying the webhook pod 01/17/23 15:06:18.268
STEP: Wait for the deployment to be ready 01/17/23 15:06:18.279
Jan 17 15:06:18.284: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/17/23 15:06:20.295
STEP: Verifying the service has paired with the endpoint 01/17/23 15:06:20.306
Jan 17 15:06:21.306: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307
STEP: Registering the crd webhook via the AdmissionRegistration API 01/17/23 15:06:21.31
STEP: Creating a custom resource definition that should be denied by the webhook 01/17/23 15:06:21.324
Jan 17 15:06:21.324: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 17 15:06:21.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5375" for this suite. 01/17/23 15:06:21.342
STEP: Destroying namespace "webhook-5375-markers" for this suite. 01/17/23 15:06:21.348
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","completed":71,"skipped":1362,"failed":0}
------------------------------
• [3.509 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:06:17.945
    Jan 17 15:06:17.945: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename webhook 01/17/23 15:06:17.946
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:06:17.97
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:06:17.972
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/17/23 15:06:17.998
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/17/23 15:06:18.257
    STEP: Deploying the webhook pod 01/17/23 15:06:18.268
    STEP: Wait for the deployment to be ready 01/17/23 15:06:18.279
    Jan 17 15:06:18.284: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/17/23 15:06:20.295
    STEP: Verifying the service has paired with the endpoint 01/17/23 15:06:20.306
    Jan 17 15:06:21.306: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:307
    STEP: Registering the crd webhook via the AdmissionRegistration API 01/17/23 15:06:21.31
    STEP: Creating a custom resource definition that should be denied by the webhook 01/17/23 15:06:21.324
    Jan 17 15:06:21.324: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 17 15:06:21.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-5375" for this suite. 01/17/23 15:06:21.342
    STEP: Destroying namespace "webhook-5375-markers" for this suite. 01/17/23 15:06:21.348
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:06:21.454
Jan 17 15:06:21.454: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename deployment 01/17/23 15:06:21.455
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:06:21.507
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:06:21.509
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
Jan 17 15:06:21.531: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jan 17 15:06:26.536: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/17/23 15:06:26.536
Jan 17 15:06:26.536: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jan 17 15:06:28.541: INFO: Creating deployment "test-rollover-deployment"
Jan 17 15:06:28.549: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jan 17 15:06:30.557: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jan 17 15:06:30.561: INFO: Ensure that both replica sets have 1 created replica
Jan 17 15:06:30.568: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jan 17 15:06:30.577: INFO: Updating deployment test-rollover-deployment
Jan 17 15:06:30.577: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jan 17 15:06:32.584: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jan 17 15:06:32.589: INFO: Make sure deployment "test-rollover-deployment" is complete
Jan 17 15:06:32.594: INFO: all replica sets need to contain the pod-template-hash label
Jan 17 15:06:32.594: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 15, 6, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 15, 6, 28, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 15, 6, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 15, 6, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 17 15:06:34.602: INFO: all replica sets need to contain the pod-template-hash label
Jan 17 15:06:34.602: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 15, 6, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 15, 6, 28, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 15, 6, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 15, 6, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 17 15:06:36.601: INFO: all replica sets need to contain the pod-template-hash label
Jan 17 15:06:36.601: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 15, 6, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 15, 6, 28, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 15, 6, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 15, 6, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 17 15:06:38.603: INFO: all replica sets need to contain the pod-template-hash label
Jan 17 15:06:38.603: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 15, 6, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 15, 6, 28, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 15, 6, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 15, 6, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 17 15:06:40.602: INFO: all replica sets need to contain the pod-template-hash label
Jan 17 15:06:40.603: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 15, 6, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 15, 6, 28, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 15, 6, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 15, 6, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 17 15:06:42.601: INFO: 
Jan 17 15:06:42.601: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 17 15:06:42.609: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-3449  5a8936cb-ad42-44ad-8c6b-fbab60c4a378 81297 2 2023-01-17 15:06:28 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-17 15:06:30 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 15:06:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003a3b968 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-17 15:06:28 +0000 UTC,LastTransitionTime:2023-01-17 15:06:28 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6d45fd857b" has successfully progressed.,LastUpdateTime:2023-01-17 15:06:42 +0000 UTC,LastTransitionTime:2023-01-17 15:06:28 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 17 15:06:42.611: INFO: New ReplicaSet "test-rollover-deployment-6d45fd857b" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6d45fd857b  deployment-3449  28cd0d28-1643-474f-8ebe-2ab1ca654ca9 81287 2 2023-01-17 15:06:30 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 5a8936cb-ad42-44ad-8c6b-fbab60c4a378 0xc003a3bf87 0xc003a3bf88}] [] [{kube-controller-manager Update apps/v1 2023-01-17 15:06:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5a8936cb-ad42-44ad-8c6b-fbab60c4a378\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 15:06:42 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6d45fd857b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002f50038 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 17 15:06:42.611: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jan 17 15:06:42.611: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-3449  b35f2b31-6ad7-40af-a3ea-e61679be8cb3 81296 2 2023-01-17 15:06:21 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 5a8936cb-ad42-44ad-8c6b-fbab60c4a378 0xc003a3bd27 0xc003a3bd28}] [] [{e2e.test Update apps/v1 2023-01-17 15:06:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 15:06:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5a8936cb-ad42-44ad-8c6b-fbab60c4a378\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-17 15:06:42 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003a3bdf8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 17 15:06:42.611: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-59b9df946d  deployment-3449  ffd6d35a-a42f-45ea-b2dd-65742313a992 81194 2 2023-01-17 15:06:28 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 5a8936cb-ad42-44ad-8c6b-fbab60c4a378 0xc003a3be67 0xc003a3be68}] [] [{kube-controller-manager Update apps/v1 2023-01-17 15:06:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5a8936cb-ad42-44ad-8c6b-fbab60c4a378\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 15:06:30 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 59b9df946d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003a3bf18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 17 15:06:42.614: INFO: Pod "test-rollover-deployment-6d45fd857b-5pv9w" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6d45fd857b-5pv9w test-rollover-deployment-6d45fd857b- deployment-3449  608ab717-4bbc-4970-b746-a43e73080066 81216 0 2023-01-17 15:06:30 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.2.53/23"],"mac_address":"0a:58:0a:80:02:35","gateway_ips":["10.128.2.1"],"ip_address":"10.128.2.53/23","gateway_ip":"10.128.2.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.2.53"
    ],
    "mac": "0a:58:0a:80:02:35",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.2.53"
    ],
    "mac": "0a:58:0a:80:02:35",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-rollover-deployment-6d45fd857b 28cd0d28-1643-474f-8ebe-2ab1ca654ca9 0xc000935327 0xc000935328}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:06:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:06:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"28cd0d28-1643-474f-8ebe-2ab1ca654ca9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-17 15:06:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-17 15:06:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.2.53\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wrk77,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wrk77,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-165-14.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c38,c22,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-nhgbl,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:06:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:06:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:06:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:06:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.165.14,PodIP:10.128.2.53,StartTime:2023-01-17 15:06:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 15:06:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:37dd9ad84b34913b4bc57e00b0a28e8b00f18ca6a5ecec69461aa57402e5c787,ContainerID:cri-o://66e6752413a26a6ea042086edbd2454cf1d95823d855e4450e686a1ff27c6375,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.2.53,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan 17 15:06:42.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3449" for this suite. 01/17/23 15:06:42.618
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","completed":72,"skipped":1376,"failed":0}
------------------------------
• [SLOW TEST] [21.169 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:06:21.454
    Jan 17 15:06:21.454: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename deployment 01/17/23 15:06:21.455
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:06:21.507
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:06:21.509
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    Jan 17 15:06:21.531: INFO: Pod name rollover-pod: Found 0 pods out of 1
    Jan 17 15:06:26.536: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/17/23 15:06:26.536
    Jan 17 15:06:26.536: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    Jan 17 15:06:28.541: INFO: Creating deployment "test-rollover-deployment"
    Jan 17 15:06:28.549: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    Jan 17 15:06:30.557: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    Jan 17 15:06:30.561: INFO: Ensure that both replica sets have 1 created replica
    Jan 17 15:06:30.568: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    Jan 17 15:06:30.577: INFO: Updating deployment test-rollover-deployment
    Jan 17 15:06:30.577: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    Jan 17 15:06:32.584: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    Jan 17 15:06:32.589: INFO: Make sure deployment "test-rollover-deployment" is complete
    Jan 17 15:06:32.594: INFO: all replica sets need to contain the pod-template-hash label
    Jan 17 15:06:32.594: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 15, 6, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 15, 6, 28, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 15, 6, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 15, 6, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 17 15:06:34.602: INFO: all replica sets need to contain the pod-template-hash label
    Jan 17 15:06:34.602: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 15, 6, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 15, 6, 28, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 15, 6, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 15, 6, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 17 15:06:36.601: INFO: all replica sets need to contain the pod-template-hash label
    Jan 17 15:06:36.601: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 15, 6, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 15, 6, 28, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 15, 6, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 15, 6, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 17 15:06:38.603: INFO: all replica sets need to contain the pod-template-hash label
    Jan 17 15:06:38.603: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 15, 6, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 15, 6, 28, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 15, 6, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 15, 6, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 17 15:06:40.602: INFO: all replica sets need to contain the pod-template-hash label
    Jan 17 15:06:40.603: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 15, 6, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 15, 6, 28, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 15, 6, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 15, 6, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 17 15:06:42.601: INFO: 
    Jan 17 15:06:42.601: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 17 15:06:42.609: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-3449  5a8936cb-ad42-44ad-8c6b-fbab60c4a378 81297 2 2023-01-17 15:06:28 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-17 15:06:30 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 15:06:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003a3b968 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-17 15:06:28 +0000 UTC,LastTransitionTime:2023-01-17 15:06:28 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6d45fd857b" has successfully progressed.,LastUpdateTime:2023-01-17 15:06:42 +0000 UTC,LastTransitionTime:2023-01-17 15:06:28 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan 17 15:06:42.611: INFO: New ReplicaSet "test-rollover-deployment-6d45fd857b" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6d45fd857b  deployment-3449  28cd0d28-1643-474f-8ebe-2ab1ca654ca9 81287 2 2023-01-17 15:06:30 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 5a8936cb-ad42-44ad-8c6b-fbab60c4a378 0xc003a3bf87 0xc003a3bf88}] [] [{kube-controller-manager Update apps/v1 2023-01-17 15:06:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5a8936cb-ad42-44ad-8c6b-fbab60c4a378\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 15:06:42 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6d45fd857b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002f50038 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 17 15:06:42.611: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    Jan 17 15:06:42.611: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-3449  b35f2b31-6ad7-40af-a3ea-e61679be8cb3 81296 2 2023-01-17 15:06:21 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 5a8936cb-ad42-44ad-8c6b-fbab60c4a378 0xc003a3bd27 0xc003a3bd28}] [] [{e2e.test Update apps/v1 2023-01-17 15:06:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 15:06:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5a8936cb-ad42-44ad-8c6b-fbab60c4a378\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-17 15:06:42 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003a3bdf8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 17 15:06:42.611: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-59b9df946d  deployment-3449  ffd6d35a-a42f-45ea-b2dd-65742313a992 81194 2 2023-01-17 15:06:28 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 5a8936cb-ad42-44ad-8c6b-fbab60c4a378 0xc003a3be67 0xc003a3be68}] [] [{kube-controller-manager Update apps/v1 2023-01-17 15:06:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5a8936cb-ad42-44ad-8c6b-fbab60c4a378\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 15:06:30 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 59b9df946d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003a3bf18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 17 15:06:42.614: INFO: Pod "test-rollover-deployment-6d45fd857b-5pv9w" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6d45fd857b-5pv9w test-rollover-deployment-6d45fd857b- deployment-3449  608ab717-4bbc-4970-b746-a43e73080066 81216 0 2023-01-17 15:06:30 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.2.53/23"],"mac_address":"0a:58:0a:80:02:35","gateway_ips":["10.128.2.1"],"ip_address":"10.128.2.53/23","gateway_ip":"10.128.2.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.128.2.53"
        ],
        "mac": "0a:58:0a:80:02:35",
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.128.2.53"
        ],
        "mac": "0a:58:0a:80:02:35",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-rollover-deployment-6d45fd857b 28cd0d28-1643-474f-8ebe-2ab1ca654ca9 0xc000935327 0xc000935328}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:06:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:06:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"28cd0d28-1643-474f-8ebe-2ab1ca654ca9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-17 15:06:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-17 15:06:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.2.53\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wrk77,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wrk77,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-165-14.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c38,c22,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-nhgbl,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:06:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:06:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:06:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:06:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.165.14,PodIP:10.128.2.53,StartTime:2023-01-17 15:06:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 15:06:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:37dd9ad84b34913b4bc57e00b0a28e8b00f18ca6a5ecec69461aa57402e5c787,ContainerID:cri-o://66e6752413a26a6ea042086edbd2454cf1d95823d855e4450e686a1ff27c6375,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.2.53,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan 17 15:06:42.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-3449" for this suite. 01/17/23 15:06:42.618
  << End Captured GinkgoWriter Output
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:06:42.623
Jan 17 15:06:42.623: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename svcaccounts 01/17/23 15:06:42.624
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:06:42.644
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:06:42.647
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739
Jan 17 15:06:42.657: INFO: Got root ca configmap in namespace "svcaccounts-2812"
Jan 17 15:06:42.672: INFO: Deleted root ca configmap in namespace "svcaccounts-2812"
STEP: waiting for a new root ca configmap created 01/17/23 15:06:43.173
Jan 17 15:06:43.176: INFO: Recreated root ca configmap in namespace "svcaccounts-2812"
Jan 17 15:06:43.181: INFO: Updated root ca configmap in namespace "svcaccounts-2812"
STEP: waiting for the root ca configmap reconciled 01/17/23 15:06:43.681
Jan 17 15:06:43.685: INFO: Reconciled root ca configmap in namespace "svcaccounts-2812"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jan 17 15:06:43.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2812" for this suite. 01/17/23 15:06:43.688
{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","completed":73,"skipped":1376,"failed":0}
------------------------------
• [1.071 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:06:42.623
    Jan 17 15:06:42.623: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename svcaccounts 01/17/23 15:06:42.624
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:06:42.644
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:06:42.647
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:739
    Jan 17 15:06:42.657: INFO: Got root ca configmap in namespace "svcaccounts-2812"
    Jan 17 15:06:42.672: INFO: Deleted root ca configmap in namespace "svcaccounts-2812"
    STEP: waiting for a new root ca configmap created 01/17/23 15:06:43.173
    Jan 17 15:06:43.176: INFO: Recreated root ca configmap in namespace "svcaccounts-2812"
    Jan 17 15:06:43.181: INFO: Updated root ca configmap in namespace "svcaccounts-2812"
    STEP: waiting for the root ca configmap reconciled 01/17/23 15:06:43.681
    Jan 17 15:06:43.685: INFO: Reconciled root ca configmap in namespace "svcaccounts-2812"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jan 17 15:06:43.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-2812" for this suite. 01/17/23 15:06:43.688
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:06:43.695
Jan 17 15:06:43.695: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename resourcequota 01/17/23 15:06:43.695
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:06:43.723
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:06:43.725
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150
STEP: Discovering how many secrets are in namespace by default 01/17/23 15:06:43.727
STEP: Counting existing ResourceQuota 01/17/23 15:06:49.732
STEP: Creating a ResourceQuota 01/17/23 15:06:54.736
STEP: Ensuring resource quota status is calculated 01/17/23 15:06:54.741
STEP: Creating a Secret 01/17/23 15:06:56.745
STEP: Ensuring resource quota status captures secret creation 01/17/23 15:06:56.756
STEP: Deleting a secret 01/17/23 15:06:58.761
STEP: Ensuring resource quota status released usage 01/17/23 15:06:58.767
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 17 15:07:00.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5341" for this suite. 01/17/23 15:07:00.78
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","completed":74,"skipped":1383,"failed":0}
------------------------------
• [SLOW TEST] [17.091 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:06:43.695
    Jan 17 15:06:43.695: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename resourcequota 01/17/23 15:06:43.695
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:06:43.723
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:06:43.725
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:150
    STEP: Discovering how many secrets are in namespace by default 01/17/23 15:06:43.727
    STEP: Counting existing ResourceQuota 01/17/23 15:06:49.732
    STEP: Creating a ResourceQuota 01/17/23 15:06:54.736
    STEP: Ensuring resource quota status is calculated 01/17/23 15:06:54.741
    STEP: Creating a Secret 01/17/23 15:06:56.745
    STEP: Ensuring resource quota status captures secret creation 01/17/23 15:06:56.756
    STEP: Deleting a secret 01/17/23 15:06:58.761
    STEP: Ensuring resource quota status released usage 01/17/23 15:06:58.767
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 17 15:07:00.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-5341" for this suite. 01/17/23 15:07:00.78
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:07:00.786
Jan 17 15:07:00.786: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename configmap 01/17/23 15:07:00.786
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:07:00.806
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:07:00.808
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137
STEP: Creating configMap that has name configmap-test-emptyKey-f041e01e-7faf-4078-b758-ad4ff5542f16 01/17/23 15:07:00.81
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Jan 17 15:07:00.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2528" for this suite. 01/17/23 15:07:00.819
{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","completed":75,"skipped":1398,"failed":0}
------------------------------
• [0.046 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:07:00.786
    Jan 17 15:07:00.786: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename configmap 01/17/23 15:07:00.786
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:07:00.806
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:07:00.808
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:137
    STEP: Creating configMap that has name configmap-test-emptyKey-f041e01e-7faf-4078-b758-ad4ff5542f16 01/17/23 15:07:00.81
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 17 15:07:00.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-2528" for this suite. 01/17/23 15:07:00.819
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:07:00.832
Jan 17 15:07:00.832: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename aggregator 01/17/23 15:07:00.833
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:07:00.876
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:07:00.879
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
Jan 17 15:07:00.886: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 01/17/23 15:07:00.887
Jan 17 15:07:01.425: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Jan 17 15:07:03.485: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 17 15:07:05.489: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 17 15:07:07.490: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 17 15:07:09.489: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 17 15:07:11.490: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 17 15:07:13.490: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 17 15:07:15.490: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 17 15:07:17.490: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 17 15:07:19.489: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 17 15:07:21.491: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 17 15:07:23.489: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 17 15:07:25.612: INFO: Waited 118.694724ms for the sample-apiserver to be ready to handle requests.
I0117 15:07:26.649530      22 request.go:682] Waited for 1.016571337s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/operator.openshift.io/v1
STEP: Read Status for v1alpha1.wardle.example.com 01/17/23 15:07:27.31
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 01/17/23 15:07:27.352
STEP: List APIServices 01/17/23 15:07:27.405
Jan 17 15:07:27.457: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:187
Jan 17 15:07:28.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-3746" for this suite. 01/17/23 15:07:28.303
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","completed":76,"skipped":1411,"failed":0}
------------------------------
• [SLOW TEST] [27.525 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:07:00.832
    Jan 17 15:07:00.832: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename aggregator 01/17/23 15:07:00.833
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:07:00.876
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:07:00.879
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    Jan 17 15:07:00.886: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 01/17/23 15:07:00.887
    Jan 17 15:07:01.425: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
    Jan 17 15:07:03.485: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 17 15:07:05.489: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 17 15:07:07.490: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 17 15:07:09.489: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 17 15:07:11.490: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 17 15:07:13.490: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 17 15:07:15.490: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 17 15:07:17.490: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 17 15:07:19.489: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 17 15:07:21.491: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 17 15:07:23.489: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 15, 7, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 17 15:07:25.612: INFO: Waited 118.694724ms for the sample-apiserver to be ready to handle requests.
    I0117 15:07:26.649530      22 request.go:682] Waited for 1.016571337s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/operator.openshift.io/v1
    STEP: Read Status for v1alpha1.wardle.example.com 01/17/23 15:07:27.31
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 01/17/23 15:07:27.352
    STEP: List APIServices 01/17/23 15:07:27.405
    Jan 17 15:07:27.457: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/framework.go:187
    Jan 17 15:07:28.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "aggregator-3746" for this suite. 01/17/23 15:07:28.303
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:07:28.357
Jan 17 15:07:28.357: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename init-container 01/17/23 15:07:28.358
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:07:28.388
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:07:28.391
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333
STEP: creating the pod 01/17/23 15:07:28.392
Jan 17 15:07:28.393: INFO: PodSpec: initContainers in spec.initContainers
Jan 17 15:08:11.121: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-c6af628d-5209-4274-9dd8-47995aa1f771", GenerateName:"", Namespace:"init-container-4076", SelfLink:"", UID:"c5dd25a6-80d2-4d64-8eea-cd6321453beb", ResourceVersion:"82295", Generation:0, CreationTimestamp:time.Date(2023, time.January, 17, 15, 7, 28, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"393004368"}, Annotations:map[string]string{"k8s.ovn.org/pod-networks":"{\"default\":{\"ip_addresses\":[\"10.131.0.70/23\"],\"mac_address\":\"0a:58:0a:83:00:46\",\"gateway_ips\":[\"10.131.0.1\"],\"ip_address\":\"10.131.0.70/23\",\"gateway_ip\":\"10.131.0.1\"}}", "k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"ovn-kubernetes\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.131.0.70\"\n    ],\n    \"mac\": \"0a:58:0a:83:00:46\",\n    \"default\": true,\n    \"dns\": {}\n}]", "k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"ovn-kubernetes\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.131.0.70\"\n    ],\n    \"mac\": \"0a:58:0a:83:00:46\",\n    \"default\": true,\n    \"dns\": {}\n}]", "openshift.io/scc":"anyuid"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 17, 15, 7, 28, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000d4a2d0), Subresource:""}, v1.ManagedFieldsEntry{Manager:"ip-10-0-135-246", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 17, 15, 7, 28, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000d4a390), Subresource:""}, v1.ManagedFieldsEntry{Manager:"multus", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 17, 15, 7, 28, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000d4a3c0), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 17, 15, 8, 11, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000d4a408), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-n8n4r", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc0039c1c60), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-n8n4r", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc002293a40), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-n8n4r", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc002293aa0), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.8", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-n8n4r", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc0022939e0), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc002c58ac8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-10-0-151-22.ec2.internal", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000af3490), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002c58b70)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002c58b90)}, v1.Toleration{Key:"node.kubernetes.io/memory-pressure", Operator:"Exists", Value:"", Effect:"NoSchedule", TolerationSeconds:(*int64)(nil)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc002c58bac), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc002c58bb0), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc00120c330), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 17, 15, 7, 28, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 17, 15, 7, 28, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 17, 15, 7, 28, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 17, 15, 7, 28, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.0.151.22", PodIP:"10.131.0.70", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.131.0.70"}}, StartTime:time.Date(2023, time.January, 17, 15, 7, 28, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000af3570)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000af35e0)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"cri-o://7831f07edc7c515648b0822078da3dbcd060ec6ceb8f80b5b532a8e2b4971086", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0039c1ce0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0039c1cc0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.8", ImageID:"", ContainerID:"", Started:(*bool)(0xc002c58c2f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Jan 17 15:08:11.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4076" for this suite. 01/17/23 15:08:11.127
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","completed":77,"skipped":1412,"failed":0}
------------------------------
• [SLOW TEST] [42.776 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:07:28.357
    Jan 17 15:07:28.357: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename init-container 01/17/23 15:07:28.358
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:07:28.388
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:07:28.391
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:333
    STEP: creating the pod 01/17/23 15:07:28.392
    Jan 17 15:07:28.393: INFO: PodSpec: initContainers in spec.initContainers
    Jan 17 15:08:11.121: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-c6af628d-5209-4274-9dd8-47995aa1f771", GenerateName:"", Namespace:"init-container-4076", SelfLink:"", UID:"c5dd25a6-80d2-4d64-8eea-cd6321453beb", ResourceVersion:"82295", Generation:0, CreationTimestamp:time.Date(2023, time.January, 17, 15, 7, 28, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"393004368"}, Annotations:map[string]string{"k8s.ovn.org/pod-networks":"{\"default\":{\"ip_addresses\":[\"10.131.0.70/23\"],\"mac_address\":\"0a:58:0a:83:00:46\",\"gateway_ips\":[\"10.131.0.1\"],\"ip_address\":\"10.131.0.70/23\",\"gateway_ip\":\"10.131.0.1\"}}", "k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"ovn-kubernetes\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.131.0.70\"\n    ],\n    \"mac\": \"0a:58:0a:83:00:46\",\n    \"default\": true,\n    \"dns\": {}\n}]", "k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"ovn-kubernetes\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.131.0.70\"\n    ],\n    \"mac\": \"0a:58:0a:83:00:46\",\n    \"default\": true,\n    \"dns\": {}\n}]", "openshift.io/scc":"anyuid"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 17, 15, 7, 28, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000d4a2d0), Subresource:""}, v1.ManagedFieldsEntry{Manager:"ip-10-0-135-246", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 17, 15, 7, 28, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000d4a390), Subresource:""}, v1.ManagedFieldsEntry{Manager:"multus", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 17, 15, 7, 28, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000d4a3c0), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 17, 15, 8, 11, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000d4a408), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-n8n4r", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc0039c1c60), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-n8n4r", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc002293a40), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-n8n4r", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc002293aa0), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.8", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-n8n4r", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc0022939e0), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc002c58ac8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-10-0-151-22.ec2.internal", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000af3490), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002c58b70)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002c58b90)}, v1.Toleration{Key:"node.kubernetes.io/memory-pressure", Operator:"Exists", Value:"", Effect:"NoSchedule", TolerationSeconds:(*int64)(nil)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc002c58bac), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc002c58bb0), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc00120c330), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 17, 15, 7, 28, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 17, 15, 7, 28, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 17, 15, 7, 28, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 17, 15, 7, 28, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.0.151.22", PodIP:"10.131.0.70", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.131.0.70"}}, StartTime:time.Date(2023, time.January, 17, 15, 7, 28, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000af3570)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000af35e0)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"cri-o://7831f07edc7c515648b0822078da3dbcd060ec6ceb8f80b5b532a8e2b4971086", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0039c1ce0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0039c1cc0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.8", ImageID:"", ContainerID:"", Started:(*bool)(0xc002c58c2f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan 17 15:08:11.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-4076" for this suite. 01/17/23 15:08:11.127
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1413
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:08:11.134
Jan 17 15:08:11.134: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename kubectl 01/17/23 15:08:11.134
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:08:11.156
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:08:11.159
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1413
STEP: creating Agnhost RC 01/17/23 15:08:11.161
Jan 17 15:08:11.161: INFO: namespace kubectl-3773
Jan 17 15:08:11.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-3773 create -f -'
Jan 17 15:08:13.328: INFO: stderr: ""
Jan 17 15:08:13.328: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 01/17/23 15:08:13.328
Jan 17 15:08:14.332: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 17 15:08:14.332: INFO: Found 0 / 1
Jan 17 15:08:15.332: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 17 15:08:15.332: INFO: Found 1 / 1
Jan 17 15:08:15.332: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan 17 15:08:15.335: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 17 15:08:15.335: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 17 15:08:15.335: INFO: wait on agnhost-primary startup in kubectl-3773 
Jan 17 15:08:15.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-3773 logs agnhost-primary-c8jg4 agnhost-primary'
Jan 17 15:08:15.392: INFO: stderr: ""
Jan 17 15:08:15.392: INFO: stdout: "Paused\n"
STEP: exposing RC 01/17/23 15:08:15.392
Jan 17 15:08:15.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-3773 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Jan 17 15:08:15.456: INFO: stderr: ""
Jan 17 15:08:15.456: INFO: stdout: "service/rm2 exposed\n"
Jan 17 15:08:15.464: INFO: Service rm2 in namespace kubectl-3773 found.
STEP: exposing service 01/17/23 15:08:17.47
Jan 17 15:08:17.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-3773 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Jan 17 15:08:17.527: INFO: stderr: ""
Jan 17 15:08:17.527: INFO: stdout: "service/rm3 exposed\n"
Jan 17 15:08:17.531: INFO: Service rm3 in namespace kubectl-3773 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 17 15:08:19.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3773" for this suite. 01/17/23 15:08:19.542
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","completed":78,"skipped":1427,"failed":0}
------------------------------
• [SLOW TEST] [8.413 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1407
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:08:11.134
    Jan 17 15:08:11.134: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename kubectl 01/17/23 15:08:11.134
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:08:11.156
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:08:11.159
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1413
    STEP: creating Agnhost RC 01/17/23 15:08:11.161
    Jan 17 15:08:11.161: INFO: namespace kubectl-3773
    Jan 17 15:08:11.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-3773 create -f -'
    Jan 17 15:08:13.328: INFO: stderr: ""
    Jan 17 15:08:13.328: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 01/17/23 15:08:13.328
    Jan 17 15:08:14.332: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 17 15:08:14.332: INFO: Found 0 / 1
    Jan 17 15:08:15.332: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 17 15:08:15.332: INFO: Found 1 / 1
    Jan 17 15:08:15.332: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Jan 17 15:08:15.335: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 17 15:08:15.335: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jan 17 15:08:15.335: INFO: wait on agnhost-primary startup in kubectl-3773 
    Jan 17 15:08:15.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-3773 logs agnhost-primary-c8jg4 agnhost-primary'
    Jan 17 15:08:15.392: INFO: stderr: ""
    Jan 17 15:08:15.392: INFO: stdout: "Paused\n"
    STEP: exposing RC 01/17/23 15:08:15.392
    Jan 17 15:08:15.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-3773 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    Jan 17 15:08:15.456: INFO: stderr: ""
    Jan 17 15:08:15.456: INFO: stdout: "service/rm2 exposed\n"
    Jan 17 15:08:15.464: INFO: Service rm2 in namespace kubectl-3773 found.
    STEP: exposing service 01/17/23 15:08:17.47
    Jan 17 15:08:17.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-3773 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    Jan 17 15:08:17.527: INFO: stderr: ""
    Jan 17 15:08:17.527: INFO: stdout: "service/rm3 exposed\n"
    Jan 17 15:08:17.531: INFO: Service rm3 in namespace kubectl-3773 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 17 15:08:19.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-3773" for this suite. 01/17/23 15:08:19.542
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:08:19.548
Jan 17 15:08:19.548: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename lease-test 01/17/23 15:08:19.549
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:08:19.576
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:08:19.578
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/framework.go:187
Jan 17 15:08:19.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-332" for this suite. 01/17/23 15:08:19.678
{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","completed":79,"skipped":1454,"failed":0}
------------------------------
• [0.136 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:08:19.548
    Jan 17 15:08:19.548: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename lease-test 01/17/23 15:08:19.549
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:08:19.576
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:08:19.578
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/framework.go:187
    Jan 17 15:08:19.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "lease-test-332" for this suite. 01/17/23 15:08:19.678
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:08:19.685
Jan 17 15:08:19.685: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename downward-api 01/17/23 15:08:19.685
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:08:19.704
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:08:19.706
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52
STEP: Creating a pod to test downward API volume plugin 01/17/23 15:08:19.709
Jan 17 15:08:19.740: INFO: Waiting up to 5m0s for pod "downwardapi-volume-edfd8217-3763-4a98-93a5-17abc924a1c4" in namespace "downward-api-1770" to be "Succeeded or Failed"
Jan 17 15:08:19.744: INFO: Pod "downwardapi-volume-edfd8217-3763-4a98-93a5-17abc924a1c4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.326425ms
Jan 17 15:08:21.750: INFO: Pod "downwardapi-volume-edfd8217-3763-4a98-93a5-17abc924a1c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009700897s
Jan 17 15:08:23.749: INFO: Pod "downwardapi-volume-edfd8217-3763-4a98-93a5-17abc924a1c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008693216s
STEP: Saw pod success 01/17/23 15:08:23.749
Jan 17 15:08:23.749: INFO: Pod "downwardapi-volume-edfd8217-3763-4a98-93a5-17abc924a1c4" satisfied condition "Succeeded or Failed"
Jan 17 15:08:23.752: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod downwardapi-volume-edfd8217-3763-4a98-93a5-17abc924a1c4 container client-container: <nil>
STEP: delete the pod 01/17/23 15:08:23.761
Jan 17 15:08:23.775: INFO: Waiting for pod downwardapi-volume-edfd8217-3763-4a98-93a5-17abc924a1c4 to disappear
Jan 17 15:08:23.777: INFO: Pod downwardapi-volume-edfd8217-3763-4a98-93a5-17abc924a1c4 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 17 15:08:23.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1770" for this suite. 01/17/23 15:08:23.781
{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","completed":80,"skipped":1454,"failed":0}
------------------------------
• [4.103 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:08:19.685
    Jan 17 15:08:19.685: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename downward-api 01/17/23 15:08:19.685
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:08:19.704
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:08:19.706
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:52
    STEP: Creating a pod to test downward API volume plugin 01/17/23 15:08:19.709
    Jan 17 15:08:19.740: INFO: Waiting up to 5m0s for pod "downwardapi-volume-edfd8217-3763-4a98-93a5-17abc924a1c4" in namespace "downward-api-1770" to be "Succeeded or Failed"
    Jan 17 15:08:19.744: INFO: Pod "downwardapi-volume-edfd8217-3763-4a98-93a5-17abc924a1c4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.326425ms
    Jan 17 15:08:21.750: INFO: Pod "downwardapi-volume-edfd8217-3763-4a98-93a5-17abc924a1c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009700897s
    Jan 17 15:08:23.749: INFO: Pod "downwardapi-volume-edfd8217-3763-4a98-93a5-17abc924a1c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008693216s
    STEP: Saw pod success 01/17/23 15:08:23.749
    Jan 17 15:08:23.749: INFO: Pod "downwardapi-volume-edfd8217-3763-4a98-93a5-17abc924a1c4" satisfied condition "Succeeded or Failed"
    Jan 17 15:08:23.752: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod downwardapi-volume-edfd8217-3763-4a98-93a5-17abc924a1c4 container client-container: <nil>
    STEP: delete the pod 01/17/23 15:08:23.761
    Jan 17 15:08:23.775: INFO: Waiting for pod downwardapi-volume-edfd8217-3763-4a98-93a5-17abc924a1c4 to disappear
    Jan 17 15:08:23.777: INFO: Pod downwardapi-volume-edfd8217-3763-4a98-93a5-17abc924a1c4 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 17 15:08:23.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-1770" for this suite. 01/17/23 15:08:23.781
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:08:23.788
Jan 17 15:08:23.788: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename webhook 01/17/23 15:08:23.789
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:08:23.815
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:08:23.817
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/17/23 15:08:23.846
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/17/23 15:08:24.31
STEP: Deploying the webhook pod 01/17/23 15:08:24.321
STEP: Wait for the deployment to be ready 01/17/23 15:08:24.332
Jan 17 15:08:24.339: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/17/23 15:08:26.349
STEP: Verifying the service has paired with the endpoint 01/17/23 15:08:26.358
Jan 17 15:08:27.358: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116
STEP: fetching the /apis discovery document 01/17/23 15:08:27.367
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 01/17/23 15:08:27.368
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 01/17/23 15:08:27.368
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 01/17/23 15:08:27.368
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 01/17/23 15:08:27.369
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 01/17/23 15:08:27.369
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 01/17/23 15:08:27.369
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 17 15:08:27.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2813" for this suite. 01/17/23 15:08:27.375
STEP: Destroying namespace "webhook-2813-markers" for this suite. 01/17/23 15:08:27.38
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","completed":81,"skipped":1464,"failed":0}
------------------------------
• [3.664 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:08:23.788
    Jan 17 15:08:23.788: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename webhook 01/17/23 15:08:23.789
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:08:23.815
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:08:23.817
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/17/23 15:08:23.846
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/17/23 15:08:24.31
    STEP: Deploying the webhook pod 01/17/23 15:08:24.321
    STEP: Wait for the deployment to be ready 01/17/23 15:08:24.332
    Jan 17 15:08:24.339: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/17/23 15:08:26.349
    STEP: Verifying the service has paired with the endpoint 01/17/23 15:08:26.358
    Jan 17 15:08:27.358: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:116
    STEP: fetching the /apis discovery document 01/17/23 15:08:27.367
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 01/17/23 15:08:27.368
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 01/17/23 15:08:27.368
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 01/17/23 15:08:27.368
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 01/17/23 15:08:27.369
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 01/17/23 15:08:27.369
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 01/17/23 15:08:27.369
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 17 15:08:27.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-2813" for this suite. 01/17/23 15:08:27.375
    STEP: Destroying namespace "webhook-2813-markers" for this suite. 01/17/23 15:08:27.38
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:08:27.453
Jan 17 15:08:27.453: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename secrets 01/17/23 15:08:27.454
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:08:27.492
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:08:27.494
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78
STEP: Creating secret with name secret-test-map-91b5b55c-b61d-4c69-bb40-244940f3d760 01/17/23 15:08:27.496
STEP: Creating a pod to test consume secrets 01/17/23 15:08:27.507
Jan 17 15:08:27.524: INFO: Waiting up to 5m0s for pod "pod-secrets-d8e66bd5-c34f-459a-a724-953e2a52aba1" in namespace "secrets-4684" to be "Succeeded or Failed"
Jan 17 15:08:27.527: INFO: Pod "pod-secrets-d8e66bd5-c34f-459a-a724-953e2a52aba1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.07219ms
Jan 17 15:08:29.532: INFO: Pod "pod-secrets-d8e66bd5-c34f-459a-a724-953e2a52aba1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007305886s
Jan 17 15:08:31.532: INFO: Pod "pod-secrets-d8e66bd5-c34f-459a-a724-953e2a52aba1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008073429s
STEP: Saw pod success 01/17/23 15:08:31.533
Jan 17 15:08:31.533: INFO: Pod "pod-secrets-d8e66bd5-c34f-459a-a724-953e2a52aba1" satisfied condition "Succeeded or Failed"
Jan 17 15:08:31.535: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-secrets-d8e66bd5-c34f-459a-a724-953e2a52aba1 container secret-volume-test: <nil>
STEP: delete the pod 01/17/23 15:08:31.54
Jan 17 15:08:31.550: INFO: Waiting for pod pod-secrets-d8e66bd5-c34f-459a-a724-953e2a52aba1 to disappear
Jan 17 15:08:31.552: INFO: Pod pod-secrets-d8e66bd5-c34f-459a-a724-953e2a52aba1 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan 17 15:08:31.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4684" for this suite. 01/17/23 15:08:31.556
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":82,"skipped":1483,"failed":0}
------------------------------
• [4.109 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:08:27.453
    Jan 17 15:08:27.453: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename secrets 01/17/23 15:08:27.454
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:08:27.492
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:08:27.494
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:78
    STEP: Creating secret with name secret-test-map-91b5b55c-b61d-4c69-bb40-244940f3d760 01/17/23 15:08:27.496
    STEP: Creating a pod to test consume secrets 01/17/23 15:08:27.507
    Jan 17 15:08:27.524: INFO: Waiting up to 5m0s for pod "pod-secrets-d8e66bd5-c34f-459a-a724-953e2a52aba1" in namespace "secrets-4684" to be "Succeeded or Failed"
    Jan 17 15:08:27.527: INFO: Pod "pod-secrets-d8e66bd5-c34f-459a-a724-953e2a52aba1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.07219ms
    Jan 17 15:08:29.532: INFO: Pod "pod-secrets-d8e66bd5-c34f-459a-a724-953e2a52aba1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007305886s
    Jan 17 15:08:31.532: INFO: Pod "pod-secrets-d8e66bd5-c34f-459a-a724-953e2a52aba1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008073429s
    STEP: Saw pod success 01/17/23 15:08:31.533
    Jan 17 15:08:31.533: INFO: Pod "pod-secrets-d8e66bd5-c34f-459a-a724-953e2a52aba1" satisfied condition "Succeeded or Failed"
    Jan 17 15:08:31.535: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-secrets-d8e66bd5-c34f-459a-a724-953e2a52aba1 container secret-volume-test: <nil>
    STEP: delete the pod 01/17/23 15:08:31.54
    Jan 17 15:08:31.550: INFO: Waiting for pod pod-secrets-d8e66bd5-c34f-459a-a724-953e2a52aba1 to disappear
    Jan 17 15:08:31.552: INFO: Pod pod-secrets-d8e66bd5-c34f-459a-a724-953e2a52aba1 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan 17 15:08:31.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-4684" for this suite. 01/17/23 15:08:31.556
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:08:31.563
Jan 17 15:08:31.563: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename cronjob 01/17/23 15:08:31.563
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:08:31.59
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:08:31.592
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 01/17/23 15:08:31.595
W0117 15:08:31.603246      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring a job is scheduled 01/17/23 15:08:31.603
STEP: Ensuring exactly one is scheduled 01/17/23 15:09:01.608
STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/17/23 15:09:01.611
STEP: Ensuring the job is replaced with a new one 01/17/23 15:09:01.613
STEP: Removing cronjob 01/17/23 15:10:01.617
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Jan 17 15:10:01.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-5660" for this suite. 01/17/23 15:10:01.627
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","completed":83,"skipped":1502,"failed":0}
------------------------------
• [SLOW TEST] [90.071 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:08:31.563
    Jan 17 15:08:31.563: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename cronjob 01/17/23 15:08:31.563
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:08:31.59
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:08:31.592
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 01/17/23 15:08:31.595
    W0117 15:08:31.603246      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Ensuring a job is scheduled 01/17/23 15:08:31.603
    STEP: Ensuring exactly one is scheduled 01/17/23 15:09:01.608
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/17/23 15:09:01.611
    STEP: Ensuring the job is replaced with a new one 01/17/23 15:09:01.613
    STEP: Removing cronjob 01/17/23 15:10:01.617
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Jan 17 15:10:01.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-5660" for this suite. 01/17/23 15:10:01.627
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:10:01.635
Jan 17 15:10:01.635: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename services 01/17/23 15:10:01.636
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:10:01.663
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:10:01.665
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173
STEP: creating service in namespace services-8170 01/17/23 15:10:01.667
Jan 17 15:10:01.746: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-8170" to be "running and ready"
Jan 17 15:10:01.750: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 3.993049ms
Jan 17 15:10:01.750: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Jan 17 15:10:03.753: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.006990845s
Jan 17 15:10:03.753: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
Jan 17 15:10:03.753: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
Jan 17 15:10:03.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-8170 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Jan 17 15:10:03.903: INFO: rc: 7
Jan 17 15:10:03.916: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jan 17 15:10:03.919: INFO: Pod kube-proxy-mode-detector no longer exists
Jan 17 15:10:03.919: INFO: Couldn't detect KubeProxy mode - test failure may be expected: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-8170 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode:
Command stdout:

stderr:
+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode
command terminated with exit code 7

error:
exit status 7
STEP: creating service affinity-clusterip-timeout in namespace services-8170 01/17/23 15:10:03.919
STEP: creating replication controller affinity-clusterip-timeout in namespace services-8170 01/17/23 15:10:03.93
I0117 15:10:03.935873      22 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-8170, replica count: 3
I0117 15:10:06.986704      22 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 17 15:10:07.003: INFO: Creating new exec pod
Jan 17 15:10:07.055: INFO: Waiting up to 5m0s for pod "execpod-affinitykfnnw" in namespace "services-8170" to be "running"
Jan 17 15:10:07.058: INFO: Pod "execpod-affinitykfnnw": Phase="Pending", Reason="", readiness=false. Elapsed: 3.039428ms
Jan 17 15:10:09.065: INFO: Pod "execpod-affinitykfnnw": Phase="Running", Reason="", readiness=true. Elapsed: 2.009883163s
Jan 17 15:10:09.065: INFO: Pod "execpod-affinitykfnnw" satisfied condition "running"
Jan 17 15:10:10.066: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-8170 exec execpod-affinitykfnnw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
Jan 17 15:10:11.214: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Jan 17 15:10:11.214: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 17 15:10:11.214: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-8170 exec execpod-affinitykfnnw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.42.255 80'
Jan 17 15:10:11.309: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.42.255 80\nConnection to 172.30.42.255 80 port [tcp/http] succeeded!\n"
Jan 17 15:10:11.309: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 17 15:10:11.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-8170 exec execpod-affinitykfnnw -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.30.42.255:80/ ; done'
Jan 17 15:10:11.454: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.42.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.42.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.42.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.42.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.42.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.42.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.42.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.42.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.42.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.42.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.42.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.42.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.42.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.42.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.42.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.42.255:80/\n"
Jan 17 15:10:11.454: INFO: stdout: "\naffinity-clusterip-timeout-d56qz\naffinity-clusterip-timeout-d56qz\naffinity-clusterip-timeout-d56qz\naffinity-clusterip-timeout-d56qz\naffinity-clusterip-timeout-d56qz\naffinity-clusterip-timeout-d56qz\naffinity-clusterip-timeout-d56qz\naffinity-clusterip-timeout-d56qz\naffinity-clusterip-timeout-d56qz\naffinity-clusterip-timeout-d56qz\naffinity-clusterip-timeout-d56qz\naffinity-clusterip-timeout-d56qz\naffinity-clusterip-timeout-d56qz\naffinity-clusterip-timeout-d56qz\naffinity-clusterip-timeout-d56qz\naffinity-clusterip-timeout-d56qz"
Jan 17 15:10:11.454: INFO: Received response from host: affinity-clusterip-timeout-d56qz
Jan 17 15:10:11.454: INFO: Received response from host: affinity-clusterip-timeout-d56qz
Jan 17 15:10:11.454: INFO: Received response from host: affinity-clusterip-timeout-d56qz
Jan 17 15:10:11.454: INFO: Received response from host: affinity-clusterip-timeout-d56qz
Jan 17 15:10:11.454: INFO: Received response from host: affinity-clusterip-timeout-d56qz
Jan 17 15:10:11.454: INFO: Received response from host: affinity-clusterip-timeout-d56qz
Jan 17 15:10:11.454: INFO: Received response from host: affinity-clusterip-timeout-d56qz
Jan 17 15:10:11.454: INFO: Received response from host: affinity-clusterip-timeout-d56qz
Jan 17 15:10:11.454: INFO: Received response from host: affinity-clusterip-timeout-d56qz
Jan 17 15:10:11.454: INFO: Received response from host: affinity-clusterip-timeout-d56qz
Jan 17 15:10:11.454: INFO: Received response from host: affinity-clusterip-timeout-d56qz
Jan 17 15:10:11.454: INFO: Received response from host: affinity-clusterip-timeout-d56qz
Jan 17 15:10:11.454: INFO: Received response from host: affinity-clusterip-timeout-d56qz
Jan 17 15:10:11.454: INFO: Received response from host: affinity-clusterip-timeout-d56qz
Jan 17 15:10:11.454: INFO: Received response from host: affinity-clusterip-timeout-d56qz
Jan 17 15:10:11.454: INFO: Received response from host: affinity-clusterip-timeout-d56qz
Jan 17 15:10:11.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-8170 exec execpod-affinitykfnnw -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.30.42.255:80/'
Jan 17 15:10:11.560: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.30.42.255:80/\n"
Jan 17 15:10:11.560: INFO: stdout: "affinity-clusterip-timeout-d56qz"
Jan 17 15:10:31.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-8170 exec execpod-affinitykfnnw -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.30.42.255:80/'
Jan 17 15:10:32.720: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.30.42.255:80/\n"
Jan 17 15:10:32.720: INFO: stdout: "affinity-clusterip-timeout-tg4ml"
Jan 17 15:10:32.720: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-8170, will wait for the garbage collector to delete the pods 01/17/23 15:10:32.731
Jan 17 15:10:32.793: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 6.287741ms
Jan 17 15:10:32.895: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 101.760384ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 17 15:10:35.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8170" for this suite. 01/17/23 15:10:35.118
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","completed":84,"skipped":1526,"failed":0}
------------------------------
• [SLOW TEST] [33.492 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:10:01.635
    Jan 17 15:10:01.635: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename services 01/17/23 15:10:01.636
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:10:01.663
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:10:01.665
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2173
    STEP: creating service in namespace services-8170 01/17/23 15:10:01.667
    Jan 17 15:10:01.746: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-8170" to be "running and ready"
    Jan 17 15:10:01.750: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 3.993049ms
    Jan 17 15:10:01.750: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 15:10:03.753: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.006990845s
    Jan 17 15:10:03.753: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
    Jan 17 15:10:03.753: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
    Jan 17 15:10:03.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-8170 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
    Jan 17 15:10:03.903: INFO: rc: 7
    Jan 17 15:10:03.916: INFO: Waiting for pod kube-proxy-mode-detector to disappear
    Jan 17 15:10:03.919: INFO: Pod kube-proxy-mode-detector no longer exists
    Jan 17 15:10:03.919: INFO: Couldn't detect KubeProxy mode - test failure may be expected: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-8170 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode:
    Command stdout:

    stderr:
    + curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode
    command terminated with exit code 7

    error:
    exit status 7
    STEP: creating service affinity-clusterip-timeout in namespace services-8170 01/17/23 15:10:03.919
    STEP: creating replication controller affinity-clusterip-timeout in namespace services-8170 01/17/23 15:10:03.93
    I0117 15:10:03.935873      22 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-8170, replica count: 3
    I0117 15:10:06.986704      22 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 17 15:10:07.003: INFO: Creating new exec pod
    Jan 17 15:10:07.055: INFO: Waiting up to 5m0s for pod "execpod-affinitykfnnw" in namespace "services-8170" to be "running"
    Jan 17 15:10:07.058: INFO: Pod "execpod-affinitykfnnw": Phase="Pending", Reason="", readiness=false. Elapsed: 3.039428ms
    Jan 17 15:10:09.065: INFO: Pod "execpod-affinitykfnnw": Phase="Running", Reason="", readiness=true. Elapsed: 2.009883163s
    Jan 17 15:10:09.065: INFO: Pod "execpod-affinitykfnnw" satisfied condition "running"
    Jan 17 15:10:10.066: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-8170 exec execpod-affinitykfnnw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
    Jan 17 15:10:11.214: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
    Jan 17 15:10:11.214: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 17 15:10:11.214: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-8170 exec execpod-affinitykfnnw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.42.255 80'
    Jan 17 15:10:11.309: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.42.255 80\nConnection to 172.30.42.255 80 port [tcp/http] succeeded!\n"
    Jan 17 15:10:11.309: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 17 15:10:11.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-8170 exec execpod-affinitykfnnw -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.30.42.255:80/ ; done'
    Jan 17 15:10:11.454: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.42.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.42.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.42.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.42.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.42.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.42.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.42.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.42.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.42.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.42.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.42.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.42.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.42.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.42.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.42.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.42.255:80/\n"
    Jan 17 15:10:11.454: INFO: stdout: "\naffinity-clusterip-timeout-d56qz\naffinity-clusterip-timeout-d56qz\naffinity-clusterip-timeout-d56qz\naffinity-clusterip-timeout-d56qz\naffinity-clusterip-timeout-d56qz\naffinity-clusterip-timeout-d56qz\naffinity-clusterip-timeout-d56qz\naffinity-clusterip-timeout-d56qz\naffinity-clusterip-timeout-d56qz\naffinity-clusterip-timeout-d56qz\naffinity-clusterip-timeout-d56qz\naffinity-clusterip-timeout-d56qz\naffinity-clusterip-timeout-d56qz\naffinity-clusterip-timeout-d56qz\naffinity-clusterip-timeout-d56qz\naffinity-clusterip-timeout-d56qz"
    Jan 17 15:10:11.454: INFO: Received response from host: affinity-clusterip-timeout-d56qz
    Jan 17 15:10:11.454: INFO: Received response from host: affinity-clusterip-timeout-d56qz
    Jan 17 15:10:11.454: INFO: Received response from host: affinity-clusterip-timeout-d56qz
    Jan 17 15:10:11.454: INFO: Received response from host: affinity-clusterip-timeout-d56qz
    Jan 17 15:10:11.454: INFO: Received response from host: affinity-clusterip-timeout-d56qz
    Jan 17 15:10:11.454: INFO: Received response from host: affinity-clusterip-timeout-d56qz
    Jan 17 15:10:11.454: INFO: Received response from host: affinity-clusterip-timeout-d56qz
    Jan 17 15:10:11.454: INFO: Received response from host: affinity-clusterip-timeout-d56qz
    Jan 17 15:10:11.454: INFO: Received response from host: affinity-clusterip-timeout-d56qz
    Jan 17 15:10:11.454: INFO: Received response from host: affinity-clusterip-timeout-d56qz
    Jan 17 15:10:11.454: INFO: Received response from host: affinity-clusterip-timeout-d56qz
    Jan 17 15:10:11.454: INFO: Received response from host: affinity-clusterip-timeout-d56qz
    Jan 17 15:10:11.454: INFO: Received response from host: affinity-clusterip-timeout-d56qz
    Jan 17 15:10:11.454: INFO: Received response from host: affinity-clusterip-timeout-d56qz
    Jan 17 15:10:11.454: INFO: Received response from host: affinity-clusterip-timeout-d56qz
    Jan 17 15:10:11.454: INFO: Received response from host: affinity-clusterip-timeout-d56qz
    Jan 17 15:10:11.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-8170 exec execpod-affinitykfnnw -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.30.42.255:80/'
    Jan 17 15:10:11.560: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.30.42.255:80/\n"
    Jan 17 15:10:11.560: INFO: stdout: "affinity-clusterip-timeout-d56qz"
    Jan 17 15:10:31.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-8170 exec execpod-affinitykfnnw -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.30.42.255:80/'
    Jan 17 15:10:32.720: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.30.42.255:80/\n"
    Jan 17 15:10:32.720: INFO: stdout: "affinity-clusterip-timeout-tg4ml"
    Jan 17 15:10:32.720: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-8170, will wait for the garbage collector to delete the pods 01/17/23 15:10:32.731
    Jan 17 15:10:32.793: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 6.287741ms
    Jan 17 15:10:32.895: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 101.760384ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 17 15:10:35.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-8170" for this suite. 01/17/23 15:10:35.118
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:10:35.127
Jan 17 15:10:35.127: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename secrets 01/17/23 15:10:35.127
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:10:35.167
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:10:35.169
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204
Jan 17 15:10:35.183: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-fb5a782a-095f-4b7c-88a3-5a509cd9feba 01/17/23 15:10:35.183
STEP: Creating secret with name s-test-opt-upd-e4f0acd7-7ac0-485b-b441-b296756d41ef 01/17/23 15:10:35.187
STEP: Creating the pod 01/17/23 15:10:35.191
Jan 17 15:10:35.216: INFO: Waiting up to 5m0s for pod "pod-secrets-d6871b70-fd8c-4043-bea0-5f9297724c2d" in namespace "secrets-358" to be "running and ready"
Jan 17 15:10:35.224: INFO: Pod "pod-secrets-d6871b70-fd8c-4043-bea0-5f9297724c2d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.319591ms
Jan 17 15:10:35.224: INFO: The phase of Pod pod-secrets-d6871b70-fd8c-4043-bea0-5f9297724c2d is Pending, waiting for it to be Running (with Ready = true)
Jan 17 15:10:37.228: INFO: Pod "pod-secrets-d6871b70-fd8c-4043-bea0-5f9297724c2d": Phase="Running", Reason="", readiness=true. Elapsed: 2.012071411s
Jan 17 15:10:37.228: INFO: The phase of Pod pod-secrets-d6871b70-fd8c-4043-bea0-5f9297724c2d is Running (Ready = true)
Jan 17 15:10:37.228: INFO: Pod "pod-secrets-d6871b70-fd8c-4043-bea0-5f9297724c2d" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-fb5a782a-095f-4b7c-88a3-5a509cd9feba 01/17/23 15:10:37.252
STEP: Updating secret s-test-opt-upd-e4f0acd7-7ac0-485b-b441-b296756d41ef 01/17/23 15:10:37.257
STEP: Creating secret with name s-test-opt-create-ca557924-cb30-4f3b-a357-944e347cb299 01/17/23 15:10:37.261
STEP: waiting to observe update in volume 01/17/23 15:10:37.265
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan 17 15:10:39.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-358" for this suite. 01/17/23 15:10:39.295
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":85,"skipped":1531,"failed":0}
------------------------------
• [4.175 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:10:35.127
    Jan 17 15:10:35.127: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename secrets 01/17/23 15:10:35.127
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:10:35.167
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:10:35.169
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:204
    Jan 17 15:10:35.183: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
    STEP: Creating secret with name s-test-opt-del-fb5a782a-095f-4b7c-88a3-5a509cd9feba 01/17/23 15:10:35.183
    STEP: Creating secret with name s-test-opt-upd-e4f0acd7-7ac0-485b-b441-b296756d41ef 01/17/23 15:10:35.187
    STEP: Creating the pod 01/17/23 15:10:35.191
    Jan 17 15:10:35.216: INFO: Waiting up to 5m0s for pod "pod-secrets-d6871b70-fd8c-4043-bea0-5f9297724c2d" in namespace "secrets-358" to be "running and ready"
    Jan 17 15:10:35.224: INFO: Pod "pod-secrets-d6871b70-fd8c-4043-bea0-5f9297724c2d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.319591ms
    Jan 17 15:10:35.224: INFO: The phase of Pod pod-secrets-d6871b70-fd8c-4043-bea0-5f9297724c2d is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 15:10:37.228: INFO: Pod "pod-secrets-d6871b70-fd8c-4043-bea0-5f9297724c2d": Phase="Running", Reason="", readiness=true. Elapsed: 2.012071411s
    Jan 17 15:10:37.228: INFO: The phase of Pod pod-secrets-d6871b70-fd8c-4043-bea0-5f9297724c2d is Running (Ready = true)
    Jan 17 15:10:37.228: INFO: Pod "pod-secrets-d6871b70-fd8c-4043-bea0-5f9297724c2d" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-fb5a782a-095f-4b7c-88a3-5a509cd9feba 01/17/23 15:10:37.252
    STEP: Updating secret s-test-opt-upd-e4f0acd7-7ac0-485b-b441-b296756d41ef 01/17/23 15:10:37.257
    STEP: Creating secret with name s-test-opt-create-ca557924-cb30-4f3b-a357-944e347cb299 01/17/23 15:10:37.261
    STEP: waiting to observe update in volume 01/17/23 15:10:37.265
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan 17 15:10:39.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-358" for this suite. 01/17/23 15:10:39.295
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:10:39.303
Jan 17 15:10:39.303: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename configmap 01/17/23 15:10:39.303
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:10:39.34
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:10:39.343
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88
STEP: Creating configMap with name configmap-test-volume-map-f39f8ae9-f9e7-4696-bf73-0a5844ef7f82 01/17/23 15:10:39.344
STEP: Creating a pod to test consume configMaps 01/17/23 15:10:39.355
Jan 17 15:10:39.387: INFO: Waiting up to 5m0s for pod "pod-configmaps-94817314-5eef-445a-9546-146228e9d8ba" in namespace "configmap-113" to be "Succeeded or Failed"
Jan 17 15:10:39.392: INFO: Pod "pod-configmaps-94817314-5eef-445a-9546-146228e9d8ba": Phase="Pending", Reason="", readiness=false. Elapsed: 5.346502ms
Jan 17 15:10:41.397: INFO: Pod "pod-configmaps-94817314-5eef-445a-9546-146228e9d8ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010184837s
Jan 17 15:10:43.397: INFO: Pod "pod-configmaps-94817314-5eef-445a-9546-146228e9d8ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009863981s
STEP: Saw pod success 01/17/23 15:10:43.397
Jan 17 15:10:43.397: INFO: Pod "pod-configmaps-94817314-5eef-445a-9546-146228e9d8ba" satisfied condition "Succeeded or Failed"
Jan 17 15:10:43.399: INFO: Trying to get logs from node ip-10-0-165-14.ec2.internal pod pod-configmaps-94817314-5eef-445a-9546-146228e9d8ba container agnhost-container: <nil>
STEP: delete the pod 01/17/23 15:10:43.41
Jan 17 15:10:43.420: INFO: Waiting for pod pod-configmaps-94817314-5eef-445a-9546-146228e9d8ba to disappear
Jan 17 15:10:43.422: INFO: Pod pod-configmaps-94817314-5eef-445a-9546-146228e9d8ba no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 17 15:10:43.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-113" for this suite. 01/17/23 15:10:43.426
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":86,"skipped":1555,"failed":0}
------------------------------
• [4.128 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:10:39.303
    Jan 17 15:10:39.303: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename configmap 01/17/23 15:10:39.303
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:10:39.34
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:10:39.343
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:88
    STEP: Creating configMap with name configmap-test-volume-map-f39f8ae9-f9e7-4696-bf73-0a5844ef7f82 01/17/23 15:10:39.344
    STEP: Creating a pod to test consume configMaps 01/17/23 15:10:39.355
    Jan 17 15:10:39.387: INFO: Waiting up to 5m0s for pod "pod-configmaps-94817314-5eef-445a-9546-146228e9d8ba" in namespace "configmap-113" to be "Succeeded or Failed"
    Jan 17 15:10:39.392: INFO: Pod "pod-configmaps-94817314-5eef-445a-9546-146228e9d8ba": Phase="Pending", Reason="", readiness=false. Elapsed: 5.346502ms
    Jan 17 15:10:41.397: INFO: Pod "pod-configmaps-94817314-5eef-445a-9546-146228e9d8ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010184837s
    Jan 17 15:10:43.397: INFO: Pod "pod-configmaps-94817314-5eef-445a-9546-146228e9d8ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009863981s
    STEP: Saw pod success 01/17/23 15:10:43.397
    Jan 17 15:10:43.397: INFO: Pod "pod-configmaps-94817314-5eef-445a-9546-146228e9d8ba" satisfied condition "Succeeded or Failed"
    Jan 17 15:10:43.399: INFO: Trying to get logs from node ip-10-0-165-14.ec2.internal pod pod-configmaps-94817314-5eef-445a-9546-146228e9d8ba container agnhost-container: <nil>
    STEP: delete the pod 01/17/23 15:10:43.41
    Jan 17 15:10:43.420: INFO: Waiting for pod pod-configmaps-94817314-5eef-445a-9546-146228e9d8ba to disappear
    Jan 17 15:10:43.422: INFO: Pod pod-configmaps-94817314-5eef-445a-9546-146228e9d8ba no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 17 15:10:43.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-113" for this suite. 01/17/23 15:10:43.426
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:10:43.431
Jan 17 15:10:43.431: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename services 01/17/23 15:10:43.432
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:10:43.458
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:10:43.461
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268
STEP: creating service nodeport-test with type=NodePort in namespace services-9426 01/17/23 15:10:43.462
STEP: creating replication controller nodeport-test in namespace services-9426 01/17/23 15:10:43.493
I0117 15:10:43.502762      22 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-9426, replica count: 2
I0117 15:10:46.553946      22 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 17 15:10:46.553: INFO: Creating new exec pod
Jan 17 15:10:46.567: INFO: Waiting up to 5m0s for pod "execpod77926" in namespace "services-9426" to be "running"
Jan 17 15:10:46.569: INFO: Pod "execpod77926": Phase="Pending", Reason="", readiness=false. Elapsed: 2.472166ms
Jan 17 15:10:48.573: INFO: Pod "execpod77926": Phase="Running", Reason="", readiness=true. Elapsed: 2.006522582s
Jan 17 15:10:48.573: INFO: Pod "execpod77926" satisfied condition "running"
Jan 17 15:10:49.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-9426 exec execpod77926 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 17 15:10:49.707: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 17 15:10:49.707: INFO: stdout: ""
Jan 17 15:10:50.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-9426 exec execpod77926 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 17 15:10:50.820: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 17 15:10:50.820: INFO: stdout: "nodeport-test-vmwp5"
Jan 17 15:10:50.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-9426 exec execpod77926 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.195.249 80'
Jan 17 15:10:50.950: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.195.249 80\nConnection to 172.30.195.249 80 port [tcp/http] succeeded!\n"
Jan 17 15:10:50.950: INFO: stdout: ""
Jan 17 15:10:51.950: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-9426 exec execpod77926 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.195.249 80'
Jan 17 15:10:52.060: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.195.249 80\nConnection to 172.30.195.249 80 port [tcp/http] succeeded!\n"
Jan 17 15:10:52.060: INFO: stdout: ""
Jan 17 15:10:52.950: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-9426 exec execpod77926 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.195.249 80'
Jan 17 15:10:53.044: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.195.249 80\nConnection to 172.30.195.249 80 port [tcp/http] succeeded!\n"
Jan 17 15:10:53.044: INFO: stdout: "nodeport-test-g4hv9"
Jan 17 15:10:53.044: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-9426 exec execpod77926 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.139.213 32178'
Jan 17 15:10:53.170: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.139.213 32178\nConnection to 10.0.139.213 32178 port [tcp/*] succeeded!\n"
Jan 17 15:10:53.170: INFO: stdout: "nodeport-test-vmwp5"
Jan 17 15:10:53.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-9426 exec execpod77926 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.151.22 32178'
Jan 17 15:10:53.282: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.151.22 32178\nConnection to 10.0.151.22 32178 port [tcp/*] succeeded!\n"
Jan 17 15:10:53.282: INFO: stdout: ""
Jan 17 15:10:54.283: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-9426 exec execpod77926 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.151.22 32178'
Jan 17 15:10:54.403: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.151.22 32178\nConnection to 10.0.151.22 32178 port [tcp/*] succeeded!\n"
Jan 17 15:10:54.403: INFO: stdout: "nodeport-test-vmwp5"
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 17 15:10:54.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9426" for this suite. 01/17/23 15:10:54.408
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","completed":87,"skipped":1557,"failed":0}
------------------------------
• [SLOW TEST] [10.986 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:10:43.431
    Jan 17 15:10:43.431: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename services 01/17/23 15:10:43.432
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:10:43.458
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:10:43.461
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1268
    STEP: creating service nodeport-test with type=NodePort in namespace services-9426 01/17/23 15:10:43.462
    STEP: creating replication controller nodeport-test in namespace services-9426 01/17/23 15:10:43.493
    I0117 15:10:43.502762      22 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-9426, replica count: 2
    I0117 15:10:46.553946      22 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 17 15:10:46.553: INFO: Creating new exec pod
    Jan 17 15:10:46.567: INFO: Waiting up to 5m0s for pod "execpod77926" in namespace "services-9426" to be "running"
    Jan 17 15:10:46.569: INFO: Pod "execpod77926": Phase="Pending", Reason="", readiness=false. Elapsed: 2.472166ms
    Jan 17 15:10:48.573: INFO: Pod "execpod77926": Phase="Running", Reason="", readiness=true. Elapsed: 2.006522582s
    Jan 17 15:10:48.573: INFO: Pod "execpod77926" satisfied condition "running"
    Jan 17 15:10:49.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-9426 exec execpod77926 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 17 15:10:49.707: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 17 15:10:49.707: INFO: stdout: ""
    Jan 17 15:10:50.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-9426 exec execpod77926 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 17 15:10:50.820: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 17 15:10:50.820: INFO: stdout: "nodeport-test-vmwp5"
    Jan 17 15:10:50.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-9426 exec execpod77926 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.195.249 80'
    Jan 17 15:10:50.950: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.195.249 80\nConnection to 172.30.195.249 80 port [tcp/http] succeeded!\n"
    Jan 17 15:10:50.950: INFO: stdout: ""
    Jan 17 15:10:51.950: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-9426 exec execpod77926 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.195.249 80'
    Jan 17 15:10:52.060: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.195.249 80\nConnection to 172.30.195.249 80 port [tcp/http] succeeded!\n"
    Jan 17 15:10:52.060: INFO: stdout: ""
    Jan 17 15:10:52.950: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-9426 exec execpod77926 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.195.249 80'
    Jan 17 15:10:53.044: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.195.249 80\nConnection to 172.30.195.249 80 port [tcp/http] succeeded!\n"
    Jan 17 15:10:53.044: INFO: stdout: "nodeport-test-g4hv9"
    Jan 17 15:10:53.044: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-9426 exec execpod77926 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.139.213 32178'
    Jan 17 15:10:53.170: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.139.213 32178\nConnection to 10.0.139.213 32178 port [tcp/*] succeeded!\n"
    Jan 17 15:10:53.170: INFO: stdout: "nodeport-test-vmwp5"
    Jan 17 15:10:53.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-9426 exec execpod77926 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.151.22 32178'
    Jan 17 15:10:53.282: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.151.22 32178\nConnection to 10.0.151.22 32178 port [tcp/*] succeeded!\n"
    Jan 17 15:10:53.282: INFO: stdout: ""
    Jan 17 15:10:54.283: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-9426 exec execpod77926 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.151.22 32178'
    Jan 17 15:10:54.403: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.151.22 32178\nConnection to 10.0.151.22 32178 port [tcp/*] succeeded!\n"
    Jan 17 15:10:54.403: INFO: stdout: "nodeport-test-vmwp5"
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 17 15:10:54.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-9426" for this suite. 01/17/23 15:10:54.408
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:51
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:10:54.418
Jan 17 15:10:54.418: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename container-runtime 01/17/23 15:10:54.419
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:10:54.451
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:10:54.453
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:51
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 01/17/23 15:10:54.483
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 01/17/23 15:11:10.556
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 01/17/23 15:11:10.559
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 01/17/23 15:11:10.565
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 01/17/23 15:11:10.565
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 01/17/23 15:11:10.595
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 01/17/23 15:11:12.613
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 01/17/23 15:11:14.624
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 01/17/23 15:11:14.63
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 01/17/23 15:11:14.63
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 01/17/23 15:11:14.657
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 01/17/23 15:11:15.664
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 01/17/23 15:11:17.676
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 01/17/23 15:11:17.682
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 01/17/23 15:11:17.682
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Jan 17 15:11:17.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1781" for this suite. 01/17/23 15:11:17.711
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","completed":88,"skipped":1585,"failed":0}
------------------------------
• [SLOW TEST] [23.299 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    when starting a container that exits
    test/e2e/common/node/runtime.go:44
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:10:54.418
    Jan 17 15:10:54.418: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename container-runtime 01/17/23 15:10:54.419
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:10:54.451
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:10:54.453
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:51
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 01/17/23 15:10:54.483
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 01/17/23 15:11:10.556
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 01/17/23 15:11:10.559
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 01/17/23 15:11:10.565
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 01/17/23 15:11:10.565
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 01/17/23 15:11:10.595
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 01/17/23 15:11:12.613
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 01/17/23 15:11:14.624
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 01/17/23 15:11:14.63
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 01/17/23 15:11:14.63
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 01/17/23 15:11:14.657
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 01/17/23 15:11:15.664
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 01/17/23 15:11:17.676
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 01/17/23 15:11:17.682
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 01/17/23 15:11:17.682
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Jan 17 15:11:17.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-1781" for this suite. 01/17/23 15:11:17.711
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:11:17.718
Jan 17 15:11:17.718: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename var-expansion 01/17/23 15:11:17.719
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:11:17.738
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:11:17.742
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91
STEP: Creating a pod to test substitution in container's args 01/17/23 15:11:17.744
Jan 17 15:11:17.772: INFO: Waiting up to 5m0s for pod "var-expansion-c575b928-a4a5-422f-8e31-0696258e91a8" in namespace "var-expansion-156" to be "Succeeded or Failed"
Jan 17 15:11:17.775: INFO: Pod "var-expansion-c575b928-a4a5-422f-8e31-0696258e91a8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.368077ms
Jan 17 15:11:19.782: INFO: Pod "var-expansion-c575b928-a4a5-422f-8e31-0696258e91a8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009937001s
Jan 17 15:11:21.780: INFO: Pod "var-expansion-c575b928-a4a5-422f-8e31-0696258e91a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00775088s
STEP: Saw pod success 01/17/23 15:11:21.78
Jan 17 15:11:21.780: INFO: Pod "var-expansion-c575b928-a4a5-422f-8e31-0696258e91a8" satisfied condition "Succeeded or Failed"
Jan 17 15:11:21.784: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod var-expansion-c575b928-a4a5-422f-8e31-0696258e91a8 container dapi-container: <nil>
STEP: delete the pod 01/17/23 15:11:21.79
Jan 17 15:11:21.804: INFO: Waiting for pod var-expansion-c575b928-a4a5-422f-8e31-0696258e91a8 to disappear
Jan 17 15:11:21.807: INFO: Pod var-expansion-c575b928-a4a5-422f-8e31-0696258e91a8 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan 17 15:11:21.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-156" for this suite. 01/17/23 15:11:21.812
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","completed":89,"skipped":1603,"failed":0}
------------------------------
• [4.101 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:11:17.718
    Jan 17 15:11:17.718: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename var-expansion 01/17/23 15:11:17.719
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:11:17.738
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:11:17.742
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:91
    STEP: Creating a pod to test substitution in container's args 01/17/23 15:11:17.744
    Jan 17 15:11:17.772: INFO: Waiting up to 5m0s for pod "var-expansion-c575b928-a4a5-422f-8e31-0696258e91a8" in namespace "var-expansion-156" to be "Succeeded or Failed"
    Jan 17 15:11:17.775: INFO: Pod "var-expansion-c575b928-a4a5-422f-8e31-0696258e91a8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.368077ms
    Jan 17 15:11:19.782: INFO: Pod "var-expansion-c575b928-a4a5-422f-8e31-0696258e91a8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009937001s
    Jan 17 15:11:21.780: INFO: Pod "var-expansion-c575b928-a4a5-422f-8e31-0696258e91a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00775088s
    STEP: Saw pod success 01/17/23 15:11:21.78
    Jan 17 15:11:21.780: INFO: Pod "var-expansion-c575b928-a4a5-422f-8e31-0696258e91a8" satisfied condition "Succeeded or Failed"
    Jan 17 15:11:21.784: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod var-expansion-c575b928-a4a5-422f-8e31-0696258e91a8 container dapi-container: <nil>
    STEP: delete the pod 01/17/23 15:11:21.79
    Jan 17 15:11:21.804: INFO: Waiting for pod var-expansion-c575b928-a4a5-422f-8e31-0696258e91a8 to disappear
    Jan 17 15:11:21.807: INFO: Pod var-expansion-c575b928-a4a5-422f-8e31-0696258e91a8 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan 17 15:11:21.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-156" for this suite. 01/17/23 15:11:21.812
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:11:21.82
Jan 17 15:11:21.820: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename hostport 01/17/23 15:11:21.821
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:11:21.84
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:11:21.843
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 01/17/23 15:11:21.854
Jan 17 15:11:21.876: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-1958" to be "running and ready"
Jan 17 15:11:21.881: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.917187ms
Jan 17 15:11:21.881: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 15:11:23.885: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.008645271s
Jan 17 15:11:23.885: INFO: The phase of Pod pod1 is Running (Ready = true)
Jan 17 15:11:23.885: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.0.165.14 on the node which pod1 resides and expect scheduled 01/17/23 15:11:23.885
Jan 17 15:11:23.899: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-1958" to be "running and ready"
Jan 17 15:11:23.902: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.863525ms
Jan 17 15:11:23.902: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 15:11:25.906: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.006910496s
Jan 17 15:11:25.906: INFO: The phase of Pod pod2 is Running (Ready = true)
Jan 17 15:11:25.906: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.0.165.14 but use UDP protocol on the node which pod2 resides 01/17/23 15:11:25.906
Jan 17 15:11:25.920: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-1958" to be "running and ready"
Jan 17 15:11:25.923: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.133491ms
Jan 17 15:11:25.923: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 15:11:27.927: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.007522617s
Jan 17 15:11:27.927: INFO: The phase of Pod pod3 is Running (Ready = true)
Jan 17 15:11:27.927: INFO: Pod "pod3" satisfied condition "running and ready"
Jan 17 15:11:27.945: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-1958" to be "running and ready"
Jan 17 15:11:27.966: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 20.892317ms
Jan 17 15:11:27.966: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Jan 17 15:11:29.971: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.025556659s
Jan 17 15:11:29.971: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
Jan 17 15:11:29.971: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 01/17/23 15:11:29.974
Jan 17 15:11:29.974: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.0.165.14 http://127.0.0.1:54323/hostname] Namespace:hostport-1958 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 15:11:29.974: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
Jan 17 15:11:29.974: INFO: ExecWithOptions: Clientset creation
Jan 17 15:11:29.974: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/hostport-1958/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.0.165.14+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.165.14, port: 54323 01/17/23 15:11:30.057
Jan 17 15:11:30.057: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.0.165.14:54323/hostname] Namespace:hostport-1958 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 15:11:30.057: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
Jan 17 15:11:30.058: INFO: ExecWithOptions: Clientset creation
Jan 17 15:11:30.058: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/hostport-1958/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.0.165.14%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.165.14, port: 54323 UDP 01/17/23 15:11:30.152
Jan 17 15:11:30.152: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.0.165.14 54323] Namespace:hostport-1958 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 15:11:30.152: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
Jan 17 15:11:30.152: INFO: ExecWithOptions: Clientset creation
Jan 17 15:11:30.152: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/hostport-1958/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.0.165.14+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/framework.go:187
Jan 17 15:11:35.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-1958" for this suite. 01/17/23 15:11:35.254
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","completed":90,"skipped":1651,"failed":0}
------------------------------
• [SLOW TEST] [13.442 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:11:21.82
    Jan 17 15:11:21.820: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename hostport 01/17/23 15:11:21.821
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:11:21.84
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:11:21.843
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 01/17/23 15:11:21.854
    Jan 17 15:11:21.876: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-1958" to be "running and ready"
    Jan 17 15:11:21.881: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.917187ms
    Jan 17 15:11:21.881: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 15:11:23.885: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.008645271s
    Jan 17 15:11:23.885: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jan 17 15:11:23.885: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.0.165.14 on the node which pod1 resides and expect scheduled 01/17/23 15:11:23.885
    Jan 17 15:11:23.899: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-1958" to be "running and ready"
    Jan 17 15:11:23.902: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.863525ms
    Jan 17 15:11:23.902: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 15:11:25.906: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.006910496s
    Jan 17 15:11:25.906: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jan 17 15:11:25.906: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.0.165.14 but use UDP protocol on the node which pod2 resides 01/17/23 15:11:25.906
    Jan 17 15:11:25.920: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-1958" to be "running and ready"
    Jan 17 15:11:25.923: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.133491ms
    Jan 17 15:11:25.923: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 15:11:27.927: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.007522617s
    Jan 17 15:11:27.927: INFO: The phase of Pod pod3 is Running (Ready = true)
    Jan 17 15:11:27.927: INFO: Pod "pod3" satisfied condition "running and ready"
    Jan 17 15:11:27.945: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-1958" to be "running and ready"
    Jan 17 15:11:27.966: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 20.892317ms
    Jan 17 15:11:27.966: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 15:11:29.971: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.025556659s
    Jan 17 15:11:29.971: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    Jan 17 15:11:29.971: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 01/17/23 15:11:29.974
    Jan 17 15:11:29.974: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.0.165.14 http://127.0.0.1:54323/hostname] Namespace:hostport-1958 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 17 15:11:29.974: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    Jan 17 15:11:29.974: INFO: ExecWithOptions: Clientset creation
    Jan 17 15:11:29.974: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/hostport-1958/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.0.165.14+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.165.14, port: 54323 01/17/23 15:11:30.057
    Jan 17 15:11:30.057: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.0.165.14:54323/hostname] Namespace:hostport-1958 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 17 15:11:30.057: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    Jan 17 15:11:30.058: INFO: ExecWithOptions: Clientset creation
    Jan 17 15:11:30.058: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/hostport-1958/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.0.165.14%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.165.14, port: 54323 UDP 01/17/23 15:11:30.152
    Jan 17 15:11:30.152: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.0.165.14 54323] Namespace:hostport-1958 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 17 15:11:30.152: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    Jan 17 15:11:30.152: INFO: ExecWithOptions: Clientset creation
    Jan 17 15:11:30.152: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/hostport-1958/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.0.165.14+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/framework.go:187
    Jan 17 15:11:35.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "hostport-1958" for this suite. 01/17/23 15:11:35.254
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:11:35.263
Jan 17 15:11:35.263: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename var-expansion 01/17/23 15:11:35.263
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:11:35.298
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:11:35.301
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111
STEP: Creating a pod to test substitution in volume subpath 01/17/23 15:11:35.304
Jan 17 15:11:35.329: INFO: Waiting up to 5m0s for pod "var-expansion-13dca04b-8b86-4724-9958-e03b5523c3c1" in namespace "var-expansion-2384" to be "Succeeded or Failed"
Jan 17 15:11:35.333: INFO: Pod "var-expansion-13dca04b-8b86-4724-9958-e03b5523c3c1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.853905ms
Jan 17 15:11:37.337: INFO: Pod "var-expansion-13dca04b-8b86-4724-9958-e03b5523c3c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00763856s
Jan 17 15:11:39.352: INFO: Pod "var-expansion-13dca04b-8b86-4724-9958-e03b5523c3c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022890639s
STEP: Saw pod success 01/17/23 15:11:39.352
Jan 17 15:11:39.352: INFO: Pod "var-expansion-13dca04b-8b86-4724-9958-e03b5523c3c1" satisfied condition "Succeeded or Failed"
Jan 17 15:11:39.363: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod var-expansion-13dca04b-8b86-4724-9958-e03b5523c3c1 container dapi-container: <nil>
STEP: delete the pod 01/17/23 15:11:39.374
Jan 17 15:11:39.402: INFO: Waiting for pod var-expansion-13dca04b-8b86-4724-9958-e03b5523c3c1 to disappear
Jan 17 15:11:39.408: INFO: Pod var-expansion-13dca04b-8b86-4724-9958-e03b5523c3c1 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan 17 15:11:39.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2384" for this suite. 01/17/23 15:11:39.423
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","completed":91,"skipped":1651,"failed":0}
------------------------------
• [4.170 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:11:35.263
    Jan 17 15:11:35.263: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename var-expansion 01/17/23 15:11:35.263
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:11:35.298
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:11:35.301
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:111
    STEP: Creating a pod to test substitution in volume subpath 01/17/23 15:11:35.304
    Jan 17 15:11:35.329: INFO: Waiting up to 5m0s for pod "var-expansion-13dca04b-8b86-4724-9958-e03b5523c3c1" in namespace "var-expansion-2384" to be "Succeeded or Failed"
    Jan 17 15:11:35.333: INFO: Pod "var-expansion-13dca04b-8b86-4724-9958-e03b5523c3c1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.853905ms
    Jan 17 15:11:37.337: INFO: Pod "var-expansion-13dca04b-8b86-4724-9958-e03b5523c3c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00763856s
    Jan 17 15:11:39.352: INFO: Pod "var-expansion-13dca04b-8b86-4724-9958-e03b5523c3c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022890639s
    STEP: Saw pod success 01/17/23 15:11:39.352
    Jan 17 15:11:39.352: INFO: Pod "var-expansion-13dca04b-8b86-4724-9958-e03b5523c3c1" satisfied condition "Succeeded or Failed"
    Jan 17 15:11:39.363: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod var-expansion-13dca04b-8b86-4724-9958-e03b5523c3c1 container dapi-container: <nil>
    STEP: delete the pod 01/17/23 15:11:39.374
    Jan 17 15:11:39.402: INFO: Waiting for pod var-expansion-13dca04b-8b86-4724-9958-e03b5523c3c1 to disappear
    Jan 17 15:11:39.408: INFO: Pod var-expansion-13dca04b-8b86-4724-9958-e03b5523c3c1 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan 17 15:11:39.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-2384" for this suite. 01/17/23 15:11:39.423
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:11:39.433
Jan 17 15:11:39.433: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename subpath 01/17/23 15:11:39.434
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:11:39.483
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:11:39.488
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/17/23 15:11:39.492
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-rzqf 01/17/23 15:11:39.532
STEP: Creating a pod to test atomic-volume-subpath 01/17/23 15:11:39.532
Jan 17 15:11:39.582: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-rzqf" in namespace "subpath-951" to be "Succeeded or Failed"
Jan 17 15:11:39.607: INFO: Pod "pod-subpath-test-downwardapi-rzqf": Phase="Pending", Reason="", readiness=false. Elapsed: 24.840859ms
Jan 17 15:11:41.612: INFO: Pod "pod-subpath-test-downwardapi-rzqf": Phase="Running", Reason="", readiness=true. Elapsed: 2.029314003s
Jan 17 15:11:43.612: INFO: Pod "pod-subpath-test-downwardapi-rzqf": Phase="Running", Reason="", readiness=true. Elapsed: 4.029708742s
Jan 17 15:11:45.611: INFO: Pod "pod-subpath-test-downwardapi-rzqf": Phase="Running", Reason="", readiness=true. Elapsed: 6.02905403s
Jan 17 15:11:47.612: INFO: Pod "pod-subpath-test-downwardapi-rzqf": Phase="Running", Reason="", readiness=true. Elapsed: 8.029493016s
Jan 17 15:11:49.612: INFO: Pod "pod-subpath-test-downwardapi-rzqf": Phase="Running", Reason="", readiness=true. Elapsed: 10.029931451s
Jan 17 15:11:51.613: INFO: Pod "pod-subpath-test-downwardapi-rzqf": Phase="Running", Reason="", readiness=true. Elapsed: 12.031097426s
Jan 17 15:11:53.611: INFO: Pod "pod-subpath-test-downwardapi-rzqf": Phase="Running", Reason="", readiness=true. Elapsed: 14.028715494s
Jan 17 15:11:55.612: INFO: Pod "pod-subpath-test-downwardapi-rzqf": Phase="Running", Reason="", readiness=true. Elapsed: 16.029727978s
Jan 17 15:11:57.612: INFO: Pod "pod-subpath-test-downwardapi-rzqf": Phase="Running", Reason="", readiness=true. Elapsed: 18.030074578s
Jan 17 15:11:59.612: INFO: Pod "pod-subpath-test-downwardapi-rzqf": Phase="Running", Reason="", readiness=true. Elapsed: 20.029715804s
Jan 17 15:12:01.612: INFO: Pod "pod-subpath-test-downwardapi-rzqf": Phase="Running", Reason="", readiness=false. Elapsed: 22.029919251s
Jan 17 15:12:03.610: INFO: Pod "pod-subpath-test-downwardapi-rzqf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.028214374s
STEP: Saw pod success 01/17/23 15:12:03.61
Jan 17 15:12:03.611: INFO: Pod "pod-subpath-test-downwardapi-rzqf" satisfied condition "Succeeded or Failed"
Jan 17 15:12:03.613: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-subpath-test-downwardapi-rzqf container test-container-subpath-downwardapi-rzqf: <nil>
STEP: delete the pod 01/17/23 15:12:03.62
Jan 17 15:12:03.632: INFO: Waiting for pod pod-subpath-test-downwardapi-rzqf to disappear
Jan 17 15:12:03.635: INFO: Pod pod-subpath-test-downwardapi-rzqf no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-rzqf 01/17/23 15:12:03.635
Jan 17 15:12:03.635: INFO: Deleting pod "pod-subpath-test-downwardapi-rzqf" in namespace "subpath-951"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Jan 17 15:12:03.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-951" for this suite. 01/17/23 15:12:03.642
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]","completed":92,"skipped":1657,"failed":0}
------------------------------
• [SLOW TEST] [24.215 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:11:39.433
    Jan 17 15:11:39.433: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename subpath 01/17/23 15:11:39.434
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:11:39.483
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:11:39.488
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/17/23 15:11:39.492
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-rzqf 01/17/23 15:11:39.532
    STEP: Creating a pod to test atomic-volume-subpath 01/17/23 15:11:39.532
    Jan 17 15:11:39.582: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-rzqf" in namespace "subpath-951" to be "Succeeded or Failed"
    Jan 17 15:11:39.607: INFO: Pod "pod-subpath-test-downwardapi-rzqf": Phase="Pending", Reason="", readiness=false. Elapsed: 24.840859ms
    Jan 17 15:11:41.612: INFO: Pod "pod-subpath-test-downwardapi-rzqf": Phase="Running", Reason="", readiness=true. Elapsed: 2.029314003s
    Jan 17 15:11:43.612: INFO: Pod "pod-subpath-test-downwardapi-rzqf": Phase="Running", Reason="", readiness=true. Elapsed: 4.029708742s
    Jan 17 15:11:45.611: INFO: Pod "pod-subpath-test-downwardapi-rzqf": Phase="Running", Reason="", readiness=true. Elapsed: 6.02905403s
    Jan 17 15:11:47.612: INFO: Pod "pod-subpath-test-downwardapi-rzqf": Phase="Running", Reason="", readiness=true. Elapsed: 8.029493016s
    Jan 17 15:11:49.612: INFO: Pod "pod-subpath-test-downwardapi-rzqf": Phase="Running", Reason="", readiness=true. Elapsed: 10.029931451s
    Jan 17 15:11:51.613: INFO: Pod "pod-subpath-test-downwardapi-rzqf": Phase="Running", Reason="", readiness=true. Elapsed: 12.031097426s
    Jan 17 15:11:53.611: INFO: Pod "pod-subpath-test-downwardapi-rzqf": Phase="Running", Reason="", readiness=true. Elapsed: 14.028715494s
    Jan 17 15:11:55.612: INFO: Pod "pod-subpath-test-downwardapi-rzqf": Phase="Running", Reason="", readiness=true. Elapsed: 16.029727978s
    Jan 17 15:11:57.612: INFO: Pod "pod-subpath-test-downwardapi-rzqf": Phase="Running", Reason="", readiness=true. Elapsed: 18.030074578s
    Jan 17 15:11:59.612: INFO: Pod "pod-subpath-test-downwardapi-rzqf": Phase="Running", Reason="", readiness=true. Elapsed: 20.029715804s
    Jan 17 15:12:01.612: INFO: Pod "pod-subpath-test-downwardapi-rzqf": Phase="Running", Reason="", readiness=false. Elapsed: 22.029919251s
    Jan 17 15:12:03.610: INFO: Pod "pod-subpath-test-downwardapi-rzqf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.028214374s
    STEP: Saw pod success 01/17/23 15:12:03.61
    Jan 17 15:12:03.611: INFO: Pod "pod-subpath-test-downwardapi-rzqf" satisfied condition "Succeeded or Failed"
    Jan 17 15:12:03.613: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-subpath-test-downwardapi-rzqf container test-container-subpath-downwardapi-rzqf: <nil>
    STEP: delete the pod 01/17/23 15:12:03.62
    Jan 17 15:12:03.632: INFO: Waiting for pod pod-subpath-test-downwardapi-rzqf to disappear
    Jan 17 15:12:03.635: INFO: Pod pod-subpath-test-downwardapi-rzqf no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-rzqf 01/17/23 15:12:03.635
    Jan 17 15:12:03.635: INFO: Deleting pod "pod-subpath-test-downwardapi-rzqf" in namespace "subpath-951"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Jan 17 15:12:03.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-951" for this suite. 01/17/23 15:12:03.642
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:12:03.648
Jan 17 15:12:03.648: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename services 01/17/23 15:12:03.649
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:12:03.68
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:12:03.682
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641
STEP: creating a collection of services 01/17/23 15:12:03.684
Jan 17 15:12:03.684: INFO: Creating e2e-svc-a-gdlmr
Jan 17 15:12:03.699: INFO: Creating e2e-svc-b-hcvws
Jan 17 15:12:03.720: INFO: Creating e2e-svc-c-blv8q
STEP: deleting service collection 01/17/23 15:12:03.751
Jan 17 15:12:03.820: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 17 15:12:03.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4214" for this suite. 01/17/23 15:12:03.825
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should delete a collection of services [Conformance]","completed":93,"skipped":1666,"failed":0}
------------------------------
• [0.186 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:12:03.648
    Jan 17 15:12:03.648: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename services 01/17/23 15:12:03.649
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:12:03.68
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:12:03.682
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3641
    STEP: creating a collection of services 01/17/23 15:12:03.684
    Jan 17 15:12:03.684: INFO: Creating e2e-svc-a-gdlmr
    Jan 17 15:12:03.699: INFO: Creating e2e-svc-b-hcvws
    Jan 17 15:12:03.720: INFO: Creating e2e-svc-c-blv8q
    STEP: deleting service collection 01/17/23 15:12:03.751
    Jan 17 15:12:03.820: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 17 15:12:03.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-4214" for this suite. 01/17/23 15:12:03.825
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:12:03.834
Jan 17 15:12:03.835: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename pods 01/17/23 15:12:03.836
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:12:03.867
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:12:03.87
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343
STEP: creating the pod 01/17/23 15:12:03.873
STEP: submitting the pod to kubernetes 01/17/23 15:12:03.873
Jan 17 15:12:03.919: INFO: Waiting up to 5m0s for pod "pod-update-6d887bdf-9347-4eec-88b8-5ec27affb3ba" in namespace "pods-8473" to be "running and ready"
Jan 17 15:12:03.925: INFO: Pod "pod-update-6d887bdf-9347-4eec-88b8-5ec27affb3ba": Phase="Pending", Reason="", readiness=false. Elapsed: 5.963883ms
Jan 17 15:12:03.925: INFO: The phase of Pod pod-update-6d887bdf-9347-4eec-88b8-5ec27affb3ba is Pending, waiting for it to be Running (with Ready = true)
Jan 17 15:12:05.928: INFO: Pod "pod-update-6d887bdf-9347-4eec-88b8-5ec27affb3ba": Phase="Running", Reason="", readiness=true. Elapsed: 2.009456589s
Jan 17 15:12:05.928: INFO: The phase of Pod pod-update-6d887bdf-9347-4eec-88b8-5ec27affb3ba is Running (Ready = true)
Jan 17 15:12:05.928: INFO: Pod "pod-update-6d887bdf-9347-4eec-88b8-5ec27affb3ba" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 01/17/23 15:12:05.931
STEP: updating the pod 01/17/23 15:12:05.934
Jan 17 15:12:06.451: INFO: Successfully updated pod "pod-update-6d887bdf-9347-4eec-88b8-5ec27affb3ba"
Jan 17 15:12:06.451: INFO: Waiting up to 5m0s for pod "pod-update-6d887bdf-9347-4eec-88b8-5ec27affb3ba" in namespace "pods-8473" to be "running"
Jan 17 15:12:06.454: INFO: Pod "pod-update-6d887bdf-9347-4eec-88b8-5ec27affb3ba": Phase="Running", Reason="", readiness=true. Elapsed: 3.017421ms
Jan 17 15:12:06.454: INFO: Pod "pod-update-6d887bdf-9347-4eec-88b8-5ec27affb3ba" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 01/17/23 15:12:06.454
Jan 17 15:12:06.457: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 17 15:12:06.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8473" for this suite. 01/17/23 15:12:06.461
{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","completed":94,"skipped":1667,"failed":0}
------------------------------
• [2.633 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:12:03.834
    Jan 17 15:12:03.835: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename pods 01/17/23 15:12:03.836
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:12:03.867
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:12:03.87
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:343
    STEP: creating the pod 01/17/23 15:12:03.873
    STEP: submitting the pod to kubernetes 01/17/23 15:12:03.873
    Jan 17 15:12:03.919: INFO: Waiting up to 5m0s for pod "pod-update-6d887bdf-9347-4eec-88b8-5ec27affb3ba" in namespace "pods-8473" to be "running and ready"
    Jan 17 15:12:03.925: INFO: Pod "pod-update-6d887bdf-9347-4eec-88b8-5ec27affb3ba": Phase="Pending", Reason="", readiness=false. Elapsed: 5.963883ms
    Jan 17 15:12:03.925: INFO: The phase of Pod pod-update-6d887bdf-9347-4eec-88b8-5ec27affb3ba is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 15:12:05.928: INFO: Pod "pod-update-6d887bdf-9347-4eec-88b8-5ec27affb3ba": Phase="Running", Reason="", readiness=true. Elapsed: 2.009456589s
    Jan 17 15:12:05.928: INFO: The phase of Pod pod-update-6d887bdf-9347-4eec-88b8-5ec27affb3ba is Running (Ready = true)
    Jan 17 15:12:05.928: INFO: Pod "pod-update-6d887bdf-9347-4eec-88b8-5ec27affb3ba" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 01/17/23 15:12:05.931
    STEP: updating the pod 01/17/23 15:12:05.934
    Jan 17 15:12:06.451: INFO: Successfully updated pod "pod-update-6d887bdf-9347-4eec-88b8-5ec27affb3ba"
    Jan 17 15:12:06.451: INFO: Waiting up to 5m0s for pod "pod-update-6d887bdf-9347-4eec-88b8-5ec27affb3ba" in namespace "pods-8473" to be "running"
    Jan 17 15:12:06.454: INFO: Pod "pod-update-6d887bdf-9347-4eec-88b8-5ec27affb3ba": Phase="Running", Reason="", readiness=true. Elapsed: 3.017421ms
    Jan 17 15:12:06.454: INFO: Pod "pod-update-6d887bdf-9347-4eec-88b8-5ec27affb3ba" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 01/17/23 15:12:06.454
    Jan 17 15:12:06.457: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 17 15:12:06.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-8473" for this suite. 01/17/23 15:12:06.461
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:12:06.469
Jan 17 15:12:06.469: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename events 01/17/23 15:12:06.469
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:12:06.496
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:12:06.499
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 01/17/23 15:12:06.501
Jan 17 15:12:06.508: INFO: created test-event-1
Jan 17 15:12:06.514: INFO: created test-event-2
Jan 17 15:12:06.518: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 01/17/23 15:12:06.518
STEP: delete collection of events 01/17/23 15:12:06.523
Jan 17 15:12:06.523: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 01/17/23 15:12:06.558
Jan 17 15:12:06.558: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
Jan 17 15:12:06.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-657" for this suite. 01/17/23 15:12:06.572
{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","completed":95,"skipped":1693,"failed":0}
------------------------------
• [0.110 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:12:06.469
    Jan 17 15:12:06.469: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename events 01/17/23 15:12:06.469
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:12:06.496
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:12:06.499
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 01/17/23 15:12:06.501
    Jan 17 15:12:06.508: INFO: created test-event-1
    Jan 17 15:12:06.514: INFO: created test-event-2
    Jan 17 15:12:06.518: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 01/17/23 15:12:06.518
    STEP: delete collection of events 01/17/23 15:12:06.523
    Jan 17 15:12:06.523: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 01/17/23 15:12:06.558
    Jan 17 15:12:06.558: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:187
    Jan 17 15:12:06.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-657" for this suite. 01/17/23 15:12:06.572
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:12:06.58
Jan 17 15:12:06.580: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename emptydir 01/17/23 15:12:06.58
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:12:06.606
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:12:06.608
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106
STEP: Creating a pod to test emptydir 0666 on tmpfs 01/17/23 15:12:06.61
Jan 17 15:12:06.676: INFO: Waiting up to 5m0s for pod "pod-a4ed1ad8-d0c3-4557-958b-2d2ce57833a9" in namespace "emptydir-9207" to be "Succeeded or Failed"
Jan 17 15:12:06.683: INFO: Pod "pod-a4ed1ad8-d0c3-4557-958b-2d2ce57833a9": Phase="Pending", Reason="", readiness=false. Elapsed: 7.457226ms
Jan 17 15:12:08.686: INFO: Pod "pod-a4ed1ad8-d0c3-4557-958b-2d2ce57833a9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010763625s
Jan 17 15:12:10.687: INFO: Pod "pod-a4ed1ad8-d0c3-4557-958b-2d2ce57833a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011158347s
STEP: Saw pod success 01/17/23 15:12:10.687
Jan 17 15:12:10.687: INFO: Pod "pod-a4ed1ad8-d0c3-4557-958b-2d2ce57833a9" satisfied condition "Succeeded or Failed"
Jan 17 15:12:10.690: INFO: Trying to get logs from node ip-10-0-165-14.ec2.internal pod pod-a4ed1ad8-d0c3-4557-958b-2d2ce57833a9 container test-container: <nil>
STEP: delete the pod 01/17/23 15:12:10.695
Jan 17 15:12:10.708: INFO: Waiting for pod pod-a4ed1ad8-d0c3-4557-958b-2d2ce57833a9 to disappear
Jan 17 15:12:10.710: INFO: Pod pod-a4ed1ad8-d0c3-4557-958b-2d2ce57833a9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 17 15:12:10.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9207" for this suite. 01/17/23 15:12:10.714
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":96,"skipped":1714,"failed":0}
------------------------------
• [4.141 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:12:06.58
    Jan 17 15:12:06.580: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename emptydir 01/17/23 15:12:06.58
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:12:06.606
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:12:06.608
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:106
    STEP: Creating a pod to test emptydir 0666 on tmpfs 01/17/23 15:12:06.61
    Jan 17 15:12:06.676: INFO: Waiting up to 5m0s for pod "pod-a4ed1ad8-d0c3-4557-958b-2d2ce57833a9" in namespace "emptydir-9207" to be "Succeeded or Failed"
    Jan 17 15:12:06.683: INFO: Pod "pod-a4ed1ad8-d0c3-4557-958b-2d2ce57833a9": Phase="Pending", Reason="", readiness=false. Elapsed: 7.457226ms
    Jan 17 15:12:08.686: INFO: Pod "pod-a4ed1ad8-d0c3-4557-958b-2d2ce57833a9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010763625s
    Jan 17 15:12:10.687: INFO: Pod "pod-a4ed1ad8-d0c3-4557-958b-2d2ce57833a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011158347s
    STEP: Saw pod success 01/17/23 15:12:10.687
    Jan 17 15:12:10.687: INFO: Pod "pod-a4ed1ad8-d0c3-4557-958b-2d2ce57833a9" satisfied condition "Succeeded or Failed"
    Jan 17 15:12:10.690: INFO: Trying to get logs from node ip-10-0-165-14.ec2.internal pod pod-a4ed1ad8-d0c3-4557-958b-2d2ce57833a9 container test-container: <nil>
    STEP: delete the pod 01/17/23 15:12:10.695
    Jan 17 15:12:10.708: INFO: Waiting for pod pod-a4ed1ad8-d0c3-4557-958b-2d2ce57833a9 to disappear
    Jan 17 15:12:10.710: INFO: Pod pod-a4ed1ad8-d0c3-4557-958b-2d2ce57833a9 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 17 15:12:10.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-9207" for this suite. 01/17/23 15:12:10.714
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:12:10.722
Jan 17 15:12:10.722: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename events 01/17/23 15:12:10.723
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:12:10.74
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:12:10.742
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 01/17/23 15:12:10.744
STEP: listing all events in all namespaces 01/17/23 15:12:10.756
STEP: patching the test event 01/17/23 15:12:10.829
STEP: fetching the test event 01/17/23 15:12:10.836
STEP: updating the test event 01/17/23 15:12:10.838
STEP: getting the test event 01/17/23 15:12:10.848
STEP: deleting the test event 01/17/23 15:12:10.851
STEP: listing all events in all namespaces 01/17/23 15:12:10.86
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
Jan 17 15:12:10.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-3033" for this suite. 01/17/23 15:12:10.924
{"msg":"PASSED [sig-instrumentation] Events should manage the lifecycle of an event [Conformance]","completed":97,"skipped":1737,"failed":0}
------------------------------
• [0.207 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:12:10.722
    Jan 17 15:12:10.722: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename events 01/17/23 15:12:10.723
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:12:10.74
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:12:10.742
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 01/17/23 15:12:10.744
    STEP: listing all events in all namespaces 01/17/23 15:12:10.756
    STEP: patching the test event 01/17/23 15:12:10.829
    STEP: fetching the test event 01/17/23 15:12:10.836
    STEP: updating the test event 01/17/23 15:12:10.838
    STEP: getting the test event 01/17/23 15:12:10.848
    STEP: deleting the test event 01/17/23 15:12:10.851
    STEP: listing all events in all namespaces 01/17/23 15:12:10.86
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:187
    Jan 17 15:12:10.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-3033" for this suite. 01/17/23 15:12:10.924
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:12:10.929
Jan 17 15:12:10.929: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename resourcequota 01/17/23 15:12:10.93
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:12:10.949
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:12:10.951
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382
STEP: Counting existing ResourceQuota 01/17/23 15:12:10.953
STEP: Creating a ResourceQuota 01/17/23 15:12:15.963
STEP: Ensuring resource quota status is calculated 01/17/23 15:12:15.969
STEP: Creating a ReplicationController 01/17/23 15:12:17.974
STEP: Ensuring resource quota status captures replication controller creation 01/17/23 15:12:18.005
STEP: Deleting a ReplicationController 01/17/23 15:12:20.011
STEP: Ensuring resource quota status released usage 01/17/23 15:12:20.017
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 17 15:12:22.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-650" for this suite. 01/17/23 15:12:22.025
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","completed":98,"skipped":1746,"failed":0}
------------------------------
• [SLOW TEST] [11.104 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:12:10.929
    Jan 17 15:12:10.929: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename resourcequota 01/17/23 15:12:10.93
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:12:10.949
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:12:10.951
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:382
    STEP: Counting existing ResourceQuota 01/17/23 15:12:10.953
    STEP: Creating a ResourceQuota 01/17/23 15:12:15.963
    STEP: Ensuring resource quota status is calculated 01/17/23 15:12:15.969
    STEP: Creating a ReplicationController 01/17/23 15:12:17.974
    STEP: Ensuring resource quota status captures replication controller creation 01/17/23 15:12:18.005
    STEP: Deleting a ReplicationController 01/17/23 15:12:20.011
    STEP: Ensuring resource quota status released usage 01/17/23 15:12:20.017
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 17 15:12:22.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-650" for this suite. 01/17/23 15:12:22.025
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1810
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:12:22.033
Jan 17 15:12:22.033: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename kubectl 01/17/23 15:12:22.034
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:12:22.068
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:12:22.07
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1810
STEP: Starting the proxy 01/17/23 15:12:22.072
Jan 17 15:12:22.072: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-41 proxy --unix-socket=/tmp/kubectl-proxy-unix1173458386/test'
STEP: retrieving proxy /api/ output 01/17/23 15:12:22.101
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 17 15:12:22.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-41" for this suite. 01/17/23 15:12:22.107
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","completed":99,"skipped":1765,"failed":0}
------------------------------
• [0.083 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1778
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:12:22.033
    Jan 17 15:12:22.033: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename kubectl 01/17/23 15:12:22.034
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:12:22.068
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:12:22.07
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1810
    STEP: Starting the proxy 01/17/23 15:12:22.072
    Jan 17 15:12:22.072: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-41 proxy --unix-socket=/tmp/kubectl-proxy-unix1173458386/test'
    STEP: retrieving proxy /api/ output 01/17/23 15:12:22.101
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 17 15:12:22.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-41" for this suite. 01/17/23 15:12:22.107
  << End Captured GinkgoWriter Output
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1590
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:12:22.116
Jan 17 15:12:22.116: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename kubectl 01/17/23 15:12:22.117
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:12:22.152
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:12:22.155
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1570
STEP: creating an pod 01/17/23 15:12:22.157
Jan 17 15:12:22.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-508 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.40 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Jan 17 15:12:22.219: INFO: stderr: ""
Jan 17 15:12:22.219: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1590
STEP: Waiting for log generator to start. 01/17/23 15:12:22.219
Jan 17 15:12:22.219: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Jan 17 15:12:22.219: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-508" to be "running and ready, or succeeded"
Jan 17 15:12:22.224: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.398431ms
Jan 17 15:12:22.224: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on '' to be 'Running' but was 'Pending'
Jan 17 15:12:24.229: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.010017594s
Jan 17 15:12:24.229: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Jan 17 15:12:24.229: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 01/17/23 15:12:24.229
Jan 17 15:12:24.229: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-508 logs logs-generator logs-generator'
Jan 17 15:12:24.281: INFO: stderr: ""
Jan 17 15:12:24.281: INFO: stdout: "I0117 15:12:22.915354       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/p6x 345\nI0117 15:12:23.115478       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/8zc 588\nI0117 15:12:23.316011       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/kube-system/pods/q2kj 309\nI0117 15:12:23.516309       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/t8sh 474\nI0117 15:12:23.715551       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/fkp6 406\nI0117 15:12:23.915843       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/jnb5 356\nI0117 15:12:24.116149       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/br4 386\n"
STEP: limiting log lines 01/17/23 15:12:24.281
Jan 17 15:12:24.281: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-508 logs logs-generator logs-generator --tail=1'
Jan 17 15:12:24.333: INFO: stderr: ""
Jan 17 15:12:24.333: INFO: stdout: "I0117 15:12:24.315703       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/96q2 433\n"
Jan 17 15:12:24.333: INFO: got output "I0117 15:12:24.315703       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/96q2 433\n"
STEP: limiting log bytes 01/17/23 15:12:24.333
Jan 17 15:12:24.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-508 logs logs-generator logs-generator --limit-bytes=1'
Jan 17 15:12:24.388: INFO: stderr: ""
Jan 17 15:12:24.388: INFO: stdout: "I"
Jan 17 15:12:24.388: INFO: got output "I"
STEP: exposing timestamps 01/17/23 15:12:24.388
Jan 17 15:12:24.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-508 logs logs-generator logs-generator --tail=1 --timestamps'
Jan 17 15:12:24.444: INFO: stderr: ""
Jan 17 15:12:24.444: INFO: stdout: "2023-01-17T15:12:24.315744418Z I0117 15:12:24.315703       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/96q2 433\n"
Jan 17 15:12:24.444: INFO: got output "2023-01-17T15:12:24.315744418Z I0117 15:12:24.315703       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/96q2 433\n"
STEP: restricting to a time range 01/17/23 15:12:24.444
Jan 17 15:12:26.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-508 logs logs-generator logs-generator --since=1s'
Jan 17 15:12:26.999: INFO: stderr: ""
Jan 17 15:12:26.999: INFO: stdout: "I0117 15:12:26.116006       1 logs_generator.go:76] 16 GET /api/v1/namespaces/default/pods/5lh 551\nI0117 15:12:26.316326       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/54f5 547\nI0117 15:12:26.515562       1 logs_generator.go:76] 18 POST /api/v1/namespaces/kube-system/pods/vnn5 349\nI0117 15:12:26.715865       1 logs_generator.go:76] 19 POST /api/v1/namespaces/kube-system/pods/47w 393\nI0117 15:12:26.916011       1 logs_generator.go:76] 20 POST /api/v1/namespaces/kube-system/pods/cc5 286\n"
Jan 17 15:12:26.999: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-508 logs logs-generator logs-generator --since=24h'
Jan 17 15:12:27.054: INFO: stderr: ""
Jan 17 15:12:27.054: INFO: stdout: "I0117 15:12:22.915354       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/p6x 345\nI0117 15:12:23.115478       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/8zc 588\nI0117 15:12:23.316011       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/kube-system/pods/q2kj 309\nI0117 15:12:23.516309       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/t8sh 474\nI0117 15:12:23.715551       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/fkp6 406\nI0117 15:12:23.915843       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/jnb5 356\nI0117 15:12:24.116149       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/br4 386\nI0117 15:12:24.315703       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/96q2 433\nI0117 15:12:24.516011       1 logs_generator.go:76] 8 GET /api/v1/namespaces/default/pods/sjd 492\nI0117 15:12:24.716315       1 logs_generator.go:76] 9 POST /api/v1/namespaces/default/pods/j7fj 332\nI0117 15:12:24.915563       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/kube-system/pods/27xx 261\nI0117 15:12:25.115882       1 logs_generator.go:76] 11 POST /api/v1/namespaces/ns/pods/4fk 314\nI0117 15:12:25.316021       1 logs_generator.go:76] 12 GET /api/v1/namespaces/ns/pods/sttg 322\nI0117 15:12:25.516325       1 logs_generator.go:76] 13 GET /api/v1/namespaces/default/pods/2wdx 406\nI0117 15:12:25.715588       1 logs_generator.go:76] 14 GET /api/v1/namespaces/default/pods/g5f 566\nI0117 15:12:25.915874       1 logs_generator.go:76] 15 POST /api/v1/namespaces/default/pods/8c4 582\nI0117 15:12:26.116006       1 logs_generator.go:76] 16 GET /api/v1/namespaces/default/pods/5lh 551\nI0117 15:12:26.316326       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/54f5 547\nI0117 15:12:26.515562       1 logs_generator.go:76] 18 POST /api/v1/namespaces/kube-system/pods/vnn5 349\nI0117 15:12:26.715865       1 logs_generator.go:76] 19 POST /api/v1/namespaces/kube-system/pods/47w 393\nI0117 15:12:26.916011       1 logs_generator.go:76] 20 POST /api/v1/namespaces/kube-system/pods/cc5 286\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1575
Jan 17 15:12:27.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-508 delete pod logs-generator'
Jan 17 15:12:28.014: INFO: stderr: ""
Jan 17 15:12:28.014: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 17 15:12:28.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-508" for this suite. 01/17/23 15:12:28.018
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","completed":100,"skipped":1765,"failed":0}
------------------------------
• [SLOW TEST] [5.908 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1567
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1590

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:12:22.116
    Jan 17 15:12:22.116: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename kubectl 01/17/23 15:12:22.117
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:12:22.152
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:12:22.155
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1570
    STEP: creating an pod 01/17/23 15:12:22.157
    Jan 17 15:12:22.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-508 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.40 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    Jan 17 15:12:22.219: INFO: stderr: ""
    Jan 17 15:12:22.219: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1590
    STEP: Waiting for log generator to start. 01/17/23 15:12:22.219
    Jan 17 15:12:22.219: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    Jan 17 15:12:22.219: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-508" to be "running and ready, or succeeded"
    Jan 17 15:12:22.224: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.398431ms
    Jan 17 15:12:22.224: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on '' to be 'Running' but was 'Pending'
    Jan 17 15:12:24.229: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.010017594s
    Jan 17 15:12:24.229: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    Jan 17 15:12:24.229: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 01/17/23 15:12:24.229
    Jan 17 15:12:24.229: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-508 logs logs-generator logs-generator'
    Jan 17 15:12:24.281: INFO: stderr: ""
    Jan 17 15:12:24.281: INFO: stdout: "I0117 15:12:22.915354       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/p6x 345\nI0117 15:12:23.115478       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/8zc 588\nI0117 15:12:23.316011       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/kube-system/pods/q2kj 309\nI0117 15:12:23.516309       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/t8sh 474\nI0117 15:12:23.715551       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/fkp6 406\nI0117 15:12:23.915843       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/jnb5 356\nI0117 15:12:24.116149       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/br4 386\n"
    STEP: limiting log lines 01/17/23 15:12:24.281
    Jan 17 15:12:24.281: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-508 logs logs-generator logs-generator --tail=1'
    Jan 17 15:12:24.333: INFO: stderr: ""
    Jan 17 15:12:24.333: INFO: stdout: "I0117 15:12:24.315703       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/96q2 433\n"
    Jan 17 15:12:24.333: INFO: got output "I0117 15:12:24.315703       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/96q2 433\n"
    STEP: limiting log bytes 01/17/23 15:12:24.333
    Jan 17 15:12:24.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-508 logs logs-generator logs-generator --limit-bytes=1'
    Jan 17 15:12:24.388: INFO: stderr: ""
    Jan 17 15:12:24.388: INFO: stdout: "I"
    Jan 17 15:12:24.388: INFO: got output "I"
    STEP: exposing timestamps 01/17/23 15:12:24.388
    Jan 17 15:12:24.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-508 logs logs-generator logs-generator --tail=1 --timestamps'
    Jan 17 15:12:24.444: INFO: stderr: ""
    Jan 17 15:12:24.444: INFO: stdout: "2023-01-17T15:12:24.315744418Z I0117 15:12:24.315703       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/96q2 433\n"
    Jan 17 15:12:24.444: INFO: got output "2023-01-17T15:12:24.315744418Z I0117 15:12:24.315703       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/96q2 433\n"
    STEP: restricting to a time range 01/17/23 15:12:24.444
    Jan 17 15:12:26.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-508 logs logs-generator logs-generator --since=1s'
    Jan 17 15:12:26.999: INFO: stderr: ""
    Jan 17 15:12:26.999: INFO: stdout: "I0117 15:12:26.116006       1 logs_generator.go:76] 16 GET /api/v1/namespaces/default/pods/5lh 551\nI0117 15:12:26.316326       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/54f5 547\nI0117 15:12:26.515562       1 logs_generator.go:76] 18 POST /api/v1/namespaces/kube-system/pods/vnn5 349\nI0117 15:12:26.715865       1 logs_generator.go:76] 19 POST /api/v1/namespaces/kube-system/pods/47w 393\nI0117 15:12:26.916011       1 logs_generator.go:76] 20 POST /api/v1/namespaces/kube-system/pods/cc5 286\n"
    Jan 17 15:12:26.999: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-508 logs logs-generator logs-generator --since=24h'
    Jan 17 15:12:27.054: INFO: stderr: ""
    Jan 17 15:12:27.054: INFO: stdout: "I0117 15:12:22.915354       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/p6x 345\nI0117 15:12:23.115478       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/8zc 588\nI0117 15:12:23.316011       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/kube-system/pods/q2kj 309\nI0117 15:12:23.516309       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/t8sh 474\nI0117 15:12:23.715551       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/fkp6 406\nI0117 15:12:23.915843       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/jnb5 356\nI0117 15:12:24.116149       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/br4 386\nI0117 15:12:24.315703       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/96q2 433\nI0117 15:12:24.516011       1 logs_generator.go:76] 8 GET /api/v1/namespaces/default/pods/sjd 492\nI0117 15:12:24.716315       1 logs_generator.go:76] 9 POST /api/v1/namespaces/default/pods/j7fj 332\nI0117 15:12:24.915563       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/kube-system/pods/27xx 261\nI0117 15:12:25.115882       1 logs_generator.go:76] 11 POST /api/v1/namespaces/ns/pods/4fk 314\nI0117 15:12:25.316021       1 logs_generator.go:76] 12 GET /api/v1/namespaces/ns/pods/sttg 322\nI0117 15:12:25.516325       1 logs_generator.go:76] 13 GET /api/v1/namespaces/default/pods/2wdx 406\nI0117 15:12:25.715588       1 logs_generator.go:76] 14 GET /api/v1/namespaces/default/pods/g5f 566\nI0117 15:12:25.915874       1 logs_generator.go:76] 15 POST /api/v1/namespaces/default/pods/8c4 582\nI0117 15:12:26.116006       1 logs_generator.go:76] 16 GET /api/v1/namespaces/default/pods/5lh 551\nI0117 15:12:26.316326       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/54f5 547\nI0117 15:12:26.515562       1 logs_generator.go:76] 18 POST /api/v1/namespaces/kube-system/pods/vnn5 349\nI0117 15:12:26.715865       1 logs_generator.go:76] 19 POST /api/v1/namespaces/kube-system/pods/47w 393\nI0117 15:12:26.916011       1 logs_generator.go:76] 20 POST /api/v1/namespaces/kube-system/pods/cc5 286\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1575
    Jan 17 15:12:27.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-508 delete pod logs-generator'
    Jan 17 15:12:28.014: INFO: stderr: ""
    Jan 17 15:12:28.014: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 17 15:12:28.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-508" for this suite. 01/17/23 15:12:28.018
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:231
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:12:28.024
Jan 17 15:12:28.024: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename container-runtime 01/17/23 15:12:28.025
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:12:28.051
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:12:28.053
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:231
STEP: create the container 01/17/23 15:12:28.055
STEP: wait for the container to reach Succeeded 01/17/23 15:12:28.079
STEP: get the container status 01/17/23 15:12:32.1
STEP: the container should be terminated 01/17/23 15:12:32.104
STEP: the termination message should be set 01/17/23 15:12:32.104
Jan 17 15:12:32.104: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 01/17/23 15:12:32.104
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Jan 17 15:12:32.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7217" for this suite. 01/17/23 15:12:32.124
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":101,"skipped":1767,"failed":0}
------------------------------
• [4.110 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:231

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:12:28.024
    Jan 17 15:12:28.024: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename container-runtime 01/17/23 15:12:28.025
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:12:28.051
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:12:28.053
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:231
    STEP: create the container 01/17/23 15:12:28.055
    STEP: wait for the container to reach Succeeded 01/17/23 15:12:28.079
    STEP: get the container status 01/17/23 15:12:32.1
    STEP: the container should be terminated 01/17/23 15:12:32.104
    STEP: the termination message should be set 01/17/23 15:12:32.104
    Jan 17 15:12:32.104: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 01/17/23 15:12:32.104
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Jan 17 15:12:32.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-7217" for this suite. 01/17/23 15:12:32.124
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:12:32.135
Jan 17 15:12:32.135: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename services 01/17/23 15:12:32.136
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:12:32.167
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:12:32.17
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189
STEP: creating service in namespace services-2711 01/17/23 15:12:32.172
STEP: creating service affinity-clusterip-transition in namespace services-2711 01/17/23 15:12:32.172
STEP: creating replication controller affinity-clusterip-transition in namespace services-2711 01/17/23 15:12:32.187
I0117 15:12:32.203601      22 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-2711, replica count: 3
I0117 15:12:35.255077      22 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 17 15:12:35.263: INFO: Creating new exec pod
Jan 17 15:12:35.280: INFO: Waiting up to 5m0s for pod "execpod-affinityhm77r" in namespace "services-2711" to be "running"
Jan 17 15:12:35.283: INFO: Pod "execpod-affinityhm77r": Phase="Pending", Reason="", readiness=false. Elapsed: 3.386819ms
Jan 17 15:12:37.287: INFO: Pod "execpod-affinityhm77r": Phase="Running", Reason="", readiness=true. Elapsed: 2.007091228s
Jan 17 15:12:37.287: INFO: Pod "execpod-affinityhm77r" satisfied condition "running"
Jan 17 15:12:38.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-2711 exec execpod-affinityhm77r -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
Jan 17 15:12:39.438: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Jan 17 15:12:39.438: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 17 15:12:39.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-2711 exec execpod-affinityhm77r -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.18.22 80'
Jan 17 15:12:39.558: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.18.22 80\nConnection to 172.30.18.22 80 port [tcp/http] succeeded!\n"
Jan 17 15:12:39.558: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 17 15:12:39.568: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-2711 exec execpod-affinityhm77r -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.30.18.22:80/ ; done'
Jan 17 15:12:39.726: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.18.22:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.18.22:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.18.22:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.18.22:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.18.22:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.18.22:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.18.22:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.18.22:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.18.22:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.18.22:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.18.22:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.18.22:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.18.22:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.18.22:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.18.22:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.18.22:80/\n"
Jan 17 15:12:39.726: INFO: stdout: "\naffinity-clusterip-transition-kg9v9\naffinity-clusterip-transition-fr8b5\naffinity-clusterip-transition-z8qh7\naffinity-clusterip-transition-z8qh7\naffinity-clusterip-transition-z8qh7\naffinity-clusterip-transition-fr8b5\naffinity-clusterip-transition-z8qh7\naffinity-clusterip-transition-fr8b5\naffinity-clusterip-transition-z8qh7\naffinity-clusterip-transition-z8qh7\naffinity-clusterip-transition-z8qh7\naffinity-clusterip-transition-kg9v9\naffinity-clusterip-transition-kg9v9\naffinity-clusterip-transition-kg9v9\naffinity-clusterip-transition-fr8b5\naffinity-clusterip-transition-kg9v9"
Jan 17 15:12:39.726: INFO: Received response from host: affinity-clusterip-transition-kg9v9
Jan 17 15:12:39.726: INFO: Received response from host: affinity-clusterip-transition-fr8b5
Jan 17 15:12:39.726: INFO: Received response from host: affinity-clusterip-transition-z8qh7
Jan 17 15:12:39.726: INFO: Received response from host: affinity-clusterip-transition-z8qh7
Jan 17 15:12:39.726: INFO: Received response from host: affinity-clusterip-transition-z8qh7
Jan 17 15:12:39.726: INFO: Received response from host: affinity-clusterip-transition-fr8b5
Jan 17 15:12:39.726: INFO: Received response from host: affinity-clusterip-transition-z8qh7
Jan 17 15:12:39.726: INFO: Received response from host: affinity-clusterip-transition-fr8b5
Jan 17 15:12:39.726: INFO: Received response from host: affinity-clusterip-transition-z8qh7
Jan 17 15:12:39.726: INFO: Received response from host: affinity-clusterip-transition-z8qh7
Jan 17 15:12:39.726: INFO: Received response from host: affinity-clusterip-transition-z8qh7
Jan 17 15:12:39.726: INFO: Received response from host: affinity-clusterip-transition-kg9v9
Jan 17 15:12:39.726: INFO: Received response from host: affinity-clusterip-transition-kg9v9
Jan 17 15:12:39.726: INFO: Received response from host: affinity-clusterip-transition-kg9v9
Jan 17 15:12:39.726: INFO: Received response from host: affinity-clusterip-transition-fr8b5
Jan 17 15:12:39.726: INFO: Received response from host: affinity-clusterip-transition-kg9v9
Jan 17 15:12:39.734: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-2711 exec execpod-affinityhm77r -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.30.18.22:80/ ; done'
Jan 17 15:12:39.889: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.18.22:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.18.22:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.18.22:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.18.22:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.18.22:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.18.22:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.18.22:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.18.22:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.18.22:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.18.22:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.18.22:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.18.22:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.18.22:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.18.22:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.18.22:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.18.22:80/\n"
Jan 17 15:12:39.889: INFO: stdout: "\naffinity-clusterip-transition-fr8b5\naffinity-clusterip-transition-fr8b5\naffinity-clusterip-transition-fr8b5\naffinity-clusterip-transition-fr8b5\naffinity-clusterip-transition-fr8b5\naffinity-clusterip-transition-fr8b5\naffinity-clusterip-transition-fr8b5\naffinity-clusterip-transition-fr8b5\naffinity-clusterip-transition-fr8b5\naffinity-clusterip-transition-fr8b5\naffinity-clusterip-transition-fr8b5\naffinity-clusterip-transition-fr8b5\naffinity-clusterip-transition-fr8b5\naffinity-clusterip-transition-fr8b5\naffinity-clusterip-transition-fr8b5\naffinity-clusterip-transition-fr8b5"
Jan 17 15:12:39.889: INFO: Received response from host: affinity-clusterip-transition-fr8b5
Jan 17 15:12:39.889: INFO: Received response from host: affinity-clusterip-transition-fr8b5
Jan 17 15:12:39.889: INFO: Received response from host: affinity-clusterip-transition-fr8b5
Jan 17 15:12:39.889: INFO: Received response from host: affinity-clusterip-transition-fr8b5
Jan 17 15:12:39.889: INFO: Received response from host: affinity-clusterip-transition-fr8b5
Jan 17 15:12:39.889: INFO: Received response from host: affinity-clusterip-transition-fr8b5
Jan 17 15:12:39.889: INFO: Received response from host: affinity-clusterip-transition-fr8b5
Jan 17 15:12:39.889: INFO: Received response from host: affinity-clusterip-transition-fr8b5
Jan 17 15:12:39.889: INFO: Received response from host: affinity-clusterip-transition-fr8b5
Jan 17 15:12:39.889: INFO: Received response from host: affinity-clusterip-transition-fr8b5
Jan 17 15:12:39.889: INFO: Received response from host: affinity-clusterip-transition-fr8b5
Jan 17 15:12:39.889: INFO: Received response from host: affinity-clusterip-transition-fr8b5
Jan 17 15:12:39.889: INFO: Received response from host: affinity-clusterip-transition-fr8b5
Jan 17 15:12:39.889: INFO: Received response from host: affinity-clusterip-transition-fr8b5
Jan 17 15:12:39.889: INFO: Received response from host: affinity-clusterip-transition-fr8b5
Jan 17 15:12:39.889: INFO: Received response from host: affinity-clusterip-transition-fr8b5
Jan 17 15:12:39.889: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-2711, will wait for the garbage collector to delete the pods 01/17/23 15:12:39.915
Jan 17 15:12:39.976: INFO: Deleting ReplicationController affinity-clusterip-transition took: 6.72172ms
Jan 17 15:12:40.077: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.995673ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 17 15:12:42.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2711" for this suite. 01/17/23 15:12:42.412
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","completed":102,"skipped":1804,"failed":0}
------------------------------
• [SLOW TEST] [10.287 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:12:32.135
    Jan 17 15:12:32.135: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename services 01/17/23 15:12:32.136
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:12:32.167
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:12:32.17
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2189
    STEP: creating service in namespace services-2711 01/17/23 15:12:32.172
    STEP: creating service affinity-clusterip-transition in namespace services-2711 01/17/23 15:12:32.172
    STEP: creating replication controller affinity-clusterip-transition in namespace services-2711 01/17/23 15:12:32.187
    I0117 15:12:32.203601      22 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-2711, replica count: 3
    I0117 15:12:35.255077      22 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 17 15:12:35.263: INFO: Creating new exec pod
    Jan 17 15:12:35.280: INFO: Waiting up to 5m0s for pod "execpod-affinityhm77r" in namespace "services-2711" to be "running"
    Jan 17 15:12:35.283: INFO: Pod "execpod-affinityhm77r": Phase="Pending", Reason="", readiness=false. Elapsed: 3.386819ms
    Jan 17 15:12:37.287: INFO: Pod "execpod-affinityhm77r": Phase="Running", Reason="", readiness=true. Elapsed: 2.007091228s
    Jan 17 15:12:37.287: INFO: Pod "execpod-affinityhm77r" satisfied condition "running"
    Jan 17 15:12:38.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-2711 exec execpod-affinityhm77r -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
    Jan 17 15:12:39.438: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    Jan 17 15:12:39.438: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 17 15:12:39.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-2711 exec execpod-affinityhm77r -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.18.22 80'
    Jan 17 15:12:39.558: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.18.22 80\nConnection to 172.30.18.22 80 port [tcp/http] succeeded!\n"
    Jan 17 15:12:39.558: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 17 15:12:39.568: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-2711 exec execpod-affinityhm77r -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.30.18.22:80/ ; done'
    Jan 17 15:12:39.726: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.18.22:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.18.22:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.18.22:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.18.22:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.18.22:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.18.22:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.18.22:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.18.22:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.18.22:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.18.22:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.18.22:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.18.22:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.18.22:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.18.22:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.18.22:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.18.22:80/\n"
    Jan 17 15:12:39.726: INFO: stdout: "\naffinity-clusterip-transition-kg9v9\naffinity-clusterip-transition-fr8b5\naffinity-clusterip-transition-z8qh7\naffinity-clusterip-transition-z8qh7\naffinity-clusterip-transition-z8qh7\naffinity-clusterip-transition-fr8b5\naffinity-clusterip-transition-z8qh7\naffinity-clusterip-transition-fr8b5\naffinity-clusterip-transition-z8qh7\naffinity-clusterip-transition-z8qh7\naffinity-clusterip-transition-z8qh7\naffinity-clusterip-transition-kg9v9\naffinity-clusterip-transition-kg9v9\naffinity-clusterip-transition-kg9v9\naffinity-clusterip-transition-fr8b5\naffinity-clusterip-transition-kg9v9"
    Jan 17 15:12:39.726: INFO: Received response from host: affinity-clusterip-transition-kg9v9
    Jan 17 15:12:39.726: INFO: Received response from host: affinity-clusterip-transition-fr8b5
    Jan 17 15:12:39.726: INFO: Received response from host: affinity-clusterip-transition-z8qh7
    Jan 17 15:12:39.726: INFO: Received response from host: affinity-clusterip-transition-z8qh7
    Jan 17 15:12:39.726: INFO: Received response from host: affinity-clusterip-transition-z8qh7
    Jan 17 15:12:39.726: INFO: Received response from host: affinity-clusterip-transition-fr8b5
    Jan 17 15:12:39.726: INFO: Received response from host: affinity-clusterip-transition-z8qh7
    Jan 17 15:12:39.726: INFO: Received response from host: affinity-clusterip-transition-fr8b5
    Jan 17 15:12:39.726: INFO: Received response from host: affinity-clusterip-transition-z8qh7
    Jan 17 15:12:39.726: INFO: Received response from host: affinity-clusterip-transition-z8qh7
    Jan 17 15:12:39.726: INFO: Received response from host: affinity-clusterip-transition-z8qh7
    Jan 17 15:12:39.726: INFO: Received response from host: affinity-clusterip-transition-kg9v9
    Jan 17 15:12:39.726: INFO: Received response from host: affinity-clusterip-transition-kg9v9
    Jan 17 15:12:39.726: INFO: Received response from host: affinity-clusterip-transition-kg9v9
    Jan 17 15:12:39.726: INFO: Received response from host: affinity-clusterip-transition-fr8b5
    Jan 17 15:12:39.726: INFO: Received response from host: affinity-clusterip-transition-kg9v9
    Jan 17 15:12:39.734: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-2711 exec execpod-affinityhm77r -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.30.18.22:80/ ; done'
    Jan 17 15:12:39.889: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.18.22:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.18.22:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.18.22:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.18.22:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.18.22:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.18.22:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.18.22:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.18.22:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.18.22:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.18.22:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.18.22:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.18.22:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.18.22:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.18.22:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.18.22:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.18.22:80/\n"
    Jan 17 15:12:39.889: INFO: stdout: "\naffinity-clusterip-transition-fr8b5\naffinity-clusterip-transition-fr8b5\naffinity-clusterip-transition-fr8b5\naffinity-clusterip-transition-fr8b5\naffinity-clusterip-transition-fr8b5\naffinity-clusterip-transition-fr8b5\naffinity-clusterip-transition-fr8b5\naffinity-clusterip-transition-fr8b5\naffinity-clusterip-transition-fr8b5\naffinity-clusterip-transition-fr8b5\naffinity-clusterip-transition-fr8b5\naffinity-clusterip-transition-fr8b5\naffinity-clusterip-transition-fr8b5\naffinity-clusterip-transition-fr8b5\naffinity-clusterip-transition-fr8b5\naffinity-clusterip-transition-fr8b5"
    Jan 17 15:12:39.889: INFO: Received response from host: affinity-clusterip-transition-fr8b5
    Jan 17 15:12:39.889: INFO: Received response from host: affinity-clusterip-transition-fr8b5
    Jan 17 15:12:39.889: INFO: Received response from host: affinity-clusterip-transition-fr8b5
    Jan 17 15:12:39.889: INFO: Received response from host: affinity-clusterip-transition-fr8b5
    Jan 17 15:12:39.889: INFO: Received response from host: affinity-clusterip-transition-fr8b5
    Jan 17 15:12:39.889: INFO: Received response from host: affinity-clusterip-transition-fr8b5
    Jan 17 15:12:39.889: INFO: Received response from host: affinity-clusterip-transition-fr8b5
    Jan 17 15:12:39.889: INFO: Received response from host: affinity-clusterip-transition-fr8b5
    Jan 17 15:12:39.889: INFO: Received response from host: affinity-clusterip-transition-fr8b5
    Jan 17 15:12:39.889: INFO: Received response from host: affinity-clusterip-transition-fr8b5
    Jan 17 15:12:39.889: INFO: Received response from host: affinity-clusterip-transition-fr8b5
    Jan 17 15:12:39.889: INFO: Received response from host: affinity-clusterip-transition-fr8b5
    Jan 17 15:12:39.889: INFO: Received response from host: affinity-clusterip-transition-fr8b5
    Jan 17 15:12:39.889: INFO: Received response from host: affinity-clusterip-transition-fr8b5
    Jan 17 15:12:39.889: INFO: Received response from host: affinity-clusterip-transition-fr8b5
    Jan 17 15:12:39.889: INFO: Received response from host: affinity-clusterip-transition-fr8b5
    Jan 17 15:12:39.889: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-2711, will wait for the garbage collector to delete the pods 01/17/23 15:12:39.915
    Jan 17 15:12:39.976: INFO: Deleting ReplicationController affinity-clusterip-transition took: 6.72172ms
    Jan 17 15:12:40.077: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.995673ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 17 15:12:42.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-2711" for this suite. 01/17/23 15:12:42.412
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:12:42.423
Jan 17 15:12:42.423: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename crd-publish-openapi 01/17/23 15:12:42.423
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:12:42.446
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:12:42.449
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235
Jan 17 15:12:42.451: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/17/23 15:12:49.057
Jan 17 15:12:49.057: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=crd-publish-openapi-850 --namespace=crd-publish-openapi-850 create -f -'
Jan 17 15:12:50.090: INFO: stderr: ""
Jan 17 15:12:50.090: INFO: stdout: "e2e-test-crd-publish-openapi-5185-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jan 17 15:12:50.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=crd-publish-openapi-850 --namespace=crd-publish-openapi-850 delete e2e-test-crd-publish-openapi-5185-crds test-cr'
Jan 17 15:12:50.142: INFO: stderr: ""
Jan 17 15:12:50.142: INFO: stdout: "e2e-test-crd-publish-openapi-5185-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Jan 17 15:12:50.142: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=crd-publish-openapi-850 --namespace=crd-publish-openapi-850 apply -f -'
Jan 17 15:12:51.212: INFO: stderr: ""
Jan 17 15:12:51.212: INFO: stdout: "e2e-test-crd-publish-openapi-5185-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jan 17 15:12:51.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=crd-publish-openapi-850 --namespace=crd-publish-openapi-850 delete e2e-test-crd-publish-openapi-5185-crds test-cr'
Jan 17 15:12:51.284: INFO: stderr: ""
Jan 17 15:12:51.284: INFO: stdout: "e2e-test-crd-publish-openapi-5185-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 01/17/23 15:12:51.284
Jan 17 15:12:51.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=crd-publish-openapi-850 explain e2e-test-crd-publish-openapi-5185-crds'
Jan 17 15:12:51.544: INFO: stderr: ""
Jan 17 15:12:51.544: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5185-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 17 15:12:58.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-850" for this suite. 01/17/23 15:12:58.05
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","completed":103,"skipped":1839,"failed":0}
------------------------------
• [SLOW TEST] [15.634 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:12:42.423
    Jan 17 15:12:42.423: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename crd-publish-openapi 01/17/23 15:12:42.423
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:12:42.446
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:12:42.449
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:235
    Jan 17 15:12:42.451: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/17/23 15:12:49.057
    Jan 17 15:12:49.057: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=crd-publish-openapi-850 --namespace=crd-publish-openapi-850 create -f -'
    Jan 17 15:12:50.090: INFO: stderr: ""
    Jan 17 15:12:50.090: INFO: stdout: "e2e-test-crd-publish-openapi-5185-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Jan 17 15:12:50.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=crd-publish-openapi-850 --namespace=crd-publish-openapi-850 delete e2e-test-crd-publish-openapi-5185-crds test-cr'
    Jan 17 15:12:50.142: INFO: stderr: ""
    Jan 17 15:12:50.142: INFO: stdout: "e2e-test-crd-publish-openapi-5185-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    Jan 17 15:12:50.142: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=crd-publish-openapi-850 --namespace=crd-publish-openapi-850 apply -f -'
    Jan 17 15:12:51.212: INFO: stderr: ""
    Jan 17 15:12:51.212: INFO: stdout: "e2e-test-crd-publish-openapi-5185-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Jan 17 15:12:51.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=crd-publish-openapi-850 --namespace=crd-publish-openapi-850 delete e2e-test-crd-publish-openapi-5185-crds test-cr'
    Jan 17 15:12:51.284: INFO: stderr: ""
    Jan 17 15:12:51.284: INFO: stdout: "e2e-test-crd-publish-openapi-5185-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 01/17/23 15:12:51.284
    Jan 17 15:12:51.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=crd-publish-openapi-850 explain e2e-test-crd-publish-openapi-5185-crds'
    Jan 17 15:12:51.544: INFO: stderr: ""
    Jan 17 15:12:51.544: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5185-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 17 15:12:58.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-850" for this suite. 01/17/23 15:12:58.05
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:12:58.058
Jan 17 15:12:58.058: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename gc 01/17/23 15:12:58.058
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:12:58.081
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:12:58.083
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
Jan 17 15:12:58.183: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"2c1fc1a7-f1e4-4bbe-b09c-0a6d6db082cd", Controller:(*bool)(0xc0066733b2), BlockOwnerDeletion:(*bool)(0xc0066733b3)}}
Jan 17 15:12:58.190: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"8d04184e-260f-45d2-abae-1ed703a27e91", Controller:(*bool)(0xc006673952), BlockOwnerDeletion:(*bool)(0xc006673953)}}
Jan 17 15:12:58.210: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"e2eebbb6-ffaa-41d5-a302-cae5cca3ce93", Controller:(*bool)(0xc006673f02), BlockOwnerDeletion:(*bool)(0xc006673f03)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan 17 15:13:03.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8614" for this suite. 01/17/23 15:13:03.23
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","completed":104,"skipped":1843,"failed":0}
------------------------------
• [SLOW TEST] [5.180 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:12:58.058
    Jan 17 15:12:58.058: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename gc 01/17/23 15:12:58.058
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:12:58.081
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:12:58.083
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    Jan 17 15:12:58.183: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"2c1fc1a7-f1e4-4bbe-b09c-0a6d6db082cd", Controller:(*bool)(0xc0066733b2), BlockOwnerDeletion:(*bool)(0xc0066733b3)}}
    Jan 17 15:12:58.190: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"8d04184e-260f-45d2-abae-1ed703a27e91", Controller:(*bool)(0xc006673952), BlockOwnerDeletion:(*bool)(0xc006673953)}}
    Jan 17 15:12:58.210: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"e2eebbb6-ffaa-41d5-a302-cae5cca3ce93", Controller:(*bool)(0xc006673f02), BlockOwnerDeletion:(*bool)(0xc006673f03)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan 17 15:13:03.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-8614" for this suite. 01/17/23 15:13:03.23
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:13:03.238
Jan 17 15:13:03.238: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename projected 01/17/23 15:13:03.238
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:13:03.259
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:13:03.262
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123
Jan 17 15:13:03.278: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating projection with configMap that has name projected-configmap-test-upd-3d9e3d86-abba-4884-a367-1ebf241d9f77 01/17/23 15:13:03.278
STEP: Creating the pod 01/17/23 15:13:03.282
Jan 17 15:13:03.304: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-af4703b2-ae8d-4bae-beb3-21165e68b0e1" in namespace "projected-1844" to be "running and ready"
Jan 17 15:13:03.309: INFO: Pod "pod-projected-configmaps-af4703b2-ae8d-4bae-beb3-21165e68b0e1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.867216ms
Jan 17 15:13:03.309: INFO: The phase of Pod pod-projected-configmaps-af4703b2-ae8d-4bae-beb3-21165e68b0e1 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 15:13:05.313: INFO: Pod "pod-projected-configmaps-af4703b2-ae8d-4bae-beb3-21165e68b0e1": Phase="Running", Reason="", readiness=true. Elapsed: 2.008523621s
Jan 17 15:13:05.313: INFO: The phase of Pod pod-projected-configmaps-af4703b2-ae8d-4bae-beb3-21165e68b0e1 is Running (Ready = true)
Jan 17 15:13:05.313: INFO: Pod "pod-projected-configmaps-af4703b2-ae8d-4bae-beb3-21165e68b0e1" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-3d9e3d86-abba-4884-a367-1ebf241d9f77 01/17/23 15:13:05.321
STEP: waiting to observe update in volume 01/17/23 15:13:05.326
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan 17 15:13:07.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1844" for this suite. 01/17/23 15:13:07.344
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","completed":105,"skipped":1849,"failed":0}
------------------------------
• [4.112 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:13:03.238
    Jan 17 15:13:03.238: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename projected 01/17/23 15:13:03.238
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:13:03.259
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:13:03.262
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:123
    Jan 17 15:13:03.278: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-3d9e3d86-abba-4884-a367-1ebf241d9f77 01/17/23 15:13:03.278
    STEP: Creating the pod 01/17/23 15:13:03.282
    Jan 17 15:13:03.304: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-af4703b2-ae8d-4bae-beb3-21165e68b0e1" in namespace "projected-1844" to be "running and ready"
    Jan 17 15:13:03.309: INFO: Pod "pod-projected-configmaps-af4703b2-ae8d-4bae-beb3-21165e68b0e1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.867216ms
    Jan 17 15:13:03.309: INFO: The phase of Pod pod-projected-configmaps-af4703b2-ae8d-4bae-beb3-21165e68b0e1 is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 15:13:05.313: INFO: Pod "pod-projected-configmaps-af4703b2-ae8d-4bae-beb3-21165e68b0e1": Phase="Running", Reason="", readiness=true. Elapsed: 2.008523621s
    Jan 17 15:13:05.313: INFO: The phase of Pod pod-projected-configmaps-af4703b2-ae8d-4bae-beb3-21165e68b0e1 is Running (Ready = true)
    Jan 17 15:13:05.313: INFO: Pod "pod-projected-configmaps-af4703b2-ae8d-4bae-beb3-21165e68b0e1" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-3d9e3d86-abba-4884-a367-1ebf241d9f77 01/17/23 15:13:05.321
    STEP: waiting to observe update in volume 01/17/23 15:13:05.326
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan 17 15:13:07.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1844" for this suite. 01/17/23 15:13:07.344
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:13:07.35
Jan 17 15:13:07.350: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename emptydir 01/17/23 15:13:07.351
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:13:07.374
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:13:07.377
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86
STEP: Creating a pod to test emptydir volume type on tmpfs 01/17/23 15:13:07.38
Jan 17 15:13:07.401: INFO: Waiting up to 5m0s for pod "pod-50da7784-fbe1-49ac-aae3-1fe97b41fe5f" in namespace "emptydir-1724" to be "Succeeded or Failed"
Jan 17 15:13:07.405: INFO: Pod "pod-50da7784-fbe1-49ac-aae3-1fe97b41fe5f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.579151ms
Jan 17 15:13:09.409: INFO: Pod "pod-50da7784-fbe1-49ac-aae3-1fe97b41fe5f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00743836s
Jan 17 15:13:11.410: INFO: Pod "pod-50da7784-fbe1-49ac-aae3-1fe97b41fe5f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009186337s
STEP: Saw pod success 01/17/23 15:13:11.41
Jan 17 15:13:11.410: INFO: Pod "pod-50da7784-fbe1-49ac-aae3-1fe97b41fe5f" satisfied condition "Succeeded or Failed"
Jan 17 15:13:11.413: INFO: Trying to get logs from node ip-10-0-165-14.ec2.internal pod pod-50da7784-fbe1-49ac-aae3-1fe97b41fe5f container test-container: <nil>
STEP: delete the pod 01/17/23 15:13:11.423
Jan 17 15:13:11.436: INFO: Waiting for pod pod-50da7784-fbe1-49ac-aae3-1fe97b41fe5f to disappear
Jan 17 15:13:11.439: INFO: Pod pod-50da7784-fbe1-49ac-aae3-1fe97b41fe5f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 17 15:13:11.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1724" for this suite. 01/17/23 15:13:11.443
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","completed":106,"skipped":1859,"failed":0}
------------------------------
• [4.099 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:13:07.35
    Jan 17 15:13:07.350: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename emptydir 01/17/23 15:13:07.351
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:13:07.374
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:13:07.377
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:86
    STEP: Creating a pod to test emptydir volume type on tmpfs 01/17/23 15:13:07.38
    Jan 17 15:13:07.401: INFO: Waiting up to 5m0s for pod "pod-50da7784-fbe1-49ac-aae3-1fe97b41fe5f" in namespace "emptydir-1724" to be "Succeeded or Failed"
    Jan 17 15:13:07.405: INFO: Pod "pod-50da7784-fbe1-49ac-aae3-1fe97b41fe5f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.579151ms
    Jan 17 15:13:09.409: INFO: Pod "pod-50da7784-fbe1-49ac-aae3-1fe97b41fe5f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00743836s
    Jan 17 15:13:11.410: INFO: Pod "pod-50da7784-fbe1-49ac-aae3-1fe97b41fe5f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009186337s
    STEP: Saw pod success 01/17/23 15:13:11.41
    Jan 17 15:13:11.410: INFO: Pod "pod-50da7784-fbe1-49ac-aae3-1fe97b41fe5f" satisfied condition "Succeeded or Failed"
    Jan 17 15:13:11.413: INFO: Trying to get logs from node ip-10-0-165-14.ec2.internal pod pod-50da7784-fbe1-49ac-aae3-1fe97b41fe5f container test-container: <nil>
    STEP: delete the pod 01/17/23 15:13:11.423
    Jan 17 15:13:11.436: INFO: Waiting for pod pod-50da7784-fbe1-49ac-aae3-1fe97b41fe5f to disappear
    Jan 17 15:13:11.439: INFO: Pod pod-50da7784-fbe1-49ac-aae3-1fe97b41fe5f no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 17 15:13:11.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-1724" for this suite. 01/17/23 15:13:11.443
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:13:11.45
Jan 17 15:13:11.450: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename daemonsets 01/17/23 15:13:11.45
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:13:11.47
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:13:11.473
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431
Jan 17 15:13:11.528: INFO: Create a RollingUpdate DaemonSet
Jan 17 15:13:11.533: INFO: Check that daemon pods launch on every node of the cluster
Jan 17 15:13:11.542: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 15:13:11.542: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 15:13:11.542: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 15:13:11.545: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 17 15:13:11.545: INFO: Node ip-10-0-139-213.ec2.internal is running 0 daemon pod, expected 1
Jan 17 15:13:12.550: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 15:13:12.550: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 15:13:12.550: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 15:13:12.554: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 17 15:13:12.554: INFO: Node ip-10-0-139-213.ec2.internal is running 0 daemon pod, expected 1
Jan 17 15:13:13.550: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 15:13:13.550: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 15:13:13.550: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 15:13:13.553: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 17 15:13:13.553: INFO: Node ip-10-0-139-213.ec2.internal is running 0 daemon pod, expected 1
Jan 17 15:13:14.550: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 15:13:14.550: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 15:13:14.550: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 15:13:14.553: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 17 15:13:14.553: INFO: Node ip-10-0-139-213.ec2.internal is running 0 daemon pod, expected 1
Jan 17 15:13:15.550: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 15:13:15.550: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 15:13:15.550: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 15:13:15.553: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan 17 15:13:15.553: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
Jan 17 15:13:15.553: INFO: Update the DaemonSet to trigger a rollout
Jan 17 15:13:15.574: INFO: Updating DaemonSet daemon-set
Jan 17 15:13:16.593: INFO: Roll back the DaemonSet before rollout is complete
Jan 17 15:13:16.603: INFO: Updating DaemonSet daemon-set
Jan 17 15:13:16.603: INFO: Make sure DaemonSet rollback is complete
Jan 17 15:13:16.606: INFO: Wrong image for pod: daemon-set-6jlsd. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
Jan 17 15:13:16.606: INFO: Pod daemon-set-6jlsd is not available
Jan 17 15:13:16.610: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 15:13:16.610: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 15:13:16.610: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 15:13:17.618: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 15:13:17.618: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 15:13:17.618: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 15:13:18.626: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 15:13:18.626: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 15:13:18.626: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 15:13:19.614: INFO: Pod daemon-set-6kk9h is not available
Jan 17 15:13:19.619: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 15:13:19.619: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 15:13:19.619: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 01/17/23 15:13:19.625
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-833, will wait for the garbage collector to delete the pods 01/17/23 15:13:19.625
Jan 17 15:13:19.684: INFO: Deleting DaemonSet.extensions daemon-set took: 5.906614ms
Jan 17 15:13:19.784: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.387001ms
Jan 17 15:13:21.388: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 17 15:13:21.388: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 17 15:13:21.391: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"86902"},"items":null}

Jan 17 15:13:21.393: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"86902"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan 17 15:13:21.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-833" for this suite. 01/17/23 15:13:21.41
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","completed":107,"skipped":1861,"failed":0}
------------------------------
• [SLOW TEST] [9.968 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:13:11.45
    Jan 17 15:13:11.450: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename daemonsets 01/17/23 15:13:11.45
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:13:11.47
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:13:11.473
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:431
    Jan 17 15:13:11.528: INFO: Create a RollingUpdate DaemonSet
    Jan 17 15:13:11.533: INFO: Check that daemon pods launch on every node of the cluster
    Jan 17 15:13:11.542: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 15:13:11.542: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 15:13:11.542: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 15:13:11.545: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 17 15:13:11.545: INFO: Node ip-10-0-139-213.ec2.internal is running 0 daemon pod, expected 1
    Jan 17 15:13:12.550: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 15:13:12.550: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 15:13:12.550: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 15:13:12.554: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 17 15:13:12.554: INFO: Node ip-10-0-139-213.ec2.internal is running 0 daemon pod, expected 1
    Jan 17 15:13:13.550: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 15:13:13.550: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 15:13:13.550: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 15:13:13.553: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 17 15:13:13.553: INFO: Node ip-10-0-139-213.ec2.internal is running 0 daemon pod, expected 1
    Jan 17 15:13:14.550: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 15:13:14.550: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 15:13:14.550: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 15:13:14.553: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 17 15:13:14.553: INFO: Node ip-10-0-139-213.ec2.internal is running 0 daemon pod, expected 1
    Jan 17 15:13:15.550: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 15:13:15.550: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 15:13:15.550: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 15:13:15.553: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jan 17 15:13:15.553: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    Jan 17 15:13:15.553: INFO: Update the DaemonSet to trigger a rollout
    Jan 17 15:13:15.574: INFO: Updating DaemonSet daemon-set
    Jan 17 15:13:16.593: INFO: Roll back the DaemonSet before rollout is complete
    Jan 17 15:13:16.603: INFO: Updating DaemonSet daemon-set
    Jan 17 15:13:16.603: INFO: Make sure DaemonSet rollback is complete
    Jan 17 15:13:16.606: INFO: Wrong image for pod: daemon-set-6jlsd. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
    Jan 17 15:13:16.606: INFO: Pod daemon-set-6jlsd is not available
    Jan 17 15:13:16.610: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 15:13:16.610: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 15:13:16.610: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 15:13:17.618: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 15:13:17.618: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 15:13:17.618: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 15:13:18.626: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 15:13:18.626: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 15:13:18.626: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 15:13:19.614: INFO: Pod daemon-set-6kk9h is not available
    Jan 17 15:13:19.619: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 15:13:19.619: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 15:13:19.619: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 01/17/23 15:13:19.625
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-833, will wait for the garbage collector to delete the pods 01/17/23 15:13:19.625
    Jan 17 15:13:19.684: INFO: Deleting DaemonSet.extensions daemon-set took: 5.906614ms
    Jan 17 15:13:19.784: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.387001ms
    Jan 17 15:13:21.388: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 17 15:13:21.388: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 17 15:13:21.391: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"86902"},"items":null}

    Jan 17 15:13:21.393: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"86902"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan 17 15:13:21.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-833" for this suite. 01/17/23 15:13:21.41
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1711
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:13:21.418
Jan 17 15:13:21.418: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename kubectl 01/17/23 15:13:21.419
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:13:21.441
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:13:21.443
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1698
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1711
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/17/23 15:13:21.446
Jan 17 15:13:21.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-3655 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2'
Jan 17 15:13:21.512: INFO: stderr: ""
Jan 17 15:13:21.512: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 01/17/23 15:13:21.512
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1702
Jan 17 15:13:21.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-3655 delete pods e2e-test-httpd-pod'
Jan 17 15:13:23.854: INFO: stderr: ""
Jan 17 15:13:23.854: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 17 15:13:23.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3655" for this suite. 01/17/23 15:13:23.858
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","completed":108,"skipped":1878,"failed":0}
------------------------------
• [2.448 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1695
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1711

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:13:21.418
    Jan 17 15:13:21.418: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename kubectl 01/17/23 15:13:21.419
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:13:21.441
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:13:21.443
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1698
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1711
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/17/23 15:13:21.446
    Jan 17 15:13:21.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-3655 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2'
    Jan 17 15:13:21.512: INFO: stderr: ""
    Jan 17 15:13:21.512: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 01/17/23 15:13:21.512
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1702
    Jan 17 15:13:21.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-3655 delete pods e2e-test-httpd-pod'
    Jan 17 15:13:23.854: INFO: stderr: ""
    Jan 17 15:13:23.854: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 17 15:13:23.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-3655" for this suite. 01/17/23 15:13:23.858
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:13:23.867
Jan 17 15:13:23.867: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename projected 01/17/23 15:13:23.867
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:13:23.893
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:13:23.898
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248
STEP: Creating a pod to test downward API volume plugin 01/17/23 15:13:23.9
Jan 17 15:13:23.923: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fcb82e77-51da-4bdb-a6c4-40f3301cbdfe" in namespace "projected-9755" to be "Succeeded or Failed"
Jan 17 15:13:23.927: INFO: Pod "downwardapi-volume-fcb82e77-51da-4bdb-a6c4-40f3301cbdfe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.346924ms
Jan 17 15:13:25.931: INFO: Pod "downwardapi-volume-fcb82e77-51da-4bdb-a6c4-40f3301cbdfe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008177457s
Jan 17 15:13:27.932: INFO: Pod "downwardapi-volume-fcb82e77-51da-4bdb-a6c4-40f3301cbdfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00902344s
STEP: Saw pod success 01/17/23 15:13:27.932
Jan 17 15:13:27.932: INFO: Pod "downwardapi-volume-fcb82e77-51da-4bdb-a6c4-40f3301cbdfe" satisfied condition "Succeeded or Failed"
Jan 17 15:13:27.935: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod downwardapi-volume-fcb82e77-51da-4bdb-a6c4-40f3301cbdfe container client-container: <nil>
STEP: delete the pod 01/17/23 15:13:27.941
Jan 17 15:13:27.954: INFO: Waiting for pod downwardapi-volume-fcb82e77-51da-4bdb-a6c4-40f3301cbdfe to disappear
Jan 17 15:13:27.957: INFO: Pod downwardapi-volume-fcb82e77-51da-4bdb-a6c4-40f3301cbdfe no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 17 15:13:27.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9755" for this suite. 01/17/23 15:13:27.961
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","completed":109,"skipped":1891,"failed":0}
------------------------------
• [4.100 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:13:23.867
    Jan 17 15:13:23.867: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename projected 01/17/23 15:13:23.867
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:13:23.893
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:13:23.898
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:248
    STEP: Creating a pod to test downward API volume plugin 01/17/23 15:13:23.9
    Jan 17 15:13:23.923: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fcb82e77-51da-4bdb-a6c4-40f3301cbdfe" in namespace "projected-9755" to be "Succeeded or Failed"
    Jan 17 15:13:23.927: INFO: Pod "downwardapi-volume-fcb82e77-51da-4bdb-a6c4-40f3301cbdfe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.346924ms
    Jan 17 15:13:25.931: INFO: Pod "downwardapi-volume-fcb82e77-51da-4bdb-a6c4-40f3301cbdfe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008177457s
    Jan 17 15:13:27.932: INFO: Pod "downwardapi-volume-fcb82e77-51da-4bdb-a6c4-40f3301cbdfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00902344s
    STEP: Saw pod success 01/17/23 15:13:27.932
    Jan 17 15:13:27.932: INFO: Pod "downwardapi-volume-fcb82e77-51da-4bdb-a6c4-40f3301cbdfe" satisfied condition "Succeeded or Failed"
    Jan 17 15:13:27.935: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod downwardapi-volume-fcb82e77-51da-4bdb-a6c4-40f3301cbdfe container client-container: <nil>
    STEP: delete the pod 01/17/23 15:13:27.941
    Jan 17 15:13:27.954: INFO: Waiting for pod downwardapi-volume-fcb82e77-51da-4bdb-a6c4-40f3301cbdfe to disappear
    Jan 17 15:13:27.957: INFO: Pod downwardapi-volume-fcb82e77-51da-4bdb-a6c4-40f3301cbdfe no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 17 15:13:27.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9755" for this suite. 01/17/23 15:13:27.961
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:13:27.967
Jan 17 15:13:27.967: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename daemonsets 01/17/23 15:13:27.968
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:13:27.991
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:13:27.995
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165
STEP: Creating simple DaemonSet "daemon-set" 01/17/23 15:13:28.025
STEP: Check that daemon pods launch on every node of the cluster. 01/17/23 15:13:28.032
Jan 17 15:13:28.036: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 15:13:28.036: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 15:13:28.036: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 15:13:28.040: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 17 15:13:28.040: INFO: Node ip-10-0-139-213.ec2.internal is running 0 daemon pod, expected 1
Jan 17 15:13:29.044: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 15:13:29.044: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 15:13:29.044: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 15:13:29.052: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 17 15:13:29.052: INFO: Node ip-10-0-139-213.ec2.internal is running 0 daemon pod, expected 1
Jan 17 15:13:30.045: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 15:13:30.045: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 15:13:30.045: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 15:13:30.048: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan 17 15:13:30.048: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 01/17/23 15:13:30.051
Jan 17 15:13:30.066: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 15:13:30.066: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 15:13:30.066: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 15:13:30.069: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 17 15:13:30.069: INFO: Node ip-10-0-151-22.ec2.internal is running 0 daemon pod, expected 1
Jan 17 15:13:31.074: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 15:13:31.074: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 15:13:31.074: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 15:13:31.078: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 17 15:13:31.078: INFO: Node ip-10-0-151-22.ec2.internal is running 0 daemon pod, expected 1
Jan 17 15:13:32.074: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 15:13:32.074: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 15:13:32.074: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 15:13:32.077: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 17 15:13:32.077: INFO: Node ip-10-0-151-22.ec2.internal is running 0 daemon pod, expected 1
Jan 17 15:13:33.075: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 15:13:33.076: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 15:13:33.076: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 15:13:33.079: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan 17 15:13:33.079: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 01/17/23 15:13:33.082
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4574, will wait for the garbage collector to delete the pods 01/17/23 15:13:33.082
Jan 17 15:13:33.143: INFO: Deleting DaemonSet.extensions daemon-set took: 8.071545ms
Jan 17 15:13:33.243: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.437124ms
Jan 17 15:13:35.947: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 17 15:13:35.947: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 17 15:13:35.950: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"87308"},"items":null}

Jan 17 15:13:35.954: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"87308"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan 17 15:13:35.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4574" for this suite. 01/17/23 15:13:35.971
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","completed":110,"skipped":1921,"failed":0}
------------------------------
• [SLOW TEST] [8.010 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:13:27.967
    Jan 17 15:13:27.967: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename daemonsets 01/17/23 15:13:27.968
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:13:27.991
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:13:27.995
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:165
    STEP: Creating simple DaemonSet "daemon-set" 01/17/23 15:13:28.025
    STEP: Check that daemon pods launch on every node of the cluster. 01/17/23 15:13:28.032
    Jan 17 15:13:28.036: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 15:13:28.036: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 15:13:28.036: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 15:13:28.040: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 17 15:13:28.040: INFO: Node ip-10-0-139-213.ec2.internal is running 0 daemon pod, expected 1
    Jan 17 15:13:29.044: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 15:13:29.044: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 15:13:29.044: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 15:13:29.052: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 17 15:13:29.052: INFO: Node ip-10-0-139-213.ec2.internal is running 0 daemon pod, expected 1
    Jan 17 15:13:30.045: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 15:13:30.045: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 15:13:30.045: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 15:13:30.048: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jan 17 15:13:30.048: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 01/17/23 15:13:30.051
    Jan 17 15:13:30.066: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 15:13:30.066: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 15:13:30.066: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 15:13:30.069: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 17 15:13:30.069: INFO: Node ip-10-0-151-22.ec2.internal is running 0 daemon pod, expected 1
    Jan 17 15:13:31.074: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 15:13:31.074: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 15:13:31.074: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 15:13:31.078: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 17 15:13:31.078: INFO: Node ip-10-0-151-22.ec2.internal is running 0 daemon pod, expected 1
    Jan 17 15:13:32.074: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 15:13:32.074: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 15:13:32.074: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 15:13:32.077: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 17 15:13:32.077: INFO: Node ip-10-0-151-22.ec2.internal is running 0 daemon pod, expected 1
    Jan 17 15:13:33.075: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 15:13:33.076: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 15:13:33.076: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 15:13:33.079: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jan 17 15:13:33.079: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 01/17/23 15:13:33.082
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4574, will wait for the garbage collector to delete the pods 01/17/23 15:13:33.082
    Jan 17 15:13:33.143: INFO: Deleting DaemonSet.extensions daemon-set took: 8.071545ms
    Jan 17 15:13:33.243: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.437124ms
    Jan 17 15:13:35.947: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 17 15:13:35.947: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 17 15:13:35.950: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"87308"},"items":null}

    Jan 17 15:13:35.954: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"87308"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan 17 15:13:35.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-4574" for this suite. 01/17/23 15:13:35.971
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:13:35.978
Jan 17 15:13:35.978: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename pods 01/17/23 15:13:35.978
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:13:36.01
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:13:36.013
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443
Jan 17 15:13:36.087: INFO: Waiting up to 5m0s for pod "server-envvars-b067c7e6-a26b-4bd3-9fdf-3fb3083d8af4" in namespace "pods-1288" to be "running and ready"
Jan 17 15:13:36.110: INFO: Pod "server-envvars-b067c7e6-a26b-4bd3-9fdf-3fb3083d8af4": Phase="Pending", Reason="", readiness=false. Elapsed: 22.342449ms
Jan 17 15:13:36.110: INFO: The phase of Pod server-envvars-b067c7e6-a26b-4bd3-9fdf-3fb3083d8af4 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 15:13:38.114: INFO: Pod "server-envvars-b067c7e6-a26b-4bd3-9fdf-3fb3083d8af4": Phase="Running", Reason="", readiness=true. Elapsed: 2.026651811s
Jan 17 15:13:38.114: INFO: The phase of Pod server-envvars-b067c7e6-a26b-4bd3-9fdf-3fb3083d8af4 is Running (Ready = true)
Jan 17 15:13:38.114: INFO: Pod "server-envvars-b067c7e6-a26b-4bd3-9fdf-3fb3083d8af4" satisfied condition "running and ready"
Jan 17 15:13:38.155: INFO: Waiting up to 5m0s for pod "client-envvars-a4c5fdd8-4444-4626-b8a4-62c633ec9972" in namespace "pods-1288" to be "Succeeded or Failed"
Jan 17 15:13:38.161: INFO: Pod "client-envvars-a4c5fdd8-4444-4626-b8a4-62c633ec9972": Phase="Pending", Reason="", readiness=false. Elapsed: 5.781749ms
Jan 17 15:13:40.164: INFO: Pod "client-envvars-a4c5fdd8-4444-4626-b8a4-62c633ec9972": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009322687s
Jan 17 15:13:42.166: INFO: Pod "client-envvars-a4c5fdd8-4444-4626-b8a4-62c633ec9972": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010751753s
STEP: Saw pod success 01/17/23 15:13:42.166
Jan 17 15:13:42.166: INFO: Pod "client-envvars-a4c5fdd8-4444-4626-b8a4-62c633ec9972" satisfied condition "Succeeded or Failed"
Jan 17 15:13:42.169: INFO: Trying to get logs from node ip-10-0-165-14.ec2.internal pod client-envvars-a4c5fdd8-4444-4626-b8a4-62c633ec9972 container env3cont: <nil>
STEP: delete the pod 01/17/23 15:13:42.175
Jan 17 15:13:42.188: INFO: Waiting for pod client-envvars-a4c5fdd8-4444-4626-b8a4-62c633ec9972 to disappear
Jan 17 15:13:42.191: INFO: Pod client-envvars-a4c5fdd8-4444-4626-b8a4-62c633ec9972 no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 17 15:13:42.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1288" for this suite. 01/17/23 15:13:42.194
{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","completed":111,"skipped":1944,"failed":0}
------------------------------
• [SLOW TEST] [6.222 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:13:35.978
    Jan 17 15:13:35.978: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename pods 01/17/23 15:13:35.978
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:13:36.01
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:13:36.013
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:443
    Jan 17 15:13:36.087: INFO: Waiting up to 5m0s for pod "server-envvars-b067c7e6-a26b-4bd3-9fdf-3fb3083d8af4" in namespace "pods-1288" to be "running and ready"
    Jan 17 15:13:36.110: INFO: Pod "server-envvars-b067c7e6-a26b-4bd3-9fdf-3fb3083d8af4": Phase="Pending", Reason="", readiness=false. Elapsed: 22.342449ms
    Jan 17 15:13:36.110: INFO: The phase of Pod server-envvars-b067c7e6-a26b-4bd3-9fdf-3fb3083d8af4 is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 15:13:38.114: INFO: Pod "server-envvars-b067c7e6-a26b-4bd3-9fdf-3fb3083d8af4": Phase="Running", Reason="", readiness=true. Elapsed: 2.026651811s
    Jan 17 15:13:38.114: INFO: The phase of Pod server-envvars-b067c7e6-a26b-4bd3-9fdf-3fb3083d8af4 is Running (Ready = true)
    Jan 17 15:13:38.114: INFO: Pod "server-envvars-b067c7e6-a26b-4bd3-9fdf-3fb3083d8af4" satisfied condition "running and ready"
    Jan 17 15:13:38.155: INFO: Waiting up to 5m0s for pod "client-envvars-a4c5fdd8-4444-4626-b8a4-62c633ec9972" in namespace "pods-1288" to be "Succeeded or Failed"
    Jan 17 15:13:38.161: INFO: Pod "client-envvars-a4c5fdd8-4444-4626-b8a4-62c633ec9972": Phase="Pending", Reason="", readiness=false. Elapsed: 5.781749ms
    Jan 17 15:13:40.164: INFO: Pod "client-envvars-a4c5fdd8-4444-4626-b8a4-62c633ec9972": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009322687s
    Jan 17 15:13:42.166: INFO: Pod "client-envvars-a4c5fdd8-4444-4626-b8a4-62c633ec9972": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010751753s
    STEP: Saw pod success 01/17/23 15:13:42.166
    Jan 17 15:13:42.166: INFO: Pod "client-envvars-a4c5fdd8-4444-4626-b8a4-62c633ec9972" satisfied condition "Succeeded or Failed"
    Jan 17 15:13:42.169: INFO: Trying to get logs from node ip-10-0-165-14.ec2.internal pod client-envvars-a4c5fdd8-4444-4626-b8a4-62c633ec9972 container env3cont: <nil>
    STEP: delete the pod 01/17/23 15:13:42.175
    Jan 17 15:13:42.188: INFO: Waiting for pod client-envvars-a4c5fdd8-4444-4626-b8a4-62c633ec9972 to disappear
    Jan 17 15:13:42.191: INFO: Pod client-envvars-a4c5fdd8-4444-4626-b8a4-62c633ec9972 no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 17 15:13:42.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-1288" for this suite. 01/17/23 15:13:42.194
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:315
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:13:42.201
Jan 17 15:13:42.201: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename statefulset 01/17/23 15:13:42.201
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:13:42.224
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:13:42.227
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-4781 01/17/23 15:13:42.229
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:315
STEP: Creating a new StatefulSet 01/17/23 15:13:42.236
Jan 17 15:13:42.251: INFO: Found 0 stateful pods, waiting for 3
Jan 17 15:13:52.255: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 17 15:13:52.255: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 17 15:13:52.255: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 01/17/23 15:13:52.264
Jan 17 15:13:52.283: INFO: Updating stateful set ss2
STEP: Creating a new revision 01/17/23 15:13:52.283
STEP: Not applying an update when the partition is greater than the number of replicas 01/17/23 15:14:02.306
STEP: Performing a canary update 01/17/23 15:14:02.306
Jan 17 15:14:02.325: INFO: Updating stateful set ss2
Jan 17 15:14:02.331: INFO: Waiting for Pod statefulset-4781/ss2-2 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
STEP: Restoring Pods to the correct revision when they are deleted 01/17/23 15:14:12.338
Jan 17 15:14:12.374: INFO: Found 1 stateful pods, waiting for 3
Jan 17 15:14:22.381: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 17 15:14:22.381: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 17 15:14:22.381: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 01/17/23 15:14:22.387
Jan 17 15:14:22.408: INFO: Updating stateful set ss2
Jan 17 15:14:22.414: INFO: Waiting for Pod statefulset-4781/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
Jan 17 15:14:32.442: INFO: Updating stateful set ss2
Jan 17 15:14:32.448: INFO: Waiting for StatefulSet statefulset-4781/ss2 to complete update
Jan 17 15:14:32.448: INFO: Waiting for Pod statefulset-4781/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 17 15:14:42.455: INFO: Deleting all statefulset in ns statefulset-4781
Jan 17 15:14:42.458: INFO: Scaling statefulset ss2 to 0
Jan 17 15:14:52.479: INFO: Waiting for statefulset status.replicas updated to 0
Jan 17 15:14:52.482: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan 17 15:14:52.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4781" for this suite. 01/17/23 15:14:52.503
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","completed":112,"skipped":1955,"failed":0}
------------------------------
• [SLOW TEST] [70.310 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:315

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:13:42.201
    Jan 17 15:13:42.201: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename statefulset 01/17/23 15:13:42.201
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:13:42.224
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:13:42.227
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-4781 01/17/23 15:13:42.229
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:315
    STEP: Creating a new StatefulSet 01/17/23 15:13:42.236
    Jan 17 15:13:42.251: INFO: Found 0 stateful pods, waiting for 3
    Jan 17 15:13:52.255: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 17 15:13:52.255: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 17 15:13:52.255: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 01/17/23 15:13:52.264
    Jan 17 15:13:52.283: INFO: Updating stateful set ss2
    STEP: Creating a new revision 01/17/23 15:13:52.283
    STEP: Not applying an update when the partition is greater than the number of replicas 01/17/23 15:14:02.306
    STEP: Performing a canary update 01/17/23 15:14:02.306
    Jan 17 15:14:02.325: INFO: Updating stateful set ss2
    Jan 17 15:14:02.331: INFO: Waiting for Pod statefulset-4781/ss2-2 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    STEP: Restoring Pods to the correct revision when they are deleted 01/17/23 15:14:12.338
    Jan 17 15:14:12.374: INFO: Found 1 stateful pods, waiting for 3
    Jan 17 15:14:22.381: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 17 15:14:22.381: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 17 15:14:22.381: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 01/17/23 15:14:22.387
    Jan 17 15:14:22.408: INFO: Updating stateful set ss2
    Jan 17 15:14:22.414: INFO: Waiting for Pod statefulset-4781/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    Jan 17 15:14:32.442: INFO: Updating stateful set ss2
    Jan 17 15:14:32.448: INFO: Waiting for StatefulSet statefulset-4781/ss2 to complete update
    Jan 17 15:14:32.448: INFO: Waiting for Pod statefulset-4781/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan 17 15:14:42.455: INFO: Deleting all statefulset in ns statefulset-4781
    Jan 17 15:14:42.458: INFO: Scaling statefulset ss2 to 0
    Jan 17 15:14:52.479: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 17 15:14:52.482: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan 17 15:14:52.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-4781" for this suite. 01/17/23 15:14:52.503
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:14:52.512
Jan 17 15:14:52.512: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename emptydir 01/17/23 15:14:52.513
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:14:52.545
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:14:52.551
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226
STEP: Creating Pod 01/17/23 15:14:52.553
Jan 17 15:14:52.577: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-25586519-878d-4611-912d-212c20dfa91e" in namespace "emptydir-9416" to be "running"
Jan 17 15:14:52.581: INFO: Pod "pod-sharedvolume-25586519-878d-4611-912d-212c20dfa91e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.748913ms
Jan 17 15:14:54.587: INFO: Pod "pod-sharedvolume-25586519-878d-4611-912d-212c20dfa91e": Phase="Running", Reason="", readiness=false. Elapsed: 2.010186921s
Jan 17 15:14:54.587: INFO: Pod "pod-sharedvolume-25586519-878d-4611-912d-212c20dfa91e" satisfied condition "running"
STEP: Reading file content from the nginx-container 01/17/23 15:14:54.587
Jan 17 15:14:54.587: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-9416 PodName:pod-sharedvolume-25586519-878d-4611-912d-212c20dfa91e ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 15:14:54.587: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
Jan 17 15:14:54.587: INFO: ExecWithOptions: Clientset creation
Jan 17 15:14:54.587: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/emptydir-9416/pods/pod-sharedvolume-25586519-878d-4611-912d-212c20dfa91e/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Jan 17 15:14:54.665: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 17 15:14:54.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9416" for this suite. 01/17/23 15:14:54.678
{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","completed":113,"skipped":2011,"failed":0}
------------------------------
• [2.173 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:14:52.512
    Jan 17 15:14:52.512: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename emptydir 01/17/23 15:14:52.513
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:14:52.545
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:14:52.551
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:226
    STEP: Creating Pod 01/17/23 15:14:52.553
    Jan 17 15:14:52.577: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-25586519-878d-4611-912d-212c20dfa91e" in namespace "emptydir-9416" to be "running"
    Jan 17 15:14:52.581: INFO: Pod "pod-sharedvolume-25586519-878d-4611-912d-212c20dfa91e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.748913ms
    Jan 17 15:14:54.587: INFO: Pod "pod-sharedvolume-25586519-878d-4611-912d-212c20dfa91e": Phase="Running", Reason="", readiness=false. Elapsed: 2.010186921s
    Jan 17 15:14:54.587: INFO: Pod "pod-sharedvolume-25586519-878d-4611-912d-212c20dfa91e" satisfied condition "running"
    STEP: Reading file content from the nginx-container 01/17/23 15:14:54.587
    Jan 17 15:14:54.587: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-9416 PodName:pod-sharedvolume-25586519-878d-4611-912d-212c20dfa91e ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 17 15:14:54.587: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    Jan 17 15:14:54.587: INFO: ExecWithOptions: Clientset creation
    Jan 17 15:14:54.587: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/emptydir-9416/pods/pod-sharedvolume-25586519-878d-4611-912d-212c20dfa91e/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    Jan 17 15:14:54.665: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 17 15:14:54.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-9416" for this suite. 01/17/23 15:14:54.678
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:14:54.686
Jan 17 15:14:54.686: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename resourcequota 01/17/23 15:14:54.686
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:14:54.71
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:14:54.713
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680
STEP: Creating a ResourceQuota with terminating scope 01/17/23 15:14:54.716
STEP: Ensuring ResourceQuota status is calculated 01/17/23 15:14:54.726
STEP: Creating a ResourceQuota with not terminating scope 01/17/23 15:14:56.73
STEP: Ensuring ResourceQuota status is calculated 01/17/23 15:14:56.736
STEP: Creating a long running pod 01/17/23 15:14:58.74
STEP: Ensuring resource quota with not terminating scope captures the pod usage 01/17/23 15:14:58.759
STEP: Ensuring resource quota with terminating scope ignored the pod usage 01/17/23 15:15:00.764
STEP: Deleting the pod 01/17/23 15:15:02.768
STEP: Ensuring resource quota status released the pod usage 01/17/23 15:15:02.781
STEP: Creating a terminating pod 01/17/23 15:15:04.784
STEP: Ensuring resource quota with terminating scope captures the pod usage 01/17/23 15:15:04.801
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 01/17/23 15:15:06.805
STEP: Deleting the pod 01/17/23 15:15:08.809
STEP: Ensuring resource quota status released the pod usage 01/17/23 15:15:08.824
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 17 15:15:10.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6144" for this suite. 01/17/23 15:15:10.832
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","completed":114,"skipped":2048,"failed":0}
------------------------------
• [SLOW TEST] [16.152 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:14:54.686
    Jan 17 15:14:54.686: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename resourcequota 01/17/23 15:14:54.686
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:14:54.71
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:14:54.713
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:680
    STEP: Creating a ResourceQuota with terminating scope 01/17/23 15:14:54.716
    STEP: Ensuring ResourceQuota status is calculated 01/17/23 15:14:54.726
    STEP: Creating a ResourceQuota with not terminating scope 01/17/23 15:14:56.73
    STEP: Ensuring ResourceQuota status is calculated 01/17/23 15:14:56.736
    STEP: Creating a long running pod 01/17/23 15:14:58.74
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 01/17/23 15:14:58.759
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 01/17/23 15:15:00.764
    STEP: Deleting the pod 01/17/23 15:15:02.768
    STEP: Ensuring resource quota status released the pod usage 01/17/23 15:15:02.781
    STEP: Creating a terminating pod 01/17/23 15:15:04.784
    STEP: Ensuring resource quota with terminating scope captures the pod usage 01/17/23 15:15:04.801
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 01/17/23 15:15:06.805
    STEP: Deleting the pod 01/17/23 15:15:08.809
    STEP: Ensuring resource quota status released the pod usage 01/17/23 15:15:08.824
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 17 15:15:10.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-6144" for this suite. 01/17/23 15:15:10.832
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:15:10.84
Jan 17 15:15:10.840: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename webhook 01/17/23 15:15:10.84
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:15:10.888
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:15:10.89
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/17/23 15:15:10.944
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/17/23 15:15:11.233
STEP: Deploying the webhook pod 01/17/23 15:15:11.24
STEP: Wait for the deployment to be ready 01/17/23 15:15:11.25
Jan 17 15:15:11.256: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/17/23 15:15:13.267
STEP: Verifying the service has paired with the endpoint 01/17/23 15:15:13.276
Jan 17 15:15:14.276: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 01/17/23 15:15:14.28
STEP: create a configmap that should be updated by the webhook 01/17/23 15:15:14.293
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 17 15:15:14.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2653" for this suite. 01/17/23 15:15:14.313
STEP: Destroying namespace "webhook-2653-markers" for this suite. 01/17/23 15:15:14.319
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","completed":115,"skipped":2112,"failed":0}
------------------------------
• [3.542 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:15:10.84
    Jan 17 15:15:10.840: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename webhook 01/17/23 15:15:10.84
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:15:10.888
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:15:10.89
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/17/23 15:15:10.944
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/17/23 15:15:11.233
    STEP: Deploying the webhook pod 01/17/23 15:15:11.24
    STEP: Wait for the deployment to be ready 01/17/23 15:15:11.25
    Jan 17 15:15:11.256: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/17/23 15:15:13.267
    STEP: Verifying the service has paired with the endpoint 01/17/23 15:15:13.276
    Jan 17 15:15:14.276: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:251
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 01/17/23 15:15:14.28
    STEP: create a configmap that should be updated by the webhook 01/17/23 15:15:14.293
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 17 15:15:14.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-2653" for this suite. 01/17/23 15:15:14.313
    STEP: Destroying namespace "webhook-2653-markers" for this suite. 01/17/23 15:15:14.319
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:15:14.383
Jan 17 15:15:14.383: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename replicaset 01/17/23 15:15:14.384
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:15:14.409
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:15:14.413
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
Jan 17 15:15:14.443: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 17 15:15:19.447: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/17/23 15:15:19.447
STEP: Scaling up "test-rs" replicaset  01/17/23 15:15:19.447
Jan 17 15:15:19.455: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 01/17/23 15:15:19.455
W0117 15:15:19.462307      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jan 17 15:15:19.463: INFO: observed ReplicaSet test-rs in namespace replicaset-5595 with ReadyReplicas 1, AvailableReplicas 1
Jan 17 15:15:19.487: INFO: observed ReplicaSet test-rs in namespace replicaset-5595 with ReadyReplicas 1, AvailableReplicas 1
Jan 17 15:15:19.532: INFO: observed ReplicaSet test-rs in namespace replicaset-5595 with ReadyReplicas 1, AvailableReplicas 1
Jan 17 15:15:19.539: INFO: observed ReplicaSet test-rs in namespace replicaset-5595 with ReadyReplicas 1, AvailableReplicas 1
Jan 17 15:15:20.635: INFO: observed ReplicaSet test-rs in namespace replicaset-5595 with ReadyReplicas 2, AvailableReplicas 2
Jan 17 15:15:20.665: INFO: observed Replicaset test-rs in namespace replicaset-5595 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jan 17 15:15:20.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-5595" for this suite. 01/17/23 15:15:20.67
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","completed":116,"skipped":2157,"failed":0}
------------------------------
• [SLOW TEST] [6.292 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:15:14.383
    Jan 17 15:15:14.383: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename replicaset 01/17/23 15:15:14.384
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:15:14.409
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:15:14.413
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    Jan 17 15:15:14.443: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan 17 15:15:19.447: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/17/23 15:15:19.447
    STEP: Scaling up "test-rs" replicaset  01/17/23 15:15:19.447
    Jan 17 15:15:19.455: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 01/17/23 15:15:19.455
    W0117 15:15:19.462307      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jan 17 15:15:19.463: INFO: observed ReplicaSet test-rs in namespace replicaset-5595 with ReadyReplicas 1, AvailableReplicas 1
    Jan 17 15:15:19.487: INFO: observed ReplicaSet test-rs in namespace replicaset-5595 with ReadyReplicas 1, AvailableReplicas 1
    Jan 17 15:15:19.532: INFO: observed ReplicaSet test-rs in namespace replicaset-5595 with ReadyReplicas 1, AvailableReplicas 1
    Jan 17 15:15:19.539: INFO: observed ReplicaSet test-rs in namespace replicaset-5595 with ReadyReplicas 1, AvailableReplicas 1
    Jan 17 15:15:20.635: INFO: observed ReplicaSet test-rs in namespace replicaset-5595 with ReadyReplicas 2, AvailableReplicas 2
    Jan 17 15:15:20.665: INFO: observed Replicaset test-rs in namespace replicaset-5595 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jan 17 15:15:20.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-5595" for this suite. 01/17/23 15:15:20.67
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:15:20.676
Jan 17 15:15:20.676: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename configmap 01/17/23 15:15:20.676
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:15:20.701
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:15:20.707
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174
Jan 17 15:15:20.715: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-305f1934-f65c-4f5e-9105-0736e1cc94e1 01/17/23 15:15:20.715
STEP: Creating the pod 01/17/23 15:15:20.72
Jan 17 15:15:20.758: INFO: Waiting up to 5m0s for pod "pod-configmaps-8dbad9e5-3de0-40b0-99a5-2e47966f47c8" in namespace "configmap-5882" to be "running"
Jan 17 15:15:20.761: INFO: Pod "pod-configmaps-8dbad9e5-3de0-40b0-99a5-2e47966f47c8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.2309ms
Jan 17 15:15:22.766: INFO: Pod "pod-configmaps-8dbad9e5-3de0-40b0-99a5-2e47966f47c8": Phase="Running", Reason="", readiness=false. Elapsed: 2.00771559s
Jan 17 15:15:22.766: INFO: Pod "pod-configmaps-8dbad9e5-3de0-40b0-99a5-2e47966f47c8" satisfied condition "running"
STEP: Waiting for pod with text data 01/17/23 15:15:22.766
STEP: Waiting for pod with binary data 01/17/23 15:15:22.775
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 17 15:15:22.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5882" for this suite. 01/17/23 15:15:22.785
{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","completed":117,"skipped":2162,"failed":0}
------------------------------
• [2.115 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:15:20.676
    Jan 17 15:15:20.676: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename configmap 01/17/23 15:15:20.676
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:15:20.701
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:15:20.707
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:174
    Jan 17 15:15:20.715: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
    STEP: Creating configMap with name configmap-test-upd-305f1934-f65c-4f5e-9105-0736e1cc94e1 01/17/23 15:15:20.715
    STEP: Creating the pod 01/17/23 15:15:20.72
    Jan 17 15:15:20.758: INFO: Waiting up to 5m0s for pod "pod-configmaps-8dbad9e5-3de0-40b0-99a5-2e47966f47c8" in namespace "configmap-5882" to be "running"
    Jan 17 15:15:20.761: INFO: Pod "pod-configmaps-8dbad9e5-3de0-40b0-99a5-2e47966f47c8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.2309ms
    Jan 17 15:15:22.766: INFO: Pod "pod-configmaps-8dbad9e5-3de0-40b0-99a5-2e47966f47c8": Phase="Running", Reason="", readiness=false. Elapsed: 2.00771559s
    Jan 17 15:15:22.766: INFO: Pod "pod-configmaps-8dbad9e5-3de0-40b0-99a5-2e47966f47c8" satisfied condition "running"
    STEP: Waiting for pod with text data 01/17/23 15:15:22.766
    STEP: Waiting for pod with binary data 01/17/23 15:15:22.775
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 17 15:15:22.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-5882" for this suite. 01/17/23 15:15:22.785
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:15:22.792
Jan 17 15:15:22.792: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename sched-pred 01/17/23 15:15:22.793
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:15:22.818
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:15:22.82
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jan 17 15:15:22.822: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 17 15:15:22.841: INFO: Waiting for terminating namespaces to be deleted...
Jan 17 15:15:22.848: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-139-213.ec2.internal before test
Jan 17 15:15:22.876: INFO: aws-ebs-csi-driver-node-5tmvr from openshift-cluster-csi-drivers started at 2023-01-17 12:55:50 +0000 UTC (3 container statuses recorded)
Jan 17 15:15:22.876: INFO: 	Container csi-driver ready: true, restart count 0
Jan 17 15:15:22.876: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Jan 17 15:15:22.876: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
Jan 17 15:15:22.876: INFO: tuned-jzlnj from openshift-cluster-node-tuning-operator started at 2023-01-17 12:55:50 +0000 UTC (1 container statuses recorded)
Jan 17 15:15:22.876: INFO: 	Container tuned ready: true, restart count 0
Jan 17 15:15:22.876: INFO: downloads-8d695cd69-ltk55 from openshift-console started at 2023-01-17 12:57:39 +0000 UTC (1 container statuses recorded)
Jan 17 15:15:22.876: INFO: 	Container download-server ready: true, restart count 0
Jan 17 15:15:22.876: INFO: dns-default-jgzr9 from openshift-dns started at 2023-01-17 12:56:45 +0000 UTC (2 container statuses recorded)
Jan 17 15:15:22.876: INFO: 	Container dns ready: true, restart count 0
Jan 17 15:15:22.876: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 17 15:15:22.876: INFO: node-resolver-gkxnp from openshift-dns started at 2023-01-17 12:55:50 +0000 UTC (1 container statuses recorded)
Jan 17 15:15:22.876: INFO: 	Container dns-node-resolver ready: true, restart count 0
Jan 17 15:15:22.876: INFO: image-registry-6d685bd45d-mk7wb from openshift-image-registry started at 2023-01-17 12:56:45 +0000 UTC (1 container statuses recorded)
Jan 17 15:15:22.876: INFO: 	Container registry ready: true, restart count 0
Jan 17 15:15:22.876: INFO: node-ca-4l49x from openshift-image-registry started at 2023-01-17 12:55:50 +0000 UTC (1 container statuses recorded)
Jan 17 15:15:22.876: INFO: 	Container node-ca ready: true, restart count 0
Jan 17 15:15:22.876: INFO: ingress-canary-xqzwm from openshift-ingress-canary started at 2023-01-17 12:56:45 +0000 UTC (1 container statuses recorded)
Jan 17 15:15:22.876: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Jan 17 15:15:22.876: INFO: router-default-c95cc587f-9clvn from openshift-ingress started at 2023-01-17 12:56:45 +0000 UTC (1 container statuses recorded)
Jan 17 15:15:22.876: INFO: 	Container router ready: true, restart count 0
Jan 17 15:15:22.876: INFO: machine-config-daemon-6hjqr from openshift-machine-config-operator started at 2023-01-17 12:55:50 +0000 UTC (2 container statuses recorded)
Jan 17 15:15:22.876: INFO: 	Container machine-config-daemon ready: true, restart count 0
Jan 17 15:15:22.876: INFO: 	Container oauth-proxy ready: true, restart count 0
Jan 17 15:15:22.876: INFO: alertmanager-main-0 from openshift-monitoring started at 2023-01-17 12:59:05 +0000 UTC (6 container statuses recorded)
Jan 17 15:15:22.876: INFO: 	Container alertmanager ready: true, restart count 1
Jan 17 15:15:22.876: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jan 17 15:15:22.876: INFO: 	Container config-reloader ready: true, restart count 0
Jan 17 15:15:22.876: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 17 15:15:22.876: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Jan 17 15:15:22.876: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jan 17 15:15:22.876: INFO: node-exporter-lwbc8 from openshift-monitoring started at 2023-01-17 12:56:55 +0000 UTC (2 container statuses recorded)
Jan 17 15:15:22.876: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 17 15:15:22.876: INFO: 	Container node-exporter ready: true, restart count 0
Jan 17 15:15:22.876: INFO: prometheus-adapter-cd9bc68fc-dstxm from openshift-monitoring started at 2023-01-17 12:57:45 +0000 UTC (1 container statuses recorded)
Jan 17 15:15:22.876: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jan 17 15:15:22.876: INFO: prometheus-k8s-0 from openshift-monitoring started at 2023-01-17 12:59:05 +0000 UTC (6 container statuses recorded)
Jan 17 15:15:22.876: INFO: 	Container config-reloader ready: true, restart count 0
Jan 17 15:15:22.876: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 17 15:15:22.876: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Jan 17 15:15:22.876: INFO: 	Container prometheus ready: true, restart count 0
Jan 17 15:15:22.876: INFO: 	Container prometheus-proxy ready: true, restart count 0
Jan 17 15:15:22.876: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jan 17 15:15:22.876: INFO: thanos-querier-5fccfc4877-prbhx from openshift-monitoring started at 2023-01-17 12:57:02 +0000 UTC (6 container statuses recorded)
Jan 17 15:15:22.876: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 17 15:15:22.876: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Jan 17 15:15:22.876: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Jan 17 15:15:22.876: INFO: 	Container oauth-proxy ready: true, restart count 0
Jan 17 15:15:22.876: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jan 17 15:15:22.876: INFO: 	Container thanos-query ready: true, restart count 0
Jan 17 15:15:22.876: INFO: multus-additional-cni-plugins-bpc6w from openshift-multus started at 2023-01-17 12:55:50 +0000 UTC (1 container statuses recorded)
Jan 17 15:15:22.876: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Jan 17 15:15:22.876: INFO: multus-tt5fs from openshift-multus started at 2023-01-17 12:55:50 +0000 UTC (1 container statuses recorded)
Jan 17 15:15:22.876: INFO: 	Container kube-multus ready: true, restart count 0
Jan 17 15:15:22.876: INFO: network-metrics-daemon-22p8j from openshift-multus started at 2023-01-17 12:55:50 +0000 UTC (2 container statuses recorded)
Jan 17 15:15:22.876: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 17 15:15:22.876: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Jan 17 15:15:22.876: INFO: network-check-target-k685r from openshift-network-diagnostics started at 2023-01-17 12:55:50 +0000 UTC (1 container statuses recorded)
Jan 17 15:15:22.876: INFO: 	Container network-check-target-container ready: true, restart count 0
Jan 17 15:15:22.876: INFO: ovnkube-node-7n8jx from openshift-ovn-kubernetes started at 2023-01-17 12:55:50 +0000 UTC (5 container statuses recorded)
Jan 17 15:15:22.876: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 17 15:15:22.876: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 0
Jan 17 15:15:22.876: INFO: 	Container ovn-acl-logging ready: true, restart count 0
Jan 17 15:15:22.876: INFO: 	Container ovn-controller ready: true, restart count 0
Jan 17 15:15:22.876: INFO: 	Container ovnkube-node ready: true, restart count 0
Jan 17 15:15:22.876: INFO: test-rs-zjl5f from replicaset-5595 started at 2023-01-17 15:15:19 +0000 UTC (2 container statuses recorded)
Jan 17 15:15:22.876: INFO: 	Container httpd ready: true, restart count 0
Jan 17 15:15:22.876: INFO: 	Container test-rs ready: true, restart count 0
Jan 17 15:15:22.876: INFO: sonobuoy-systemd-logs-daemon-set-329d7d7181d04e86-mhx5d from sonobuoy started at 2023-01-17 14:47:25 +0000 UTC (2 container statuses recorded)
Jan 17 15:15:22.876: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 17 15:15:22.876: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 17 15:15:22.876: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-151-22.ec2.internal before test
Jan 17 15:15:23.009: INFO: pod-configmaps-8dbad9e5-3de0-40b0-99a5-2e47966f47c8 from configmap-5882 started at 2023-01-17 15:15:20 +0000 UTC (2 container statuses recorded)
Jan 17 15:15:23.009: INFO: 	Container agnhost-container ready: true, restart count 0
Jan 17 15:15:23.009: INFO: 	Container configmap-volume-binary-test ready: false, restart count 0
Jan 17 15:15:23.009: INFO: aws-ebs-csi-driver-node-mx922 from openshift-cluster-csi-drivers started at 2023-01-17 12:51:50 +0000 UTC (3 container statuses recorded)
Jan 17 15:15:23.009: INFO: 	Container csi-driver ready: true, restart count 0
Jan 17 15:15:23.009: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Jan 17 15:15:23.009: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Jan 17 15:15:23.009: INFO: tuned-9hhcs from openshift-cluster-node-tuning-operator started at 2023-01-17 12:51:50 +0000 UTC (1 container statuses recorded)
Jan 17 15:15:23.009: INFO: 	Container tuned ready: true, restart count 0
Jan 17 15:15:23.009: INFO: downloads-8d695cd69-w4jfq from openshift-console started at 2023-01-17 12:57:39 +0000 UTC (1 container statuses recorded)
Jan 17 15:15:23.009: INFO: 	Container download-server ready: true, restart count 0
Jan 17 15:15:23.009: INFO: dns-default-d977v from openshift-dns started at 2023-01-17 12:52:21 +0000 UTC (2 container statuses recorded)
Jan 17 15:15:23.009: INFO: 	Container dns ready: true, restart count 0
Jan 17 15:15:23.009: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 17 15:15:23.009: INFO: node-resolver-fxksv from openshift-dns started at 2023-01-17 12:51:50 +0000 UTC (1 container statuses recorded)
Jan 17 15:15:23.009: INFO: 	Container dns-node-resolver ready: true, restart count 0
Jan 17 15:15:23.009: INFO: node-ca-tpp6r from openshift-image-registry started at 2023-01-17 12:54:40 +0000 UTC (1 container statuses recorded)
Jan 17 15:15:23.009: INFO: 	Container node-ca ready: true, restart count 0
Jan 17 15:15:23.009: INFO: ingress-canary-d45j9 from openshift-ingress-canary started at 2023-01-17 12:54:09 +0000 UTC (1 container statuses recorded)
Jan 17 15:15:23.009: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Jan 17 15:15:23.009: INFO: router-default-c95cc587f-24fmv from openshift-ingress started at 2023-01-17 12:52:21 +0000 UTC (1 container statuses recorded)
Jan 17 15:15:23.009: INFO: 	Container router ready: true, restart count 1
Jan 17 15:15:23.009: INFO: machine-config-daemon-bgrjb from openshift-machine-config-operator started at 2023-01-17 12:51:50 +0000 UTC (2 container statuses recorded)
Jan 17 15:15:23.009: INFO: 	Container machine-config-daemon ready: true, restart count 0
Jan 17 15:15:23.009: INFO: 	Container oauth-proxy ready: true, restart count 0
Jan 17 15:15:23.009: INFO: alertmanager-main-1 from openshift-monitoring started at 2023-01-17 12:58:32 +0000 UTC (6 container statuses recorded)
Jan 17 15:15:23.009: INFO: 	Container alertmanager ready: true, restart count 1
Jan 17 15:15:23.009: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jan 17 15:15:23.009: INFO: 	Container config-reloader ready: true, restart count 0
Jan 17 15:15:23.009: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 17 15:15:23.009: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Jan 17 15:15:23.009: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jan 17 15:15:23.009: INFO: node-exporter-snggg from openshift-monitoring started at 2023-01-17 12:56:55 +0000 UTC (2 container statuses recorded)
Jan 17 15:15:23.009: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 17 15:15:23.009: INFO: 	Container node-exporter ready: true, restart count 0
Jan 17 15:15:23.009: INFO: prometheus-adapter-cd9bc68fc-fr52q from openshift-monitoring started at 2023-01-17 12:57:45 +0000 UTC (1 container statuses recorded)
Jan 17 15:15:23.009: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jan 17 15:15:23.009: INFO: prometheus-operator-admission-webhook-7d4759d465-skshz from openshift-monitoring started at 2023-01-17 12:52:21 +0000 UTC (1 container statuses recorded)
Jan 17 15:15:23.009: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Jan 17 15:15:23.009: INFO: multus-additional-cni-plugins-grzml from openshift-multus started at 2023-01-17 12:51:50 +0000 UTC (1 container statuses recorded)
Jan 17 15:15:23.009: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Jan 17 15:15:23.009: INFO: multus-qspfd from openshift-multus started at 2023-01-17 12:51:50 +0000 UTC (1 container statuses recorded)
Jan 17 15:15:23.009: INFO: 	Container kube-multus ready: true, restart count 0
Jan 17 15:15:23.009: INFO: network-metrics-daemon-x4zrh from openshift-multus started at 2023-01-17 12:51:50 +0000 UTC (2 container statuses recorded)
Jan 17 15:15:23.009: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 17 15:15:23.009: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Jan 17 15:15:23.009: INFO: network-check-source-746dd6c885-ndrxh from openshift-network-diagnostics started at 2023-01-17 12:52:21 +0000 UTC (1 container statuses recorded)
Jan 17 15:15:23.009: INFO: 	Container check-endpoints ready: true, restart count 0
Jan 17 15:15:23.009: INFO: network-check-target-f794b from openshift-network-diagnostics started at 2023-01-17 12:51:50 +0000 UTC (1 container statuses recorded)
Jan 17 15:15:23.009: INFO: 	Container network-check-target-container ready: true, restart count 0
Jan 17 15:15:23.009: INFO: collect-profiles-27899445-5rxw6 from openshift-operator-lifecycle-manager started at 2023-01-17 14:45:00 +0000 UTC (1 container statuses recorded)
Jan 17 15:15:23.009: INFO: 	Container collect-profiles ready: false, restart count 0
Jan 17 15:15:23.009: INFO: collect-profiles-27899460-6mxgm from openshift-operator-lifecycle-manager started at 2023-01-17 15:00:00 +0000 UTC (1 container statuses recorded)
Jan 17 15:15:23.009: INFO: 	Container collect-profiles ready: false, restart count 0
Jan 17 15:15:23.009: INFO: collect-profiles-27899475-99859 from openshift-operator-lifecycle-manager started at 2023-01-17 15:15:00 +0000 UTC (1 container statuses recorded)
Jan 17 15:15:23.009: INFO: 	Container collect-profiles ready: false, restart count 0
Jan 17 15:15:23.009: INFO: ovnkube-node-bks72 from openshift-ovn-kubernetes started at 2023-01-17 12:51:50 +0000 UTC (5 container statuses recorded)
Jan 17 15:15:23.009: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 17 15:15:23.009: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 0
Jan 17 15:15:23.009: INFO: 	Container ovn-acl-logging ready: true, restart count 0
Jan 17 15:15:23.009: INFO: 	Container ovn-controller ready: true, restart count 0
Jan 17 15:15:23.009: INFO: 	Container ovnkube-node ready: true, restart count 0
Jan 17 15:15:23.009: INFO: test-rs-r2std from replicaset-5595 started at 2023-01-17 15:15:14 +0000 UTC (1 container statuses recorded)
Jan 17 15:15:23.009: INFO: 	Container httpd ready: true, restart count 0
Jan 17 15:15:23.009: INFO: sonobuoy from sonobuoy started at 2023-01-17 14:47:22 +0000 UTC (1 container statuses recorded)
Jan 17 15:15:23.009: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 17 15:15:23.009: INFO: sonobuoy-e2e-job-0a9b408aa2f34bad from sonobuoy started at 2023-01-17 14:47:25 +0000 UTC (2 container statuses recorded)
Jan 17 15:15:23.009: INFO: 	Container e2e ready: true, restart count 0
Jan 17 15:15:23.009: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 17 15:15:23.009: INFO: sonobuoy-systemd-logs-daemon-set-329d7d7181d04e86-8knzd from sonobuoy started at 2023-01-17 14:47:25 +0000 UTC (2 container statuses recorded)
Jan 17 15:15:23.009: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 17 15:15:23.009: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 17 15:15:23.009: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-165-14.ec2.internal before test
Jan 17 15:15:23.146: INFO: aws-ebs-csi-driver-node-x9fxp from openshift-cluster-csi-drivers started at 2023-01-17 12:55:53 +0000 UTC (3 container statuses recorded)
Jan 17 15:15:23.146: INFO: 	Container csi-driver ready: true, restart count 0
Jan 17 15:15:23.146: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Jan 17 15:15:23.146: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Jan 17 15:15:23.146: INFO: tuned-cgskr from openshift-cluster-node-tuning-operator started at 2023-01-17 12:55:53 +0000 UTC (1 container statuses recorded)
Jan 17 15:15:23.146: INFO: 	Container tuned ready: true, restart count 0
Jan 17 15:15:23.146: INFO: dns-default-8q4m7 from openshift-dns started at 2023-01-17 12:56:45 +0000 UTC (2 container statuses recorded)
Jan 17 15:15:23.146: INFO: 	Container dns ready: true, restart count 0
Jan 17 15:15:23.146: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 17 15:15:23.146: INFO: node-resolver-dvw44 from openshift-dns started at 2023-01-17 12:55:53 +0000 UTC (1 container statuses recorded)
Jan 17 15:15:23.146: INFO: 	Container dns-node-resolver ready: true, restart count 0
Jan 17 15:15:23.146: INFO: image-registry-6d685bd45d-g2mxt from openshift-image-registry started at 2023-01-17 12:56:45 +0000 UTC (1 container statuses recorded)
Jan 17 15:15:23.146: INFO: 	Container registry ready: true, restart count 0
Jan 17 15:15:23.146: INFO: node-ca-45nfl from openshift-image-registry started at 2023-01-17 12:55:53 +0000 UTC (1 container statuses recorded)
Jan 17 15:15:23.146: INFO: 	Container node-ca ready: true, restart count 0
Jan 17 15:15:23.146: INFO: ingress-canary-q6p6t from openshift-ingress-canary started at 2023-01-17 12:56:45 +0000 UTC (1 container statuses recorded)
Jan 17 15:15:23.146: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Jan 17 15:15:23.146: INFO: machine-config-daemon-v7v4j from openshift-machine-config-operator started at 2023-01-17 12:55:53 +0000 UTC (2 container statuses recorded)
Jan 17 15:15:23.146: INFO: 	Container machine-config-daemon ready: true, restart count 0
Jan 17 15:15:23.146: INFO: 	Container oauth-proxy ready: true, restart count 0
Jan 17 15:15:23.146: INFO: kube-state-metrics-75455b796c-rgbdl from openshift-monitoring started at 2023-01-17 12:56:55 +0000 UTC (3 container statuses recorded)
Jan 17 15:15:23.146: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jan 17 15:15:23.146: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jan 17 15:15:23.146: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jan 17 15:15:23.146: INFO: node-exporter-pdxfc from openshift-monitoring started at 2023-01-17 12:56:55 +0000 UTC (2 container statuses recorded)
Jan 17 15:15:23.146: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 17 15:15:23.146: INFO: 	Container node-exporter ready: true, restart count 0
Jan 17 15:15:23.146: INFO: openshift-state-metrics-5ff95d844f-dl27q from openshift-monitoring started at 2023-01-17 12:56:55 +0000 UTC (3 container statuses recorded)
Jan 17 15:15:23.146: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jan 17 15:15:23.146: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jan 17 15:15:23.146: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Jan 17 15:15:23.146: INFO: prometheus-k8s-1 from openshift-monitoring started at 2023-01-17 12:58:37 +0000 UTC (6 container statuses recorded)
Jan 17 15:15:23.146: INFO: 	Container config-reloader ready: true, restart count 0
Jan 17 15:15:23.146: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 17 15:15:23.146: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Jan 17 15:15:23.146: INFO: 	Container prometheus ready: true, restart count 0
Jan 17 15:15:23.146: INFO: 	Container prometheus-proxy ready: true, restart count 0
Jan 17 15:15:23.146: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jan 17 15:15:23.146: INFO: prometheus-operator-admission-webhook-7d4759d465-bw8tr from openshift-monitoring started at 2023-01-17 12:56:45 +0000 UTC (1 container statuses recorded)
Jan 17 15:15:23.146: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Jan 17 15:15:23.146: INFO: telemeter-client-585b5cf5bf-zlltk from openshift-monitoring started at 2023-01-17 12:57:00 +0000 UTC (3 container statuses recorded)
Jan 17 15:15:23.146: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 17 15:15:23.146: INFO: 	Container reload ready: true, restart count 0
Jan 17 15:15:23.146: INFO: 	Container telemeter-client ready: true, restart count 0
Jan 17 15:15:23.146: INFO: thanos-querier-5fccfc4877-lc5nk from openshift-monitoring started at 2023-01-17 12:57:02 +0000 UTC (6 container statuses recorded)
Jan 17 15:15:23.146: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 17 15:15:23.146: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Jan 17 15:15:23.146: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Jan 17 15:15:23.146: INFO: 	Container oauth-proxy ready: true, restart count 0
Jan 17 15:15:23.146: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jan 17 15:15:23.146: INFO: 	Container thanos-query ready: true, restart count 0
Jan 17 15:15:23.146: INFO: multus-additional-cni-plugins-zg84j from openshift-multus started at 2023-01-17 12:55:53 +0000 UTC (1 container statuses recorded)
Jan 17 15:15:23.146: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Jan 17 15:15:23.146: INFO: multus-jq6qc from openshift-multus started at 2023-01-17 12:55:53 +0000 UTC (1 container statuses recorded)
Jan 17 15:15:23.146: INFO: 	Container kube-multus ready: true, restart count 0
Jan 17 15:15:23.146: INFO: network-metrics-daemon-gngg5 from openshift-multus started at 2023-01-17 12:55:53 +0000 UTC (2 container statuses recorded)
Jan 17 15:15:23.146: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 17 15:15:23.146: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Jan 17 15:15:23.146: INFO: network-check-target-v9g9x from openshift-network-diagnostics started at 2023-01-17 12:55:53 +0000 UTC (1 container statuses recorded)
Jan 17 15:15:23.146: INFO: 	Container network-check-target-container ready: true, restart count 0
Jan 17 15:15:23.146: INFO: ovnkube-node-q54mr from openshift-ovn-kubernetes started at 2023-01-17 12:55:53 +0000 UTC (5 container statuses recorded)
Jan 17 15:15:23.146: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 17 15:15:23.146: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 0
Jan 17 15:15:23.146: INFO: 	Container ovn-acl-logging ready: true, restart count 0
Jan 17 15:15:23.146: INFO: 	Container ovn-controller ready: true, restart count 0
Jan 17 15:15:23.146: INFO: 	Container ovnkube-node ready: true, restart count 0
Jan 17 15:15:23.146: INFO: test-rs-6vm6z from replicaset-5595 started at 2023-01-17 15:15:19 +0000 UTC (1 container statuses recorded)
Jan 17 15:15:23.146: INFO: 	Container httpd ready: true, restart count 0
Jan 17 15:15:23.146: INFO: sonobuoy-systemd-logs-daemon-set-329d7d7181d04e86-qcrqh from sonobuoy started at 2023-01-17 14:47:25 +0000 UTC (2 container statuses recorded)
Jan 17 15:15:23.146: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 17 15:15:23.146: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461
STEP: Trying to launch a pod without a label to get a node which can launch it. 01/17/23 15:15:23.146
Jan 17 15:15:23.162: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-7734" to be "running"
Jan 17 15:15:23.167: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 5.069856ms
Jan 17 15:15:25.171: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.008690328s
Jan 17 15:15:25.171: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 01/17/23 15:15:25.174
STEP: Trying to apply a random label on the found node. 01/17/23 15:15:25.195
STEP: verifying the node has the label kubernetes.io/e2e-1b25c85d-cc89-4a95-ae5e-9b7acf0a64c9 42 01/17/23 15:15:25.207
STEP: Trying to relaunch the pod, now with labels. 01/17/23 15:15:25.212
Jan 17 15:15:25.234: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-7734" to be "not pending"
Jan 17 15:15:25.245: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 10.5882ms
Jan 17 15:15:27.249: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.014464213s
Jan 17 15:15:27.249: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-1b25c85d-cc89-4a95-ae5e-9b7acf0a64c9 off the node ip-10-0-165-14.ec2.internal 01/17/23 15:15:27.252
STEP: verifying the node doesn't have the label kubernetes.io/e2e-1b25c85d-cc89-4a95-ae5e-9b7acf0a64c9 01/17/23 15:15:27.263
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Jan 17 15:15:27.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7734" for this suite. 01/17/23 15:15:27.276
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","completed":118,"skipped":2201,"failed":0}
------------------------------
• [4.495 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:15:22.792
    Jan 17 15:15:22.792: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename sched-pred 01/17/23 15:15:22.793
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:15:22.818
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:15:22.82
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Jan 17 15:15:22.822: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan 17 15:15:22.841: INFO: Waiting for terminating namespaces to be deleted...
    Jan 17 15:15:22.848: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-139-213.ec2.internal before test
    Jan 17 15:15:22.876: INFO: aws-ebs-csi-driver-node-5tmvr from openshift-cluster-csi-drivers started at 2023-01-17 12:55:50 +0000 UTC (3 container statuses recorded)
    Jan 17 15:15:22.876: INFO: 	Container csi-driver ready: true, restart count 0
    Jan 17 15:15:22.876: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    Jan 17 15:15:22.876: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
    Jan 17 15:15:22.876: INFO: tuned-jzlnj from openshift-cluster-node-tuning-operator started at 2023-01-17 12:55:50 +0000 UTC (1 container statuses recorded)
    Jan 17 15:15:22.876: INFO: 	Container tuned ready: true, restart count 0
    Jan 17 15:15:22.876: INFO: downloads-8d695cd69-ltk55 from openshift-console started at 2023-01-17 12:57:39 +0000 UTC (1 container statuses recorded)
    Jan 17 15:15:22.876: INFO: 	Container download-server ready: true, restart count 0
    Jan 17 15:15:22.876: INFO: dns-default-jgzr9 from openshift-dns started at 2023-01-17 12:56:45 +0000 UTC (2 container statuses recorded)
    Jan 17 15:15:22.876: INFO: 	Container dns ready: true, restart count 0
    Jan 17 15:15:22.876: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 17 15:15:22.876: INFO: node-resolver-gkxnp from openshift-dns started at 2023-01-17 12:55:50 +0000 UTC (1 container statuses recorded)
    Jan 17 15:15:22.876: INFO: 	Container dns-node-resolver ready: true, restart count 0
    Jan 17 15:15:22.876: INFO: image-registry-6d685bd45d-mk7wb from openshift-image-registry started at 2023-01-17 12:56:45 +0000 UTC (1 container statuses recorded)
    Jan 17 15:15:22.876: INFO: 	Container registry ready: true, restart count 0
    Jan 17 15:15:22.876: INFO: node-ca-4l49x from openshift-image-registry started at 2023-01-17 12:55:50 +0000 UTC (1 container statuses recorded)
    Jan 17 15:15:22.876: INFO: 	Container node-ca ready: true, restart count 0
    Jan 17 15:15:22.876: INFO: ingress-canary-xqzwm from openshift-ingress-canary started at 2023-01-17 12:56:45 +0000 UTC (1 container statuses recorded)
    Jan 17 15:15:22.876: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    Jan 17 15:15:22.876: INFO: router-default-c95cc587f-9clvn from openshift-ingress started at 2023-01-17 12:56:45 +0000 UTC (1 container statuses recorded)
    Jan 17 15:15:22.876: INFO: 	Container router ready: true, restart count 0
    Jan 17 15:15:22.876: INFO: machine-config-daemon-6hjqr from openshift-machine-config-operator started at 2023-01-17 12:55:50 +0000 UTC (2 container statuses recorded)
    Jan 17 15:15:22.876: INFO: 	Container machine-config-daemon ready: true, restart count 0
    Jan 17 15:15:22.876: INFO: 	Container oauth-proxy ready: true, restart count 0
    Jan 17 15:15:22.876: INFO: alertmanager-main-0 from openshift-monitoring started at 2023-01-17 12:59:05 +0000 UTC (6 container statuses recorded)
    Jan 17 15:15:22.876: INFO: 	Container alertmanager ready: true, restart count 1
    Jan 17 15:15:22.876: INFO: 	Container alertmanager-proxy ready: true, restart count 0
    Jan 17 15:15:22.876: INFO: 	Container config-reloader ready: true, restart count 0
    Jan 17 15:15:22.876: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 17 15:15:22.876: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
    Jan 17 15:15:22.876: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Jan 17 15:15:22.876: INFO: node-exporter-lwbc8 from openshift-monitoring started at 2023-01-17 12:56:55 +0000 UTC (2 container statuses recorded)
    Jan 17 15:15:22.876: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 17 15:15:22.876: INFO: 	Container node-exporter ready: true, restart count 0
    Jan 17 15:15:22.876: INFO: prometheus-adapter-cd9bc68fc-dstxm from openshift-monitoring started at 2023-01-17 12:57:45 +0000 UTC (1 container statuses recorded)
    Jan 17 15:15:22.876: INFO: 	Container prometheus-adapter ready: true, restart count 0
    Jan 17 15:15:22.876: INFO: prometheus-k8s-0 from openshift-monitoring started at 2023-01-17 12:59:05 +0000 UTC (6 container statuses recorded)
    Jan 17 15:15:22.876: INFO: 	Container config-reloader ready: true, restart count 0
    Jan 17 15:15:22.876: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 17 15:15:22.876: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
    Jan 17 15:15:22.876: INFO: 	Container prometheus ready: true, restart count 0
    Jan 17 15:15:22.876: INFO: 	Container prometheus-proxy ready: true, restart count 0
    Jan 17 15:15:22.876: INFO: 	Container thanos-sidecar ready: true, restart count 0
    Jan 17 15:15:22.876: INFO: thanos-querier-5fccfc4877-prbhx from openshift-monitoring started at 2023-01-17 12:57:02 +0000 UTC (6 container statuses recorded)
    Jan 17 15:15:22.876: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 17 15:15:22.876: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
    Jan 17 15:15:22.876: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
    Jan 17 15:15:22.876: INFO: 	Container oauth-proxy ready: true, restart count 0
    Jan 17 15:15:22.876: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Jan 17 15:15:22.876: INFO: 	Container thanos-query ready: true, restart count 0
    Jan 17 15:15:22.876: INFO: multus-additional-cni-plugins-bpc6w from openshift-multus started at 2023-01-17 12:55:50 +0000 UTC (1 container statuses recorded)
    Jan 17 15:15:22.876: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    Jan 17 15:15:22.876: INFO: multus-tt5fs from openshift-multus started at 2023-01-17 12:55:50 +0000 UTC (1 container statuses recorded)
    Jan 17 15:15:22.876: INFO: 	Container kube-multus ready: true, restart count 0
    Jan 17 15:15:22.876: INFO: network-metrics-daemon-22p8j from openshift-multus started at 2023-01-17 12:55:50 +0000 UTC (2 container statuses recorded)
    Jan 17 15:15:22.876: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 17 15:15:22.876: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    Jan 17 15:15:22.876: INFO: network-check-target-k685r from openshift-network-diagnostics started at 2023-01-17 12:55:50 +0000 UTC (1 container statuses recorded)
    Jan 17 15:15:22.876: INFO: 	Container network-check-target-container ready: true, restart count 0
    Jan 17 15:15:22.876: INFO: ovnkube-node-7n8jx from openshift-ovn-kubernetes started at 2023-01-17 12:55:50 +0000 UTC (5 container statuses recorded)
    Jan 17 15:15:22.876: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 17 15:15:22.876: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 0
    Jan 17 15:15:22.876: INFO: 	Container ovn-acl-logging ready: true, restart count 0
    Jan 17 15:15:22.876: INFO: 	Container ovn-controller ready: true, restart count 0
    Jan 17 15:15:22.876: INFO: 	Container ovnkube-node ready: true, restart count 0
    Jan 17 15:15:22.876: INFO: test-rs-zjl5f from replicaset-5595 started at 2023-01-17 15:15:19 +0000 UTC (2 container statuses recorded)
    Jan 17 15:15:22.876: INFO: 	Container httpd ready: true, restart count 0
    Jan 17 15:15:22.876: INFO: 	Container test-rs ready: true, restart count 0
    Jan 17 15:15:22.876: INFO: sonobuoy-systemd-logs-daemon-set-329d7d7181d04e86-mhx5d from sonobuoy started at 2023-01-17 14:47:25 +0000 UTC (2 container statuses recorded)
    Jan 17 15:15:22.876: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 17 15:15:22.876: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 17 15:15:22.876: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-151-22.ec2.internal before test
    Jan 17 15:15:23.009: INFO: pod-configmaps-8dbad9e5-3de0-40b0-99a5-2e47966f47c8 from configmap-5882 started at 2023-01-17 15:15:20 +0000 UTC (2 container statuses recorded)
    Jan 17 15:15:23.009: INFO: 	Container agnhost-container ready: true, restart count 0
    Jan 17 15:15:23.009: INFO: 	Container configmap-volume-binary-test ready: false, restart count 0
    Jan 17 15:15:23.009: INFO: aws-ebs-csi-driver-node-mx922 from openshift-cluster-csi-drivers started at 2023-01-17 12:51:50 +0000 UTC (3 container statuses recorded)
    Jan 17 15:15:23.009: INFO: 	Container csi-driver ready: true, restart count 0
    Jan 17 15:15:23.009: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    Jan 17 15:15:23.009: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Jan 17 15:15:23.009: INFO: tuned-9hhcs from openshift-cluster-node-tuning-operator started at 2023-01-17 12:51:50 +0000 UTC (1 container statuses recorded)
    Jan 17 15:15:23.009: INFO: 	Container tuned ready: true, restart count 0
    Jan 17 15:15:23.009: INFO: downloads-8d695cd69-w4jfq from openshift-console started at 2023-01-17 12:57:39 +0000 UTC (1 container statuses recorded)
    Jan 17 15:15:23.009: INFO: 	Container download-server ready: true, restart count 0
    Jan 17 15:15:23.009: INFO: dns-default-d977v from openshift-dns started at 2023-01-17 12:52:21 +0000 UTC (2 container statuses recorded)
    Jan 17 15:15:23.009: INFO: 	Container dns ready: true, restart count 0
    Jan 17 15:15:23.009: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 17 15:15:23.009: INFO: node-resolver-fxksv from openshift-dns started at 2023-01-17 12:51:50 +0000 UTC (1 container statuses recorded)
    Jan 17 15:15:23.009: INFO: 	Container dns-node-resolver ready: true, restart count 0
    Jan 17 15:15:23.009: INFO: node-ca-tpp6r from openshift-image-registry started at 2023-01-17 12:54:40 +0000 UTC (1 container statuses recorded)
    Jan 17 15:15:23.009: INFO: 	Container node-ca ready: true, restart count 0
    Jan 17 15:15:23.009: INFO: ingress-canary-d45j9 from openshift-ingress-canary started at 2023-01-17 12:54:09 +0000 UTC (1 container statuses recorded)
    Jan 17 15:15:23.009: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    Jan 17 15:15:23.009: INFO: router-default-c95cc587f-24fmv from openshift-ingress started at 2023-01-17 12:52:21 +0000 UTC (1 container statuses recorded)
    Jan 17 15:15:23.009: INFO: 	Container router ready: true, restart count 1
    Jan 17 15:15:23.009: INFO: machine-config-daemon-bgrjb from openshift-machine-config-operator started at 2023-01-17 12:51:50 +0000 UTC (2 container statuses recorded)
    Jan 17 15:15:23.009: INFO: 	Container machine-config-daemon ready: true, restart count 0
    Jan 17 15:15:23.009: INFO: 	Container oauth-proxy ready: true, restart count 0
    Jan 17 15:15:23.009: INFO: alertmanager-main-1 from openshift-monitoring started at 2023-01-17 12:58:32 +0000 UTC (6 container statuses recorded)
    Jan 17 15:15:23.009: INFO: 	Container alertmanager ready: true, restart count 1
    Jan 17 15:15:23.009: INFO: 	Container alertmanager-proxy ready: true, restart count 0
    Jan 17 15:15:23.009: INFO: 	Container config-reloader ready: true, restart count 0
    Jan 17 15:15:23.009: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 17 15:15:23.009: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
    Jan 17 15:15:23.009: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Jan 17 15:15:23.009: INFO: node-exporter-snggg from openshift-monitoring started at 2023-01-17 12:56:55 +0000 UTC (2 container statuses recorded)
    Jan 17 15:15:23.009: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 17 15:15:23.009: INFO: 	Container node-exporter ready: true, restart count 0
    Jan 17 15:15:23.009: INFO: prometheus-adapter-cd9bc68fc-fr52q from openshift-monitoring started at 2023-01-17 12:57:45 +0000 UTC (1 container statuses recorded)
    Jan 17 15:15:23.009: INFO: 	Container prometheus-adapter ready: true, restart count 0
    Jan 17 15:15:23.009: INFO: prometheus-operator-admission-webhook-7d4759d465-skshz from openshift-monitoring started at 2023-01-17 12:52:21 +0000 UTC (1 container statuses recorded)
    Jan 17 15:15:23.009: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
    Jan 17 15:15:23.009: INFO: multus-additional-cni-plugins-grzml from openshift-multus started at 2023-01-17 12:51:50 +0000 UTC (1 container statuses recorded)
    Jan 17 15:15:23.009: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    Jan 17 15:15:23.009: INFO: multus-qspfd from openshift-multus started at 2023-01-17 12:51:50 +0000 UTC (1 container statuses recorded)
    Jan 17 15:15:23.009: INFO: 	Container kube-multus ready: true, restart count 0
    Jan 17 15:15:23.009: INFO: network-metrics-daemon-x4zrh from openshift-multus started at 2023-01-17 12:51:50 +0000 UTC (2 container statuses recorded)
    Jan 17 15:15:23.009: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 17 15:15:23.009: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    Jan 17 15:15:23.009: INFO: network-check-source-746dd6c885-ndrxh from openshift-network-diagnostics started at 2023-01-17 12:52:21 +0000 UTC (1 container statuses recorded)
    Jan 17 15:15:23.009: INFO: 	Container check-endpoints ready: true, restart count 0
    Jan 17 15:15:23.009: INFO: network-check-target-f794b from openshift-network-diagnostics started at 2023-01-17 12:51:50 +0000 UTC (1 container statuses recorded)
    Jan 17 15:15:23.009: INFO: 	Container network-check-target-container ready: true, restart count 0
    Jan 17 15:15:23.009: INFO: collect-profiles-27899445-5rxw6 from openshift-operator-lifecycle-manager started at 2023-01-17 14:45:00 +0000 UTC (1 container statuses recorded)
    Jan 17 15:15:23.009: INFO: 	Container collect-profiles ready: false, restart count 0
    Jan 17 15:15:23.009: INFO: collect-profiles-27899460-6mxgm from openshift-operator-lifecycle-manager started at 2023-01-17 15:00:00 +0000 UTC (1 container statuses recorded)
    Jan 17 15:15:23.009: INFO: 	Container collect-profiles ready: false, restart count 0
    Jan 17 15:15:23.009: INFO: collect-profiles-27899475-99859 from openshift-operator-lifecycle-manager started at 2023-01-17 15:15:00 +0000 UTC (1 container statuses recorded)
    Jan 17 15:15:23.009: INFO: 	Container collect-profiles ready: false, restart count 0
    Jan 17 15:15:23.009: INFO: ovnkube-node-bks72 from openshift-ovn-kubernetes started at 2023-01-17 12:51:50 +0000 UTC (5 container statuses recorded)
    Jan 17 15:15:23.009: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 17 15:15:23.009: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 0
    Jan 17 15:15:23.009: INFO: 	Container ovn-acl-logging ready: true, restart count 0
    Jan 17 15:15:23.009: INFO: 	Container ovn-controller ready: true, restart count 0
    Jan 17 15:15:23.009: INFO: 	Container ovnkube-node ready: true, restart count 0
    Jan 17 15:15:23.009: INFO: test-rs-r2std from replicaset-5595 started at 2023-01-17 15:15:14 +0000 UTC (1 container statuses recorded)
    Jan 17 15:15:23.009: INFO: 	Container httpd ready: true, restart count 0
    Jan 17 15:15:23.009: INFO: sonobuoy from sonobuoy started at 2023-01-17 14:47:22 +0000 UTC (1 container statuses recorded)
    Jan 17 15:15:23.009: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan 17 15:15:23.009: INFO: sonobuoy-e2e-job-0a9b408aa2f34bad from sonobuoy started at 2023-01-17 14:47:25 +0000 UTC (2 container statuses recorded)
    Jan 17 15:15:23.009: INFO: 	Container e2e ready: true, restart count 0
    Jan 17 15:15:23.009: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 17 15:15:23.009: INFO: sonobuoy-systemd-logs-daemon-set-329d7d7181d04e86-8knzd from sonobuoy started at 2023-01-17 14:47:25 +0000 UTC (2 container statuses recorded)
    Jan 17 15:15:23.009: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 17 15:15:23.009: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 17 15:15:23.009: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-165-14.ec2.internal before test
    Jan 17 15:15:23.146: INFO: aws-ebs-csi-driver-node-x9fxp from openshift-cluster-csi-drivers started at 2023-01-17 12:55:53 +0000 UTC (3 container statuses recorded)
    Jan 17 15:15:23.146: INFO: 	Container csi-driver ready: true, restart count 0
    Jan 17 15:15:23.146: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    Jan 17 15:15:23.146: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Jan 17 15:15:23.146: INFO: tuned-cgskr from openshift-cluster-node-tuning-operator started at 2023-01-17 12:55:53 +0000 UTC (1 container statuses recorded)
    Jan 17 15:15:23.146: INFO: 	Container tuned ready: true, restart count 0
    Jan 17 15:15:23.146: INFO: dns-default-8q4m7 from openshift-dns started at 2023-01-17 12:56:45 +0000 UTC (2 container statuses recorded)
    Jan 17 15:15:23.146: INFO: 	Container dns ready: true, restart count 0
    Jan 17 15:15:23.146: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 17 15:15:23.146: INFO: node-resolver-dvw44 from openshift-dns started at 2023-01-17 12:55:53 +0000 UTC (1 container statuses recorded)
    Jan 17 15:15:23.146: INFO: 	Container dns-node-resolver ready: true, restart count 0
    Jan 17 15:15:23.146: INFO: image-registry-6d685bd45d-g2mxt from openshift-image-registry started at 2023-01-17 12:56:45 +0000 UTC (1 container statuses recorded)
    Jan 17 15:15:23.146: INFO: 	Container registry ready: true, restart count 0
    Jan 17 15:15:23.146: INFO: node-ca-45nfl from openshift-image-registry started at 2023-01-17 12:55:53 +0000 UTC (1 container statuses recorded)
    Jan 17 15:15:23.146: INFO: 	Container node-ca ready: true, restart count 0
    Jan 17 15:15:23.146: INFO: ingress-canary-q6p6t from openshift-ingress-canary started at 2023-01-17 12:56:45 +0000 UTC (1 container statuses recorded)
    Jan 17 15:15:23.146: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    Jan 17 15:15:23.146: INFO: machine-config-daemon-v7v4j from openshift-machine-config-operator started at 2023-01-17 12:55:53 +0000 UTC (2 container statuses recorded)
    Jan 17 15:15:23.146: INFO: 	Container machine-config-daemon ready: true, restart count 0
    Jan 17 15:15:23.146: INFO: 	Container oauth-proxy ready: true, restart count 0
    Jan 17 15:15:23.146: INFO: kube-state-metrics-75455b796c-rgbdl from openshift-monitoring started at 2023-01-17 12:56:55 +0000 UTC (3 container statuses recorded)
    Jan 17 15:15:23.146: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
    Jan 17 15:15:23.146: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
    Jan 17 15:15:23.146: INFO: 	Container kube-state-metrics ready: true, restart count 0
    Jan 17 15:15:23.146: INFO: node-exporter-pdxfc from openshift-monitoring started at 2023-01-17 12:56:55 +0000 UTC (2 container statuses recorded)
    Jan 17 15:15:23.146: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 17 15:15:23.146: INFO: 	Container node-exporter ready: true, restart count 0
    Jan 17 15:15:23.146: INFO: openshift-state-metrics-5ff95d844f-dl27q from openshift-monitoring started at 2023-01-17 12:56:55 +0000 UTC (3 container statuses recorded)
    Jan 17 15:15:23.146: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
    Jan 17 15:15:23.146: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
    Jan 17 15:15:23.146: INFO: 	Container openshift-state-metrics ready: true, restart count 0
    Jan 17 15:15:23.146: INFO: prometheus-k8s-1 from openshift-monitoring started at 2023-01-17 12:58:37 +0000 UTC (6 container statuses recorded)
    Jan 17 15:15:23.146: INFO: 	Container config-reloader ready: true, restart count 0
    Jan 17 15:15:23.146: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 17 15:15:23.146: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
    Jan 17 15:15:23.146: INFO: 	Container prometheus ready: true, restart count 0
    Jan 17 15:15:23.146: INFO: 	Container prometheus-proxy ready: true, restart count 0
    Jan 17 15:15:23.146: INFO: 	Container thanos-sidecar ready: true, restart count 0
    Jan 17 15:15:23.146: INFO: prometheus-operator-admission-webhook-7d4759d465-bw8tr from openshift-monitoring started at 2023-01-17 12:56:45 +0000 UTC (1 container statuses recorded)
    Jan 17 15:15:23.146: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
    Jan 17 15:15:23.146: INFO: telemeter-client-585b5cf5bf-zlltk from openshift-monitoring started at 2023-01-17 12:57:00 +0000 UTC (3 container statuses recorded)
    Jan 17 15:15:23.146: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 17 15:15:23.146: INFO: 	Container reload ready: true, restart count 0
    Jan 17 15:15:23.146: INFO: 	Container telemeter-client ready: true, restart count 0
    Jan 17 15:15:23.146: INFO: thanos-querier-5fccfc4877-lc5nk from openshift-monitoring started at 2023-01-17 12:57:02 +0000 UTC (6 container statuses recorded)
    Jan 17 15:15:23.146: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 17 15:15:23.146: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
    Jan 17 15:15:23.146: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
    Jan 17 15:15:23.146: INFO: 	Container oauth-proxy ready: true, restart count 0
    Jan 17 15:15:23.146: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Jan 17 15:15:23.146: INFO: 	Container thanos-query ready: true, restart count 0
    Jan 17 15:15:23.146: INFO: multus-additional-cni-plugins-zg84j from openshift-multus started at 2023-01-17 12:55:53 +0000 UTC (1 container statuses recorded)
    Jan 17 15:15:23.146: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    Jan 17 15:15:23.146: INFO: multus-jq6qc from openshift-multus started at 2023-01-17 12:55:53 +0000 UTC (1 container statuses recorded)
    Jan 17 15:15:23.146: INFO: 	Container kube-multus ready: true, restart count 0
    Jan 17 15:15:23.146: INFO: network-metrics-daemon-gngg5 from openshift-multus started at 2023-01-17 12:55:53 +0000 UTC (2 container statuses recorded)
    Jan 17 15:15:23.146: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 17 15:15:23.146: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    Jan 17 15:15:23.146: INFO: network-check-target-v9g9x from openshift-network-diagnostics started at 2023-01-17 12:55:53 +0000 UTC (1 container statuses recorded)
    Jan 17 15:15:23.146: INFO: 	Container network-check-target-container ready: true, restart count 0
    Jan 17 15:15:23.146: INFO: ovnkube-node-q54mr from openshift-ovn-kubernetes started at 2023-01-17 12:55:53 +0000 UTC (5 container statuses recorded)
    Jan 17 15:15:23.146: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 17 15:15:23.146: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 0
    Jan 17 15:15:23.146: INFO: 	Container ovn-acl-logging ready: true, restart count 0
    Jan 17 15:15:23.146: INFO: 	Container ovn-controller ready: true, restart count 0
    Jan 17 15:15:23.146: INFO: 	Container ovnkube-node ready: true, restart count 0
    Jan 17 15:15:23.146: INFO: test-rs-6vm6z from replicaset-5595 started at 2023-01-17 15:15:19 +0000 UTC (1 container statuses recorded)
    Jan 17 15:15:23.146: INFO: 	Container httpd ready: true, restart count 0
    Jan 17 15:15:23.146: INFO: sonobuoy-systemd-logs-daemon-set-329d7d7181d04e86-qcrqh from sonobuoy started at 2023-01-17 14:47:25 +0000 UTC (2 container statuses recorded)
    Jan 17 15:15:23.146: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 17 15:15:23.146: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:461
    STEP: Trying to launch a pod without a label to get a node which can launch it. 01/17/23 15:15:23.146
    Jan 17 15:15:23.162: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-7734" to be "running"
    Jan 17 15:15:23.167: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 5.069856ms
    Jan 17 15:15:25.171: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.008690328s
    Jan 17 15:15:25.171: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 01/17/23 15:15:25.174
    STEP: Trying to apply a random label on the found node. 01/17/23 15:15:25.195
    STEP: verifying the node has the label kubernetes.io/e2e-1b25c85d-cc89-4a95-ae5e-9b7acf0a64c9 42 01/17/23 15:15:25.207
    STEP: Trying to relaunch the pod, now with labels. 01/17/23 15:15:25.212
    Jan 17 15:15:25.234: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-7734" to be "not pending"
    Jan 17 15:15:25.245: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 10.5882ms
    Jan 17 15:15:27.249: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.014464213s
    Jan 17 15:15:27.249: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-1b25c85d-cc89-4a95-ae5e-9b7acf0a64c9 off the node ip-10-0-165-14.ec2.internal 01/17/23 15:15:27.252
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-1b25c85d-cc89-4a95-ae5e-9b7acf0a64c9 01/17/23 15:15:27.263
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Jan 17 15:15:27.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-7734" for this suite. 01/17/23 15:15:27.276
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:15:27.288
Jan 17 15:15:27.288: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename svcaccounts 01/17/23 15:15:27.288
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:15:27.323
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:15:27.325
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75
Jan 17 15:15:27.359: INFO: Waiting up to 5m0s for pod "pod-service-account-bb403824-e84a-4920-b1b8-bc99368f3386" in namespace "svcaccounts-8347" to be "running"
Jan 17 15:15:27.366: INFO: Pod "pod-service-account-bb403824-e84a-4920-b1b8-bc99368f3386": Phase="Pending", Reason="", readiness=false. Elapsed: 6.642475ms
Jan 17 15:15:29.370: INFO: Pod "pod-service-account-bb403824-e84a-4920-b1b8-bc99368f3386": Phase="Running", Reason="", readiness=true. Elapsed: 2.010978824s
Jan 17 15:15:29.370: INFO: Pod "pod-service-account-bb403824-e84a-4920-b1b8-bc99368f3386" satisfied condition "running"
STEP: reading a file in the container 01/17/23 15:15:29.37
Jan 17 15:15:29.370: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8347 pod-service-account-bb403824-e84a-4920-b1b8-bc99368f3386 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 01/17/23 15:15:29.472
Jan 17 15:15:29.472: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8347 pod-service-account-bb403824-e84a-4920-b1b8-bc99368f3386 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 01/17/23 15:15:29.566
Jan 17 15:15:29.566: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8347 pod-service-account-bb403824-e84a-4920-b1b8-bc99368f3386 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Jan 17 15:15:29.737: INFO: Got root ca configmap in namespace "svcaccounts-8347"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jan 17 15:15:29.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8347" for this suite. 01/17/23 15:15:29.744
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","completed":119,"skipped":2213,"failed":0}
------------------------------
• [2.462 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:15:27.288
    Jan 17 15:15:27.288: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename svcaccounts 01/17/23 15:15:27.288
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:15:27.323
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:15:27.325
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:75
    Jan 17 15:15:27.359: INFO: Waiting up to 5m0s for pod "pod-service-account-bb403824-e84a-4920-b1b8-bc99368f3386" in namespace "svcaccounts-8347" to be "running"
    Jan 17 15:15:27.366: INFO: Pod "pod-service-account-bb403824-e84a-4920-b1b8-bc99368f3386": Phase="Pending", Reason="", readiness=false. Elapsed: 6.642475ms
    Jan 17 15:15:29.370: INFO: Pod "pod-service-account-bb403824-e84a-4920-b1b8-bc99368f3386": Phase="Running", Reason="", readiness=true. Elapsed: 2.010978824s
    Jan 17 15:15:29.370: INFO: Pod "pod-service-account-bb403824-e84a-4920-b1b8-bc99368f3386" satisfied condition "running"
    STEP: reading a file in the container 01/17/23 15:15:29.37
    Jan 17 15:15:29.370: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8347 pod-service-account-bb403824-e84a-4920-b1b8-bc99368f3386 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 01/17/23 15:15:29.472
    Jan 17 15:15:29.472: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8347 pod-service-account-bb403824-e84a-4920-b1b8-bc99368f3386 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 01/17/23 15:15:29.566
    Jan 17 15:15:29.566: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8347 pod-service-account-bb403824-e84a-4920-b1b8-bc99368f3386 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    Jan 17 15:15:29.737: INFO: Got root ca configmap in namespace "svcaccounts-8347"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jan 17 15:15:29.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-8347" for this suite. 01/17/23 15:15:29.744
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:15:29.751
Jan 17 15:15:29.751: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename deployment 01/17/23 15:15:29.752
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:15:29.789
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:15:29.793
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
Jan 17 15:15:29.795: INFO: Creating simple deployment test-new-deployment
W0117 15:15:29.802177      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 17 15:15:29.814: INFO: deployment "test-new-deployment" doesn't have the required revision set
STEP: getting scale subresource 01/17/23 15:15:31.826
STEP: updating a scale subresource 01/17/23 15:15:31.829
STEP: verifying the deployment Spec.Replicas was modified 01/17/23 15:15:31.834
STEP: Patch a scale subresource 01/17/23 15:15:31.837
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 17 15:15:31.849: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-5450  9234ac65-d3aa-4d2d-abae-10c3f4147250 89210 3 2023-01-17 15:15:29 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-01-17 15:15:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 15:15:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00661b138 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-17 15:15:31 +0000 UTC,LastTransitionTime:2023-01-17 15:15:31 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-845c8977d9" has successfully progressed.,LastUpdateTime:2023-01-17 15:15:31 +0000 UTC,LastTransitionTime:2023-01-17 15:15:29 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 17 15:15:31.854: INFO: New ReplicaSet "test-new-deployment-845c8977d9" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-845c8977d9  deployment-5450  66c8543d-5eb0-4861-9b00-fae744631af1 89209 2 2023-01-17 15:15:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 9234ac65-d3aa-4d2d-abae-10c3f4147250 0xc006639da7 0xc006639da8}] [] [{kube-controller-manager Update apps/v1 2023-01-17 15:15:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9234ac65-d3aa-4d2d-abae-10c3f4147250\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 15:15:31 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006639e38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 17 15:15:31.857: INFO: Pod "test-new-deployment-845c8977d9-ghb8j" is not available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-ghb8j test-new-deployment-845c8977d9- deployment-5450  ba231c75-efac-4c3d-b281-d415119866b9 89213 0 2023-01-17 15:15:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 66c8543d-5eb0-4861-9b00-fae744631af1 0xc0066e8257 0xc0066e8258}] [] [{kube-controller-manager Update v1 2023-01-17 15:15:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"66c8543d-5eb0-4861-9b00-fae744631af1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6mvwp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6mvwp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c44,c29,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-nchbx,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 15:15:31.857: INFO: Pod "test-new-deployment-845c8977d9-lhshl" is available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-lhshl test-new-deployment-845c8977d9- deployment-5450  10a051ba-7f02-4acc-8dac-b7f680b8906a 89205 0 2023-01-17 15:15:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.131.0.108/23"],"mac_address":"0a:58:0a:83:00:6c","gateway_ips":["10.131.0.1"],"ip_address":"10.131.0.108/23","gateway_ip":"10.131.0.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.131.0.108"
    ],
    "mac": "0a:58:0a:83:00:6c",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.131.0.108"
    ],
    "mac": "0a:58:0a:83:00:6c",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 66c8543d-5eb0-4861-9b00-fae744631af1 0xc0066e83f7 0xc0066e83f8}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:15:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:15:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"66c8543d-5eb0-4861-9b00-fae744631af1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-17 15:15:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-17 15:15:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.0.108\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4h9wj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4h9wj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-151-22.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c44,c29,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.151.22,PodIP:10.131.0.108,StartTime:2023-01-17 15:15:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 15:15:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://1747c2dfe0720c377337c196483d67233b17da284e40092a6be897d64b8a15f5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.131.0.108,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan 17 15:15:31.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5450" for this suite. 01/17/23 15:15:31.865
{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","completed":120,"skipped":2250,"failed":0}
------------------------------
• [2.124 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:15:29.751
    Jan 17 15:15:29.751: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename deployment 01/17/23 15:15:29.752
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:15:29.789
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:15:29.793
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    Jan 17 15:15:29.795: INFO: Creating simple deployment test-new-deployment
    W0117 15:15:29.802177      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jan 17 15:15:29.814: INFO: deployment "test-new-deployment" doesn't have the required revision set
    STEP: getting scale subresource 01/17/23 15:15:31.826
    STEP: updating a scale subresource 01/17/23 15:15:31.829
    STEP: verifying the deployment Spec.Replicas was modified 01/17/23 15:15:31.834
    STEP: Patch a scale subresource 01/17/23 15:15:31.837
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 17 15:15:31.849: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-5450  9234ac65-d3aa-4d2d-abae-10c3f4147250 89210 3 2023-01-17 15:15:29 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-01-17 15:15:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 15:15:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00661b138 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-17 15:15:31 +0000 UTC,LastTransitionTime:2023-01-17 15:15:31 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-845c8977d9" has successfully progressed.,LastUpdateTime:2023-01-17 15:15:31 +0000 UTC,LastTransitionTime:2023-01-17 15:15:29 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan 17 15:15:31.854: INFO: New ReplicaSet "test-new-deployment-845c8977d9" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-845c8977d9  deployment-5450  66c8543d-5eb0-4861-9b00-fae744631af1 89209 2 2023-01-17 15:15:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 9234ac65-d3aa-4d2d-abae-10c3f4147250 0xc006639da7 0xc006639da8}] [] [{kube-controller-manager Update apps/v1 2023-01-17 15:15:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9234ac65-d3aa-4d2d-abae-10c3f4147250\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 15:15:31 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006639e38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 17 15:15:31.857: INFO: Pod "test-new-deployment-845c8977d9-ghb8j" is not available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-ghb8j test-new-deployment-845c8977d9- deployment-5450  ba231c75-efac-4c3d-b281-d415119866b9 89213 0 2023-01-17 15:15:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 66c8543d-5eb0-4861-9b00-fae744631af1 0xc0066e8257 0xc0066e8258}] [] [{kube-controller-manager Update v1 2023-01-17 15:15:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"66c8543d-5eb0-4861-9b00-fae744631af1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6mvwp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6mvwp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c44,c29,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-nchbx,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 17 15:15:31.857: INFO: Pod "test-new-deployment-845c8977d9-lhshl" is available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-lhshl test-new-deployment-845c8977d9- deployment-5450  10a051ba-7f02-4acc-8dac-b7f680b8906a 89205 0 2023-01-17 15:15:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.131.0.108/23"],"mac_address":"0a:58:0a:83:00:6c","gateway_ips":["10.131.0.1"],"ip_address":"10.131.0.108/23","gateway_ip":"10.131.0.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.131.0.108"
        ],
        "mac": "0a:58:0a:83:00:6c",
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.131.0.108"
        ],
        "mac": "0a:58:0a:83:00:6c",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 66c8543d-5eb0-4861-9b00-fae744631af1 0xc0066e83f7 0xc0066e83f8}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:15:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:15:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"66c8543d-5eb0-4861-9b00-fae744631af1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-17 15:15:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-17 15:15:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.0.108\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4h9wj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4h9wj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-151-22.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c44,c29,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.151.22,PodIP:10.131.0.108,StartTime:2023-01-17 15:15:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 15:15:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://1747c2dfe0720c377337c196483d67233b17da284e40092a6be897d64b8a15f5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.131.0.108,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan 17 15:15:31.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-5450" for this suite. 01/17/23 15:15:31.865
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:15:31.875
Jan 17 15:15:31.875: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename dns 01/17/23 15:15:31.876
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:15:31.903
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:15:31.907
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 01/17/23 15:15:31.91
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6123.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6123.svc.cluster.local; sleep 1; done
 01/17/23 15:15:31.923
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6123.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6123.svc.cluster.local; sleep 1; done
 01/17/23 15:15:31.923
STEP: creating a pod to probe DNS 01/17/23 15:15:31.923
STEP: submitting the pod to kubernetes 01/17/23 15:15:31.923
Jan 17 15:15:31.942: INFO: Waiting up to 15m0s for pod "dns-test-6424dccd-3997-4d91-a034-5e9764cede5f" in namespace "dns-6123" to be "running"
Jan 17 15:15:31.945: INFO: Pod "dns-test-6424dccd-3997-4d91-a034-5e9764cede5f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.919311ms
Jan 17 15:15:33.948: INFO: Pod "dns-test-6424dccd-3997-4d91-a034-5e9764cede5f": Phase="Running", Reason="", readiness=true. Elapsed: 2.006582761s
Jan 17 15:15:33.948: INFO: Pod "dns-test-6424dccd-3997-4d91-a034-5e9764cede5f" satisfied condition "running"
STEP: retrieving the pod 01/17/23 15:15:33.948
STEP: looking for the results for each expected name from probers 01/17/23 15:15:33.952
Jan 17 15:15:33.960: INFO: DNS probes using dns-test-6424dccd-3997-4d91-a034-5e9764cede5f succeeded

STEP: deleting the pod 01/17/23 15:15:33.96
STEP: changing the externalName to bar.example.com 01/17/23 15:15:33.973
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6123.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6123.svc.cluster.local; sleep 1; done
 01/17/23 15:15:33.982
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6123.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6123.svc.cluster.local; sleep 1; done
 01/17/23 15:15:33.982
STEP: creating a second pod to probe DNS 01/17/23 15:15:33.982
STEP: submitting the pod to kubernetes 01/17/23 15:15:33.982
Jan 17 15:15:33.995: INFO: Waiting up to 15m0s for pod "dns-test-d8d0ddda-0dd5-4546-9746-c05d1f255383" in namespace "dns-6123" to be "running"
Jan 17 15:15:34.014: INFO: Pod "dns-test-d8d0ddda-0dd5-4546-9746-c05d1f255383": Phase="Pending", Reason="", readiness=false. Elapsed: 19.12391ms
Jan 17 15:15:36.019: INFO: Pod "dns-test-d8d0ddda-0dd5-4546-9746-c05d1f255383": Phase="Running", Reason="", readiness=true. Elapsed: 2.024146099s
Jan 17 15:15:36.019: INFO: Pod "dns-test-d8d0ddda-0dd5-4546-9746-c05d1f255383" satisfied condition "running"
STEP: retrieving the pod 01/17/23 15:15:36.019
STEP: looking for the results for each expected name from probers 01/17/23 15:15:36.022
Jan 17 15:15:36.028: INFO: File wheezy_udp@dns-test-service-3.dns-6123.svc.cluster.local from pod  dns-6123/dns-test-d8d0ddda-0dd5-4546-9746-c05d1f255383 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 17 15:15:36.032: INFO: File jessie_udp@dns-test-service-3.dns-6123.svc.cluster.local from pod  dns-6123/dns-test-d8d0ddda-0dd5-4546-9746-c05d1f255383 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 17 15:15:36.032: INFO: Lookups using dns-6123/dns-test-d8d0ddda-0dd5-4546-9746-c05d1f255383 failed for: [wheezy_udp@dns-test-service-3.dns-6123.svc.cluster.local jessie_udp@dns-test-service-3.dns-6123.svc.cluster.local]

Jan 17 15:15:41.040: INFO: DNS probes using dns-test-d8d0ddda-0dd5-4546-9746-c05d1f255383 succeeded

STEP: deleting the pod 01/17/23 15:15:41.04
STEP: changing the service to type=ClusterIP 01/17/23 15:15:41.053
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6123.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-6123.svc.cluster.local; sleep 1; done
 01/17/23 15:15:41.07
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6123.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-6123.svc.cluster.local; sleep 1; done
 01/17/23 15:15:41.07
STEP: creating a third pod to probe DNS 01/17/23 15:15:41.07
STEP: submitting the pod to kubernetes 01/17/23 15:15:41.073
Jan 17 15:15:41.087: INFO: Waiting up to 15m0s for pod "dns-test-b6b053fd-4f2f-4e7c-b9fb-8beb502eb794" in namespace "dns-6123" to be "running"
Jan 17 15:15:41.091: INFO: Pod "dns-test-b6b053fd-4f2f-4e7c-b9fb-8beb502eb794": Phase="Pending", Reason="", readiness=false. Elapsed: 3.233488ms
Jan 17 15:15:43.094: INFO: Pod "dns-test-b6b053fd-4f2f-4e7c-b9fb-8beb502eb794": Phase="Running", Reason="", readiness=true. Elapsed: 2.006996723s
Jan 17 15:15:43.094: INFO: Pod "dns-test-b6b053fd-4f2f-4e7c-b9fb-8beb502eb794" satisfied condition "running"
STEP: retrieving the pod 01/17/23 15:15:43.094
STEP: looking for the results for each expected name from probers 01/17/23 15:15:43.098
Jan 17 15:15:43.107: INFO: DNS probes using dns-test-b6b053fd-4f2f-4e7c-b9fb-8beb502eb794 succeeded

STEP: deleting the pod 01/17/23 15:15:43.107
STEP: deleting the test externalName service 01/17/23 15:15:43.12
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan 17 15:15:43.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6123" for this suite. 01/17/23 15:15:43.157
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","completed":121,"skipped":2263,"failed":0}
------------------------------
• [SLOW TEST] [11.292 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:15:31.875
    Jan 17 15:15:31.875: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename dns 01/17/23 15:15:31.876
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:15:31.903
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:15:31.907
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 01/17/23 15:15:31.91
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6123.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6123.svc.cluster.local; sleep 1; done
     01/17/23 15:15:31.923
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6123.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6123.svc.cluster.local; sleep 1; done
     01/17/23 15:15:31.923
    STEP: creating a pod to probe DNS 01/17/23 15:15:31.923
    STEP: submitting the pod to kubernetes 01/17/23 15:15:31.923
    Jan 17 15:15:31.942: INFO: Waiting up to 15m0s for pod "dns-test-6424dccd-3997-4d91-a034-5e9764cede5f" in namespace "dns-6123" to be "running"
    Jan 17 15:15:31.945: INFO: Pod "dns-test-6424dccd-3997-4d91-a034-5e9764cede5f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.919311ms
    Jan 17 15:15:33.948: INFO: Pod "dns-test-6424dccd-3997-4d91-a034-5e9764cede5f": Phase="Running", Reason="", readiness=true. Elapsed: 2.006582761s
    Jan 17 15:15:33.948: INFO: Pod "dns-test-6424dccd-3997-4d91-a034-5e9764cede5f" satisfied condition "running"
    STEP: retrieving the pod 01/17/23 15:15:33.948
    STEP: looking for the results for each expected name from probers 01/17/23 15:15:33.952
    Jan 17 15:15:33.960: INFO: DNS probes using dns-test-6424dccd-3997-4d91-a034-5e9764cede5f succeeded

    STEP: deleting the pod 01/17/23 15:15:33.96
    STEP: changing the externalName to bar.example.com 01/17/23 15:15:33.973
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6123.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6123.svc.cluster.local; sleep 1; done
     01/17/23 15:15:33.982
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6123.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6123.svc.cluster.local; sleep 1; done
     01/17/23 15:15:33.982
    STEP: creating a second pod to probe DNS 01/17/23 15:15:33.982
    STEP: submitting the pod to kubernetes 01/17/23 15:15:33.982
    Jan 17 15:15:33.995: INFO: Waiting up to 15m0s for pod "dns-test-d8d0ddda-0dd5-4546-9746-c05d1f255383" in namespace "dns-6123" to be "running"
    Jan 17 15:15:34.014: INFO: Pod "dns-test-d8d0ddda-0dd5-4546-9746-c05d1f255383": Phase="Pending", Reason="", readiness=false. Elapsed: 19.12391ms
    Jan 17 15:15:36.019: INFO: Pod "dns-test-d8d0ddda-0dd5-4546-9746-c05d1f255383": Phase="Running", Reason="", readiness=true. Elapsed: 2.024146099s
    Jan 17 15:15:36.019: INFO: Pod "dns-test-d8d0ddda-0dd5-4546-9746-c05d1f255383" satisfied condition "running"
    STEP: retrieving the pod 01/17/23 15:15:36.019
    STEP: looking for the results for each expected name from probers 01/17/23 15:15:36.022
    Jan 17 15:15:36.028: INFO: File wheezy_udp@dns-test-service-3.dns-6123.svc.cluster.local from pod  dns-6123/dns-test-d8d0ddda-0dd5-4546-9746-c05d1f255383 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan 17 15:15:36.032: INFO: File jessie_udp@dns-test-service-3.dns-6123.svc.cluster.local from pod  dns-6123/dns-test-d8d0ddda-0dd5-4546-9746-c05d1f255383 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan 17 15:15:36.032: INFO: Lookups using dns-6123/dns-test-d8d0ddda-0dd5-4546-9746-c05d1f255383 failed for: [wheezy_udp@dns-test-service-3.dns-6123.svc.cluster.local jessie_udp@dns-test-service-3.dns-6123.svc.cluster.local]

    Jan 17 15:15:41.040: INFO: DNS probes using dns-test-d8d0ddda-0dd5-4546-9746-c05d1f255383 succeeded

    STEP: deleting the pod 01/17/23 15:15:41.04
    STEP: changing the service to type=ClusterIP 01/17/23 15:15:41.053
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6123.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-6123.svc.cluster.local; sleep 1; done
     01/17/23 15:15:41.07
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6123.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-6123.svc.cluster.local; sleep 1; done
     01/17/23 15:15:41.07
    STEP: creating a third pod to probe DNS 01/17/23 15:15:41.07
    STEP: submitting the pod to kubernetes 01/17/23 15:15:41.073
    Jan 17 15:15:41.087: INFO: Waiting up to 15m0s for pod "dns-test-b6b053fd-4f2f-4e7c-b9fb-8beb502eb794" in namespace "dns-6123" to be "running"
    Jan 17 15:15:41.091: INFO: Pod "dns-test-b6b053fd-4f2f-4e7c-b9fb-8beb502eb794": Phase="Pending", Reason="", readiness=false. Elapsed: 3.233488ms
    Jan 17 15:15:43.094: INFO: Pod "dns-test-b6b053fd-4f2f-4e7c-b9fb-8beb502eb794": Phase="Running", Reason="", readiness=true. Elapsed: 2.006996723s
    Jan 17 15:15:43.094: INFO: Pod "dns-test-b6b053fd-4f2f-4e7c-b9fb-8beb502eb794" satisfied condition "running"
    STEP: retrieving the pod 01/17/23 15:15:43.094
    STEP: looking for the results for each expected name from probers 01/17/23 15:15:43.098
    Jan 17 15:15:43.107: INFO: DNS probes using dns-test-b6b053fd-4f2f-4e7c-b9fb-8beb502eb794 succeeded

    STEP: deleting the pod 01/17/23 15:15:43.107
    STEP: deleting the test externalName service 01/17/23 15:15:43.12
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan 17 15:15:43.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-6123" for this suite. 01/17/23 15:15:43.157
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:15:43.168
Jan 17 15:15:43.168: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename webhook 01/17/23 15:15:43.169
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:15:43.208
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:15:43.211
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/17/23 15:15:43.241
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/17/23 15:15:43.766
STEP: Deploying the webhook pod 01/17/23 15:15:43.777
STEP: Wait for the deployment to be ready 01/17/23 15:15:43.788
Jan 17 15:15:43.797: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/17/23 15:15:45.807
STEP: Verifying the service has paired with the endpoint 01/17/23 15:15:45.817
Jan 17 15:15:46.817: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655
STEP: Listing all of the created validation webhooks 01/17/23 15:15:46.876
STEP: Creating a configMap that should be mutated 01/17/23 15:15:46.887
STEP: Deleting the collection of validation webhooks 01/17/23 15:15:46.913
STEP: Creating a configMap that should not be mutated 01/17/23 15:15:46.973
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 17 15:15:46.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8568" for this suite. 01/17/23 15:15:46.994
STEP: Destroying namespace "webhook-8568-markers" for this suite. 01/17/23 15:15:47.006
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","completed":122,"skipped":2277,"failed":0}
------------------------------
• [3.915 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:15:43.168
    Jan 17 15:15:43.168: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename webhook 01/17/23 15:15:43.169
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:15:43.208
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:15:43.211
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/17/23 15:15:43.241
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/17/23 15:15:43.766
    STEP: Deploying the webhook pod 01/17/23 15:15:43.777
    STEP: Wait for the deployment to be ready 01/17/23 15:15:43.788
    Jan 17 15:15:43.797: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/17/23 15:15:45.807
    STEP: Verifying the service has paired with the endpoint 01/17/23 15:15:45.817
    Jan 17 15:15:46.817: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:655
    STEP: Listing all of the created validation webhooks 01/17/23 15:15:46.876
    STEP: Creating a configMap that should be mutated 01/17/23 15:15:46.887
    STEP: Deleting the collection of validation webhooks 01/17/23 15:15:46.913
    STEP: Creating a configMap that should not be mutated 01/17/23 15:15:46.973
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 17 15:15:46.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-8568" for this suite. 01/17/23 15:15:46.994
    STEP: Destroying namespace "webhook-8568-markers" for this suite. 01/17/23 15:15:47.006
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:15:47.083
Jan 17 15:15:47.083: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename pods 01/17/23 15:15:47.083
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:15:47.138
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:15:47.141
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844
STEP: Create set of pods 01/17/23 15:15:47.144
Jan 17 15:15:47.205: INFO: created test-pod-1
Jan 17 15:15:47.241: INFO: created test-pod-2
Jan 17 15:15:47.257: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 01/17/23 15:15:47.257
Jan 17 15:15:47.257: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-5962' to be running and ready
Jan 17 15:15:47.274: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 17 15:15:47.274: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 17 15:15:47.274: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 17 15:15:47.274: INFO: 0 / 3 pods in namespace 'pods-5962' are running and ready (0 seconds elapsed)
Jan 17 15:15:47.274: INFO: expected 0 pod replicas in namespace 'pods-5962', 0 are Running and Ready.
Jan 17 15:15:47.274: INFO: POD         NODE                         PHASE    GRACE  CONDITIONS
Jan 17 15:15:47.274: INFO: test-pod-1  ip-10-0-151-22.ec2.internal  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 15:15:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 15:15:47 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 15:15:47 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 15:15:47 +0000 UTC  }]
Jan 17 15:15:47.274: INFO: test-pod-2  ip-10-0-165-14.ec2.internal  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 15:15:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 15:15:47 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 15:15:47 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 15:15:47 +0000 UTC  }]
Jan 17 15:15:47.274: INFO: test-pod-3  ip-10-0-151-22.ec2.internal  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 15:15:47 +0000 UTC  }]
Jan 17 15:15:47.274: INFO: 
Jan 17 15:15:49.284: INFO: 3 / 3 pods in namespace 'pods-5962' are running and ready (2 seconds elapsed)
Jan 17 15:15:49.284: INFO: expected 0 pod replicas in namespace 'pods-5962', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 01/17/23 15:15:49.306
Jan 17 15:15:49.309: INFO: Pod quantity 3 is different from expected quantity 0
Jan 17 15:15:50.313: INFO: Pod quantity 3 is different from expected quantity 0
Jan 17 15:15:51.312: INFO: Pod quantity 1 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 17 15:15:52.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5962" for this suite. 01/17/23 15:15:52.317
{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","completed":123,"skipped":2279,"failed":0}
------------------------------
• [SLOW TEST] [5.241 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:15:47.083
    Jan 17 15:15:47.083: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename pods 01/17/23 15:15:47.083
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:15:47.138
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:15:47.141
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:844
    STEP: Create set of pods 01/17/23 15:15:47.144
    Jan 17 15:15:47.205: INFO: created test-pod-1
    Jan 17 15:15:47.241: INFO: created test-pod-2
    Jan 17 15:15:47.257: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 01/17/23 15:15:47.257
    Jan 17 15:15:47.257: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-5962' to be running and ready
    Jan 17 15:15:47.274: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 17 15:15:47.274: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 17 15:15:47.274: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 17 15:15:47.274: INFO: 0 / 3 pods in namespace 'pods-5962' are running and ready (0 seconds elapsed)
    Jan 17 15:15:47.274: INFO: expected 0 pod replicas in namespace 'pods-5962', 0 are Running and Ready.
    Jan 17 15:15:47.274: INFO: POD         NODE                         PHASE    GRACE  CONDITIONS
    Jan 17 15:15:47.274: INFO: test-pod-1  ip-10-0-151-22.ec2.internal  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 15:15:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 15:15:47 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 15:15:47 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 15:15:47 +0000 UTC  }]
    Jan 17 15:15:47.274: INFO: test-pod-2  ip-10-0-165-14.ec2.internal  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 15:15:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 15:15:47 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 15:15:47 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 15:15:47 +0000 UTC  }]
    Jan 17 15:15:47.274: INFO: test-pod-3  ip-10-0-151-22.ec2.internal  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 15:15:47 +0000 UTC  }]
    Jan 17 15:15:47.274: INFO: 
    Jan 17 15:15:49.284: INFO: 3 / 3 pods in namespace 'pods-5962' are running and ready (2 seconds elapsed)
    Jan 17 15:15:49.284: INFO: expected 0 pod replicas in namespace 'pods-5962', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 01/17/23 15:15:49.306
    Jan 17 15:15:49.309: INFO: Pod quantity 3 is different from expected quantity 0
    Jan 17 15:15:50.313: INFO: Pod quantity 3 is different from expected quantity 0
    Jan 17 15:15:51.312: INFO: Pod quantity 1 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 17 15:15:52.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-5962" for this suite. 01/17/23 15:15:52.317
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:15:52.324
Jan 17 15:15:52.325: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename deployment 01/17/23 15:15:52.325
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:15:52.362
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:15:52.365
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
Jan 17 15:15:52.368: INFO: Creating deployment "webserver-deployment"
W0117 15:15:52.378179      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 17 15:15:52.378: INFO: Waiting for observed generation 1
Jan 17 15:15:54.389: INFO: Waiting for all required pods to come up
Jan 17 15:15:54.393: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 01/17/23 15:15:54.393
Jan 17 15:15:54.393: INFO: Waiting for deployment "webserver-deployment" to complete
Jan 17 15:15:54.399: INFO: Updating deployment "webserver-deployment" with a non-existent image
Jan 17 15:15:54.420: INFO: Updating deployment webserver-deployment
Jan 17 15:15:54.420: INFO: Waiting for observed generation 2
Jan 17 15:15:56.440: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jan 17 15:15:56.443: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jan 17 15:15:56.446: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jan 17 15:15:56.455: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jan 17 15:15:56.455: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jan 17 15:15:56.458: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jan 17 15:15:56.463: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Jan 17 15:15:56.463: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Jan 17 15:15:56.472: INFO: Updating deployment webserver-deployment
Jan 17 15:15:56.472: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Jan 17 15:15:56.479: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jan 17 15:15:56.482: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 17 15:15:58.493: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-8007  cbd30a98-467d-449e-86b3-1fb346af6e76 90462 3 2023-01-17 15:15:52 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 15:15:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0085b3ab8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:20,UnavailableReplicas:13,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-17 15:15:56 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-69b7448995" is progressing.,LastUpdateTime:2023-01-17 15:15:58 +0000 UTC,LastTransitionTime:2023-01-17 15:15:52 +0000 UTC,},},ReadyReplicas:20,CollisionCount:nil,},}

Jan 17 15:15:58.497: INFO: New ReplicaSet "webserver-deployment-69b7448995" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-69b7448995  deployment-8007  c99fb177-715b-4337-8ab3-969406af806e 90258 3 2023-01-17 15:15:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment cbd30a98-467d-449e-86b3-1fb346af6e76 0xc0085b3ed7 0xc0085b3ed8}] [] [{kube-controller-manager Update apps/v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cbd30a98-467d-449e-86b3-1fb346af6e76\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 69b7448995,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0085b3f78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 17 15:15:58.497: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Jan 17 15:15:58.497: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-845c8977d9  deployment-8007  2a4fbabb-944d-4901-83ea-d017bac845a2 90460 3 2023-01-17 15:15:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment cbd30a98-467d-449e-86b3-1fb346af6e76 0xc0085b3fd7 0xc0085b3fd8}] [] [{kube-controller-manager Update apps/v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cbd30a98-467d-449e-86b3-1fb346af6e76\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 15:15:58 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0064b4068 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:20,AvailableReplicas:20,Conditions:[]ReplicaSetCondition{},},}
Jan 17 15:15:58.503: INFO: Pod "webserver-deployment-69b7448995-55svr" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-55svr webserver-deployment-69b7448995- deployment-8007  5cfa81fe-15f9-4424-9137-863463e6d56c 90147 0 2023-01-17 15:15:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.131.0.119/23"],"mac_address":"0a:58:0a:83:00:77","gateway_ips":["10.131.0.1"],"ip_address":"10.131.0.119/23","gateway_ip":"10.131.0.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.131.0.119"
    ],
    "mac": "0a:58:0a:83:00:77",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.131.0.119"
    ],
    "mac": "0a:58:0a:83:00:77",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c99fb177-715b-4337-8ab3-969406af806e 0xc008abcf67 0xc008abcf68}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:15:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:15:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c99fb177-715b-4337-8ab3-969406af806e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-17 15:15:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.0.119\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9qwhq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9qwhq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-151-22.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lsx8j,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.151.22,PodIP:10.131.0.119,StartTime:2023-01-17 15:15:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = reading manifest 404 in docker.io/library/webserver: errors:
denied: requested access to the resource is denied
unauthorized: authentication required
,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.131.0.119,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 15:15:58.503: INFO: Pod "webserver-deployment-69b7448995-6l7vj" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-6l7vj webserver-deployment-69b7448995- deployment-8007  80b31d53-ea1f-4ba5-87ce-cb0b3abd3ca5 90466 0 2023-01-17 15:15:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.131.0.123/23"],"mac_address":"0a:58:0a:83:00:7b","gateway_ips":["10.131.0.1"],"ip_address":"10.131.0.123/23","gateway_ip":"10.131.0.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.131.0.123"
    ],
    "mac": "0a:58:0a:83:00:7b",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.131.0.123"
    ],
    "mac": "0a:58:0a:83:00:7b",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c99fb177-715b-4337-8ab3-969406af806e 0xc008abd207 0xc008abd208}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c99fb177-715b-4337-8ab3-969406af806e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-17 15:15:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-17 15:15:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.0.123\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zfn5d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zfn5d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-151-22.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lsx8j,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.151.22,PodIP:10.131.0.123,StartTime:2023-01-17 15:15:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = reading manifest 404 in docker.io/library/webserver: errors:
denied: requested access to the resource is denied
unauthorized: authentication required
,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.131.0.123,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 15:15:58.503: INFO: Pod "webserver-deployment-69b7448995-9fx8n" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-9fx8n webserver-deployment-69b7448995- deployment-8007  04678b4c-6880-4065-8a64-917010e6d166 90283 0 2023-01-17 15:15:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.129.2.49/23"],"mac_address":"0a:58:0a:81:02:31","gateway_ips":["10.129.2.1"],"ip_address":"10.129.2.49/23","gateway_ip":"10.129.2.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.129.2.49"
    ],
    "mac": "0a:58:0a:81:02:31",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.129.2.49"
    ],
    "mac": "0a:58:0a:81:02:31",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c99fb177-715b-4337-8ab3-969406af806e 0xc008abd497 0xc008abd498}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:15:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:15:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c99fb177-715b-4337-8ab3-969406af806e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-17 15:15:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.129.2.49\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7x68r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7x68r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-139-213.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lsx8j,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.139.213,PodIP:10.129.2.49,StartTime:2023-01-17 15:15:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "webserver:404",},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.129.2.49,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 15:15:58.503: INFO: Pod "webserver-deployment-69b7448995-b8xvt" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-b8xvt webserver-deployment-69b7448995- deployment-8007  cb9f382c-c7f9-4dad-901d-10b5945d2d47 90398 0 2023-01-17 15:15:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.129.2.53/23"],"mac_address":"0a:58:0a:81:02:35","gateway_ips":["10.129.2.1"],"ip_address":"10.129.2.53/23","gateway_ip":"10.129.2.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.129.2.53"
    ],
    "mac": "0a:58:0a:81:02:35",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.129.2.53"
    ],
    "mac": "0a:58:0a:81:02:35",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c99fb177-715b-4337-8ab3-969406af806e 0xc008abd727 0xc008abd728}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c99fb177-715b-4337-8ab3-969406af806e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 15:15:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.129.2.53\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-01-17 15:15:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mjvwl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mjvwl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-139-213.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lsx8j,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.139.213,PodIP:10.129.2.53,StartTime:2023-01-17 15:15:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = reading manifest 404 in docker.io/library/webserver: errors:
denied: requested access to the resource is denied
unauthorized: authentication required
,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.129.2.53,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 15:15:58.503: INFO: Pod "webserver-deployment-69b7448995-cmxwc" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-cmxwc webserver-deployment-69b7448995- deployment-8007  1646e83b-1fad-4e8e-b38d-591a2f91743c 90350 0 2023-01-17 15:15:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.129.2.50/23"],"mac_address":"0a:58:0a:81:02:32","gateway_ips":["10.129.2.1"],"ip_address":"10.129.2.50/23","gateway_ip":"10.129.2.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.129.2.50"
    ],
    "mac": "0a:58:0a:81:02:32",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.129.2.50"
    ],
    "mac": "0a:58:0a:81:02:32",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c99fb177-715b-4337-8ab3-969406af806e 0xc008abd9b7 0xc008abd9b8}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c99fb177-715b-4337-8ab3-969406af806e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {multus Update v1 2023-01-17 15:15:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cqt2d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cqt2d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-139-213.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lsx8j,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.139.213,PodIP:,StartTime:2023-01-17 15:15:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 15:15:58.503: INFO: Pod "webserver-deployment-69b7448995-ghnms" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-ghnms webserver-deployment-69b7448995- deployment-8007  7f132718-f673-4b7d-90b9-2495affea8ba 90381 0 2023-01-17 15:15:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.129.2.55/23"],"mac_address":"0a:58:0a:81:02:37","gateway_ips":["10.129.2.1"],"ip_address":"10.129.2.55/23","gateway_ip":"10.129.2.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.129.2.55"
    ],
    "mac": "0a:58:0a:81:02:37",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.129.2.55"
    ],
    "mac": "0a:58:0a:81:02:37",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c99fb177-715b-4337-8ab3-969406af806e 0xc008abdc17 0xc008abdc18}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c99fb177-715b-4337-8ab3-969406af806e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 15:15:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.129.2.55\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-01-17 15:15:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hxjmw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hxjmw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-139-213.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lsx8j,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.139.213,PodIP:10.129.2.55,StartTime:2023-01-17 15:15:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = reading manifest 404 in docker.io/library/webserver: errors:
denied: requested access to the resource is denied
unauthorized: authentication required
,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.129.2.55,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 15:15:58.503: INFO: Pod "webserver-deployment-69b7448995-gsqpf" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-gsqpf webserver-deployment-69b7448995- deployment-8007  f2772cf3-daef-445a-8d8e-c5e6863258bb 90358 0 2023-01-17 15:15:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.2.84/23"],"mac_address":"0a:58:0a:80:02:54","gateway_ips":["10.128.2.1"],"ip_address":"10.128.2.84/23","gateway_ip":"10.128.2.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.2.84"
    ],
    "mac": "0a:58:0a:80:02:54",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.2.84"
    ],
    "mac": "0a:58:0a:80:02:54",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c99fb177-715b-4337-8ab3-969406af806e 0xc008abdea7 0xc008abdea8}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c99fb177-715b-4337-8ab3-969406af806e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {multus Update v1 2023-01-17 15:15:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j6sg4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j6sg4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-165-14.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lsx8j,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.165.14,PodIP:,StartTime:2023-01-17 15:15:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 15:15:58.503: INFO: Pod "webserver-deployment-69b7448995-kv72b" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-kv72b webserver-deployment-69b7448995- deployment-8007  86dc4af3-8597-4868-bc79-5eb59f1567d9 90465 0 2023-01-17 15:15:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.131.0.128/23"],"mac_address":"0a:58:0a:83:00:80","gateway_ips":["10.131.0.1"],"ip_address":"10.131.0.128/23","gateway_ip":"10.131.0.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.131.0.128"
    ],
    "mac": "0a:58:0a:83:00:80",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.131.0.128"
    ],
    "mac": "0a:58:0a:83:00:80",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c99fb177-715b-4337-8ab3-969406af806e 0xc008cae107 0xc008cae108}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c99fb177-715b-4337-8ab3-969406af806e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-17 15:15:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-17 15:15:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.0.128\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dzxcd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dzxcd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-151-22.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lsx8j,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.151.22,PodIP:10.131.0.128,StartTime:2023-01-17 15:15:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = reading manifest 404 in docker.io/library/webserver: errors:
denied: requested access to the resource is denied
unauthorized: authentication required
,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.131.0.128,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 15:15:58.504: INFO: Pod "webserver-deployment-69b7448995-kvwlc" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-kvwlc webserver-deployment-69b7448995- deployment-8007  c7d0e164-154d-4e4e-8ba8-3cb14ed95f25 90369 0 2023-01-17 15:15:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.2.85/23"],"mac_address":"0a:58:0a:80:02:55","gateway_ips":["10.128.2.1"],"ip_address":"10.128.2.85/23","gateway_ip":"10.128.2.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.2.85"
    ],
    "mac": "0a:58:0a:80:02:55",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.2.85"
    ],
    "mac": "0a:58:0a:80:02:55",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c99fb177-715b-4337-8ab3-969406af806e 0xc008cae397 0xc008cae398}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c99fb177-715b-4337-8ab3-969406af806e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {multus Update v1 2023-01-17 15:15:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pzd85,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pzd85,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-165-14.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lsx8j,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.165.14,PodIP:,StartTime:2023-01-17 15:15:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 15:15:58.504: INFO: Pod "webserver-deployment-69b7448995-qkx56" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-qkx56 webserver-deployment-69b7448995- deployment-8007  90cd80bc-4dbf-45a7-a310-96942f09a12a 90287 0 2023-01-17 15:15:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.2.82/23"],"mac_address":"0a:58:0a:80:02:52","gateway_ips":["10.128.2.1"],"ip_address":"10.128.2.82/23","gateway_ip":"10.128.2.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.2.82"
    ],
    "mac": "0a:58:0a:80:02:52",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.2.82"
    ],
    "mac": "0a:58:0a:80:02:52",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c99fb177-715b-4337-8ab3-969406af806e 0xc008cae5f7 0xc008cae5f8}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:15:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:15:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c99fb177-715b-4337-8ab3-969406af806e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-17 15:15:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.2.82\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tgh28,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tgh28,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-165-14.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lsx8j,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.165.14,PodIP:10.128.2.82,StartTime:2023-01-17 15:15:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "webserver:404",},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.2.82,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 15:15:58.504: INFO: Pod "webserver-deployment-69b7448995-vmwdw" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-vmwdw webserver-deployment-69b7448995- deployment-8007  a6c66e39-1f38-432a-9d6d-403a724daf19 90294 0 2023-01-17 15:15:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.2.81/23"],"mac_address":"0a:58:0a:80:02:51","gateway_ips":["10.128.2.1"],"ip_address":"10.128.2.81/23","gateway_ip":"10.128.2.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.2.81"
    ],
    "mac": "0a:58:0a:80:02:51",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.2.81"
    ],
    "mac": "0a:58:0a:80:02:51",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c99fb177-715b-4337-8ab3-969406af806e 0xc008cae887 0xc008cae888}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:15:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:15:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c99fb177-715b-4337-8ab3-969406af806e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-17 15:15:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.2.81\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-87rcv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-87rcv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-165-14.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lsx8j,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.165.14,PodIP:10.128.2.81,StartTime:2023-01-17 15:15:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "webserver:404",},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.2.81,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 15:15:58.504: INFO: Pod "webserver-deployment-69b7448995-x78pm" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-x78pm webserver-deployment-69b7448995- deployment-8007  20db1c31-3236-431f-b133-eaedd9153b7a 90461 0 2023-01-17 15:15:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.131.0.124/23"],"mac_address":"0a:58:0a:83:00:7c","gateway_ips":["10.131.0.1"],"ip_address":"10.131.0.124/23","gateway_ip":"10.131.0.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.131.0.124"
    ],
    "mac": "0a:58:0a:83:00:7c",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.131.0.124"
    ],
    "mac": "0a:58:0a:83:00:7c",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c99fb177-715b-4337-8ab3-969406af806e 0xc008caeb17 0xc008caeb18}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c99fb177-715b-4337-8ab3-969406af806e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-17 15:15:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-17 15:15:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.0.124\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bnc6t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bnc6t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-151-22.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lsx8j,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.151.22,PodIP:10.131.0.124,StartTime:2023-01-17 15:15:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = reading manifest 404 in docker.io/library/webserver: errors:
denied: requested access to the resource is denied
unauthorized: authentication required
,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.131.0.124,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 15:15:58.504: INFO: Pod "webserver-deployment-69b7448995-zgtkh" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-zgtkh webserver-deployment-69b7448995- deployment-8007  9a257db0-3589-4f57-8add-6be59a7ee170 90318 0 2023-01-17 15:15:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.131.0.120/23"],"mac_address":"0a:58:0a:83:00:78","gateway_ips":["10.131.0.1"],"ip_address":"10.131.0.120/23","gateway_ip":"10.131.0.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.131.0.120"
    ],
    "mac": "0a:58:0a:83:00:78",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.131.0.120"
    ],
    "mac": "0a:58:0a:83:00:78",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c99fb177-715b-4337-8ab3-969406af806e 0xc008caeda7 0xc008caeda8}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:15:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:15:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c99fb177-715b-4337-8ab3-969406af806e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-17 15:15:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-17 15:15:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.0.120\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zkrrh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zkrrh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-151-22.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lsx8j,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.151.22,PodIP:10.131.0.120,StartTime:2023-01-17 15:15:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "webserver:404",},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.131.0.120,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 15:15:58.504: INFO: Pod "webserver-deployment-845c8977d9-27n5c" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-27n5c webserver-deployment-845c8977d9- deployment-8007  f0f2f4ee-02df-4127-8fe0-16ef5652aa43 90455 0 2023-01-17 15:15:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.131.0.121/23"],"mac_address":"0a:58:0a:83:00:79","gateway_ips":["10.131.0.1"],"ip_address":"10.131.0.121/23","gateway_ip":"10.131.0.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.131.0.121"
    ],
    "mac": "0a:58:0a:83:00:79",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.131.0.121"
    ],
    "mac": "0a:58:0a:83:00:79",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 2a4fbabb-944d-4901-83ea-d017bac845a2 0xc008caf037 0xc008caf038}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2a4fbabb-944d-4901-83ea-d017bac845a2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-17 15:15:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-17 15:15:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.0.121\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zjn9b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zjn9b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-151-22.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lsx8j,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.151.22,PodIP:10.131.0.121,StartTime:2023-01-17 15:15:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 15:15:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://d2c3c9515294b716a4f29d41724757670fc59676a724b4e12dfb915783329159,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.131.0.121,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 15:15:58.504: INFO: Pod "webserver-deployment-845c8977d9-56klv" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-56klv webserver-deployment-845c8977d9- deployment-8007  1a85c355-9432-47c0-ae39-00fc7ff3c6dc 90380 0 2023-01-17 15:15:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.2.83/23"],"mac_address":"0a:58:0a:80:02:53","gateway_ips":["10.128.2.1"],"ip_address":"10.128.2.83/23","gateway_ip":"10.128.2.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.2.83"
    ],
    "mac": "0a:58:0a:80:02:53",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.2.83"
    ],
    "mac": "0a:58:0a:80:02:53",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 2a4fbabb-944d-4901-83ea-d017bac845a2 0xc008caf2a7 0xc008caf2a8}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2a4fbabb-944d-4901-83ea-d017bac845a2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 15:15:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.2.83\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-01-17 15:15:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ztcqw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ztcqw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-165-14.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lsx8j,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.165.14,PodIP:10.128.2.83,StartTime:2023-01-17 15:15:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 15:15:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://49cdcac2284b2541949420e9ce42cfb8d8399d410d454c4a6d3161917bcfc0ce,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.2.83,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 15:15:58.504: INFO: Pod "webserver-deployment-845c8977d9-6v5mx" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-6v5mx webserver-deployment-845c8977d9- deployment-8007  06fcbe67-a1af-46f0-8f8d-1d6f47d95169 90010 0 2023-01-17 15:15:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.2.80/23"],"mac_address":"0a:58:0a:80:02:50","gateway_ips":["10.128.2.1"],"ip_address":"10.128.2.80/23","gateway_ip":"10.128.2.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.2.80"
    ],
    "mac": "0a:58:0a:80:02:50",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.2.80"
    ],
    "mac": "0a:58:0a:80:02:50",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 2a4fbabb-944d-4901-83ea-d017bac845a2 0xc008caf507 0xc008caf508}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:15:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:15:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2a4fbabb-944d-4901-83ea-d017bac845a2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 15:15:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.2.80\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-01-17 15:15:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9zb82,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9zb82,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-165-14.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lsx8j,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.165.14,PodIP:10.128.2.80,StartTime:2023-01-17 15:15:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 15:15:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://7e767f756dd44bd5b6302490c18d4f683255c86bba2eb01075e432e2a3c688c2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.2.80,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 15:15:58.505: INFO: Pod "webserver-deployment-845c8977d9-8726c" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-8726c webserver-deployment-845c8977d9- deployment-8007  c9a47f49-57cd-4f68-b93d-bd43cfec4231 90449 0 2023-01-17 15:15:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.131.0.126/23"],"mac_address":"0a:58:0a:83:00:7e","gateway_ips":["10.131.0.1"],"ip_address":"10.131.0.126/23","gateway_ip":"10.131.0.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.131.0.126"
    ],
    "mac": "0a:58:0a:83:00:7e",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.131.0.126"
    ],
    "mac": "0a:58:0a:83:00:7e",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 2a4fbabb-944d-4901-83ea-d017bac845a2 0xc008caf767 0xc008caf768}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2a4fbabb-944d-4901-83ea-d017bac845a2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 15:15:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.0.126\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-01-17 15:15:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p9fk7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p9fk7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-151-22.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lsx8j,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.151.22,PodIP:10.131.0.126,StartTime:2023-01-17 15:15:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 15:15:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://aff3be9d46f8f112905954eb9740333082a9047f4e2926e5530780bc24e8f3ca,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.131.0.126,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 15:15:58.505: INFO: Pod "webserver-deployment-845c8977d9-8df49" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-8df49 webserver-deployment-845c8977d9- deployment-8007  124541e6-5c7e-4fa1-9bbc-7274e20ee0fc 90458 0 2023-01-17 15:15:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.131.0.122/23"],"mac_address":"0a:58:0a:83:00:7a","gateway_ips":["10.131.0.1"],"ip_address":"10.131.0.122/23","gateway_ip":"10.131.0.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.131.0.122"
    ],
    "mac": "0a:58:0a:83:00:7a",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.131.0.122"
    ],
    "mac": "0a:58:0a:83:00:7a",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 2a4fbabb-944d-4901-83ea-d017bac845a2 0xc008caf9c7 0xc008caf9c8}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2a4fbabb-944d-4901-83ea-d017bac845a2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-17 15:15:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-17 15:15:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.0.122\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5d9fg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5d9fg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-151-22.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lsx8j,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.151.22,PodIP:10.131.0.122,StartTime:2023-01-17 15:15:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 15:15:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://e42d49d9ceac35850b750ef58ae2624164b1f61444db46b1d0cd044f37c879ef,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.131.0.122,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 15:15:58.505: INFO: Pod "webserver-deployment-845c8977d9-94jwz" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-94jwz webserver-deployment-845c8977d9- deployment-8007  7a06015e-3b89-4d94-bf81-06b5207f2dd0 90013 0 2023-01-17 15:15:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.2.78/23"],"mac_address":"0a:58:0a:80:02:4e","gateway_ips":["10.128.2.1"],"ip_address":"10.128.2.78/23","gateway_ip":"10.128.2.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.2.78"
    ],
    "mac": "0a:58:0a:80:02:4e",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.2.78"
    ],
    "mac": "0a:58:0a:80:02:4e",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 2a4fbabb-944d-4901-83ea-d017bac845a2 0xc008cafc27 0xc008cafc28}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:15:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:15:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2a4fbabb-944d-4901-83ea-d017bac845a2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 15:15:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.2.78\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-01-17 15:15:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-528kg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-528kg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-165-14.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.165.14,PodIP:10.128.2.78,StartTime:2023-01-17 15:15:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 15:15:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://df183609a49c7375ccfe4327e6cac30d0df6a4d31b7e9ef622899717a16b8d4c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.2.78,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 15:15:58.505: INFO: Pod "webserver-deployment-845c8977d9-9wbhw" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-9wbhw webserver-deployment-845c8977d9- deployment-8007  45a10e9b-07f1-4206-9fa0-b4e644155c45 90371 0 2023-01-17 15:15:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.2.86/23"],"mac_address":"0a:58:0a:80:02:56","gateway_ips":["10.128.2.1"],"ip_address":"10.128.2.86/23","gateway_ip":"10.128.2.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.2.86"
    ],
    "mac": "0a:58:0a:80:02:56",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.2.86"
    ],
    "mac": "0a:58:0a:80:02:56",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 2a4fbabb-944d-4901-83ea-d017bac845a2 0xc008cafe97 0xc008cafe98}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2a4fbabb-944d-4901-83ea-d017bac845a2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 15:15:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.2.86\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-01-17 15:15:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mqm2s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mqm2s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-165-14.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lsx8j,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.165.14,PodIP:10.128.2.86,StartTime:2023-01-17 15:15:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 15:15:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://aba8236d65491623ea8768bfb170e6e75f696239a690b2cfdff85d468cdd4a47,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.2.86,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 15:15:58.505: INFO: Pod "webserver-deployment-845c8977d9-bdfv6" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-bdfv6 webserver-deployment-845c8977d9- deployment-8007  84929c39-62d3-4e2e-9a1b-568c3108fc74 90400 0 2023-01-17 15:15:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.129.2.54/23"],"mac_address":"0a:58:0a:81:02:36","gateway_ips":["10.129.2.1"],"ip_address":"10.129.2.54/23","gateway_ip":"10.129.2.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.129.2.54"
    ],
    "mac": "0a:58:0a:81:02:36",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.129.2.54"
    ],
    "mac": "0a:58:0a:81:02:36",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 2a4fbabb-944d-4901-83ea-d017bac845a2 0xc006638107 0xc006638108}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2a4fbabb-944d-4901-83ea-d017bac845a2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 15:15:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.129.2.54\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-01-17 15:15:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xkhk8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xkhk8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-139-213.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lsx8j,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.139.213,PodIP:10.129.2.54,StartTime:2023-01-17 15:15:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 15:15:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://878358bcdf8b18423c5a3814f3c1602c3905bba9ae203286a7e3763c035102f1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.129.2.54,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 15:15:58.505: INFO: Pod "webserver-deployment-845c8977d9-cfjkl" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-cfjkl webserver-deployment-845c8977d9- deployment-8007  9e236366-b328-4973-b8c6-8ce512a49f02 90023 0 2023-01-17 15:15:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.129.2.46/23"],"mac_address":"0a:58:0a:81:02:2e","gateway_ips":["10.129.2.1"],"ip_address":"10.129.2.46/23","gateway_ip":"10.129.2.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.129.2.46"
    ],
    "mac": "0a:58:0a:81:02:2e",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.129.2.46"
    ],
    "mac": "0a:58:0a:81:02:2e",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 2a4fbabb-944d-4901-83ea-d017bac845a2 0xc006638367 0xc006638368}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:15:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:15:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2a4fbabb-944d-4901-83ea-d017bac845a2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 15:15:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.129.2.46\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-01-17 15:15:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c628c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c628c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-139-213.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.139.213,PodIP:10.129.2.46,StartTime:2023-01-17 15:15:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 15:15:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://bd7eb7766c8eb245ce7138a1ff652b8b50217183bb4c731033b69e232fa0e03e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.129.2.46,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 15:15:58.505: INFO: Pod "webserver-deployment-845c8977d9-drq79" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-drq79 webserver-deployment-845c8977d9- deployment-8007  c7d1691a-42e1-44b5-a08c-ba127a4ee1c6 90384 0 2023-01-17 15:15:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.2.88/23"],"mac_address":"0a:58:0a:80:02:58","gateway_ips":["10.128.2.1"],"ip_address":"10.128.2.88/23","gateway_ip":"10.128.2.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.2.88"
    ],
    "mac": "0a:58:0a:80:02:58",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.2.88"
    ],
    "mac": "0a:58:0a:80:02:58",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 2a4fbabb-944d-4901-83ea-d017bac845a2 0xc0066385c7 0xc0066385c8}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2a4fbabb-944d-4901-83ea-d017bac845a2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 15:15:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.2.88\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-01-17 15:15:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lnfv6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lnfv6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-165-14.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lsx8j,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.165.14,PodIP:10.128.2.88,StartTime:2023-01-17 15:15:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 15:15:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://68807b9744deb10df97f44488927a9e187416d40b9490cdfed73add319f09ecb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.2.88,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 15:15:58.505: INFO: Pod "webserver-deployment-845c8977d9-g5mj6" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-g5mj6 webserver-deployment-845c8977d9- deployment-8007  f4e4e3cb-e70c-4df9-a02a-0dd3e832f07b 90012 0 2023-01-17 15:15:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.2.79/23"],"mac_address":"0a:58:0a:80:02:4f","gateway_ips":["10.128.2.1"],"ip_address":"10.128.2.79/23","gateway_ip":"10.128.2.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.2.79"
    ],
    "mac": "0a:58:0a:80:02:4f",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.2.79"
    ],
    "mac": "0a:58:0a:80:02:4f",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 2a4fbabb-944d-4901-83ea-d017bac845a2 0xc006638837 0xc006638838}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:15:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:15:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2a4fbabb-944d-4901-83ea-d017bac845a2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 15:15:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.2.79\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-01-17 15:15:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bn6wf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bn6wf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-165-14.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lsx8j,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.165.14,PodIP:10.128.2.79,StartTime:2023-01-17 15:15:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 15:15:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://d3c9f2663d8208e9aa35dba5e99a3b83e07e71b33891aa9db4e41ae6f2374140,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.2.79,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 15:15:58.505: INFO: Pod "webserver-deployment-845c8977d9-gxzf8" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-gxzf8 webserver-deployment-845c8977d9- deployment-8007  2849a6fc-302f-4529-a2dc-9c391f551b1c 90374 0 2023-01-17 15:15:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.2.87/23"],"mac_address":"0a:58:0a:80:02:57","gateway_ips":["10.128.2.1"],"ip_address":"10.128.2.87/23","gateway_ip":"10.128.2.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.2.87"
    ],
    "mac": "0a:58:0a:80:02:57",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.2.87"
    ],
    "mac": "0a:58:0a:80:02:57",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 2a4fbabb-944d-4901-83ea-d017bac845a2 0xc006638a97 0xc006638a98}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2a4fbabb-944d-4901-83ea-d017bac845a2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 15:15:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.2.87\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-01-17 15:15:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7j6lj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7j6lj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-165-14.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lsx8j,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.165.14,PodIP:10.128.2.87,StartTime:2023-01-17 15:15:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 15:15:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://238fedac9c2e21d115629f26536532f0c1306a6d57fd5c06c2336933174e7e0c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.2.87,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 15:15:58.506: INFO: Pod "webserver-deployment-845c8977d9-hzczq" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-hzczq webserver-deployment-845c8977d9- deployment-8007  e0dbd774-85bb-4b47-b562-fa29bfa98d2e 90393 0 2023-01-17 15:15:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.129.2.51/23"],"mac_address":"0a:58:0a:81:02:33","gateway_ips":["10.129.2.1"],"ip_address":"10.129.2.51/23","gateway_ip":"10.129.2.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.129.2.51"
    ],
    "mac": "0a:58:0a:81:02:33",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.129.2.51"
    ],
    "mac": "0a:58:0a:81:02:33",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 2a4fbabb-944d-4901-83ea-d017bac845a2 0xc006638cf7 0xc006638cf8}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2a4fbabb-944d-4901-83ea-d017bac845a2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 15:15:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.129.2.51\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-01-17 15:15:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k4hml,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k4hml,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-139-213.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lsx8j,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.139.213,PodIP:10.129.2.51,StartTime:2023-01-17 15:15:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 15:15:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://7ed4a3a59e7329cac03678ff0761eece326862ce6369a5db86e2cc17201a9213,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.129.2.51,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 15:15:58.506: INFO: Pod "webserver-deployment-845c8977d9-jxcwm" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-jxcwm webserver-deployment-845c8977d9- deployment-8007  51e86f3a-30c1-48d8-813e-970b43372e6f 90016 0 2023-01-17 15:15:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.129.2.47/23"],"mac_address":"0a:58:0a:81:02:2f","gateway_ips":["10.129.2.1"],"ip_address":"10.129.2.47/23","gateway_ip":"10.129.2.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.129.2.47"
    ],
    "mac": "0a:58:0a:81:02:2f",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.129.2.47"
    ],
    "mac": "0a:58:0a:81:02:2f",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 2a4fbabb-944d-4901-83ea-d017bac845a2 0xc006638f57 0xc006638f58}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:15:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:15:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2a4fbabb-944d-4901-83ea-d017bac845a2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 15:15:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.129.2.47\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-01-17 15:15:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qvwf6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qvwf6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-139-213.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lsx8j,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.139.213,PodIP:10.129.2.47,StartTime:2023-01-17 15:15:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 15:15:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://48b3de9cf0b01a0b3544c0ce23d163ad1f1055e74786f83b2bd4e778cef12934,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.129.2.47,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 15:15:58.506: INFO: Pod "webserver-deployment-845c8977d9-kdwsd" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-kdwsd webserver-deployment-845c8977d9- deployment-8007  1bc08ceb-3149-4ebc-8d77-05a065b0ad09 90037 0 2023-01-17 15:15:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.131.0.118/23"],"mac_address":"0a:58:0a:83:00:76","gateway_ips":["10.131.0.1"],"ip_address":"10.131.0.118/23","gateway_ip":"10.131.0.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.131.0.118"
    ],
    "mac": "0a:58:0a:83:00:76",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.131.0.118"
    ],
    "mac": "0a:58:0a:83:00:76",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 2a4fbabb-944d-4901-83ea-d017bac845a2 0xc0066391b7 0xc0066391b8}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:15:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:15:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2a4fbabb-944d-4901-83ea-d017bac845a2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-17 15:15:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-17 15:15:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.0.118\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n7r5h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n7r5h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-151-22.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lsx8j,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.151.22,PodIP:10.131.0.118,StartTime:2023-01-17 15:15:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 15:15:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://575835aa4b35f100976a840de55659c4adde1933bab7262ebae006d5dc612ce0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.131.0.118,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 15:15:58.506: INFO: Pod "webserver-deployment-845c8977d9-kjm8p" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-kjm8p webserver-deployment-845c8977d9- deployment-8007  78a924ac-a6cd-4a3c-833a-516b0f5b770a 90447 0 2023-01-17 15:15:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.131.0.125/23"],"mac_address":"0a:58:0a:83:00:7d","gateway_ips":["10.131.0.1"],"ip_address":"10.131.0.125/23","gateway_ip":"10.131.0.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.131.0.125"
    ],
    "mac": "0a:58:0a:83:00:7d",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.131.0.125"
    ],
    "mac": "0a:58:0a:83:00:7d",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 2a4fbabb-944d-4901-83ea-d017bac845a2 0xc006639417 0xc006639418}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2a4fbabb-944d-4901-83ea-d017bac845a2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-17 15:15:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-17 15:15:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.0.125\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vb44v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vb44v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-151-22.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lsx8j,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.151.22,PodIP:10.131.0.125,StartTime:2023-01-17 15:15:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 15:15:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://df8beef96dae2f8774c44783a9c6755a30e65ac5463d3af39c0c4ccae489fc3c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.131.0.125,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 15:15:58.506: INFO: Pod "webserver-deployment-845c8977d9-n4w2s" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-n4w2s webserver-deployment-845c8977d9- deployment-8007  9fb5c383-269f-47a9-bb9e-29f3c0c9b418 90031 0 2023-01-17 15:15:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.131.0.116/23"],"mac_address":"0a:58:0a:83:00:74","gateway_ips":["10.131.0.1"],"ip_address":"10.131.0.116/23","gateway_ip":"10.131.0.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.131.0.116"
    ],
    "mac": "0a:58:0a:83:00:74",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.131.0.116"
    ],
    "mac": "0a:58:0a:83:00:74",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 2a4fbabb-944d-4901-83ea-d017bac845a2 0xc006639677 0xc006639678}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:15:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:15:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2a4fbabb-944d-4901-83ea-d017bac845a2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-17 15:15:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-17 15:15:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.0.116\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6cx54,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6cx54,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-151-22.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lsx8j,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.151.22,PodIP:10.131.0.116,StartTime:2023-01-17 15:15:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 15:15:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://4b844a8cb1333abe33a451f82f89e4077ce50ca3273cc545b65880c683168c3b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.131.0.116,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 15:15:58.506: INFO: Pod "webserver-deployment-845c8977d9-qtcfr" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-qtcfr webserver-deployment-845c8977d9- deployment-8007  89fdfef2-4a88-4ec3-9539-bda327f47e0d 90452 0 2023-01-17 15:15:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.131.0.127/23"],"mac_address":"0a:58:0a:83:00:7f","gateway_ips":["10.131.0.1"],"ip_address":"10.131.0.127/23","gateway_ip":"10.131.0.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.131.0.127"
    ],
    "mac": "0a:58:0a:83:00:7f",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.131.0.127"
    ],
    "mac": "0a:58:0a:83:00:7f",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 2a4fbabb-944d-4901-83ea-d017bac845a2 0xc0066398d7 0xc0066398d8}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2a4fbabb-944d-4901-83ea-d017bac845a2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-17 15:15:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-17 15:15:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.0.127\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vt8nv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vt8nv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-151-22.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lsx8j,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.151.22,PodIP:10.131.0.127,StartTime:2023-01-17 15:15:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 15:15:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://f944b513acaf7c3530ae47c7dada438b6f50079249add7365c7c4e1bd3af0ae6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.131.0.127,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 15:15:58.506: INFO: Pod "webserver-deployment-845c8977d9-vhslc" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-vhslc webserver-deployment-845c8977d9- deployment-8007  6cd64616-ef07-4553-9224-e18239fb35f6 90388 0 2023-01-17 15:15:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.129.2.52/23"],"mac_address":"0a:58:0a:81:02:34","gateway_ips":["10.129.2.1"],"ip_address":"10.129.2.52/23","gateway_ip":"10.129.2.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.129.2.52"
    ],
    "mac": "0a:58:0a:81:02:34",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.129.2.52"
    ],
    "mac": "0a:58:0a:81:02:34",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 2a4fbabb-944d-4901-83ea-d017bac845a2 0xc006639cf7 0xc006639cf8}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2a4fbabb-944d-4901-83ea-d017bac845a2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 15:15:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.129.2.52\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-01-17 15:15:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l2vzg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l2vzg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-139-213.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lsx8j,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.139.213,PodIP:10.129.2.52,StartTime:2023-01-17 15:15:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 15:15:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://a9ae880ae9f4d1475fd1716bfe896f250e5dbc0aabe6d0b2a237a671b0e3cc71,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.129.2.52,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 15:15:58.506: INFO: Pod "webserver-deployment-845c8977d9-vtl7l" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-vtl7l webserver-deployment-845c8977d9- deployment-8007  d71d60f0-ab70-4999-a4d9-4384fb052af6 90019 0 2023-01-17 15:15:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.129.2.48/23"],"mac_address":"0a:58:0a:81:02:30","gateway_ips":["10.129.2.1"],"ip_address":"10.129.2.48/23","gateway_ip":"10.129.2.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.129.2.48"
    ],
    "mac": "0a:58:0a:81:02:30",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.129.2.48"
    ],
    "mac": "0a:58:0a:81:02:30",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 2a4fbabb-944d-4901-83ea-d017bac845a2 0xc006639f57 0xc006639f58}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:15:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:15:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2a4fbabb-944d-4901-83ea-d017bac845a2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 15:15:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.129.2.48\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-01-17 15:15:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-98lq5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-98lq5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-139-213.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lsx8j,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.139.213,PodIP:10.129.2.48,StartTime:2023-01-17 15:15:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 15:15:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://5c8b6f14f274bc1150c2b4ecd105dec0ce95e35cb3dcb6153ccb4048792675e6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.129.2.48,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan 17 15:15:58.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8007" for this suite. 01/17/23 15:15:58.511
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","completed":124,"skipped":2296,"failed":0}
------------------------------
• [SLOW TEST] [6.191 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:15:52.324
    Jan 17 15:15:52.325: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename deployment 01/17/23 15:15:52.325
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:15:52.362
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:15:52.365
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    Jan 17 15:15:52.368: INFO: Creating deployment "webserver-deployment"
    W0117 15:15:52.378179      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jan 17 15:15:52.378: INFO: Waiting for observed generation 1
    Jan 17 15:15:54.389: INFO: Waiting for all required pods to come up
    Jan 17 15:15:54.393: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 01/17/23 15:15:54.393
    Jan 17 15:15:54.393: INFO: Waiting for deployment "webserver-deployment" to complete
    Jan 17 15:15:54.399: INFO: Updating deployment "webserver-deployment" with a non-existent image
    Jan 17 15:15:54.420: INFO: Updating deployment webserver-deployment
    Jan 17 15:15:54.420: INFO: Waiting for observed generation 2
    Jan 17 15:15:56.440: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    Jan 17 15:15:56.443: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    Jan 17 15:15:56.446: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Jan 17 15:15:56.455: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    Jan 17 15:15:56.455: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    Jan 17 15:15:56.458: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Jan 17 15:15:56.463: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    Jan 17 15:15:56.463: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    Jan 17 15:15:56.472: INFO: Updating deployment webserver-deployment
    Jan 17 15:15:56.472: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    Jan 17 15:15:56.479: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    Jan 17 15:15:56.482: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 17 15:15:58.493: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-8007  cbd30a98-467d-449e-86b3-1fb346af6e76 90462 3 2023-01-17 15:15:52 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 15:15:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0085b3ab8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:20,UnavailableReplicas:13,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-17 15:15:56 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-69b7448995" is progressing.,LastUpdateTime:2023-01-17 15:15:58 +0000 UTC,LastTransitionTime:2023-01-17 15:15:52 +0000 UTC,},},ReadyReplicas:20,CollisionCount:nil,},}

    Jan 17 15:15:58.497: INFO: New ReplicaSet "webserver-deployment-69b7448995" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-69b7448995  deployment-8007  c99fb177-715b-4337-8ab3-969406af806e 90258 3 2023-01-17 15:15:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment cbd30a98-467d-449e-86b3-1fb346af6e76 0xc0085b3ed7 0xc0085b3ed8}] [] [{kube-controller-manager Update apps/v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cbd30a98-467d-449e-86b3-1fb346af6e76\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 69b7448995,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0085b3f78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 17 15:15:58.497: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    Jan 17 15:15:58.497: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-845c8977d9  deployment-8007  2a4fbabb-944d-4901-83ea-d017bac845a2 90460 3 2023-01-17 15:15:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment cbd30a98-467d-449e-86b3-1fb346af6e76 0xc0085b3fd7 0xc0085b3fd8}] [] [{kube-controller-manager Update apps/v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cbd30a98-467d-449e-86b3-1fb346af6e76\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 15:15:58 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0064b4068 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:20,AvailableReplicas:20,Conditions:[]ReplicaSetCondition{},},}
    Jan 17 15:15:58.503: INFO: Pod "webserver-deployment-69b7448995-55svr" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-55svr webserver-deployment-69b7448995- deployment-8007  5cfa81fe-15f9-4424-9137-863463e6d56c 90147 0 2023-01-17 15:15:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.131.0.119/23"],"mac_address":"0a:58:0a:83:00:77","gateway_ips":["10.131.0.1"],"ip_address":"10.131.0.119/23","gateway_ip":"10.131.0.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.131.0.119"
        ],
        "mac": "0a:58:0a:83:00:77",
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.131.0.119"
        ],
        "mac": "0a:58:0a:83:00:77",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c99fb177-715b-4337-8ab3-969406af806e 0xc008abcf67 0xc008abcf68}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:15:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:15:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c99fb177-715b-4337-8ab3-969406af806e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-17 15:15:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.0.119\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9qwhq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9qwhq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-151-22.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lsx8j,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.151.22,PodIP:10.131.0.119,StartTime:2023-01-17 15:15:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = reading manifest 404 in docker.io/library/webserver: errors:
    denied: requested access to the resource is denied
    unauthorized: authentication required
    ,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.131.0.119,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 17 15:15:58.503: INFO: Pod "webserver-deployment-69b7448995-6l7vj" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-6l7vj webserver-deployment-69b7448995- deployment-8007  80b31d53-ea1f-4ba5-87ce-cb0b3abd3ca5 90466 0 2023-01-17 15:15:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.131.0.123/23"],"mac_address":"0a:58:0a:83:00:7b","gateway_ips":["10.131.0.1"],"ip_address":"10.131.0.123/23","gateway_ip":"10.131.0.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.131.0.123"
        ],
        "mac": "0a:58:0a:83:00:7b",
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.131.0.123"
        ],
        "mac": "0a:58:0a:83:00:7b",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c99fb177-715b-4337-8ab3-969406af806e 0xc008abd207 0xc008abd208}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c99fb177-715b-4337-8ab3-969406af806e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-17 15:15:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-17 15:15:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.0.123\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zfn5d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zfn5d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-151-22.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lsx8j,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.151.22,PodIP:10.131.0.123,StartTime:2023-01-17 15:15:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = reading manifest 404 in docker.io/library/webserver: errors:
    denied: requested access to the resource is denied
    unauthorized: authentication required
    ,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.131.0.123,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 17 15:15:58.503: INFO: Pod "webserver-deployment-69b7448995-9fx8n" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-9fx8n webserver-deployment-69b7448995- deployment-8007  04678b4c-6880-4065-8a64-917010e6d166 90283 0 2023-01-17 15:15:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.129.2.49/23"],"mac_address":"0a:58:0a:81:02:31","gateway_ips":["10.129.2.1"],"ip_address":"10.129.2.49/23","gateway_ip":"10.129.2.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.129.2.49"
        ],
        "mac": "0a:58:0a:81:02:31",
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.129.2.49"
        ],
        "mac": "0a:58:0a:81:02:31",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c99fb177-715b-4337-8ab3-969406af806e 0xc008abd497 0xc008abd498}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:15:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:15:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c99fb177-715b-4337-8ab3-969406af806e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-17 15:15:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.129.2.49\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7x68r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7x68r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-139-213.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lsx8j,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.139.213,PodIP:10.129.2.49,StartTime:2023-01-17 15:15:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "webserver:404",},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.129.2.49,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 17 15:15:58.503: INFO: Pod "webserver-deployment-69b7448995-b8xvt" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-b8xvt webserver-deployment-69b7448995- deployment-8007  cb9f382c-c7f9-4dad-901d-10b5945d2d47 90398 0 2023-01-17 15:15:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.129.2.53/23"],"mac_address":"0a:58:0a:81:02:35","gateway_ips":["10.129.2.1"],"ip_address":"10.129.2.53/23","gateway_ip":"10.129.2.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.129.2.53"
        ],
        "mac": "0a:58:0a:81:02:35",
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.129.2.53"
        ],
        "mac": "0a:58:0a:81:02:35",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c99fb177-715b-4337-8ab3-969406af806e 0xc008abd727 0xc008abd728}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c99fb177-715b-4337-8ab3-969406af806e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 15:15:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.129.2.53\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-01-17 15:15:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mjvwl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mjvwl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-139-213.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lsx8j,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.139.213,PodIP:10.129.2.53,StartTime:2023-01-17 15:15:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = reading manifest 404 in docker.io/library/webserver: errors:
    denied: requested access to the resource is denied
    unauthorized: authentication required
    ,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.129.2.53,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 17 15:15:58.503: INFO: Pod "webserver-deployment-69b7448995-cmxwc" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-cmxwc webserver-deployment-69b7448995- deployment-8007  1646e83b-1fad-4e8e-b38d-591a2f91743c 90350 0 2023-01-17 15:15:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.129.2.50/23"],"mac_address":"0a:58:0a:81:02:32","gateway_ips":["10.129.2.1"],"ip_address":"10.129.2.50/23","gateway_ip":"10.129.2.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.129.2.50"
        ],
        "mac": "0a:58:0a:81:02:32",
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.129.2.50"
        ],
        "mac": "0a:58:0a:81:02:32",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c99fb177-715b-4337-8ab3-969406af806e 0xc008abd9b7 0xc008abd9b8}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c99fb177-715b-4337-8ab3-969406af806e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {multus Update v1 2023-01-17 15:15:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cqt2d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cqt2d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-139-213.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lsx8j,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.139.213,PodIP:,StartTime:2023-01-17 15:15:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 17 15:15:58.503: INFO: Pod "webserver-deployment-69b7448995-ghnms" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-ghnms webserver-deployment-69b7448995- deployment-8007  7f132718-f673-4b7d-90b9-2495affea8ba 90381 0 2023-01-17 15:15:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.129.2.55/23"],"mac_address":"0a:58:0a:81:02:37","gateway_ips":["10.129.2.1"],"ip_address":"10.129.2.55/23","gateway_ip":"10.129.2.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.129.2.55"
        ],
        "mac": "0a:58:0a:81:02:37",
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.129.2.55"
        ],
        "mac": "0a:58:0a:81:02:37",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c99fb177-715b-4337-8ab3-969406af806e 0xc008abdc17 0xc008abdc18}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c99fb177-715b-4337-8ab3-969406af806e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 15:15:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.129.2.55\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-01-17 15:15:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hxjmw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hxjmw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-139-213.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lsx8j,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.139.213,PodIP:10.129.2.55,StartTime:2023-01-17 15:15:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = reading manifest 404 in docker.io/library/webserver: errors:
    denied: requested access to the resource is denied
    unauthorized: authentication required
    ,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.129.2.55,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 17 15:15:58.503: INFO: Pod "webserver-deployment-69b7448995-gsqpf" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-gsqpf webserver-deployment-69b7448995- deployment-8007  f2772cf3-daef-445a-8d8e-c5e6863258bb 90358 0 2023-01-17 15:15:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.2.84/23"],"mac_address":"0a:58:0a:80:02:54","gateway_ips":["10.128.2.1"],"ip_address":"10.128.2.84/23","gateway_ip":"10.128.2.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.128.2.84"
        ],
        "mac": "0a:58:0a:80:02:54",
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.128.2.84"
        ],
        "mac": "0a:58:0a:80:02:54",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c99fb177-715b-4337-8ab3-969406af806e 0xc008abdea7 0xc008abdea8}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c99fb177-715b-4337-8ab3-969406af806e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {multus Update v1 2023-01-17 15:15:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j6sg4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j6sg4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-165-14.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lsx8j,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.165.14,PodIP:,StartTime:2023-01-17 15:15:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 17 15:15:58.503: INFO: Pod "webserver-deployment-69b7448995-kv72b" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-kv72b webserver-deployment-69b7448995- deployment-8007  86dc4af3-8597-4868-bc79-5eb59f1567d9 90465 0 2023-01-17 15:15:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.131.0.128/23"],"mac_address":"0a:58:0a:83:00:80","gateway_ips":["10.131.0.1"],"ip_address":"10.131.0.128/23","gateway_ip":"10.131.0.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.131.0.128"
        ],
        "mac": "0a:58:0a:83:00:80",
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.131.0.128"
        ],
        "mac": "0a:58:0a:83:00:80",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c99fb177-715b-4337-8ab3-969406af806e 0xc008cae107 0xc008cae108}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c99fb177-715b-4337-8ab3-969406af806e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-17 15:15:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-17 15:15:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.0.128\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dzxcd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dzxcd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-151-22.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lsx8j,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.151.22,PodIP:10.131.0.128,StartTime:2023-01-17 15:15:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = reading manifest 404 in docker.io/library/webserver: errors:
    denied: requested access to the resource is denied
    unauthorized: authentication required
    ,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.131.0.128,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 17 15:15:58.504: INFO: Pod "webserver-deployment-69b7448995-kvwlc" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-kvwlc webserver-deployment-69b7448995- deployment-8007  c7d0e164-154d-4e4e-8ba8-3cb14ed95f25 90369 0 2023-01-17 15:15:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.2.85/23"],"mac_address":"0a:58:0a:80:02:55","gateway_ips":["10.128.2.1"],"ip_address":"10.128.2.85/23","gateway_ip":"10.128.2.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.128.2.85"
        ],
        "mac": "0a:58:0a:80:02:55",
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.128.2.85"
        ],
        "mac": "0a:58:0a:80:02:55",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c99fb177-715b-4337-8ab3-969406af806e 0xc008cae397 0xc008cae398}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c99fb177-715b-4337-8ab3-969406af806e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {multus Update v1 2023-01-17 15:15:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pzd85,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pzd85,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-165-14.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lsx8j,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.165.14,PodIP:,StartTime:2023-01-17 15:15:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 17 15:15:58.504: INFO: Pod "webserver-deployment-69b7448995-qkx56" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-qkx56 webserver-deployment-69b7448995- deployment-8007  90cd80bc-4dbf-45a7-a310-96942f09a12a 90287 0 2023-01-17 15:15:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.2.82/23"],"mac_address":"0a:58:0a:80:02:52","gateway_ips":["10.128.2.1"],"ip_address":"10.128.2.82/23","gateway_ip":"10.128.2.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.128.2.82"
        ],
        "mac": "0a:58:0a:80:02:52",
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.128.2.82"
        ],
        "mac": "0a:58:0a:80:02:52",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c99fb177-715b-4337-8ab3-969406af806e 0xc008cae5f7 0xc008cae5f8}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:15:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:15:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c99fb177-715b-4337-8ab3-969406af806e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-17 15:15:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.2.82\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tgh28,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tgh28,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-165-14.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lsx8j,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.165.14,PodIP:10.128.2.82,StartTime:2023-01-17 15:15:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "webserver:404",},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.2.82,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 17 15:15:58.504: INFO: Pod "webserver-deployment-69b7448995-vmwdw" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-vmwdw webserver-deployment-69b7448995- deployment-8007  a6c66e39-1f38-432a-9d6d-403a724daf19 90294 0 2023-01-17 15:15:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.2.81/23"],"mac_address":"0a:58:0a:80:02:51","gateway_ips":["10.128.2.1"],"ip_address":"10.128.2.81/23","gateway_ip":"10.128.2.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.128.2.81"
        ],
        "mac": "0a:58:0a:80:02:51",
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.128.2.81"
        ],
        "mac": "0a:58:0a:80:02:51",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c99fb177-715b-4337-8ab3-969406af806e 0xc008cae887 0xc008cae888}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:15:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:15:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c99fb177-715b-4337-8ab3-969406af806e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-17 15:15:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.2.81\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-87rcv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-87rcv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-165-14.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lsx8j,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.165.14,PodIP:10.128.2.81,StartTime:2023-01-17 15:15:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "webserver:404",},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.2.81,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 17 15:15:58.504: INFO: Pod "webserver-deployment-69b7448995-x78pm" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-x78pm webserver-deployment-69b7448995- deployment-8007  20db1c31-3236-431f-b133-eaedd9153b7a 90461 0 2023-01-17 15:15:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.131.0.124/23"],"mac_address":"0a:58:0a:83:00:7c","gateway_ips":["10.131.0.1"],"ip_address":"10.131.0.124/23","gateway_ip":"10.131.0.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.131.0.124"
        ],
        "mac": "0a:58:0a:83:00:7c",
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.131.0.124"
        ],
        "mac": "0a:58:0a:83:00:7c",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c99fb177-715b-4337-8ab3-969406af806e 0xc008caeb17 0xc008caeb18}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c99fb177-715b-4337-8ab3-969406af806e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-17 15:15:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-17 15:15:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.0.124\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bnc6t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bnc6t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-151-22.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lsx8j,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.151.22,PodIP:10.131.0.124,StartTime:2023-01-17 15:15:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = reading manifest 404 in docker.io/library/webserver: errors:
    denied: requested access to the resource is denied
    unauthorized: authentication required
    ,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.131.0.124,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 17 15:15:58.504: INFO: Pod "webserver-deployment-69b7448995-zgtkh" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-zgtkh webserver-deployment-69b7448995- deployment-8007  9a257db0-3589-4f57-8add-6be59a7ee170 90318 0 2023-01-17 15:15:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.131.0.120/23"],"mac_address":"0a:58:0a:83:00:78","gateway_ips":["10.131.0.1"],"ip_address":"10.131.0.120/23","gateway_ip":"10.131.0.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.131.0.120"
        ],
        "mac": "0a:58:0a:83:00:78",
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.131.0.120"
        ],
        "mac": "0a:58:0a:83:00:78",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c99fb177-715b-4337-8ab3-969406af806e 0xc008caeda7 0xc008caeda8}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:15:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:15:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c99fb177-715b-4337-8ab3-969406af806e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-17 15:15:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-17 15:15:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.0.120\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zkrrh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zkrrh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-151-22.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lsx8j,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.151.22,PodIP:10.131.0.120,StartTime:2023-01-17 15:15:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "webserver:404",},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.131.0.120,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 17 15:15:58.504: INFO: Pod "webserver-deployment-845c8977d9-27n5c" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-27n5c webserver-deployment-845c8977d9- deployment-8007  f0f2f4ee-02df-4127-8fe0-16ef5652aa43 90455 0 2023-01-17 15:15:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.131.0.121/23"],"mac_address":"0a:58:0a:83:00:79","gateway_ips":["10.131.0.1"],"ip_address":"10.131.0.121/23","gateway_ip":"10.131.0.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.131.0.121"
        ],
        "mac": "0a:58:0a:83:00:79",
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.131.0.121"
        ],
        "mac": "0a:58:0a:83:00:79",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 2a4fbabb-944d-4901-83ea-d017bac845a2 0xc008caf037 0xc008caf038}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2a4fbabb-944d-4901-83ea-d017bac845a2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-17 15:15:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-17 15:15:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.0.121\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zjn9b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zjn9b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-151-22.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lsx8j,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.151.22,PodIP:10.131.0.121,StartTime:2023-01-17 15:15:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 15:15:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://d2c3c9515294b716a4f29d41724757670fc59676a724b4e12dfb915783329159,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.131.0.121,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 17 15:15:58.504: INFO: Pod "webserver-deployment-845c8977d9-56klv" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-56klv webserver-deployment-845c8977d9- deployment-8007  1a85c355-9432-47c0-ae39-00fc7ff3c6dc 90380 0 2023-01-17 15:15:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.2.83/23"],"mac_address":"0a:58:0a:80:02:53","gateway_ips":["10.128.2.1"],"ip_address":"10.128.2.83/23","gateway_ip":"10.128.2.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.128.2.83"
        ],
        "mac": "0a:58:0a:80:02:53",
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.128.2.83"
        ],
        "mac": "0a:58:0a:80:02:53",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 2a4fbabb-944d-4901-83ea-d017bac845a2 0xc008caf2a7 0xc008caf2a8}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2a4fbabb-944d-4901-83ea-d017bac845a2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 15:15:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.2.83\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-01-17 15:15:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ztcqw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ztcqw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-165-14.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lsx8j,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.165.14,PodIP:10.128.2.83,StartTime:2023-01-17 15:15:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 15:15:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://49cdcac2284b2541949420e9ce42cfb8d8399d410d454c4a6d3161917bcfc0ce,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.2.83,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 17 15:15:58.504: INFO: Pod "webserver-deployment-845c8977d9-6v5mx" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-6v5mx webserver-deployment-845c8977d9- deployment-8007  06fcbe67-a1af-46f0-8f8d-1d6f47d95169 90010 0 2023-01-17 15:15:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.2.80/23"],"mac_address":"0a:58:0a:80:02:50","gateway_ips":["10.128.2.1"],"ip_address":"10.128.2.80/23","gateway_ip":"10.128.2.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.128.2.80"
        ],
        "mac": "0a:58:0a:80:02:50",
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.128.2.80"
        ],
        "mac": "0a:58:0a:80:02:50",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 2a4fbabb-944d-4901-83ea-d017bac845a2 0xc008caf507 0xc008caf508}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:15:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:15:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2a4fbabb-944d-4901-83ea-d017bac845a2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 15:15:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.2.80\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-01-17 15:15:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9zb82,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9zb82,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-165-14.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lsx8j,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.165.14,PodIP:10.128.2.80,StartTime:2023-01-17 15:15:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 15:15:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://7e767f756dd44bd5b6302490c18d4f683255c86bba2eb01075e432e2a3c688c2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.2.80,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 17 15:15:58.505: INFO: Pod "webserver-deployment-845c8977d9-8726c" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-8726c webserver-deployment-845c8977d9- deployment-8007  c9a47f49-57cd-4f68-b93d-bd43cfec4231 90449 0 2023-01-17 15:15:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.131.0.126/23"],"mac_address":"0a:58:0a:83:00:7e","gateway_ips":["10.131.0.1"],"ip_address":"10.131.0.126/23","gateway_ip":"10.131.0.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.131.0.126"
        ],
        "mac": "0a:58:0a:83:00:7e",
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.131.0.126"
        ],
        "mac": "0a:58:0a:83:00:7e",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 2a4fbabb-944d-4901-83ea-d017bac845a2 0xc008caf767 0xc008caf768}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2a4fbabb-944d-4901-83ea-d017bac845a2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 15:15:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.0.126\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-01-17 15:15:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p9fk7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p9fk7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-151-22.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lsx8j,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.151.22,PodIP:10.131.0.126,StartTime:2023-01-17 15:15:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 15:15:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://aff3be9d46f8f112905954eb9740333082a9047f4e2926e5530780bc24e8f3ca,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.131.0.126,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 17 15:15:58.505: INFO: Pod "webserver-deployment-845c8977d9-8df49" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-8df49 webserver-deployment-845c8977d9- deployment-8007  124541e6-5c7e-4fa1-9bbc-7274e20ee0fc 90458 0 2023-01-17 15:15:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.131.0.122/23"],"mac_address":"0a:58:0a:83:00:7a","gateway_ips":["10.131.0.1"],"ip_address":"10.131.0.122/23","gateway_ip":"10.131.0.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.131.0.122"
        ],
        "mac": "0a:58:0a:83:00:7a",
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.131.0.122"
        ],
        "mac": "0a:58:0a:83:00:7a",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 2a4fbabb-944d-4901-83ea-d017bac845a2 0xc008caf9c7 0xc008caf9c8}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2a4fbabb-944d-4901-83ea-d017bac845a2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-17 15:15:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-17 15:15:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.0.122\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5d9fg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5d9fg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-151-22.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lsx8j,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.151.22,PodIP:10.131.0.122,StartTime:2023-01-17 15:15:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 15:15:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://e42d49d9ceac35850b750ef58ae2624164b1f61444db46b1d0cd044f37c879ef,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.131.0.122,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 17 15:15:58.505: INFO: Pod "webserver-deployment-845c8977d9-94jwz" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-94jwz webserver-deployment-845c8977d9- deployment-8007  7a06015e-3b89-4d94-bf81-06b5207f2dd0 90013 0 2023-01-17 15:15:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.2.78/23"],"mac_address":"0a:58:0a:80:02:4e","gateway_ips":["10.128.2.1"],"ip_address":"10.128.2.78/23","gateway_ip":"10.128.2.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.128.2.78"
        ],
        "mac": "0a:58:0a:80:02:4e",
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.128.2.78"
        ],
        "mac": "0a:58:0a:80:02:4e",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 2a4fbabb-944d-4901-83ea-d017bac845a2 0xc008cafc27 0xc008cafc28}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:15:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:15:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2a4fbabb-944d-4901-83ea-d017bac845a2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 15:15:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.2.78\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-01-17 15:15:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-528kg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-528kg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-165-14.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.165.14,PodIP:10.128.2.78,StartTime:2023-01-17 15:15:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 15:15:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://df183609a49c7375ccfe4327e6cac30d0df6a4d31b7e9ef622899717a16b8d4c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.2.78,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 17 15:15:58.505: INFO: Pod "webserver-deployment-845c8977d9-9wbhw" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-9wbhw webserver-deployment-845c8977d9- deployment-8007  45a10e9b-07f1-4206-9fa0-b4e644155c45 90371 0 2023-01-17 15:15:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.2.86/23"],"mac_address":"0a:58:0a:80:02:56","gateway_ips":["10.128.2.1"],"ip_address":"10.128.2.86/23","gateway_ip":"10.128.2.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.128.2.86"
        ],
        "mac": "0a:58:0a:80:02:56",
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.128.2.86"
        ],
        "mac": "0a:58:0a:80:02:56",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 2a4fbabb-944d-4901-83ea-d017bac845a2 0xc008cafe97 0xc008cafe98}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2a4fbabb-944d-4901-83ea-d017bac845a2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 15:15:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.2.86\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-01-17 15:15:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mqm2s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mqm2s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-165-14.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lsx8j,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.165.14,PodIP:10.128.2.86,StartTime:2023-01-17 15:15:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 15:15:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://aba8236d65491623ea8768bfb170e6e75f696239a690b2cfdff85d468cdd4a47,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.2.86,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 17 15:15:58.505: INFO: Pod "webserver-deployment-845c8977d9-bdfv6" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-bdfv6 webserver-deployment-845c8977d9- deployment-8007  84929c39-62d3-4e2e-9a1b-568c3108fc74 90400 0 2023-01-17 15:15:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.129.2.54/23"],"mac_address":"0a:58:0a:81:02:36","gateway_ips":["10.129.2.1"],"ip_address":"10.129.2.54/23","gateway_ip":"10.129.2.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.129.2.54"
        ],
        "mac": "0a:58:0a:81:02:36",
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.129.2.54"
        ],
        "mac": "0a:58:0a:81:02:36",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 2a4fbabb-944d-4901-83ea-d017bac845a2 0xc006638107 0xc006638108}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2a4fbabb-944d-4901-83ea-d017bac845a2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 15:15:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.129.2.54\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-01-17 15:15:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xkhk8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xkhk8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-139-213.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lsx8j,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.139.213,PodIP:10.129.2.54,StartTime:2023-01-17 15:15:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 15:15:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://878358bcdf8b18423c5a3814f3c1602c3905bba9ae203286a7e3763c035102f1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.129.2.54,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 17 15:15:58.505: INFO: Pod "webserver-deployment-845c8977d9-cfjkl" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-cfjkl webserver-deployment-845c8977d9- deployment-8007  9e236366-b328-4973-b8c6-8ce512a49f02 90023 0 2023-01-17 15:15:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.129.2.46/23"],"mac_address":"0a:58:0a:81:02:2e","gateway_ips":["10.129.2.1"],"ip_address":"10.129.2.46/23","gateway_ip":"10.129.2.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.129.2.46"
        ],
        "mac": "0a:58:0a:81:02:2e",
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.129.2.46"
        ],
        "mac": "0a:58:0a:81:02:2e",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 2a4fbabb-944d-4901-83ea-d017bac845a2 0xc006638367 0xc006638368}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:15:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:15:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2a4fbabb-944d-4901-83ea-d017bac845a2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 15:15:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.129.2.46\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-01-17 15:15:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c628c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c628c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-139-213.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.139.213,PodIP:10.129.2.46,StartTime:2023-01-17 15:15:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 15:15:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://bd7eb7766c8eb245ce7138a1ff652b8b50217183bb4c731033b69e232fa0e03e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.129.2.46,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 17 15:15:58.505: INFO: Pod "webserver-deployment-845c8977d9-drq79" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-drq79 webserver-deployment-845c8977d9- deployment-8007  c7d1691a-42e1-44b5-a08c-ba127a4ee1c6 90384 0 2023-01-17 15:15:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.2.88/23"],"mac_address":"0a:58:0a:80:02:58","gateway_ips":["10.128.2.1"],"ip_address":"10.128.2.88/23","gateway_ip":"10.128.2.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.128.2.88"
        ],
        "mac": "0a:58:0a:80:02:58",
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.128.2.88"
        ],
        "mac": "0a:58:0a:80:02:58",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 2a4fbabb-944d-4901-83ea-d017bac845a2 0xc0066385c7 0xc0066385c8}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2a4fbabb-944d-4901-83ea-d017bac845a2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 15:15:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.2.88\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-01-17 15:15:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lnfv6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lnfv6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-165-14.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lsx8j,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.165.14,PodIP:10.128.2.88,StartTime:2023-01-17 15:15:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 15:15:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://68807b9744deb10df97f44488927a9e187416d40b9490cdfed73add319f09ecb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.2.88,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 17 15:15:58.505: INFO: Pod "webserver-deployment-845c8977d9-g5mj6" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-g5mj6 webserver-deployment-845c8977d9- deployment-8007  f4e4e3cb-e70c-4df9-a02a-0dd3e832f07b 90012 0 2023-01-17 15:15:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.2.79/23"],"mac_address":"0a:58:0a:80:02:4f","gateway_ips":["10.128.2.1"],"ip_address":"10.128.2.79/23","gateway_ip":"10.128.2.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.128.2.79"
        ],
        "mac": "0a:58:0a:80:02:4f",
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.128.2.79"
        ],
        "mac": "0a:58:0a:80:02:4f",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 2a4fbabb-944d-4901-83ea-d017bac845a2 0xc006638837 0xc006638838}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:15:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:15:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2a4fbabb-944d-4901-83ea-d017bac845a2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 15:15:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.2.79\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-01-17 15:15:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bn6wf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bn6wf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-165-14.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lsx8j,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.165.14,PodIP:10.128.2.79,StartTime:2023-01-17 15:15:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 15:15:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://d3c9f2663d8208e9aa35dba5e99a3b83e07e71b33891aa9db4e41ae6f2374140,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.2.79,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 17 15:15:58.505: INFO: Pod "webserver-deployment-845c8977d9-gxzf8" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-gxzf8 webserver-deployment-845c8977d9- deployment-8007  2849a6fc-302f-4529-a2dc-9c391f551b1c 90374 0 2023-01-17 15:15:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.2.87/23"],"mac_address":"0a:58:0a:80:02:57","gateway_ips":["10.128.2.1"],"ip_address":"10.128.2.87/23","gateway_ip":"10.128.2.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.128.2.87"
        ],
        "mac": "0a:58:0a:80:02:57",
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.128.2.87"
        ],
        "mac": "0a:58:0a:80:02:57",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 2a4fbabb-944d-4901-83ea-d017bac845a2 0xc006638a97 0xc006638a98}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2a4fbabb-944d-4901-83ea-d017bac845a2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 15:15:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.2.87\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-01-17 15:15:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7j6lj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7j6lj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-165-14.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lsx8j,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.165.14,PodIP:10.128.2.87,StartTime:2023-01-17 15:15:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 15:15:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://238fedac9c2e21d115629f26536532f0c1306a6d57fd5c06c2336933174e7e0c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.2.87,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 17 15:15:58.506: INFO: Pod "webserver-deployment-845c8977d9-hzczq" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-hzczq webserver-deployment-845c8977d9- deployment-8007  e0dbd774-85bb-4b47-b562-fa29bfa98d2e 90393 0 2023-01-17 15:15:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.129.2.51/23"],"mac_address":"0a:58:0a:81:02:33","gateway_ips":["10.129.2.1"],"ip_address":"10.129.2.51/23","gateway_ip":"10.129.2.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.129.2.51"
        ],
        "mac": "0a:58:0a:81:02:33",
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.129.2.51"
        ],
        "mac": "0a:58:0a:81:02:33",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 2a4fbabb-944d-4901-83ea-d017bac845a2 0xc006638cf7 0xc006638cf8}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2a4fbabb-944d-4901-83ea-d017bac845a2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 15:15:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.129.2.51\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-01-17 15:15:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k4hml,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k4hml,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-139-213.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lsx8j,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.139.213,PodIP:10.129.2.51,StartTime:2023-01-17 15:15:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 15:15:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://7ed4a3a59e7329cac03678ff0761eece326862ce6369a5db86e2cc17201a9213,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.129.2.51,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 17 15:15:58.506: INFO: Pod "webserver-deployment-845c8977d9-jxcwm" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-jxcwm webserver-deployment-845c8977d9- deployment-8007  51e86f3a-30c1-48d8-813e-970b43372e6f 90016 0 2023-01-17 15:15:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.129.2.47/23"],"mac_address":"0a:58:0a:81:02:2f","gateway_ips":["10.129.2.1"],"ip_address":"10.129.2.47/23","gateway_ip":"10.129.2.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.129.2.47"
        ],
        "mac": "0a:58:0a:81:02:2f",
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.129.2.47"
        ],
        "mac": "0a:58:0a:81:02:2f",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 2a4fbabb-944d-4901-83ea-d017bac845a2 0xc006638f57 0xc006638f58}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:15:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:15:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2a4fbabb-944d-4901-83ea-d017bac845a2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 15:15:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.129.2.47\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-01-17 15:15:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qvwf6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qvwf6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-139-213.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lsx8j,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.139.213,PodIP:10.129.2.47,StartTime:2023-01-17 15:15:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 15:15:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://48b3de9cf0b01a0b3544c0ce23d163ad1f1055e74786f83b2bd4e778cef12934,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.129.2.47,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 17 15:15:58.506: INFO: Pod "webserver-deployment-845c8977d9-kdwsd" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-kdwsd webserver-deployment-845c8977d9- deployment-8007  1bc08ceb-3149-4ebc-8d77-05a065b0ad09 90037 0 2023-01-17 15:15:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.131.0.118/23"],"mac_address":"0a:58:0a:83:00:76","gateway_ips":["10.131.0.1"],"ip_address":"10.131.0.118/23","gateway_ip":"10.131.0.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.131.0.118"
        ],
        "mac": "0a:58:0a:83:00:76",
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.131.0.118"
        ],
        "mac": "0a:58:0a:83:00:76",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 2a4fbabb-944d-4901-83ea-d017bac845a2 0xc0066391b7 0xc0066391b8}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:15:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:15:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2a4fbabb-944d-4901-83ea-d017bac845a2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-17 15:15:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-17 15:15:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.0.118\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n7r5h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n7r5h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-151-22.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lsx8j,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.151.22,PodIP:10.131.0.118,StartTime:2023-01-17 15:15:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 15:15:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://575835aa4b35f100976a840de55659c4adde1933bab7262ebae006d5dc612ce0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.131.0.118,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 17 15:15:58.506: INFO: Pod "webserver-deployment-845c8977d9-kjm8p" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-kjm8p webserver-deployment-845c8977d9- deployment-8007  78a924ac-a6cd-4a3c-833a-516b0f5b770a 90447 0 2023-01-17 15:15:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.131.0.125/23"],"mac_address":"0a:58:0a:83:00:7d","gateway_ips":["10.131.0.1"],"ip_address":"10.131.0.125/23","gateway_ip":"10.131.0.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.131.0.125"
        ],
        "mac": "0a:58:0a:83:00:7d",
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.131.0.125"
        ],
        "mac": "0a:58:0a:83:00:7d",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 2a4fbabb-944d-4901-83ea-d017bac845a2 0xc006639417 0xc006639418}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2a4fbabb-944d-4901-83ea-d017bac845a2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-17 15:15:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-17 15:15:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.0.125\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vb44v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vb44v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-151-22.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lsx8j,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.151.22,PodIP:10.131.0.125,StartTime:2023-01-17 15:15:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 15:15:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://df8beef96dae2f8774c44783a9c6755a30e65ac5463d3af39c0c4ccae489fc3c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.131.0.125,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 17 15:15:58.506: INFO: Pod "webserver-deployment-845c8977d9-n4w2s" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-n4w2s webserver-deployment-845c8977d9- deployment-8007  9fb5c383-269f-47a9-bb9e-29f3c0c9b418 90031 0 2023-01-17 15:15:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.131.0.116/23"],"mac_address":"0a:58:0a:83:00:74","gateway_ips":["10.131.0.1"],"ip_address":"10.131.0.116/23","gateway_ip":"10.131.0.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.131.0.116"
        ],
        "mac": "0a:58:0a:83:00:74",
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.131.0.116"
        ],
        "mac": "0a:58:0a:83:00:74",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 2a4fbabb-944d-4901-83ea-d017bac845a2 0xc006639677 0xc006639678}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:15:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:15:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2a4fbabb-944d-4901-83ea-d017bac845a2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-17 15:15:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-17 15:15:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.0.116\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6cx54,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6cx54,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-151-22.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lsx8j,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.151.22,PodIP:10.131.0.116,StartTime:2023-01-17 15:15:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 15:15:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://4b844a8cb1333abe33a451f82f89e4077ce50ca3273cc545b65880c683168c3b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.131.0.116,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 17 15:15:58.506: INFO: Pod "webserver-deployment-845c8977d9-qtcfr" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-qtcfr webserver-deployment-845c8977d9- deployment-8007  89fdfef2-4a88-4ec3-9539-bda327f47e0d 90452 0 2023-01-17 15:15:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.131.0.127/23"],"mac_address":"0a:58:0a:83:00:7f","gateway_ips":["10.131.0.1"],"ip_address":"10.131.0.127/23","gateway_ip":"10.131.0.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.131.0.127"
        ],
        "mac": "0a:58:0a:83:00:7f",
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.131.0.127"
        ],
        "mac": "0a:58:0a:83:00:7f",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 2a4fbabb-944d-4901-83ea-d017bac845a2 0xc0066398d7 0xc0066398d8}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2a4fbabb-944d-4901-83ea-d017bac845a2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-17 15:15:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-17 15:15:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.0.127\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vt8nv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vt8nv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-151-22.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lsx8j,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.151.22,PodIP:10.131.0.127,StartTime:2023-01-17 15:15:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 15:15:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://f944b513acaf7c3530ae47c7dada438b6f50079249add7365c7c4e1bd3af0ae6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.131.0.127,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 17 15:15:58.506: INFO: Pod "webserver-deployment-845c8977d9-vhslc" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-vhslc webserver-deployment-845c8977d9- deployment-8007  6cd64616-ef07-4553-9224-e18239fb35f6 90388 0 2023-01-17 15:15:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.129.2.52/23"],"mac_address":"0a:58:0a:81:02:34","gateway_ips":["10.129.2.1"],"ip_address":"10.129.2.52/23","gateway_ip":"10.129.2.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.129.2.52"
        ],
        "mac": "0a:58:0a:81:02:34",
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.129.2.52"
        ],
        "mac": "0a:58:0a:81:02:34",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 2a4fbabb-944d-4901-83ea-d017bac845a2 0xc006639cf7 0xc006639cf8}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:15:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2a4fbabb-944d-4901-83ea-d017bac845a2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 15:15:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.129.2.52\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-01-17 15:15:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l2vzg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l2vzg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-139-213.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lsx8j,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.139.213,PodIP:10.129.2.52,StartTime:2023-01-17 15:15:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 15:15:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://a9ae880ae9f4d1475fd1716bfe896f250e5dbc0aabe6d0b2a237a671b0e3cc71,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.129.2.52,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 17 15:15:58.506: INFO: Pod "webserver-deployment-845c8977d9-vtl7l" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-vtl7l webserver-deployment-845c8977d9- deployment-8007  d71d60f0-ab70-4999-a4d9-4384fb052af6 90019 0 2023-01-17 15:15:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.129.2.48/23"],"mac_address":"0a:58:0a:81:02:30","gateway_ips":["10.129.2.1"],"ip_address":"10.129.2.48/23","gateway_ip":"10.129.2.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.129.2.48"
        ],
        "mac": "0a:58:0a:81:02:30",
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.129.2.48"
        ],
        "mac": "0a:58:0a:81:02:30",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 2a4fbabb-944d-4901-83ea-d017bac845a2 0xc006639f57 0xc006639f58}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:15:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:15:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2a4fbabb-944d-4901-83ea-d017bac845a2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 15:15:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.129.2.48\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-01-17 15:15:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-98lq5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-98lq5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-139-213.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-lsx8j,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:15:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.139.213,PodIP:10.129.2.48,StartTime:2023-01-17 15:15:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 15:15:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://5c8b6f14f274bc1150c2b4ecd105dec0ce95e35cb3dcb6153ccb4048792675e6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.129.2.48,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan 17 15:15:58.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-8007" for this suite. 01/17/23 15:15:58.511
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:960
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:15:58.519
Jan 17 15:15:58.520: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename kubectl 01/17/23 15:15:58.52
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:15:58.545
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:15:58.549
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:960
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/17/23 15:15:58.551
Jan 17 15:15:58.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-2514 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jan 17 15:15:58.609: INFO: stderr: ""
Jan 17 15:15:58.609: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 01/17/23 15:15:58.609
Jan 17 15:15:58.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-2514 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
Jan 17 15:15:59.639: INFO: stderr: ""
Jan 17 15:15:59.639: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/17/23 15:15:59.639
Jan 17 15:15:59.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-2514 delete pods e2e-test-httpd-pod'
Jan 17 15:16:01.766: INFO: stderr: ""
Jan 17 15:16:01.766: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 17 15:16:01.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2514" for this suite. 01/17/23 15:16:01.77
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","completed":125,"skipped":2358,"failed":0}
------------------------------
• [3.258 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:954
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:960

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:15:58.519
    Jan 17 15:15:58.520: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename kubectl 01/17/23 15:15:58.52
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:15:58.545
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:15:58.549
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:960
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/17/23 15:15:58.551
    Jan 17 15:15:58.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-2514 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Jan 17 15:15:58.609: INFO: stderr: ""
    Jan 17 15:15:58.609: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 01/17/23 15:15:58.609
    Jan 17 15:15:58.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-2514 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
    Jan 17 15:15:59.639: INFO: stderr: ""
    Jan 17 15:15:59.639: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/17/23 15:15:59.639
    Jan 17 15:15:59.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-2514 delete pods e2e-test-httpd-pod'
    Jan 17 15:16:01.766: INFO: stderr: ""
    Jan 17 15:16:01.766: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 17 15:16:01.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-2514" for this suite. 01/17/23 15:16:01.77
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:16:01.778
Jan 17 15:16:01.779: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename runtimeclass 01/17/23 15:16:01.779
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:16:01.816
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:16:01.818
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 01/17/23 15:16:01.82
STEP: getting /apis/node.k8s.io 01/17/23 15:16:01.823
STEP: getting /apis/node.k8s.io/v1 01/17/23 15:16:01.824
STEP: creating 01/17/23 15:16:01.824
STEP: watching 01/17/23 15:16:01.844
Jan 17 15:16:01.844: INFO: starting watch
STEP: getting 01/17/23 15:16:01.848
STEP: listing 01/17/23 15:16:01.858
STEP: patching 01/17/23 15:16:01.865
STEP: updating 01/17/23 15:16:01.87
Jan 17 15:16:01.876: INFO: waiting for watch events with expected annotations
STEP: deleting 01/17/23 15:16:01.876
STEP: deleting a collection 01/17/23 15:16:01.889
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Jan 17 15:16:01.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-5874" for this suite. 01/17/23 15:16:01.91
{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","completed":126,"skipped":2377,"failed":0}
------------------------------
• [0.142 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:16:01.778
    Jan 17 15:16:01.779: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename runtimeclass 01/17/23 15:16:01.779
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:16:01.816
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:16:01.818
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 01/17/23 15:16:01.82
    STEP: getting /apis/node.k8s.io 01/17/23 15:16:01.823
    STEP: getting /apis/node.k8s.io/v1 01/17/23 15:16:01.824
    STEP: creating 01/17/23 15:16:01.824
    STEP: watching 01/17/23 15:16:01.844
    Jan 17 15:16:01.844: INFO: starting watch
    STEP: getting 01/17/23 15:16:01.848
    STEP: listing 01/17/23 15:16:01.858
    STEP: patching 01/17/23 15:16:01.865
    STEP: updating 01/17/23 15:16:01.87
    Jan 17 15:16:01.876: INFO: waiting for watch events with expected annotations
    STEP: deleting 01/17/23 15:16:01.876
    STEP: deleting a collection 01/17/23 15:16:01.889
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Jan 17 15:16:01.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-5874" for this suite. 01/17/23 15:16:01.91
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:16:01.922
Jan 17 15:16:01.922: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename crd-publish-openapi 01/17/23 15:16:01.923
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:16:01.962
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:16:01.969
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390
STEP: set up a multi version CRD 01/17/23 15:16:01.972
Jan 17 15:16:01.973: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: rename a version 01/17/23 15:16:16.715
STEP: check the new version name is served 01/17/23 15:16:16.725
STEP: check the old version name is removed 01/17/23 15:16:22.641
STEP: check the other version is not changed 01/17/23 15:16:24.833
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 17 15:16:35.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1339" for this suite. 01/17/23 15:16:35.693
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","completed":127,"skipped":2417,"failed":0}
------------------------------
• [SLOW TEST] [33.777 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:16:01.922
    Jan 17 15:16:01.922: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename crd-publish-openapi 01/17/23 15:16:01.923
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:16:01.962
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:16:01.969
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:390
    STEP: set up a multi version CRD 01/17/23 15:16:01.972
    Jan 17 15:16:01.973: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: rename a version 01/17/23 15:16:16.715
    STEP: check the new version name is served 01/17/23 15:16:16.725
    STEP: check the old version name is removed 01/17/23 15:16:22.641
    STEP: check the other version is not changed 01/17/23 15:16:24.833
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 17 15:16:35.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-1339" for this suite. 01/17/23 15:16:35.693
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:16:35.699
Jan 17 15:16:35.699: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename secrets 01/17/23 15:16:35.699
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:16:35.714
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:16:35.716
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88
STEP: Creating secret with name secret-test-map-78018191-4459-40a6-b4d3-0aa8626ff7d0 01/17/23 15:16:35.72
STEP: Creating a pod to test consume secrets 01/17/23 15:16:35.724
Jan 17 15:16:35.747: INFO: Waiting up to 5m0s for pod "pod-secrets-3f4c4987-a2ca-44da-ad84-479a6e2e05c2" in namespace "secrets-7620" to be "Succeeded or Failed"
Jan 17 15:16:35.755: INFO: Pod "pod-secrets-3f4c4987-a2ca-44da-ad84-479a6e2e05c2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.481996ms
Jan 17 15:16:37.762: INFO: Pod "pod-secrets-3f4c4987-a2ca-44da-ad84-479a6e2e05c2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01537753s
Jan 17 15:16:39.766: INFO: Pod "pod-secrets-3f4c4987-a2ca-44da-ad84-479a6e2e05c2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018975181s
STEP: Saw pod success 01/17/23 15:16:39.766
Jan 17 15:16:39.766: INFO: Pod "pod-secrets-3f4c4987-a2ca-44da-ad84-479a6e2e05c2" satisfied condition "Succeeded or Failed"
Jan 17 15:16:39.768: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-secrets-3f4c4987-a2ca-44da-ad84-479a6e2e05c2 container secret-volume-test: <nil>
STEP: delete the pod 01/17/23 15:16:39.784
Jan 17 15:16:39.805: INFO: Waiting for pod pod-secrets-3f4c4987-a2ca-44da-ad84-479a6e2e05c2 to disappear
Jan 17 15:16:39.808: INFO: Pod pod-secrets-3f4c4987-a2ca-44da-ad84-479a6e2e05c2 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan 17 15:16:39.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7620" for this suite. 01/17/23 15:16:39.819
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":128,"skipped":2417,"failed":0}
------------------------------
• [4.142 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:16:35.699
    Jan 17 15:16:35.699: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename secrets 01/17/23 15:16:35.699
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:16:35.714
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:16:35.716
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:88
    STEP: Creating secret with name secret-test-map-78018191-4459-40a6-b4d3-0aa8626ff7d0 01/17/23 15:16:35.72
    STEP: Creating a pod to test consume secrets 01/17/23 15:16:35.724
    Jan 17 15:16:35.747: INFO: Waiting up to 5m0s for pod "pod-secrets-3f4c4987-a2ca-44da-ad84-479a6e2e05c2" in namespace "secrets-7620" to be "Succeeded or Failed"
    Jan 17 15:16:35.755: INFO: Pod "pod-secrets-3f4c4987-a2ca-44da-ad84-479a6e2e05c2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.481996ms
    Jan 17 15:16:37.762: INFO: Pod "pod-secrets-3f4c4987-a2ca-44da-ad84-479a6e2e05c2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01537753s
    Jan 17 15:16:39.766: INFO: Pod "pod-secrets-3f4c4987-a2ca-44da-ad84-479a6e2e05c2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018975181s
    STEP: Saw pod success 01/17/23 15:16:39.766
    Jan 17 15:16:39.766: INFO: Pod "pod-secrets-3f4c4987-a2ca-44da-ad84-479a6e2e05c2" satisfied condition "Succeeded or Failed"
    Jan 17 15:16:39.768: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-secrets-3f4c4987-a2ca-44da-ad84-479a6e2e05c2 container secret-volume-test: <nil>
    STEP: delete the pod 01/17/23 15:16:39.784
    Jan 17 15:16:39.805: INFO: Waiting for pod pod-secrets-3f4c4987-a2ca-44da-ad84-479a6e2e05c2 to disappear
    Jan 17 15:16:39.808: INFO: Pod pod-secrets-3f4c4987-a2ca-44da-ad84-479a6e2e05c2 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan 17 15:16:39.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-7620" for this suite. 01/17/23 15:16:39.819
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:16:39.841
Jan 17 15:16:39.841: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename emptydir 01/17/23 15:16:39.842
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:16:39.856
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:16:39.858
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156
STEP: Creating a pod to test emptydir volume type on node default medium 01/17/23 15:16:39.869
Jan 17 15:16:39.898: INFO: Waiting up to 5m0s for pod "pod-b077bb19-c1f4-4d8d-8b4f-af002174b35b" in namespace "emptydir-1944" to be "Succeeded or Failed"
Jan 17 15:16:39.910: INFO: Pod "pod-b077bb19-c1f4-4d8d-8b4f-af002174b35b": Phase="Pending", Reason="", readiness=false. Elapsed: 11.556657ms
Jan 17 15:16:41.915: INFO: Pod "pod-b077bb19-c1f4-4d8d-8b4f-af002174b35b": Phase="Running", Reason="", readiness=false. Elapsed: 2.016323123s
Jan 17 15:16:43.915: INFO: Pod "pod-b077bb19-c1f4-4d8d-8b4f-af002174b35b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016382075s
STEP: Saw pod success 01/17/23 15:16:43.915
Jan 17 15:16:43.915: INFO: Pod "pod-b077bb19-c1f4-4d8d-8b4f-af002174b35b" satisfied condition "Succeeded or Failed"
Jan 17 15:16:43.918: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-b077bb19-c1f4-4d8d-8b4f-af002174b35b container test-container: <nil>
STEP: delete the pod 01/17/23 15:16:43.923
Jan 17 15:16:43.932: INFO: Waiting for pod pod-b077bb19-c1f4-4d8d-8b4f-af002174b35b to disappear
Jan 17 15:16:43.934: INFO: Pod pod-b077bb19-c1f4-4d8d-8b4f-af002174b35b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 17 15:16:43.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1944" for this suite. 01/17/23 15:16:43.938
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","completed":129,"skipped":2424,"failed":0}
------------------------------
• [4.111 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:16:39.841
    Jan 17 15:16:39.841: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename emptydir 01/17/23 15:16:39.842
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:16:39.856
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:16:39.858
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:156
    STEP: Creating a pod to test emptydir volume type on node default medium 01/17/23 15:16:39.869
    Jan 17 15:16:39.898: INFO: Waiting up to 5m0s for pod "pod-b077bb19-c1f4-4d8d-8b4f-af002174b35b" in namespace "emptydir-1944" to be "Succeeded or Failed"
    Jan 17 15:16:39.910: INFO: Pod "pod-b077bb19-c1f4-4d8d-8b4f-af002174b35b": Phase="Pending", Reason="", readiness=false. Elapsed: 11.556657ms
    Jan 17 15:16:41.915: INFO: Pod "pod-b077bb19-c1f4-4d8d-8b4f-af002174b35b": Phase="Running", Reason="", readiness=false. Elapsed: 2.016323123s
    Jan 17 15:16:43.915: INFO: Pod "pod-b077bb19-c1f4-4d8d-8b4f-af002174b35b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016382075s
    STEP: Saw pod success 01/17/23 15:16:43.915
    Jan 17 15:16:43.915: INFO: Pod "pod-b077bb19-c1f4-4d8d-8b4f-af002174b35b" satisfied condition "Succeeded or Failed"
    Jan 17 15:16:43.918: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-b077bb19-c1f4-4d8d-8b4f-af002174b35b container test-container: <nil>
    STEP: delete the pod 01/17/23 15:16:43.923
    Jan 17 15:16:43.932: INFO: Waiting for pod pod-b077bb19-c1f4-4d8d-8b4f-af002174b35b to disappear
    Jan 17 15:16:43.934: INFO: Pod pod-b077bb19-c1f4-4d8d-8b4f-af002174b35b no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 17 15:16:43.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-1944" for this suite. 01/17/23 15:16:43.938
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:16:43.952
Jan 17 15:16:43.952: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename downward-api 01/17/23 15:16:43.953
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:16:43.972
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:16:43.975
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220
STEP: Creating a pod to test downward API volume plugin 01/17/23 15:16:43.977
Jan 17 15:16:43.997: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f3a6adac-d277-4d82-89b0-667ad155ca87" in namespace "downward-api-3617" to be "Succeeded or Failed"
Jan 17 15:16:44.002: INFO: Pod "downwardapi-volume-f3a6adac-d277-4d82-89b0-667ad155ca87": Phase="Pending", Reason="", readiness=false. Elapsed: 4.946357ms
Jan 17 15:16:46.007: INFO: Pod "downwardapi-volume-f3a6adac-d277-4d82-89b0-667ad155ca87": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009643146s
Jan 17 15:16:48.005: INFO: Pod "downwardapi-volume-f3a6adac-d277-4d82-89b0-667ad155ca87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007879139s
STEP: Saw pod success 01/17/23 15:16:48.005
Jan 17 15:16:48.005: INFO: Pod "downwardapi-volume-f3a6adac-d277-4d82-89b0-667ad155ca87" satisfied condition "Succeeded or Failed"
Jan 17 15:16:48.009: INFO: Trying to get logs from node ip-10-0-165-14.ec2.internal pod downwardapi-volume-f3a6adac-d277-4d82-89b0-667ad155ca87 container client-container: <nil>
STEP: delete the pod 01/17/23 15:16:48.021
Jan 17 15:16:48.032: INFO: Waiting for pod downwardapi-volume-f3a6adac-d277-4d82-89b0-667ad155ca87 to disappear
Jan 17 15:16:48.035: INFO: Pod downwardapi-volume-f3a6adac-d277-4d82-89b0-667ad155ca87 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 17 15:16:48.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3617" for this suite. 01/17/23 15:16:48.04
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","completed":130,"skipped":2428,"failed":0}
------------------------------
• [4.094 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:16:43.952
    Jan 17 15:16:43.952: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename downward-api 01/17/23 15:16:43.953
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:16:43.972
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:16:43.975
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:220
    STEP: Creating a pod to test downward API volume plugin 01/17/23 15:16:43.977
    Jan 17 15:16:43.997: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f3a6adac-d277-4d82-89b0-667ad155ca87" in namespace "downward-api-3617" to be "Succeeded or Failed"
    Jan 17 15:16:44.002: INFO: Pod "downwardapi-volume-f3a6adac-d277-4d82-89b0-667ad155ca87": Phase="Pending", Reason="", readiness=false. Elapsed: 4.946357ms
    Jan 17 15:16:46.007: INFO: Pod "downwardapi-volume-f3a6adac-d277-4d82-89b0-667ad155ca87": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009643146s
    Jan 17 15:16:48.005: INFO: Pod "downwardapi-volume-f3a6adac-d277-4d82-89b0-667ad155ca87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007879139s
    STEP: Saw pod success 01/17/23 15:16:48.005
    Jan 17 15:16:48.005: INFO: Pod "downwardapi-volume-f3a6adac-d277-4d82-89b0-667ad155ca87" satisfied condition "Succeeded or Failed"
    Jan 17 15:16:48.009: INFO: Trying to get logs from node ip-10-0-165-14.ec2.internal pod downwardapi-volume-f3a6adac-d277-4d82-89b0-667ad155ca87 container client-container: <nil>
    STEP: delete the pod 01/17/23 15:16:48.021
    Jan 17 15:16:48.032: INFO: Waiting for pod downwardapi-volume-f3a6adac-d277-4d82-89b0-667ad155ca87 to disappear
    Jan 17 15:16:48.035: INFO: Pod downwardapi-volume-f3a6adac-d277-4d82-89b0-667ad155ca87 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 17 15:16:48.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-3617" for this suite. 01/17/23 15:16:48.04
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:16:48.047
Jan 17 15:16:48.047: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename downward-api 01/17/23 15:16:48.047
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:16:48.066
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:16:48.069
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67
STEP: Creating a pod to test downward API volume plugin 01/17/23 15:16:48.073
W0117 15:16:48.097026      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 17 15:16:48.097: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8deb9d81-ba82-45c8-afc5-b8fdae9b8ffd" in namespace "downward-api-8071" to be "Succeeded or Failed"
Jan 17 15:16:48.100: INFO: Pod "downwardapi-volume-8deb9d81-ba82-45c8-afc5-b8fdae9b8ffd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.509679ms
Jan 17 15:16:50.104: INFO: Pod "downwardapi-volume-8deb9d81-ba82-45c8-afc5-b8fdae9b8ffd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007835671s
Jan 17 15:16:52.105: INFO: Pod "downwardapi-volume-8deb9d81-ba82-45c8-afc5-b8fdae9b8ffd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008825351s
STEP: Saw pod success 01/17/23 15:16:52.105
Jan 17 15:16:52.106: INFO: Pod "downwardapi-volume-8deb9d81-ba82-45c8-afc5-b8fdae9b8ffd" satisfied condition "Succeeded or Failed"
Jan 17 15:16:52.108: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod downwardapi-volume-8deb9d81-ba82-45c8-afc5-b8fdae9b8ffd container client-container: <nil>
STEP: delete the pod 01/17/23 15:16:52.113
Jan 17 15:16:52.124: INFO: Waiting for pod downwardapi-volume-8deb9d81-ba82-45c8-afc5-b8fdae9b8ffd to disappear
Jan 17 15:16:52.127: INFO: Pod downwardapi-volume-8deb9d81-ba82-45c8-afc5-b8fdae9b8ffd no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 17 15:16:52.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8071" for this suite. 01/17/23 15:16:52.131
{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","completed":131,"skipped":2435,"failed":0}
------------------------------
• [4.091 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:16:48.047
    Jan 17 15:16:48.047: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename downward-api 01/17/23 15:16:48.047
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:16:48.066
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:16:48.069
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:67
    STEP: Creating a pod to test downward API volume plugin 01/17/23 15:16:48.073
    W0117 15:16:48.097026      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jan 17 15:16:48.097: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8deb9d81-ba82-45c8-afc5-b8fdae9b8ffd" in namespace "downward-api-8071" to be "Succeeded or Failed"
    Jan 17 15:16:48.100: INFO: Pod "downwardapi-volume-8deb9d81-ba82-45c8-afc5-b8fdae9b8ffd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.509679ms
    Jan 17 15:16:50.104: INFO: Pod "downwardapi-volume-8deb9d81-ba82-45c8-afc5-b8fdae9b8ffd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007835671s
    Jan 17 15:16:52.105: INFO: Pod "downwardapi-volume-8deb9d81-ba82-45c8-afc5-b8fdae9b8ffd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008825351s
    STEP: Saw pod success 01/17/23 15:16:52.105
    Jan 17 15:16:52.106: INFO: Pod "downwardapi-volume-8deb9d81-ba82-45c8-afc5-b8fdae9b8ffd" satisfied condition "Succeeded or Failed"
    Jan 17 15:16:52.108: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod downwardapi-volume-8deb9d81-ba82-45c8-afc5-b8fdae9b8ffd container client-container: <nil>
    STEP: delete the pod 01/17/23 15:16:52.113
    Jan 17 15:16:52.124: INFO: Waiting for pod downwardapi-volume-8deb9d81-ba82-45c8-afc5-b8fdae9b8ffd to disappear
    Jan 17 15:16:52.127: INFO: Pod downwardapi-volume-8deb9d81-ba82-45c8-afc5-b8fdae9b8ffd no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 17 15:16:52.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-8071" for this suite. 01/17/23 15:16:52.131
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:16:52.138
Jan 17 15:16:52.138: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename projected 01/17/23 15:16:52.138
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:16:52.156
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:16:52.16
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83
STEP: Creating a pod to test downward API volume plugin 01/17/23 15:16:52.164
Jan 17 15:16:52.205: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c50d1309-bb30-466d-89dc-488e13b40194" in namespace "projected-8310" to be "Succeeded or Failed"
Jan 17 15:16:52.208: INFO: Pod "downwardapi-volume-c50d1309-bb30-466d-89dc-488e13b40194": Phase="Pending", Reason="", readiness=false. Elapsed: 3.11341ms
Jan 17 15:16:54.212: INFO: Pod "downwardapi-volume-c50d1309-bb30-466d-89dc-488e13b40194": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006996847s
Jan 17 15:16:56.213: INFO: Pod "downwardapi-volume-c50d1309-bb30-466d-89dc-488e13b40194": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007986297s
STEP: Saw pod success 01/17/23 15:16:56.213
Jan 17 15:16:56.213: INFO: Pod "downwardapi-volume-c50d1309-bb30-466d-89dc-488e13b40194" satisfied condition "Succeeded or Failed"
Jan 17 15:16:56.217: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod downwardapi-volume-c50d1309-bb30-466d-89dc-488e13b40194 container client-container: <nil>
STEP: delete the pod 01/17/23 15:16:56.223
Jan 17 15:16:56.235: INFO: Waiting for pod downwardapi-volume-c50d1309-bb30-466d-89dc-488e13b40194 to disappear
Jan 17 15:16:56.237: INFO: Pod downwardapi-volume-c50d1309-bb30-466d-89dc-488e13b40194 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 17 15:16:56.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8310" for this suite. 01/17/23 15:16:56.242
{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","completed":132,"skipped":2438,"failed":0}
------------------------------
• [4.111 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:16:52.138
    Jan 17 15:16:52.138: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename projected 01/17/23 15:16:52.138
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:16:52.156
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:16:52.16
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:83
    STEP: Creating a pod to test downward API volume plugin 01/17/23 15:16:52.164
    Jan 17 15:16:52.205: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c50d1309-bb30-466d-89dc-488e13b40194" in namespace "projected-8310" to be "Succeeded or Failed"
    Jan 17 15:16:52.208: INFO: Pod "downwardapi-volume-c50d1309-bb30-466d-89dc-488e13b40194": Phase="Pending", Reason="", readiness=false. Elapsed: 3.11341ms
    Jan 17 15:16:54.212: INFO: Pod "downwardapi-volume-c50d1309-bb30-466d-89dc-488e13b40194": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006996847s
    Jan 17 15:16:56.213: INFO: Pod "downwardapi-volume-c50d1309-bb30-466d-89dc-488e13b40194": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007986297s
    STEP: Saw pod success 01/17/23 15:16:56.213
    Jan 17 15:16:56.213: INFO: Pod "downwardapi-volume-c50d1309-bb30-466d-89dc-488e13b40194" satisfied condition "Succeeded or Failed"
    Jan 17 15:16:56.217: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod downwardapi-volume-c50d1309-bb30-466d-89dc-488e13b40194 container client-container: <nil>
    STEP: delete the pod 01/17/23 15:16:56.223
    Jan 17 15:16:56.235: INFO: Waiting for pod downwardapi-volume-c50d1309-bb30-466d-89dc-488e13b40194 to disappear
    Jan 17 15:16:56.237: INFO: Pod downwardapi-volume-c50d1309-bb30-466d-89dc-488e13b40194 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 17 15:16:56.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8310" for this suite. 01/17/23 15:16:56.242
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:16:56.249
Jan 17 15:16:56.249: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename deployment 01/17/23 15:16:56.25
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:16:56.272
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:16:56.278
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 01/17/23 15:16:56.292
Jan 17 15:16:56.292: INFO: Creating simple deployment test-deployment-72c59
Jan 17 15:16:56.309: INFO: deployment "test-deployment-72c59" doesn't have the required revision set
STEP: Getting /status 01/17/23 15:16:58.321
Jan 17 15:16:58.326: INFO: Deployment test-deployment-72c59 has Conditions: [{Available True 2023-01-17 15:16:57 +0000 UTC 2023-01-17 15:16:57 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-01-17 15:16:57 +0000 UTC 2023-01-17 15:16:56 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-72c59-777898ffcc" has successfully progressed.}]
STEP: updating Deployment Status 01/17/23 15:16:58.326
Jan 17 15:16:58.335: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 15, 16, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 15, 16, 57, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 15, 16, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 15, 16, 56, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-72c59-777898ffcc\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 01/17/23 15:16:58.335
Jan 17 15:16:58.337: INFO: Observed &Deployment event: ADDED
Jan 17 15:16:58.337: INFO: Observed Deployment test-deployment-72c59 in namespace deployment-4592 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-17 15:16:56 +0000 UTC 2023-01-17 15:16:56 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-72c59-777898ffcc"}
Jan 17 15:16:58.337: INFO: Observed &Deployment event: MODIFIED
Jan 17 15:16:58.337: INFO: Observed Deployment test-deployment-72c59 in namespace deployment-4592 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-17 15:16:56 +0000 UTC 2023-01-17 15:16:56 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-72c59-777898ffcc"}
Jan 17 15:16:58.337: INFO: Observed Deployment test-deployment-72c59 in namespace deployment-4592 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-17 15:16:56 +0000 UTC 2023-01-17 15:16:56 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 17 15:16:58.337: INFO: Observed &Deployment event: MODIFIED
Jan 17 15:16:58.337: INFO: Observed Deployment test-deployment-72c59 in namespace deployment-4592 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-17 15:16:56 +0000 UTC 2023-01-17 15:16:56 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 17 15:16:58.337: INFO: Observed Deployment test-deployment-72c59 in namespace deployment-4592 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-17 15:16:56 +0000 UTC 2023-01-17 15:16:56 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-72c59-777898ffcc" is progressing.}
Jan 17 15:16:58.337: INFO: Observed &Deployment event: MODIFIED
Jan 17 15:16:58.337: INFO: Observed Deployment test-deployment-72c59 in namespace deployment-4592 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-17 15:16:57 +0000 UTC 2023-01-17 15:16:57 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 17 15:16:58.337: INFO: Observed Deployment test-deployment-72c59 in namespace deployment-4592 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-17 15:16:57 +0000 UTC 2023-01-17 15:16:56 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-72c59-777898ffcc" has successfully progressed.}
Jan 17 15:16:58.337: INFO: Observed &Deployment event: MODIFIED
Jan 17 15:16:58.337: INFO: Observed Deployment test-deployment-72c59 in namespace deployment-4592 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-17 15:16:57 +0000 UTC 2023-01-17 15:16:57 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 17 15:16:58.337: INFO: Observed Deployment test-deployment-72c59 in namespace deployment-4592 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-17 15:16:57 +0000 UTC 2023-01-17 15:16:56 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-72c59-777898ffcc" has successfully progressed.}
Jan 17 15:16:58.337: INFO: Found Deployment test-deployment-72c59 in namespace deployment-4592 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 17 15:16:58.337: INFO: Deployment test-deployment-72c59 has an updated status
STEP: patching the Statefulset Status 01/17/23 15:16:58.337
Jan 17 15:16:58.337: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan 17 15:16:58.342: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 01/17/23 15:16:58.342
Jan 17 15:16:58.343: INFO: Observed &Deployment event: ADDED
Jan 17 15:16:58.343: INFO: Observed deployment test-deployment-72c59 in namespace deployment-4592 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-17 15:16:56 +0000 UTC 2023-01-17 15:16:56 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-72c59-777898ffcc"}
Jan 17 15:16:58.343: INFO: Observed &Deployment event: MODIFIED
Jan 17 15:16:58.343: INFO: Observed deployment test-deployment-72c59 in namespace deployment-4592 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-17 15:16:56 +0000 UTC 2023-01-17 15:16:56 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-72c59-777898ffcc"}
Jan 17 15:16:58.343: INFO: Observed deployment test-deployment-72c59 in namespace deployment-4592 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-17 15:16:56 +0000 UTC 2023-01-17 15:16:56 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 17 15:16:58.343: INFO: Observed &Deployment event: MODIFIED
Jan 17 15:16:58.343: INFO: Observed deployment test-deployment-72c59 in namespace deployment-4592 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-17 15:16:56 +0000 UTC 2023-01-17 15:16:56 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 17 15:16:58.343: INFO: Observed deployment test-deployment-72c59 in namespace deployment-4592 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-17 15:16:56 +0000 UTC 2023-01-17 15:16:56 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-72c59-777898ffcc" is progressing.}
Jan 17 15:16:58.343: INFO: Observed &Deployment event: MODIFIED
Jan 17 15:16:58.343: INFO: Observed deployment test-deployment-72c59 in namespace deployment-4592 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-17 15:16:57 +0000 UTC 2023-01-17 15:16:57 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 17 15:16:58.343: INFO: Observed deployment test-deployment-72c59 in namespace deployment-4592 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-17 15:16:57 +0000 UTC 2023-01-17 15:16:56 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-72c59-777898ffcc" has successfully progressed.}
Jan 17 15:16:58.344: INFO: Observed &Deployment event: MODIFIED
Jan 17 15:16:58.344: INFO: Observed deployment test-deployment-72c59 in namespace deployment-4592 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-17 15:16:57 +0000 UTC 2023-01-17 15:16:57 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 17 15:16:58.344: INFO: Observed deployment test-deployment-72c59 in namespace deployment-4592 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-17 15:16:57 +0000 UTC 2023-01-17 15:16:56 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-72c59-777898ffcc" has successfully progressed.}
Jan 17 15:16:58.344: INFO: Observed deployment test-deployment-72c59 in namespace deployment-4592 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 17 15:16:58.344: INFO: Observed &Deployment event: MODIFIED
Jan 17 15:16:58.344: INFO: Found deployment test-deployment-72c59 in namespace deployment-4592 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Jan 17 15:16:58.344: INFO: Deployment test-deployment-72c59 has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 17 15:16:58.350: INFO: Deployment "test-deployment-72c59":
&Deployment{ObjectMeta:{test-deployment-72c59  deployment-4592  030f286d-3bcf-453a-aacb-d74d5dd0650c 91709 1 2023-01-17 15:16:56 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-01-17 15:16:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 15:16:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2023-01-17 15:16:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0067794f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 17 15:16:58.352: INFO: New ReplicaSet "test-deployment-72c59-777898ffcc" of Deployment "test-deployment-72c59":
&ReplicaSet{ObjectMeta:{test-deployment-72c59-777898ffcc  deployment-4592  e703933d-d5c7-4741-b0fa-bff6b31caee2 91700 1 2023-01-17 15:16:56 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-72c59 030f286d-3bcf-453a-aacb-d74d5dd0650c 0xc007378bf0 0xc007378bf1}] [] [{kube-controller-manager Update apps/v1 2023-01-17 15:16:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"030f286d-3bcf-453a-aacb-d74d5dd0650c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 15:16:57 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 777898ffcc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc007378c98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 17 15:16:58.355: INFO: Pod "test-deployment-72c59-777898ffcc-xplfl" is available:
&Pod{ObjectMeta:{test-deployment-72c59-777898ffcc-xplfl test-deployment-72c59-777898ffcc- deployment-4592  4be6e6f2-e972-4dc5-b375-1483144182a5 91699 0 2023-01-17 15:16:56 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.131.0.133/23"],"mac_address":"0a:58:0a:83:00:85","gateway_ips":["10.131.0.1"],"ip_address":"10.131.0.133/23","gateway_ip":"10.131.0.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.131.0.133"
    ],
    "mac": "0a:58:0a:83:00:85",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.131.0.133"
    ],
    "mac": "0a:58:0a:83:00:85",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-72c59-777898ffcc e703933d-d5c7-4741-b0fa-bff6b31caee2 0xc007379077 0xc007379078}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:16:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:16:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e703933d-d5c7-4741-b0fa-bff6b31caee2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-17 15:16:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-17 15:16:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.0.133\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-trft2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-trft2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-151-22.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c46,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:16:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:16:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:16:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:16:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.151.22,PodIP:10.131.0.133,StartTime:2023-01-17 15:16:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 15:16:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://52d67371d4b9867c8e5dff15bfa4ee3c07e2613895a0fa510958c646f6db7733,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.131.0.133,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan 17 15:16:58.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4592" for this suite. 01/17/23 15:16:58.361
{"msg":"PASSED [sig-apps] Deployment should validate Deployment Status endpoints [Conformance]","completed":133,"skipped":2463,"failed":0}
------------------------------
• [2.118 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:16:56.249
    Jan 17 15:16:56.249: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename deployment 01/17/23 15:16:56.25
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:16:56.272
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:16:56.278
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 01/17/23 15:16:56.292
    Jan 17 15:16:56.292: INFO: Creating simple deployment test-deployment-72c59
    Jan 17 15:16:56.309: INFO: deployment "test-deployment-72c59" doesn't have the required revision set
    STEP: Getting /status 01/17/23 15:16:58.321
    Jan 17 15:16:58.326: INFO: Deployment test-deployment-72c59 has Conditions: [{Available True 2023-01-17 15:16:57 +0000 UTC 2023-01-17 15:16:57 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-01-17 15:16:57 +0000 UTC 2023-01-17 15:16:56 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-72c59-777898ffcc" has successfully progressed.}]
    STEP: updating Deployment Status 01/17/23 15:16:58.326
    Jan 17 15:16:58.335: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 15, 16, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 15, 16, 57, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 15, 16, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 15, 16, 56, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-72c59-777898ffcc\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 01/17/23 15:16:58.335
    Jan 17 15:16:58.337: INFO: Observed &Deployment event: ADDED
    Jan 17 15:16:58.337: INFO: Observed Deployment test-deployment-72c59 in namespace deployment-4592 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-17 15:16:56 +0000 UTC 2023-01-17 15:16:56 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-72c59-777898ffcc"}
    Jan 17 15:16:58.337: INFO: Observed &Deployment event: MODIFIED
    Jan 17 15:16:58.337: INFO: Observed Deployment test-deployment-72c59 in namespace deployment-4592 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-17 15:16:56 +0000 UTC 2023-01-17 15:16:56 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-72c59-777898ffcc"}
    Jan 17 15:16:58.337: INFO: Observed Deployment test-deployment-72c59 in namespace deployment-4592 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-17 15:16:56 +0000 UTC 2023-01-17 15:16:56 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan 17 15:16:58.337: INFO: Observed &Deployment event: MODIFIED
    Jan 17 15:16:58.337: INFO: Observed Deployment test-deployment-72c59 in namespace deployment-4592 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-17 15:16:56 +0000 UTC 2023-01-17 15:16:56 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan 17 15:16:58.337: INFO: Observed Deployment test-deployment-72c59 in namespace deployment-4592 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-17 15:16:56 +0000 UTC 2023-01-17 15:16:56 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-72c59-777898ffcc" is progressing.}
    Jan 17 15:16:58.337: INFO: Observed &Deployment event: MODIFIED
    Jan 17 15:16:58.337: INFO: Observed Deployment test-deployment-72c59 in namespace deployment-4592 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-17 15:16:57 +0000 UTC 2023-01-17 15:16:57 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan 17 15:16:58.337: INFO: Observed Deployment test-deployment-72c59 in namespace deployment-4592 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-17 15:16:57 +0000 UTC 2023-01-17 15:16:56 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-72c59-777898ffcc" has successfully progressed.}
    Jan 17 15:16:58.337: INFO: Observed &Deployment event: MODIFIED
    Jan 17 15:16:58.337: INFO: Observed Deployment test-deployment-72c59 in namespace deployment-4592 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-17 15:16:57 +0000 UTC 2023-01-17 15:16:57 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan 17 15:16:58.337: INFO: Observed Deployment test-deployment-72c59 in namespace deployment-4592 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-17 15:16:57 +0000 UTC 2023-01-17 15:16:56 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-72c59-777898ffcc" has successfully progressed.}
    Jan 17 15:16:58.337: INFO: Found Deployment test-deployment-72c59 in namespace deployment-4592 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan 17 15:16:58.337: INFO: Deployment test-deployment-72c59 has an updated status
    STEP: patching the Statefulset Status 01/17/23 15:16:58.337
    Jan 17 15:16:58.337: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jan 17 15:16:58.342: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 01/17/23 15:16:58.342
    Jan 17 15:16:58.343: INFO: Observed &Deployment event: ADDED
    Jan 17 15:16:58.343: INFO: Observed deployment test-deployment-72c59 in namespace deployment-4592 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-17 15:16:56 +0000 UTC 2023-01-17 15:16:56 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-72c59-777898ffcc"}
    Jan 17 15:16:58.343: INFO: Observed &Deployment event: MODIFIED
    Jan 17 15:16:58.343: INFO: Observed deployment test-deployment-72c59 in namespace deployment-4592 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-17 15:16:56 +0000 UTC 2023-01-17 15:16:56 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-72c59-777898ffcc"}
    Jan 17 15:16:58.343: INFO: Observed deployment test-deployment-72c59 in namespace deployment-4592 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-17 15:16:56 +0000 UTC 2023-01-17 15:16:56 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan 17 15:16:58.343: INFO: Observed &Deployment event: MODIFIED
    Jan 17 15:16:58.343: INFO: Observed deployment test-deployment-72c59 in namespace deployment-4592 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-17 15:16:56 +0000 UTC 2023-01-17 15:16:56 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan 17 15:16:58.343: INFO: Observed deployment test-deployment-72c59 in namespace deployment-4592 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-17 15:16:56 +0000 UTC 2023-01-17 15:16:56 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-72c59-777898ffcc" is progressing.}
    Jan 17 15:16:58.343: INFO: Observed &Deployment event: MODIFIED
    Jan 17 15:16:58.343: INFO: Observed deployment test-deployment-72c59 in namespace deployment-4592 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-17 15:16:57 +0000 UTC 2023-01-17 15:16:57 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan 17 15:16:58.343: INFO: Observed deployment test-deployment-72c59 in namespace deployment-4592 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-17 15:16:57 +0000 UTC 2023-01-17 15:16:56 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-72c59-777898ffcc" has successfully progressed.}
    Jan 17 15:16:58.344: INFO: Observed &Deployment event: MODIFIED
    Jan 17 15:16:58.344: INFO: Observed deployment test-deployment-72c59 in namespace deployment-4592 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-17 15:16:57 +0000 UTC 2023-01-17 15:16:57 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan 17 15:16:58.344: INFO: Observed deployment test-deployment-72c59 in namespace deployment-4592 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-17 15:16:57 +0000 UTC 2023-01-17 15:16:56 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-72c59-777898ffcc" has successfully progressed.}
    Jan 17 15:16:58.344: INFO: Observed deployment test-deployment-72c59 in namespace deployment-4592 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan 17 15:16:58.344: INFO: Observed &Deployment event: MODIFIED
    Jan 17 15:16:58.344: INFO: Found deployment test-deployment-72c59 in namespace deployment-4592 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    Jan 17 15:16:58.344: INFO: Deployment test-deployment-72c59 has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 17 15:16:58.350: INFO: Deployment "test-deployment-72c59":
    &Deployment{ObjectMeta:{test-deployment-72c59  deployment-4592  030f286d-3bcf-453a-aacb-d74d5dd0650c 91709 1 2023-01-17 15:16:56 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-01-17 15:16:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 15:16:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2023-01-17 15:16:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0067794f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan 17 15:16:58.352: INFO: New ReplicaSet "test-deployment-72c59-777898ffcc" of Deployment "test-deployment-72c59":
    &ReplicaSet{ObjectMeta:{test-deployment-72c59-777898ffcc  deployment-4592  e703933d-d5c7-4741-b0fa-bff6b31caee2 91700 1 2023-01-17 15:16:56 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-72c59 030f286d-3bcf-453a-aacb-d74d5dd0650c 0xc007378bf0 0xc007378bf1}] [] [{kube-controller-manager Update apps/v1 2023-01-17 15:16:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"030f286d-3bcf-453a-aacb-d74d5dd0650c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 15:16:57 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 777898ffcc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc007378c98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 17 15:16:58.355: INFO: Pod "test-deployment-72c59-777898ffcc-xplfl" is available:
    &Pod{ObjectMeta:{test-deployment-72c59-777898ffcc-xplfl test-deployment-72c59-777898ffcc- deployment-4592  4be6e6f2-e972-4dc5-b375-1483144182a5 91699 0 2023-01-17 15:16:56 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.131.0.133/23"],"mac_address":"0a:58:0a:83:00:85","gateway_ips":["10.131.0.1"],"ip_address":"10.131.0.133/23","gateway_ip":"10.131.0.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.131.0.133"
        ],
        "mac": "0a:58:0a:83:00:85",
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.131.0.133"
        ],
        "mac": "0a:58:0a:83:00:85",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-72c59-777898ffcc e703933d-d5c7-4741-b0fa-bff6b31caee2 0xc007379077 0xc007379078}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:16:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:16:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e703933d-d5c7-4741-b0fa-bff6b31caee2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-17 15:16:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-17 15:16:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.0.133\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-trft2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-trft2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-151-22.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c46,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:16:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:16:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:16:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:16:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.151.22,PodIP:10.131.0.133,StartTime:2023-01-17 15:16:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 15:16:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://52d67371d4b9867c8e5dff15bfa4ee3c07e2613895a0fa510958c646f6db7733,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.131.0.133,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan 17 15:16:58.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-4592" for this suite. 01/17/23 15:16:58.361
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:16:58.368
Jan 17 15:16:58.368: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename replicaset 01/17/23 15:16:58.369
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:16:58.391
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:16:58.395
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 01/17/23 15:16:58.417
STEP: Verify that the required pods have come up. 01/17/23 15:16:58.429
Jan 17 15:16:58.435: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 17 15:17:03.439: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/17/23 15:17:03.439
STEP: Getting /status 01/17/23 15:17:03.439
Jan 17 15:17:03.441: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 01/17/23 15:17:03.441
Jan 17 15:17:03.450: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 01/17/23 15:17:03.45
Jan 17 15:17:03.452: INFO: Observed &ReplicaSet event: ADDED
Jan 17 15:17:03.452: INFO: Observed &ReplicaSet event: MODIFIED
Jan 17 15:17:03.452: INFO: Observed &ReplicaSet event: MODIFIED
Jan 17 15:17:03.452: INFO: Observed &ReplicaSet event: MODIFIED
Jan 17 15:17:03.452: INFO: Found replicaset test-rs in namespace replicaset-9616 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 17 15:17:03.452: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 01/17/23 15:17:03.452
Jan 17 15:17:03.452: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan 17 15:17:03.457: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 01/17/23 15:17:03.457
Jan 17 15:17:03.458: INFO: Observed &ReplicaSet event: ADDED
Jan 17 15:17:03.458: INFO: Observed &ReplicaSet event: MODIFIED
Jan 17 15:17:03.458: INFO: Observed &ReplicaSet event: MODIFIED
Jan 17 15:17:03.459: INFO: Observed &ReplicaSet event: MODIFIED
Jan 17 15:17:03.459: INFO: Observed replicaset test-rs in namespace replicaset-9616 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 17 15:17:03.459: INFO: Observed &ReplicaSet event: MODIFIED
Jan 17 15:17:03.459: INFO: Found replicaset test-rs in namespace replicaset-9616 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Jan 17 15:17:03.459: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jan 17 15:17:03.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9616" for this suite. 01/17/23 15:17:03.462
{"msg":"PASSED [sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]","completed":134,"skipped":2492,"failed":0}
------------------------------
• [SLOW TEST] [5.099 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:16:58.368
    Jan 17 15:16:58.368: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename replicaset 01/17/23 15:16:58.369
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:16:58.391
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:16:58.395
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 01/17/23 15:16:58.417
    STEP: Verify that the required pods have come up. 01/17/23 15:16:58.429
    Jan 17 15:16:58.435: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan 17 15:17:03.439: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/17/23 15:17:03.439
    STEP: Getting /status 01/17/23 15:17:03.439
    Jan 17 15:17:03.441: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 01/17/23 15:17:03.441
    Jan 17 15:17:03.450: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 01/17/23 15:17:03.45
    Jan 17 15:17:03.452: INFO: Observed &ReplicaSet event: ADDED
    Jan 17 15:17:03.452: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 17 15:17:03.452: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 17 15:17:03.452: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 17 15:17:03.452: INFO: Found replicaset test-rs in namespace replicaset-9616 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan 17 15:17:03.452: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 01/17/23 15:17:03.452
    Jan 17 15:17:03.452: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jan 17 15:17:03.457: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 01/17/23 15:17:03.457
    Jan 17 15:17:03.458: INFO: Observed &ReplicaSet event: ADDED
    Jan 17 15:17:03.458: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 17 15:17:03.458: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 17 15:17:03.459: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 17 15:17:03.459: INFO: Observed replicaset test-rs in namespace replicaset-9616 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan 17 15:17:03.459: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 17 15:17:03.459: INFO: Found replicaset test-rs in namespace replicaset-9616 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    Jan 17 15:17:03.459: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jan 17 15:17:03.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-9616" for this suite. 01/17/23 15:17:03.462
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:17:03.468
Jan 17 15:17:03.468: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename containers 01/17/23 15:17:03.468
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:17:03.487
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:17:03.491
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86
STEP: Creating a pod to test override all 01/17/23 15:17:03.493
Jan 17 15:17:03.517: INFO: Waiting up to 5m0s for pod "client-containers-065d3459-a11b-41bb-ada4-edac378566ff" in namespace "containers-8818" to be "Succeeded or Failed"
Jan 17 15:17:03.522: INFO: Pod "client-containers-065d3459-a11b-41bb-ada4-edac378566ff": Phase="Pending", Reason="", readiness=false. Elapsed: 4.773514ms
Jan 17 15:17:05.526: INFO: Pod "client-containers-065d3459-a11b-41bb-ada4-edac378566ff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008870976s
Jan 17 15:17:07.526: INFO: Pod "client-containers-065d3459-a11b-41bb-ada4-edac378566ff": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008872424s
Jan 17 15:17:09.527: INFO: Pod "client-containers-065d3459-a11b-41bb-ada4-edac378566ff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010038487s
STEP: Saw pod success 01/17/23 15:17:09.527
Jan 17 15:17:09.527: INFO: Pod "client-containers-065d3459-a11b-41bb-ada4-edac378566ff" satisfied condition "Succeeded or Failed"
Jan 17 15:17:09.530: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod client-containers-065d3459-a11b-41bb-ada4-edac378566ff container agnhost-container: <nil>
STEP: delete the pod 01/17/23 15:17:09.536
Jan 17 15:17:09.548: INFO: Waiting for pod client-containers-065d3459-a11b-41bb-ada4-edac378566ff to disappear
Jan 17 15:17:09.550: INFO: Pod client-containers-065d3459-a11b-41bb-ada4-edac378566ff no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Jan 17 15:17:09.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8818" for this suite. 01/17/23 15:17:09.556
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","completed":135,"skipped":2509,"failed":0}
------------------------------
• [SLOW TEST] [6.092 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:17:03.468
    Jan 17 15:17:03.468: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename containers 01/17/23 15:17:03.468
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:17:03.487
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:17:03.491
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:86
    STEP: Creating a pod to test override all 01/17/23 15:17:03.493
    Jan 17 15:17:03.517: INFO: Waiting up to 5m0s for pod "client-containers-065d3459-a11b-41bb-ada4-edac378566ff" in namespace "containers-8818" to be "Succeeded or Failed"
    Jan 17 15:17:03.522: INFO: Pod "client-containers-065d3459-a11b-41bb-ada4-edac378566ff": Phase="Pending", Reason="", readiness=false. Elapsed: 4.773514ms
    Jan 17 15:17:05.526: INFO: Pod "client-containers-065d3459-a11b-41bb-ada4-edac378566ff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008870976s
    Jan 17 15:17:07.526: INFO: Pod "client-containers-065d3459-a11b-41bb-ada4-edac378566ff": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008872424s
    Jan 17 15:17:09.527: INFO: Pod "client-containers-065d3459-a11b-41bb-ada4-edac378566ff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010038487s
    STEP: Saw pod success 01/17/23 15:17:09.527
    Jan 17 15:17:09.527: INFO: Pod "client-containers-065d3459-a11b-41bb-ada4-edac378566ff" satisfied condition "Succeeded or Failed"
    Jan 17 15:17:09.530: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod client-containers-065d3459-a11b-41bb-ada4-edac378566ff container agnhost-container: <nil>
    STEP: delete the pod 01/17/23 15:17:09.536
    Jan 17 15:17:09.548: INFO: Waiting for pod client-containers-065d3459-a11b-41bb-ada4-edac378566ff to disappear
    Jan 17 15:17:09.550: INFO: Pod client-containers-065d3459-a11b-41bb-ada4-edac378566ff no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Jan 17 15:17:09.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-8818" for this suite. 01/17/23 15:17:09.556
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:17:09.561
Jan 17 15:17:09.561: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename projected 01/17/23 15:17:09.561
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:17:09.576
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:17:09.578
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77
STEP: Creating projection with secret that has name projected-secret-test-map-ddfa8beb-0f13-46a2-af4d-62fca1cea3a4 01/17/23 15:17:09.581
STEP: Creating a pod to test consume secrets 01/17/23 15:17:09.589
Jan 17 15:17:09.616: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-007c077a-ee26-4790-9114-28cbe9187c71" in namespace "projected-1337" to be "Succeeded or Failed"
Jan 17 15:17:09.624: INFO: Pod "pod-projected-secrets-007c077a-ee26-4790-9114-28cbe9187c71": Phase="Pending", Reason="", readiness=false. Elapsed: 7.69786ms
Jan 17 15:17:11.627: INFO: Pod "pod-projected-secrets-007c077a-ee26-4790-9114-28cbe9187c71": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011458814s
Jan 17 15:17:13.628: INFO: Pod "pod-projected-secrets-007c077a-ee26-4790-9114-28cbe9187c71": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012437553s
Jan 17 15:17:15.629: INFO: Pod "pod-projected-secrets-007c077a-ee26-4790-9114-28cbe9187c71": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012905866s
STEP: Saw pod success 01/17/23 15:17:15.629
Jan 17 15:17:15.629: INFO: Pod "pod-projected-secrets-007c077a-ee26-4790-9114-28cbe9187c71" satisfied condition "Succeeded or Failed"
Jan 17 15:17:15.631: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-projected-secrets-007c077a-ee26-4790-9114-28cbe9187c71 container projected-secret-volume-test: <nil>
STEP: delete the pod 01/17/23 15:17:15.636
Jan 17 15:17:15.648: INFO: Waiting for pod pod-projected-secrets-007c077a-ee26-4790-9114-28cbe9187c71 to disappear
Jan 17 15:17:15.652: INFO: Pod pod-projected-secrets-007c077a-ee26-4790-9114-28cbe9187c71 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan 17 15:17:15.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1337" for this suite. 01/17/23 15:17:15.655
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":136,"skipped":2541,"failed":0}
------------------------------
• [SLOW TEST] [6.100 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:17:09.561
    Jan 17 15:17:09.561: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename projected 01/17/23 15:17:09.561
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:17:09.576
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:17:09.578
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:77
    STEP: Creating projection with secret that has name projected-secret-test-map-ddfa8beb-0f13-46a2-af4d-62fca1cea3a4 01/17/23 15:17:09.581
    STEP: Creating a pod to test consume secrets 01/17/23 15:17:09.589
    Jan 17 15:17:09.616: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-007c077a-ee26-4790-9114-28cbe9187c71" in namespace "projected-1337" to be "Succeeded or Failed"
    Jan 17 15:17:09.624: INFO: Pod "pod-projected-secrets-007c077a-ee26-4790-9114-28cbe9187c71": Phase="Pending", Reason="", readiness=false. Elapsed: 7.69786ms
    Jan 17 15:17:11.627: INFO: Pod "pod-projected-secrets-007c077a-ee26-4790-9114-28cbe9187c71": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011458814s
    Jan 17 15:17:13.628: INFO: Pod "pod-projected-secrets-007c077a-ee26-4790-9114-28cbe9187c71": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012437553s
    Jan 17 15:17:15.629: INFO: Pod "pod-projected-secrets-007c077a-ee26-4790-9114-28cbe9187c71": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012905866s
    STEP: Saw pod success 01/17/23 15:17:15.629
    Jan 17 15:17:15.629: INFO: Pod "pod-projected-secrets-007c077a-ee26-4790-9114-28cbe9187c71" satisfied condition "Succeeded or Failed"
    Jan 17 15:17:15.631: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-projected-secrets-007c077a-ee26-4790-9114-28cbe9187c71 container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/17/23 15:17:15.636
    Jan 17 15:17:15.648: INFO: Waiting for pod pod-projected-secrets-007c077a-ee26-4790-9114-28cbe9187c71 to disappear
    Jan 17 15:17:15.652: INFO: Pod pod-projected-secrets-007c077a-ee26-4790-9114-28cbe9187c71 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan 17 15:17:15.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1337" for this suite. 01/17/23 15:17:15.655
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:17:15.662
Jan 17 15:17:15.662: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename webhook 01/17/23 15:17:15.663
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:17:15.683
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:17:15.687
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/17/23 15:17:15.71
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/17/23 15:17:15.85
STEP: Deploying the webhook pod 01/17/23 15:17:15.88
STEP: Wait for the deployment to be ready 01/17/23 15:17:15.91
Jan 17 15:17:15.936: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/17/23 15:17:17.945
STEP: Verifying the service has paired with the endpoint 01/17/23 15:17:17.955
Jan 17 15:17:18.956: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220
Jan 17 15:17:18.964: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Registering the custom resource webhook via the AdmissionRegistration API 01/17/23 15:17:19.475
STEP: Creating a custom resource that should be denied by the webhook 01/17/23 15:17:19.496
STEP: Creating a custom resource whose deletion would be denied by the webhook 01/17/23 15:17:21.537
STEP: Updating the custom resource with disallowed data should be denied 01/17/23 15:17:21.545
STEP: Deleting the custom resource should be denied 01/17/23 15:17:21.569
STEP: Remove the offending key and value from the custom resource data 01/17/23 15:17:21.575
STEP: Deleting the updated custom resource should be successful 01/17/23 15:17:21.585
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 17 15:17:22.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2810" for this suite. 01/17/23 15:17:22.114
STEP: Destroying namespace "webhook-2810-markers" for this suite. 01/17/23 15:17:22.119
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","completed":137,"skipped":2568,"failed":0}
------------------------------
• [SLOW TEST] [6.576 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:17:15.662
    Jan 17 15:17:15.662: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename webhook 01/17/23 15:17:15.663
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:17:15.683
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:17:15.687
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/17/23 15:17:15.71
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/17/23 15:17:15.85
    STEP: Deploying the webhook pod 01/17/23 15:17:15.88
    STEP: Wait for the deployment to be ready 01/17/23 15:17:15.91
    Jan 17 15:17:15.936: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/17/23 15:17:17.945
    STEP: Verifying the service has paired with the endpoint 01/17/23 15:17:17.955
    Jan 17 15:17:18.956: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:220
    Jan 17 15:17:18.964: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 01/17/23 15:17:19.475
    STEP: Creating a custom resource that should be denied by the webhook 01/17/23 15:17:19.496
    STEP: Creating a custom resource whose deletion would be denied by the webhook 01/17/23 15:17:21.537
    STEP: Updating the custom resource with disallowed data should be denied 01/17/23 15:17:21.545
    STEP: Deleting the custom resource should be denied 01/17/23 15:17:21.569
    STEP: Remove the offending key and value from the custom resource data 01/17/23 15:17:21.575
    STEP: Deleting the updated custom resource should be successful 01/17/23 15:17:21.585
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 17 15:17:22.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-2810" for this suite. 01/17/23 15:17:22.114
    STEP: Destroying namespace "webhook-2810-markers" for this suite. 01/17/23 15:17:22.119
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:17:22.239
Jan 17 15:17:22.239: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename custom-resource-definition 01/17/23 15:17:22.24
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:17:22.309
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:17:22.312
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 01/17/23 15:17:22.317
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 01/17/23 15:17:22.318
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 01/17/23 15:17:22.318
STEP: fetching the /apis/apiextensions.k8s.io discovery document 01/17/23 15:17:22.318
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 01/17/23 15:17:22.343
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 01/17/23 15:17:22.343
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 01/17/23 15:17:22.344
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 17 15:17:22.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3181" for this suite. 01/17/23 15:17:22.351
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","completed":138,"skipped":2625,"failed":0}
------------------------------
• [0.122 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:17:22.239
    Jan 17 15:17:22.239: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename custom-resource-definition 01/17/23 15:17:22.24
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:17:22.309
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:17:22.312
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 01/17/23 15:17:22.317
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 01/17/23 15:17:22.318
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 01/17/23 15:17:22.318
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 01/17/23 15:17:22.318
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 01/17/23 15:17:22.343
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 01/17/23 15:17:22.343
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 01/17/23 15:17:22.344
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 17 15:17:22.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-3181" for this suite. 01/17/23 15:17:22.351
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:17:22.363
Jan 17 15:17:22.363: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename crd-publish-openapi 01/17/23 15:17:22.364
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:17:22.435
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:17:22.44
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441
STEP: set up a multi version CRD 01/17/23 15:17:22.445
Jan 17 15:17:22.445: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: mark a version not serverd 01/17/23 15:17:38.479
STEP: check the unserved version gets removed 01/17/23 15:17:38.494
STEP: check the other version is not changed 01/17/23 15:17:44.238
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 17 15:17:55.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2872" for this suite. 01/17/23 15:17:55.088
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","completed":139,"skipped":2659,"failed":0}
------------------------------
• [SLOW TEST] [32.731 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:17:22.363
    Jan 17 15:17:22.363: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename crd-publish-openapi 01/17/23 15:17:22.364
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:17:22.435
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:17:22.44
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:441
    STEP: set up a multi version CRD 01/17/23 15:17:22.445
    Jan 17 15:17:22.445: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: mark a version not serverd 01/17/23 15:17:38.479
    STEP: check the unserved version gets removed 01/17/23 15:17:38.494
    STEP: check the other version is not changed 01/17/23 15:17:44.238
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 17 15:17:55.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-2872" for this suite. 01/17/23 15:17:55.088
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:17:55.095
Jan 17 15:17:55.095: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename endpointslice 01/17/23 15:17:55.096
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:17:55.121
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:17:55.126
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65
Jan 17 15:17:55.148: INFO: Endpoints addresses: [10.0.135.246 10.0.159.167 10.0.161.62] , ports: [6443]
Jan 17 15:17:55.148: INFO: EndpointSlices addresses: [10.0.135.246 10.0.159.167 10.0.161.62] , ports: [6443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Jan 17 15:17:55.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-2788" for this suite. 01/17/23 15:17:55.155
{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","completed":140,"skipped":2669,"failed":0}
------------------------------
• [0.068 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:17:55.095
    Jan 17 15:17:55.095: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename endpointslice 01/17/23 15:17:55.096
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:17:55.121
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:17:55.126
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:65
    Jan 17 15:17:55.148: INFO: Endpoints addresses: [10.0.135.246 10.0.159.167 10.0.161.62] , ports: [6443]
    Jan 17 15:17:55.148: INFO: EndpointSlices addresses: [10.0.135.246 10.0.159.167 10.0.161.62] , ports: [6443]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Jan 17 15:17:55.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-2788" for this suite. 01/17/23 15:17:55.155
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:17:55.164
Jan 17 15:17:55.164: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename webhook 01/17/23 15:17:55.164
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:17:55.196
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:17:55.199
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/17/23 15:17:55.228
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/17/23 15:17:55.524
STEP: Deploying the webhook pod 01/17/23 15:17:55.531
STEP: Wait for the deployment to be ready 01/17/23 15:17:55.54
Jan 17 15:17:55.549: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/17/23 15:17:57.558
STEP: Verifying the service has paired with the endpoint 01/17/23 15:17:57.567
Jan 17 15:17:58.568: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380
STEP: Setting timeout (1s) shorter than webhook latency (5s) 01/17/23 15:17:58.572
STEP: Registering slow webhook via the AdmissionRegistration API 01/17/23 15:17:58.572
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 01/17/23 15:17:58.585
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 01/17/23 15:17:59.594
STEP: Registering slow webhook via the AdmissionRegistration API 01/17/23 15:17:59.594
STEP: Having no error when timeout is longer than webhook latency 01/17/23 15:18:00.621
STEP: Registering slow webhook via the AdmissionRegistration API 01/17/23 15:18:00.621
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 01/17/23 15:18:05.652
STEP: Registering slow webhook via the AdmissionRegistration API 01/17/23 15:18:05.652
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 17 15:18:10.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9913" for this suite. 01/17/23 15:18:10.684
STEP: Destroying namespace "webhook-9913-markers" for this suite. 01/17/23 15:18:10.691
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","completed":141,"skipped":2694,"failed":0}
------------------------------
• [SLOW TEST] [15.620 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:17:55.164
    Jan 17 15:17:55.164: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename webhook 01/17/23 15:17:55.164
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:17:55.196
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:17:55.199
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/17/23 15:17:55.228
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/17/23 15:17:55.524
    STEP: Deploying the webhook pod 01/17/23 15:17:55.531
    STEP: Wait for the deployment to be ready 01/17/23 15:17:55.54
    Jan 17 15:17:55.549: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/17/23 15:17:57.558
    STEP: Verifying the service has paired with the endpoint 01/17/23 15:17:57.567
    Jan 17 15:17:58.568: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:380
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 01/17/23 15:17:58.572
    STEP: Registering slow webhook via the AdmissionRegistration API 01/17/23 15:17:58.572
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 01/17/23 15:17:58.585
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 01/17/23 15:17:59.594
    STEP: Registering slow webhook via the AdmissionRegistration API 01/17/23 15:17:59.594
    STEP: Having no error when timeout is longer than webhook latency 01/17/23 15:18:00.621
    STEP: Registering slow webhook via the AdmissionRegistration API 01/17/23 15:18:00.621
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 01/17/23 15:18:05.652
    STEP: Registering slow webhook via the AdmissionRegistration API 01/17/23 15:18:05.652
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 17 15:18:10.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-9913" for this suite. 01/17/23 15:18:10.684
    STEP: Destroying namespace "webhook-9913-markers" for this suite. 01/17/23 15:18:10.691
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:18:10.785
Jan 17 15:18:10.785: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename svcaccounts 01/17/23 15:18:10.785
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:18:10.823
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:18:10.826
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272
STEP: Creating a pod to test service account token:  01/17/23 15:18:10.828
Jan 17 15:18:10.856: INFO: Waiting up to 5m0s for pod "test-pod-6331fe6a-11e6-49e3-bc67-9acc3f26d0e8" in namespace "svcaccounts-8985" to be "Succeeded or Failed"
Jan 17 15:18:10.874: INFO: Pod "test-pod-6331fe6a-11e6-49e3-bc67-9acc3f26d0e8": Phase="Pending", Reason="", readiness=false. Elapsed: 18.918448ms
Jan 17 15:18:12.878: INFO: Pod "test-pod-6331fe6a-11e6-49e3-bc67-9acc3f26d0e8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02194835s
Jan 17 15:18:14.879: INFO: Pod "test-pod-6331fe6a-11e6-49e3-bc67-9acc3f26d0e8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023302354s
STEP: Saw pod success 01/17/23 15:18:14.879
Jan 17 15:18:14.879: INFO: Pod "test-pod-6331fe6a-11e6-49e3-bc67-9acc3f26d0e8" satisfied condition "Succeeded or Failed"
Jan 17 15:18:14.882: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod test-pod-6331fe6a-11e6-49e3-bc67-9acc3f26d0e8 container agnhost-container: <nil>
STEP: delete the pod 01/17/23 15:18:14.891
Jan 17 15:18:14.904: INFO: Waiting for pod test-pod-6331fe6a-11e6-49e3-bc67-9acc3f26d0e8 to disappear
Jan 17 15:18:14.907: INFO: Pod test-pod-6331fe6a-11e6-49e3-bc67-9acc3f26d0e8 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jan 17 15:18:14.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8985" for this suite. 01/17/23 15:18:14.911
{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","completed":142,"skipped":2709,"failed":0}
------------------------------
• [4.133 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:18:10.785
    Jan 17 15:18:10.785: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename svcaccounts 01/17/23 15:18:10.785
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:18:10.823
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:18:10.826
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:272
    STEP: Creating a pod to test service account token:  01/17/23 15:18:10.828
    Jan 17 15:18:10.856: INFO: Waiting up to 5m0s for pod "test-pod-6331fe6a-11e6-49e3-bc67-9acc3f26d0e8" in namespace "svcaccounts-8985" to be "Succeeded or Failed"
    Jan 17 15:18:10.874: INFO: Pod "test-pod-6331fe6a-11e6-49e3-bc67-9acc3f26d0e8": Phase="Pending", Reason="", readiness=false. Elapsed: 18.918448ms
    Jan 17 15:18:12.878: INFO: Pod "test-pod-6331fe6a-11e6-49e3-bc67-9acc3f26d0e8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02194835s
    Jan 17 15:18:14.879: INFO: Pod "test-pod-6331fe6a-11e6-49e3-bc67-9acc3f26d0e8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023302354s
    STEP: Saw pod success 01/17/23 15:18:14.879
    Jan 17 15:18:14.879: INFO: Pod "test-pod-6331fe6a-11e6-49e3-bc67-9acc3f26d0e8" satisfied condition "Succeeded or Failed"
    Jan 17 15:18:14.882: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod test-pod-6331fe6a-11e6-49e3-bc67-9acc3f26d0e8 container agnhost-container: <nil>
    STEP: delete the pod 01/17/23 15:18:14.891
    Jan 17 15:18:14.904: INFO: Waiting for pod test-pod-6331fe6a-11e6-49e3-bc67-9acc3f26d0e8 to disappear
    Jan 17 15:18:14.907: INFO: Pod test-pod-6331fe6a-11e6-49e3-bc67-9acc3f26d0e8 no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jan 17 15:18:14.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-8985" for this suite. 01/17/23 15:18:14.911
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:608
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:18:14.919
Jan 17 15:18:14.919: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename security-context-test 01/17/23 15:18:14.919
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:18:14.952
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:18:14.955
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:608
Jan 17 15:18:14.992: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-27e18e8c-0ec4-49e8-b6c9-cc839c21efde" in namespace "security-context-test-9576" to be "Succeeded or Failed"
Jan 17 15:18:15.005: INFO: Pod "alpine-nnp-false-27e18e8c-0ec4-49e8-b6c9-cc839c21efde": Phase="Pending", Reason="", readiness=false. Elapsed: 13.597603ms
Jan 17 15:18:17.009: INFO: Pod "alpine-nnp-false-27e18e8c-0ec4-49e8-b6c9-cc839c21efde": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017603457s
Jan 17 15:18:19.010: INFO: Pod "alpine-nnp-false-27e18e8c-0ec4-49e8-b6c9-cc839c21efde": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018626132s
Jan 17 15:18:21.009: INFO: Pod "alpine-nnp-false-27e18e8c-0ec4-49e8-b6c9-cc839c21efde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017509043s
Jan 17 15:18:21.009: INFO: Pod "alpine-nnp-false-27e18e8c-0ec4-49e8-b6c9-cc839c21efde" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jan 17 15:18:21.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-9576" for this suite. 01/17/23 15:18:21.02
{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","completed":143,"skipped":2730,"failed":0}
------------------------------
• [SLOW TEST] [6.106 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:554
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:608

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:18:14.919
    Jan 17 15:18:14.919: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename security-context-test 01/17/23 15:18:14.919
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:18:14.952
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:18:14.955
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:608
    Jan 17 15:18:14.992: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-27e18e8c-0ec4-49e8-b6c9-cc839c21efde" in namespace "security-context-test-9576" to be "Succeeded or Failed"
    Jan 17 15:18:15.005: INFO: Pod "alpine-nnp-false-27e18e8c-0ec4-49e8-b6c9-cc839c21efde": Phase="Pending", Reason="", readiness=false. Elapsed: 13.597603ms
    Jan 17 15:18:17.009: INFO: Pod "alpine-nnp-false-27e18e8c-0ec4-49e8-b6c9-cc839c21efde": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017603457s
    Jan 17 15:18:19.010: INFO: Pod "alpine-nnp-false-27e18e8c-0ec4-49e8-b6c9-cc839c21efde": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018626132s
    Jan 17 15:18:21.009: INFO: Pod "alpine-nnp-false-27e18e8c-0ec4-49e8-b6c9-cc839c21efde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017509043s
    Jan 17 15:18:21.009: INFO: Pod "alpine-nnp-false-27e18e8c-0ec4-49e8-b6c9-cc839c21efde" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jan 17 15:18:21.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-9576" for this suite. 01/17/23 15:18:21.02
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:485
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:18:21.025
Jan 17 15:18:21.025: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename security-context-test 01/17/23 15:18:21.026
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:18:21.049
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:18:21.051
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:485
Jan 17 15:18:21.097: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-93759bf0-0437-4a0e-8fc3-19be6027cc11" in namespace "security-context-test-1181" to be "Succeeded or Failed"
Jan 17 15:18:21.101: INFO: Pod "busybox-readonly-false-93759bf0-0437-4a0e-8fc3-19be6027cc11": Phase="Pending", Reason="", readiness=false. Elapsed: 4.272715ms
Jan 17 15:18:23.147: INFO: Pod "busybox-readonly-false-93759bf0-0437-4a0e-8fc3-19be6027cc11": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049807605s
Jan 17 15:18:25.105: INFO: Pod "busybox-readonly-false-93759bf0-0437-4a0e-8fc3-19be6027cc11": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008597259s
Jan 17 15:18:25.105: INFO: Pod "busybox-readonly-false-93759bf0-0437-4a0e-8fc3-19be6027cc11" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jan 17 15:18:25.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-1181" for this suite. 01/17/23 15:18:25.11
{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","completed":144,"skipped":2732,"failed":0}
------------------------------
• [4.090 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:429
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:485

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:18:21.025
    Jan 17 15:18:21.025: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename security-context-test 01/17/23 15:18:21.026
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:18:21.049
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:18:21.051
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:485
    Jan 17 15:18:21.097: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-93759bf0-0437-4a0e-8fc3-19be6027cc11" in namespace "security-context-test-1181" to be "Succeeded or Failed"
    Jan 17 15:18:21.101: INFO: Pod "busybox-readonly-false-93759bf0-0437-4a0e-8fc3-19be6027cc11": Phase="Pending", Reason="", readiness=false. Elapsed: 4.272715ms
    Jan 17 15:18:23.147: INFO: Pod "busybox-readonly-false-93759bf0-0437-4a0e-8fc3-19be6027cc11": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049807605s
    Jan 17 15:18:25.105: INFO: Pod "busybox-readonly-false-93759bf0-0437-4a0e-8fc3-19be6027cc11": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008597259s
    Jan 17 15:18:25.105: INFO: Pod "busybox-readonly-false-93759bf0-0437-4a0e-8fc3-19be6027cc11" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jan 17 15:18:25.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-1181" for this suite. 01/17/23 15:18:25.11
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:18:25.116
Jan 17 15:18:25.116: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename replication-controller 01/17/23 15:18:25.117
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:18:25.136
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:18:25.138
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100
STEP: Given a ReplicationController is created 01/17/23 15:18:25.141
W0117 15:18:25.149419      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "pod-release" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pod-release" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pod-release" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pod-release" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: When the matched label of one of its pods change 01/17/23 15:18:25.149
Jan 17 15:18:25.155: INFO: Pod name pod-release: Found 0 pods out of 1
Jan 17 15:18:30.161: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 01/17/23 15:18:30.176
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Jan 17 15:18:31.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3577" for this suite. 01/17/23 15:18:31.188
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","completed":145,"skipped":2761,"failed":0}
------------------------------
• [SLOW TEST] [6.081 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:18:25.116
    Jan 17 15:18:25.116: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename replication-controller 01/17/23 15:18:25.117
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:18:25.136
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:18:25.138
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:100
    STEP: Given a ReplicationController is created 01/17/23 15:18:25.141
    W0117 15:18:25.149419      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "pod-release" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pod-release" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pod-release" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pod-release" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: When the matched label of one of its pods change 01/17/23 15:18:25.149
    Jan 17 15:18:25.155: INFO: Pod name pod-release: Found 0 pods out of 1
    Jan 17 15:18:30.161: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 01/17/23 15:18:30.176
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Jan 17 15:18:31.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-3577" for this suite. 01/17/23 15:18:31.188
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:18:31.198
Jan 17 15:18:31.198: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename resourcequota 01/17/23 15:18:31.198
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:18:31.231
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:18:31.234
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874
STEP: Creating a ResourceQuota 01/17/23 15:18:31.236
STEP: Getting a ResourceQuota 01/17/23 15:18:31.24
STEP: Updating a ResourceQuota 01/17/23 15:18:31.244
STEP: Verifying a ResourceQuota was modified 01/17/23 15:18:31.257
STEP: Deleting a ResourceQuota 01/17/23 15:18:31.26
STEP: Verifying the deleted ResourceQuota 01/17/23 15:18:31.272
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 17 15:18:31.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7513" for this suite. 01/17/23 15:18:31.289
{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","completed":146,"skipped":2781,"failed":0}
------------------------------
• [0.101 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:18:31.198
    Jan 17 15:18:31.198: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename resourcequota 01/17/23 15:18:31.198
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:18:31.231
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:18:31.234
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:874
    STEP: Creating a ResourceQuota 01/17/23 15:18:31.236
    STEP: Getting a ResourceQuota 01/17/23 15:18:31.24
    STEP: Updating a ResourceQuota 01/17/23 15:18:31.244
    STEP: Verifying a ResourceQuota was modified 01/17/23 15:18:31.257
    STEP: Deleting a ResourceQuota 01/17/23 15:18:31.26
    STEP: Verifying the deleted ResourceQuota 01/17/23 15:18:31.272
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 17 15:18:31.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-7513" for this suite. 01/17/23 15:18:31.289
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:18:31.299
Jan 17 15:18:31.299: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename emptydir 01/17/23 15:18:31.3
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:18:31.328
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:18:31.33
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146
STEP: Creating a pod to test emptydir 0777 on tmpfs 01/17/23 15:18:31.337
Jan 17 15:18:31.378: INFO: Waiting up to 5m0s for pod "pod-6522202d-afd0-44cb-9aca-0e0931711555" in namespace "emptydir-5466" to be "Succeeded or Failed"
Jan 17 15:18:31.384: INFO: Pod "pod-6522202d-afd0-44cb-9aca-0e0931711555": Phase="Pending", Reason="", readiness=false. Elapsed: 6.448507ms
Jan 17 15:18:33.388: INFO: Pod "pod-6522202d-afd0-44cb-9aca-0e0931711555": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010154015s
Jan 17 15:18:35.389: INFO: Pod "pod-6522202d-afd0-44cb-9aca-0e0931711555": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010989061s
STEP: Saw pod success 01/17/23 15:18:35.389
Jan 17 15:18:35.389: INFO: Pod "pod-6522202d-afd0-44cb-9aca-0e0931711555" satisfied condition "Succeeded or Failed"
Jan 17 15:18:35.392: INFO: Trying to get logs from node ip-10-0-139-213.ec2.internal pod pod-6522202d-afd0-44cb-9aca-0e0931711555 container test-container: <nil>
STEP: delete the pod 01/17/23 15:18:35.403
Jan 17 15:18:35.416: INFO: Waiting for pod pod-6522202d-afd0-44cb-9aca-0e0931711555 to disappear
Jan 17 15:18:35.419: INFO: Pod pod-6522202d-afd0-44cb-9aca-0e0931711555 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 17 15:18:35.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5466" for this suite. 01/17/23 15:18:35.423
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":147,"skipped":2782,"failed":0}
------------------------------
• [4.129 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:18:31.299
    Jan 17 15:18:31.299: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename emptydir 01/17/23 15:18:31.3
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:18:31.328
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:18:31.33
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:146
    STEP: Creating a pod to test emptydir 0777 on tmpfs 01/17/23 15:18:31.337
    Jan 17 15:18:31.378: INFO: Waiting up to 5m0s for pod "pod-6522202d-afd0-44cb-9aca-0e0931711555" in namespace "emptydir-5466" to be "Succeeded or Failed"
    Jan 17 15:18:31.384: INFO: Pod "pod-6522202d-afd0-44cb-9aca-0e0931711555": Phase="Pending", Reason="", readiness=false. Elapsed: 6.448507ms
    Jan 17 15:18:33.388: INFO: Pod "pod-6522202d-afd0-44cb-9aca-0e0931711555": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010154015s
    Jan 17 15:18:35.389: INFO: Pod "pod-6522202d-afd0-44cb-9aca-0e0931711555": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010989061s
    STEP: Saw pod success 01/17/23 15:18:35.389
    Jan 17 15:18:35.389: INFO: Pod "pod-6522202d-afd0-44cb-9aca-0e0931711555" satisfied condition "Succeeded or Failed"
    Jan 17 15:18:35.392: INFO: Trying to get logs from node ip-10-0-139-213.ec2.internal pod pod-6522202d-afd0-44cb-9aca-0e0931711555 container test-container: <nil>
    STEP: delete the pod 01/17/23 15:18:35.403
    Jan 17 15:18:35.416: INFO: Waiting for pod pod-6522202d-afd0-44cb-9aca-0e0931711555 to disappear
    Jan 17 15:18:35.419: INFO: Pod pod-6522202d-afd0-44cb-9aca-0e0931711555 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 17 15:18:35.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-5466" for this suite. 01/17/23 15:18:35.423
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:18:35.429
Jan 17 15:18:35.429: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename webhook 01/17/23 15:18:35.429
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:18:35.456
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:18:35.458
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/17/23 15:18:35.476
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/17/23 15:18:35.708
STEP: Deploying the webhook pod 01/17/23 15:18:35.718
STEP: Wait for the deployment to be ready 01/17/23 15:18:35.728
Jan 17 15:18:35.734: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/17/23 15:18:37.743
STEP: Verifying the service has paired with the endpoint 01/17/23 15:18:37.753
Jan 17 15:18:38.753: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507
STEP: Creating a mutating webhook configuration 01/17/23 15:18:38.757
STEP: Updating a mutating webhook configuration's rules to not include the create operation 01/17/23 15:18:38.773
STEP: Creating a configMap that should not be mutated 01/17/23 15:18:38.779
STEP: Patching a mutating webhook configuration's rules to include the create operation 01/17/23 15:18:38.789
STEP: Creating a configMap that should be mutated 01/17/23 15:18:38.796
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 17 15:18:38.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7073" for this suite. 01/17/23 15:18:38.82
STEP: Destroying namespace "webhook-7073-markers" for this suite. 01/17/23 15:18:38.83
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","completed":148,"skipped":2792,"failed":0}
------------------------------
• [3.476 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:18:35.429
    Jan 17 15:18:35.429: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename webhook 01/17/23 15:18:35.429
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:18:35.456
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:18:35.458
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/17/23 15:18:35.476
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/17/23 15:18:35.708
    STEP: Deploying the webhook pod 01/17/23 15:18:35.718
    STEP: Wait for the deployment to be ready 01/17/23 15:18:35.728
    Jan 17 15:18:35.734: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/17/23 15:18:37.743
    STEP: Verifying the service has paired with the endpoint 01/17/23 15:18:37.753
    Jan 17 15:18:38.753: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:507
    STEP: Creating a mutating webhook configuration 01/17/23 15:18:38.757
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 01/17/23 15:18:38.773
    STEP: Creating a configMap that should not be mutated 01/17/23 15:18:38.779
    STEP: Patching a mutating webhook configuration's rules to include the create operation 01/17/23 15:18:38.789
    STEP: Creating a configMap that should be mutated 01/17/23 15:18:38.796
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 17 15:18:38.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-7073" for this suite. 01/17/23 15:18:38.82
    STEP: Destroying namespace "webhook-7073-markers" for this suite. 01/17/23 15:18:38.83
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:18:38.905
Jan 17 15:18:38.905: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename replication-controller 01/17/23 15:18:38.906
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:18:38.947
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:18:38.95
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66
STEP: Creating replication controller my-hostname-basic-ccf8c604-a744-496b-aa1e-d6a843ce0abd 01/17/23 15:18:38.954
Jan 17 15:18:38.968: INFO: Pod name my-hostname-basic-ccf8c604-a744-496b-aa1e-d6a843ce0abd: Found 0 pods out of 1
Jan 17 15:18:43.972: INFO: Pod name my-hostname-basic-ccf8c604-a744-496b-aa1e-d6a843ce0abd: Found 1 pods out of 1
Jan 17 15:18:43.972: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-ccf8c604-a744-496b-aa1e-d6a843ce0abd" are running
Jan 17 15:18:43.972: INFO: Waiting up to 5m0s for pod "my-hostname-basic-ccf8c604-a744-496b-aa1e-d6a843ce0abd-dw4r7" in namespace "replication-controller-9273" to be "running"
Jan 17 15:18:43.975: INFO: Pod "my-hostname-basic-ccf8c604-a744-496b-aa1e-d6a843ce0abd-dw4r7": Phase="Running", Reason="", readiness=true. Elapsed: 3.20727ms
Jan 17 15:18:43.975: INFO: Pod "my-hostname-basic-ccf8c604-a744-496b-aa1e-d6a843ce0abd-dw4r7" satisfied condition "running"
Jan 17 15:18:43.975: INFO: Pod "my-hostname-basic-ccf8c604-a744-496b-aa1e-d6a843ce0abd-dw4r7" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-17 15:18:39 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-17 15:18:39 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-17 15:18:39 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-17 15:18:39 +0000 UTC Reason: Message:}])
Jan 17 15:18:43.975: INFO: Trying to dial the pod
Jan 17 15:18:48.989: INFO: Controller my-hostname-basic-ccf8c604-a744-496b-aa1e-d6a843ce0abd: Got expected result from replica 1 [my-hostname-basic-ccf8c604-a744-496b-aa1e-d6a843ce0abd-dw4r7]: "my-hostname-basic-ccf8c604-a744-496b-aa1e-d6a843ce0abd-dw4r7", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Jan 17 15:18:48.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9273" for this suite. 01/17/23 15:18:48.993
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","completed":149,"skipped":2792,"failed":0}
------------------------------
• [SLOW TEST] [10.095 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:18:38.905
    Jan 17 15:18:38.905: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename replication-controller 01/17/23 15:18:38.906
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:18:38.947
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:18:38.95
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:66
    STEP: Creating replication controller my-hostname-basic-ccf8c604-a744-496b-aa1e-d6a843ce0abd 01/17/23 15:18:38.954
    Jan 17 15:18:38.968: INFO: Pod name my-hostname-basic-ccf8c604-a744-496b-aa1e-d6a843ce0abd: Found 0 pods out of 1
    Jan 17 15:18:43.972: INFO: Pod name my-hostname-basic-ccf8c604-a744-496b-aa1e-d6a843ce0abd: Found 1 pods out of 1
    Jan 17 15:18:43.972: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-ccf8c604-a744-496b-aa1e-d6a843ce0abd" are running
    Jan 17 15:18:43.972: INFO: Waiting up to 5m0s for pod "my-hostname-basic-ccf8c604-a744-496b-aa1e-d6a843ce0abd-dw4r7" in namespace "replication-controller-9273" to be "running"
    Jan 17 15:18:43.975: INFO: Pod "my-hostname-basic-ccf8c604-a744-496b-aa1e-d6a843ce0abd-dw4r7": Phase="Running", Reason="", readiness=true. Elapsed: 3.20727ms
    Jan 17 15:18:43.975: INFO: Pod "my-hostname-basic-ccf8c604-a744-496b-aa1e-d6a843ce0abd-dw4r7" satisfied condition "running"
    Jan 17 15:18:43.975: INFO: Pod "my-hostname-basic-ccf8c604-a744-496b-aa1e-d6a843ce0abd-dw4r7" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-17 15:18:39 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-17 15:18:39 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-17 15:18:39 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-17 15:18:39 +0000 UTC Reason: Message:}])
    Jan 17 15:18:43.975: INFO: Trying to dial the pod
    Jan 17 15:18:48.989: INFO: Controller my-hostname-basic-ccf8c604-a744-496b-aa1e-d6a843ce0abd: Got expected result from replica 1 [my-hostname-basic-ccf8c604-a744-496b-aa1e-d6a843ce0abd-dw4r7]: "my-hostname-basic-ccf8c604-a744-496b-aa1e-d6a843ce0abd-dw4r7", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Jan 17 15:18:48.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-9273" for this suite. 01/17/23 15:18:48.993
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:97
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:18:49
Jan 17 15:18:49.000: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename container-lifecycle-hook 01/17/23 15:18:49.001
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:18:49.022
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:18:49.024
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 01/17/23 15:18:49.036
Jan 17 15:18:49.063: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4613" to be "running and ready"
Jan 17 15:18:49.067: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.489238ms
Jan 17 15:18:49.067: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 17 15:18:51.071: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.008224846s
Jan 17 15:18:51.071: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan 17 15:18:51.071: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:97
STEP: create the pod with lifecycle hook 01/17/23 15:18:51.074
Jan 17 15:18:51.085: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-4613" to be "running and ready"
Jan 17 15:18:51.088: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.141467ms
Jan 17 15:18:51.088: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 17 15:18:53.092: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.00630505s
Jan 17 15:18:53.092: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
Jan 17 15:18:53.092: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 01/17/23 15:18:53.094
STEP: delete the pod with lifecycle hook 01/17/23 15:18:53.1
Jan 17 15:18:53.107: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 17 15:18:53.110: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 17 15:18:55.111: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 17 15:18:55.114: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 17 15:18:57.111: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 17 15:18:57.115: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Jan 17 15:18:57.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4613" for this suite. 01/17/23 15:18:57.119
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","completed":150,"skipped":2793,"failed":0}
------------------------------
• [SLOW TEST] [8.126 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:18:49
    Jan 17 15:18:49.000: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/17/23 15:18:49.001
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:18:49.022
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:18:49.024
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 01/17/23 15:18:49.036
    Jan 17 15:18:49.063: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4613" to be "running and ready"
    Jan 17 15:18:49.067: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.489238ms
    Jan 17 15:18:49.067: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 15:18:51.071: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.008224846s
    Jan 17 15:18:51.071: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan 17 15:18:51.071: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:97
    STEP: create the pod with lifecycle hook 01/17/23 15:18:51.074
    Jan 17 15:18:51.085: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-4613" to be "running and ready"
    Jan 17 15:18:51.088: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.141467ms
    Jan 17 15:18:51.088: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 15:18:53.092: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.00630505s
    Jan 17 15:18:53.092: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    Jan 17 15:18:53.092: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 01/17/23 15:18:53.094
    STEP: delete the pod with lifecycle hook 01/17/23 15:18:53.1
    Jan 17 15:18:53.107: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jan 17 15:18:53.110: INFO: Pod pod-with-poststart-exec-hook still exists
    Jan 17 15:18:55.111: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jan 17 15:18:55.114: INFO: Pod pod-with-poststart-exec-hook still exists
    Jan 17 15:18:57.111: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jan 17 15:18:57.115: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Jan 17 15:18:57.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-4613" for this suite. 01/17/23 15:18:57.119
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:18:57.127
Jan 17 15:18:57.127: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename projected 01/17/23 15:18:57.127
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:18:57.162
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:18:57.165
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192
STEP: Creating a pod to test downward API volume plugin 01/17/23 15:18:57.167
Jan 17 15:18:57.204: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2f2a64d7-b1cd-4bad-a998-7655186163dc" in namespace "projected-6103" to be "Succeeded or Failed"
Jan 17 15:18:57.211: INFO: Pod "downwardapi-volume-2f2a64d7-b1cd-4bad-a998-7655186163dc": Phase="Pending", Reason="", readiness=false. Elapsed: 7.464905ms
Jan 17 15:18:59.216: INFO: Pod "downwardapi-volume-2f2a64d7-b1cd-4bad-a998-7655186163dc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011926423s
Jan 17 15:19:01.216: INFO: Pod "downwardapi-volume-2f2a64d7-b1cd-4bad-a998-7655186163dc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011913874s
Jan 17 15:19:03.216: INFO: Pod "downwardapi-volume-2f2a64d7-b1cd-4bad-a998-7655186163dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011678772s
STEP: Saw pod success 01/17/23 15:19:03.216
Jan 17 15:19:03.216: INFO: Pod "downwardapi-volume-2f2a64d7-b1cd-4bad-a998-7655186163dc" satisfied condition "Succeeded or Failed"
Jan 17 15:19:03.219: INFO: Trying to get logs from node ip-10-0-165-14.ec2.internal pod downwardapi-volume-2f2a64d7-b1cd-4bad-a998-7655186163dc container client-container: <nil>
STEP: delete the pod 01/17/23 15:19:03.227
Jan 17 15:19:03.239: INFO: Waiting for pod downwardapi-volume-2f2a64d7-b1cd-4bad-a998-7655186163dc to disappear
Jan 17 15:19:03.242: INFO: Pod downwardapi-volume-2f2a64d7-b1cd-4bad-a998-7655186163dc no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 17 15:19:03.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6103" for this suite. 01/17/23 15:19:03.246
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","completed":151,"skipped":2809,"failed":0}
------------------------------
• [SLOW TEST] [6.125 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:18:57.127
    Jan 17 15:18:57.127: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename projected 01/17/23 15:18:57.127
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:18:57.162
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:18:57.165
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:192
    STEP: Creating a pod to test downward API volume plugin 01/17/23 15:18:57.167
    Jan 17 15:18:57.204: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2f2a64d7-b1cd-4bad-a998-7655186163dc" in namespace "projected-6103" to be "Succeeded or Failed"
    Jan 17 15:18:57.211: INFO: Pod "downwardapi-volume-2f2a64d7-b1cd-4bad-a998-7655186163dc": Phase="Pending", Reason="", readiness=false. Elapsed: 7.464905ms
    Jan 17 15:18:59.216: INFO: Pod "downwardapi-volume-2f2a64d7-b1cd-4bad-a998-7655186163dc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011926423s
    Jan 17 15:19:01.216: INFO: Pod "downwardapi-volume-2f2a64d7-b1cd-4bad-a998-7655186163dc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011913874s
    Jan 17 15:19:03.216: INFO: Pod "downwardapi-volume-2f2a64d7-b1cd-4bad-a998-7655186163dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011678772s
    STEP: Saw pod success 01/17/23 15:19:03.216
    Jan 17 15:19:03.216: INFO: Pod "downwardapi-volume-2f2a64d7-b1cd-4bad-a998-7655186163dc" satisfied condition "Succeeded or Failed"
    Jan 17 15:19:03.219: INFO: Trying to get logs from node ip-10-0-165-14.ec2.internal pod downwardapi-volume-2f2a64d7-b1cd-4bad-a998-7655186163dc container client-container: <nil>
    STEP: delete the pod 01/17/23 15:19:03.227
    Jan 17 15:19:03.239: INFO: Waiting for pod downwardapi-volume-2f2a64d7-b1cd-4bad-a998-7655186163dc to disappear
    Jan 17 15:19:03.242: INFO: Pod downwardapi-volume-2f2a64d7-b1cd-4bad-a998-7655186163dc no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 17 15:19:03.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6103" for this suite. 01/17/23 15:19:03.246
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:19:03.252
Jan 17 15:19:03.252: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename events 01/17/23 15:19:03.253
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:19:03.272
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:19:03.274
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 01/17/23 15:19:03.276
STEP: get a list of Events with a label in the current namespace 01/17/23 15:19:03.305
STEP: delete a list of events 01/17/23 15:19:03.308
Jan 17 15:19:03.308: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 01/17/23 15:19:03.359
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
Jan 17 15:19:03.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5913" for this suite. 01/17/23 15:19:03.367
{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","completed":152,"skipped":2810,"failed":0}
------------------------------
• [0.122 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:19:03.252
    Jan 17 15:19:03.252: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename events 01/17/23 15:19:03.253
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:19:03.272
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:19:03.274
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 01/17/23 15:19:03.276
    STEP: get a list of Events with a label in the current namespace 01/17/23 15:19:03.305
    STEP: delete a list of events 01/17/23 15:19:03.308
    Jan 17 15:19:03.308: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 01/17/23 15:19:03.359
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:187
    Jan 17 15:19:03.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-5913" for this suite. 01/17/23 15:19:03.367
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:19:03.374
Jan 17 15:19:03.374: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename crd-webhook 01/17/23 15:19:03.375
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:19:03.412
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:19:03.416
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 01/17/23 15:19:03.419
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/17/23 15:19:03.628
STEP: Deploying the custom resource conversion webhook pod 01/17/23 15:19:03.635
STEP: Wait for the deployment to be ready 01/17/23 15:19:03.646
Jan 17 15:19:03.651: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/17/23 15:19:05.661
STEP: Verifying the service has paired with the endpoint 01/17/23 15:19:05.671
Jan 17 15:19:06.671: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
Jan 17 15:19:06.675: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Creating a v1 custom resource 01/17/23 15:19:09.28
STEP: Create a v2 custom resource 01/17/23 15:19:09.379
STEP: List CRs in v1 01/17/23 15:19:09.435
STEP: List CRs in v2 01/17/23 15:19:09.44
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 17 15:19:09.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-2899" for this suite. 01/17/23 15:19:09.963
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","completed":153,"skipped":2828,"failed":0}
------------------------------
• [SLOW TEST] [6.700 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:19:03.374
    Jan 17 15:19:03.374: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename crd-webhook 01/17/23 15:19:03.375
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:19:03.412
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:19:03.416
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 01/17/23 15:19:03.419
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/17/23 15:19:03.628
    STEP: Deploying the custom resource conversion webhook pod 01/17/23 15:19:03.635
    STEP: Wait for the deployment to be ready 01/17/23 15:19:03.646
    Jan 17 15:19:03.651: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/17/23 15:19:05.661
    STEP: Verifying the service has paired with the endpoint 01/17/23 15:19:05.671
    Jan 17 15:19:06.671: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    Jan 17 15:19:06.675: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Creating a v1 custom resource 01/17/23 15:19:09.28
    STEP: Create a v2 custom resource 01/17/23 15:19:09.379
    STEP: List CRs in v1 01/17/23 15:19:09.435
    STEP: List CRs in v2 01/17/23 15:19:09.44
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 17 15:19:09.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-webhook-2899" for this suite. 01/17/23 15:19:09.963
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:19:10.074
Jan 17 15:19:10.075: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename watch 01/17/23 15:19:10.075
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:19:10.111
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:19:10.116
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 01/17/23 15:19:10.119
STEP: starting a background goroutine to produce watch events 01/17/23 15:19:10.13
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 01/17/23 15:19:10.13
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Jan 17 15:19:12.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6186" for this suite. 01/17/23 15:19:12.93
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","completed":154,"skipped":2831,"failed":0}
------------------------------
• [2.907 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:19:10.074
    Jan 17 15:19:10.075: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename watch 01/17/23 15:19:10.075
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:19:10.111
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:19:10.116
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 01/17/23 15:19:10.119
    STEP: starting a background goroutine to produce watch events 01/17/23 15:19:10.13
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 01/17/23 15:19:10.13
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Jan 17 15:19:12.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-6186" for this suite. 01/17/23 15:19:12.93
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:19:12.982
Jan 17 15:19:12.982: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename certificates 01/17/23 15:19:12.983
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:19:13.008
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:19:13.012
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 01/17/23 15:19:13.351
STEP: getting /apis/certificates.k8s.io 01/17/23 15:19:13.353
STEP: getting /apis/certificates.k8s.io/v1 01/17/23 15:19:13.354
STEP: creating 01/17/23 15:19:13.355
STEP: getting 01/17/23 15:19:13.376
STEP: listing 01/17/23 15:19:13.379
STEP: watching 01/17/23 15:19:13.382
Jan 17 15:19:13.382: INFO: starting watch
STEP: patching 01/17/23 15:19:13.383
STEP: updating 01/17/23 15:19:13.387
Jan 17 15:19:13.393: INFO: waiting for watch events with expected annotations
Jan 17 15:19:13.393: INFO: saw patched and updated annotations
STEP: getting /approval 01/17/23 15:19:13.393
STEP: patching /approval 01/17/23 15:19:13.396
STEP: updating /approval 01/17/23 15:19:13.4
STEP: getting /status 01/17/23 15:19:13.406
STEP: patching /status 01/17/23 15:19:13.413
STEP: updating /status 01/17/23 15:19:13.419
STEP: deleting 01/17/23 15:19:13.426
STEP: deleting a collection 01/17/23 15:19:13.441
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 17 15:19:13.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-208" for this suite. 01/17/23 15:19:13.463
{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","completed":155,"skipped":2832,"failed":0}
------------------------------
• [0.487 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:19:12.982
    Jan 17 15:19:12.982: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename certificates 01/17/23 15:19:12.983
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:19:13.008
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:19:13.012
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 01/17/23 15:19:13.351
    STEP: getting /apis/certificates.k8s.io 01/17/23 15:19:13.353
    STEP: getting /apis/certificates.k8s.io/v1 01/17/23 15:19:13.354
    STEP: creating 01/17/23 15:19:13.355
    STEP: getting 01/17/23 15:19:13.376
    STEP: listing 01/17/23 15:19:13.379
    STEP: watching 01/17/23 15:19:13.382
    Jan 17 15:19:13.382: INFO: starting watch
    STEP: patching 01/17/23 15:19:13.383
    STEP: updating 01/17/23 15:19:13.387
    Jan 17 15:19:13.393: INFO: waiting for watch events with expected annotations
    Jan 17 15:19:13.393: INFO: saw patched and updated annotations
    STEP: getting /approval 01/17/23 15:19:13.393
    STEP: patching /approval 01/17/23 15:19:13.396
    STEP: updating /approval 01/17/23 15:19:13.4
    STEP: getting /status 01/17/23 15:19:13.406
    STEP: patching /status 01/17/23 15:19:13.413
    STEP: updating /status 01/17/23 15:19:13.419
    STEP: deleting 01/17/23 15:19:13.426
    STEP: deleting a collection 01/17/23 15:19:13.441
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 17 15:19:13.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "certificates-208" for this suite. 01/17/23 15:19:13.463
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:19:13.471
Jan 17 15:19:13.471: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename webhook 01/17/23 15:19:13.471
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:19:13.495
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:19:13.501
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/17/23 15:19:13.525
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/17/23 15:19:13.677
STEP: Deploying the webhook pod 01/17/23 15:19:13.687
STEP: Wait for the deployment to be ready 01/17/23 15:19:13.7
Jan 17 15:19:13.710: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/17/23 15:19:15.723
STEP: Verifying the service has paired with the endpoint 01/17/23 15:19:15.735
Jan 17 15:19:16.735: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/17/23 15:19:16.738
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/17/23 15:19:16.751
STEP: Creating a dummy validating-webhook-configuration object 01/17/23 15:19:16.762
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 01/17/23 15:19:16.769
STEP: Creating a dummy mutating-webhook-configuration object 01/17/23 15:19:16.782
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 01/17/23 15:19:16.789
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 17 15:19:16.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3561" for this suite. 01/17/23 15:19:16.809
STEP: Destroying namespace "webhook-3561-markers" for this suite. 01/17/23 15:19:16.816
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","completed":156,"skipped":2869,"failed":0}
------------------------------
• [3.414 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:19:13.471
    Jan 17 15:19:13.471: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename webhook 01/17/23 15:19:13.471
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:19:13.495
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:19:13.501
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/17/23 15:19:13.525
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/17/23 15:19:13.677
    STEP: Deploying the webhook pod 01/17/23 15:19:13.687
    STEP: Wait for the deployment to be ready 01/17/23 15:19:13.7
    Jan 17 15:19:13.710: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/17/23 15:19:15.723
    STEP: Verifying the service has paired with the endpoint 01/17/23 15:19:15.735
    Jan 17 15:19:16.735: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:276
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/17/23 15:19:16.738
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/17/23 15:19:16.751
    STEP: Creating a dummy validating-webhook-configuration object 01/17/23 15:19:16.762
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 01/17/23 15:19:16.769
    STEP: Creating a dummy mutating-webhook-configuration object 01/17/23 15:19:16.782
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 01/17/23 15:19:16.789
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 17 15:19:16.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-3561" for this suite. 01/17/23 15:19:16.809
    STEP: Destroying namespace "webhook-3561-markers" for this suite. 01/17/23 15:19:16.816
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:19:16.885
Jan 17 15:19:16.886: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename replicaset 01/17/23 15:19:16.886
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:19:16.925
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:19:16.928
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 01/17/23 15:19:16.93
Jan 17 15:19:16.942: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 17 15:19:21.946: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/17/23 15:19:21.946
STEP: getting scale subresource 01/17/23 15:19:21.946
STEP: updating a scale subresource 01/17/23 15:19:21.95
STEP: verifying the replicaset Spec.Replicas was modified 01/17/23 15:19:21.955
STEP: Patch a scale subresource 01/17/23 15:19:21.958
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jan 17 15:19:21.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4971" for this suite. 01/17/23 15:19:21.973
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","completed":157,"skipped":2873,"failed":0}
------------------------------
• [SLOW TEST] [5.097 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:19:16.885
    Jan 17 15:19:16.886: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename replicaset 01/17/23 15:19:16.886
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:19:16.925
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:19:16.928
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 01/17/23 15:19:16.93
    Jan 17 15:19:16.942: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan 17 15:19:21.946: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/17/23 15:19:21.946
    STEP: getting scale subresource 01/17/23 15:19:21.946
    STEP: updating a scale subresource 01/17/23 15:19:21.95
    STEP: verifying the replicaset Spec.Replicas was modified 01/17/23 15:19:21.955
    STEP: Patch a scale subresource 01/17/23 15:19:21.958
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jan 17 15:19:21.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-4971" for this suite. 01/17/23 15:19:21.973
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:19:21.983
Jan 17 15:19:21.983: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename projected 01/17/23 15:19:21.984
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:19:22.014
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:19:22.017
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55
STEP: Creating projection with secret that has name projected-secret-test-644c125d-c903-488f-a461-078334043547 01/17/23 15:19:22.019
STEP: Creating a pod to test consume secrets 01/17/23 15:19:22.031
Jan 17 15:19:22.082: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-301f2af4-6e1e-4008-b78f-8fcbbd240d6f" in namespace "projected-269" to be "Succeeded or Failed"
Jan 17 15:19:22.091: INFO: Pod "pod-projected-secrets-301f2af4-6e1e-4008-b78f-8fcbbd240d6f": Phase="Pending", Reason="", readiness=false. Elapsed: 9.71159ms
Jan 17 15:19:24.097: INFO: Pod "pod-projected-secrets-301f2af4-6e1e-4008-b78f-8fcbbd240d6f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014954508s
Jan 17 15:19:26.096: INFO: Pod "pod-projected-secrets-301f2af4-6e1e-4008-b78f-8fcbbd240d6f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014406581s
STEP: Saw pod success 01/17/23 15:19:26.096
Jan 17 15:19:26.096: INFO: Pod "pod-projected-secrets-301f2af4-6e1e-4008-b78f-8fcbbd240d6f" satisfied condition "Succeeded or Failed"
Jan 17 15:19:26.099: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-projected-secrets-301f2af4-6e1e-4008-b78f-8fcbbd240d6f container projected-secret-volume-test: <nil>
STEP: delete the pod 01/17/23 15:19:26.105
Jan 17 15:19:26.117: INFO: Waiting for pod pod-projected-secrets-301f2af4-6e1e-4008-b78f-8fcbbd240d6f to disappear
Jan 17 15:19:26.120: INFO: Pod pod-projected-secrets-301f2af4-6e1e-4008-b78f-8fcbbd240d6f no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan 17 15:19:26.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-269" for this suite. 01/17/23 15:19:26.124
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":158,"skipped":2906,"failed":0}
------------------------------
• [4.148 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:19:21.983
    Jan 17 15:19:21.983: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename projected 01/17/23 15:19:21.984
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:19:22.014
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:19:22.017
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:55
    STEP: Creating projection with secret that has name projected-secret-test-644c125d-c903-488f-a461-078334043547 01/17/23 15:19:22.019
    STEP: Creating a pod to test consume secrets 01/17/23 15:19:22.031
    Jan 17 15:19:22.082: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-301f2af4-6e1e-4008-b78f-8fcbbd240d6f" in namespace "projected-269" to be "Succeeded or Failed"
    Jan 17 15:19:22.091: INFO: Pod "pod-projected-secrets-301f2af4-6e1e-4008-b78f-8fcbbd240d6f": Phase="Pending", Reason="", readiness=false. Elapsed: 9.71159ms
    Jan 17 15:19:24.097: INFO: Pod "pod-projected-secrets-301f2af4-6e1e-4008-b78f-8fcbbd240d6f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014954508s
    Jan 17 15:19:26.096: INFO: Pod "pod-projected-secrets-301f2af4-6e1e-4008-b78f-8fcbbd240d6f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014406581s
    STEP: Saw pod success 01/17/23 15:19:26.096
    Jan 17 15:19:26.096: INFO: Pod "pod-projected-secrets-301f2af4-6e1e-4008-b78f-8fcbbd240d6f" satisfied condition "Succeeded or Failed"
    Jan 17 15:19:26.099: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-projected-secrets-301f2af4-6e1e-4008-b78f-8fcbbd240d6f container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/17/23 15:19:26.105
    Jan 17 15:19:26.117: INFO: Waiting for pod pod-projected-secrets-301f2af4-6e1e-4008-b78f-8fcbbd240d6f to disappear
    Jan 17 15:19:26.120: INFO: Pod pod-projected-secrets-301f2af4-6e1e-4008-b78f-8fcbbd240d6f no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan 17 15:19:26.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-269" for this suite. 01/17/23 15:19:26.124
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:19:26.131
Jan 17 15:19:26.131: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename crd-publish-openapi 01/17/23 15:19:26.132
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:19:26.163
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:19:26.165
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152
Jan 17 15:19:26.168: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/17/23 15:19:32.999
Jan 17 15:19:32.999: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=crd-publish-openapi-1026 --namespace=crd-publish-openapi-1026 create -f -'
Jan 17 15:19:33.988: INFO: stderr: ""
Jan 17 15:19:33.988: INFO: stdout: "e2e-test-crd-publish-openapi-901-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jan 17 15:19:33.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=crd-publish-openapi-1026 --namespace=crd-publish-openapi-1026 delete e2e-test-crd-publish-openapi-901-crds test-cr'
Jan 17 15:19:34.040: INFO: stderr: ""
Jan 17 15:19:34.040: INFO: stdout: "e2e-test-crd-publish-openapi-901-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Jan 17 15:19:34.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=crd-publish-openapi-1026 --namespace=crd-publish-openapi-1026 apply -f -'
Jan 17 15:19:34.953: INFO: stderr: ""
Jan 17 15:19:34.953: INFO: stdout: "e2e-test-crd-publish-openapi-901-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jan 17 15:19:34.953: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=crd-publish-openapi-1026 --namespace=crd-publish-openapi-1026 delete e2e-test-crd-publish-openapi-901-crds test-cr'
Jan 17 15:19:35.004: INFO: stderr: ""
Jan 17 15:19:35.004: INFO: stdout: "e2e-test-crd-publish-openapi-901-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 01/17/23 15:19:35.004
Jan 17 15:19:35.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=crd-publish-openapi-1026 explain e2e-test-crd-publish-openapi-901-crds'
Jan 17 15:19:35.253: INFO: stderr: ""
Jan 17 15:19:35.253: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-901-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 17 15:19:40.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1026" for this suite. 01/17/23 15:19:41.001
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","completed":159,"skipped":2914,"failed":0}
------------------------------
• [SLOW TEST] [14.876 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:19:26.131
    Jan 17 15:19:26.131: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename crd-publish-openapi 01/17/23 15:19:26.132
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:19:26.163
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:19:26.165
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:152
    Jan 17 15:19:26.168: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/17/23 15:19:32.999
    Jan 17 15:19:32.999: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=crd-publish-openapi-1026 --namespace=crd-publish-openapi-1026 create -f -'
    Jan 17 15:19:33.988: INFO: stderr: ""
    Jan 17 15:19:33.988: INFO: stdout: "e2e-test-crd-publish-openapi-901-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Jan 17 15:19:33.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=crd-publish-openapi-1026 --namespace=crd-publish-openapi-1026 delete e2e-test-crd-publish-openapi-901-crds test-cr'
    Jan 17 15:19:34.040: INFO: stderr: ""
    Jan 17 15:19:34.040: INFO: stdout: "e2e-test-crd-publish-openapi-901-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    Jan 17 15:19:34.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=crd-publish-openapi-1026 --namespace=crd-publish-openapi-1026 apply -f -'
    Jan 17 15:19:34.953: INFO: stderr: ""
    Jan 17 15:19:34.953: INFO: stdout: "e2e-test-crd-publish-openapi-901-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Jan 17 15:19:34.953: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=crd-publish-openapi-1026 --namespace=crd-publish-openapi-1026 delete e2e-test-crd-publish-openapi-901-crds test-cr'
    Jan 17 15:19:35.004: INFO: stderr: ""
    Jan 17 15:19:35.004: INFO: stdout: "e2e-test-crd-publish-openapi-901-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 01/17/23 15:19:35.004
    Jan 17 15:19:35.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=crd-publish-openapi-1026 explain e2e-test-crd-publish-openapi-901-crds'
    Jan 17 15:19:35.253: INFO: stderr: ""
    Jan 17 15:19:35.253: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-901-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 17 15:19:40.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-1026" for this suite. 01/17/23 15:19:41.001
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:19:41.007
Jan 17 15:19:41.008: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename deployment 01/17/23 15:19:41.008
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:19:41.035
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:19:41.037
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 01/17/23 15:19:41.058
STEP: waiting for Deployment to be created 01/17/23 15:19:41.066
STEP: waiting for all Replicas to be Ready 01/17/23 15:19:41.075
Jan 17 15:19:41.075: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 17 15:19:41.075: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 17 15:19:41.095: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 17 15:19:41.095: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 17 15:19:41.110: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 17 15:19:41.110: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 17 15:19:41.165: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 17 15:19:41.165: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 17 15:19:42.060: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jan 17 15:19:42.060: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jan 17 15:19:42.326: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 01/17/23 15:19:42.326
W0117 15:19:42.338996      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jan 17 15:19:42.340: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 01/17/23 15:19:42.34
Jan 17 15:19:42.341: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 0
Jan 17 15:19:42.341: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 0
Jan 17 15:19:42.341: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 0
Jan 17 15:19:42.341: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 0
Jan 17 15:19:42.341: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 0
Jan 17 15:19:42.341: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 0
Jan 17 15:19:42.341: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 0
Jan 17 15:19:42.341: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 0
Jan 17 15:19:42.341: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 1
Jan 17 15:19:42.341: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 1
Jan 17 15:19:42.341: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 2
Jan 17 15:19:42.341: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 2
Jan 17 15:19:42.341: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 2
Jan 17 15:19:42.341: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 2
Jan 17 15:19:42.352: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 2
Jan 17 15:19:42.352: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 2
Jan 17 15:19:42.369: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 2
Jan 17 15:19:42.369: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 2
Jan 17 15:19:42.376: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 1
Jan 17 15:19:42.376: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 1
Jan 17 15:19:42.394: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 1
Jan 17 15:19:42.394: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 1
Jan 17 15:19:43.471: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 2
Jan 17 15:19:43.471: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 2
Jan 17 15:19:43.492: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 1
STEP: listing Deployments 01/17/23 15:19:43.492
Jan 17 15:19:43.500: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 01/17/23 15:19:43.5
Jan 17 15:19:43.509: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 01/17/23 15:19:43.509
Jan 17 15:19:43.515: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 17 15:19:43.518: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 17 15:19:43.539: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 17 15:19:43.558: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 17 15:19:43.564: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 17 15:19:45.123: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 17 15:19:45.139: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 17 15:19:45.148: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
Jan 17 15:19:45.166: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 17 15:19:45.173: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 17 15:19:46.342: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 01/17/23 15:19:46.363
STEP: fetching the DeploymentStatus 01/17/23 15:19:46.371
Jan 17 15:19:46.375: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 1
Jan 17 15:19:46.375: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 1
Jan 17 15:19:46.375: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 1
Jan 17 15:19:46.375: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 1
Jan 17 15:19:46.375: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 1
Jan 17 15:19:46.376: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 2
Jan 17 15:19:46.376: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 2
Jan 17 15:19:46.376: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 3
Jan 17 15:19:46.376: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 2
Jan 17 15:19:46.376: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 2
Jan 17 15:19:46.376: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 3
STEP: deleting the Deployment 01/17/23 15:19:46.376
Jan 17 15:19:46.388: INFO: observed event type MODIFIED
Jan 17 15:19:46.388: INFO: observed event type MODIFIED
Jan 17 15:19:46.388: INFO: observed event type MODIFIED
Jan 17 15:19:46.388: INFO: observed event type MODIFIED
Jan 17 15:19:46.388: INFO: observed event type MODIFIED
Jan 17 15:19:46.388: INFO: observed event type MODIFIED
Jan 17 15:19:46.388: INFO: observed event type MODIFIED
Jan 17 15:19:46.389: INFO: observed event type MODIFIED
Jan 17 15:19:46.389: INFO: observed event type MODIFIED
Jan 17 15:19:46.389: INFO: observed event type MODIFIED
Jan 17 15:19:46.389: INFO: observed event type MODIFIED
Jan 17 15:19:46.389: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 17 15:19:46.391: INFO: Log out all the ReplicaSets if there is no deployment created
Jan 17 15:19:46.402: INFO: ReplicaSet "test-deployment-54cc775c4b":
&ReplicaSet{ObjectMeta:{test-deployment-54cc775c4b  deployment-4831  b5bf4998-5311-4897-abe4-32aa65a1b46c 95212 4 2023-01-17 15:19:42 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment ee8d791c-9226-4d0e-ae21-2e1fc997a99e 0xc00a9ae557 0xc00a9ae558}] [] [{kube-controller-manager Update apps/v1 2023-01-17 15:19:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ee8d791c-9226-4d0e-ae21-2e1fc997a99e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 15:19:46 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 54cc775c4b,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00a9ae5e0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Jan 17 15:19:46.405: INFO: pod: "test-deployment-54cc775c4b-thsmp":
&Pod{ObjectMeta:{test-deployment-54cc775c4b-thsmp test-deployment-54cc775c4b- deployment-4831  4c1b6d91-c2fe-4ef2-b27d-accfe5c16c48 95208 0 2023-01-17 15:19:42 +0000 UTC 2023-01-17 15:19:47 +0000 UTC 0xc00a9aea78 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.129.2.59/23"],"mac_address":"0a:58:0a:81:02:3b","gateway_ips":["10.129.2.1"],"ip_address":"10.129.2.59/23","gateway_ip":"10.129.2.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.129.2.59"
    ],
    "mac": "0a:58:0a:81:02:3b",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.129.2.59"
    ],
    "mac": "0a:58:0a:81:02:3b",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-54cc775c4b b5bf4998-5311-4897-abe4-32aa65a1b46c 0xc00a9aeab7 0xc00a9aeab8}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:19:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:19:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b5bf4998-5311-4897-abe4-32aa65a1b46c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-17 15:19:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-17 15:19:43 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.129.2.59\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tpqxl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tpqxl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-139-213.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c49,c24,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-rvrr4,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:19:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:19:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:19:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:19:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.139.213,PodIP:10.129.2.59,StartTime:2023-01-17 15:19:42 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 15:19:43 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d,ContainerID:cri-o://5607549ac6a95894257e6a516ff0c8c1d2a6c5d165a2e19c64d2ca3e02ba2af5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.129.2.59,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jan 17 15:19:46.405: INFO: pod: "test-deployment-54cc775c4b-x2hlr":
&Pod{ObjectMeta:{test-deployment-54cc775c4b-x2hlr test-deployment-54cc775c4b- deployment-4831  3a142bb4-5a23-4260-aed4-406de1c9ecfc 95166 0 2023-01-17 15:19:43 +0000 UTC 2023-01-17 15:19:46 +0000 UTC 0xc00a9aecf0 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.131.0.149/23"],"mac_address":"0a:58:0a:83:00:95","gateway_ips":["10.131.0.1"],"ip_address":"10.131.0.149/23","gateway_ip":"10.131.0.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.131.0.149"
    ],
    "mac": "0a:58:0a:83:00:95",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.131.0.149"
    ],
    "mac": "0a:58:0a:83:00:95",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-54cc775c4b b5bf4998-5311-4897-abe4-32aa65a1b46c 0xc00a9aed37 0xc00a9aed38}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:19:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:19:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b5bf4998-5311-4897-abe4-32aa65a1b46c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-17 15:19:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-17 15:19:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.0.149\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r6sbt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r6sbt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-151-22.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c49,c24,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-rvrr4,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:19:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:19:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:19:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:19:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.151.22,PodIP:10.131.0.149,StartTime:2023-01-17 15:19:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 15:19:44 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d,ContainerID:cri-o://7d2901cf52cb3bdec6fbf18849b97f26ad1acc9ed184c4940c15d1f384e6c5f6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.131.0.149,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jan 17 15:19:46.405: INFO: ReplicaSet "test-deployment-7c7d8d58c8":
&ReplicaSet{ObjectMeta:{test-deployment-7c7d8d58c8  deployment-4831  46fd0bbf-910b-49db-8696-b12b21997f7e 95204 2 2023-01-17 15:19:43 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment ee8d791c-9226-4d0e-ae21-2e1fc997a99e 0xc00a9ae647 0xc00a9ae648}] [] [{kube-controller-manager Update apps/v1 2023-01-17 15:19:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ee8d791c-9226-4d0e-ae21-2e1fc997a99e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 15:19:46 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7c7d8d58c8,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00a9ae6d0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Jan 17 15:19:46.414: INFO: pod: "test-deployment-7c7d8d58c8-7j7gp":
&Pod{ObjectMeta:{test-deployment-7c7d8d58c8-7j7gp test-deployment-7c7d8d58c8- deployment-4831  398529aa-db5a-4251-b50e-549b894d795a 95203 0 2023-01-17 15:19:45 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.2.96/23"],"mac_address":"0a:58:0a:80:02:60","gateway_ips":["10.128.2.1"],"ip_address":"10.128.2.96/23","gateway_ip":"10.128.2.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.2.96"
    ],
    "mac": "0a:58:0a:80:02:60",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.2.96"
    ],
    "mac": "0a:58:0a:80:02:60",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 46fd0bbf-910b-49db-8696-b12b21997f7e 0xc00a9dc567 0xc00a9dc568}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:19:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:19:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"46fd0bbf-910b-49db-8696-b12b21997f7e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-17 15:19:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-17 15:19:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.2.96\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pf86c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pf86c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-165-14.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c49,c24,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-rvrr4,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:19:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:19:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:19:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:19:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.165.14,PodIP:10.128.2.96,StartTime:2023-01-17 15:19:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 15:19:45 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://b81e020cafe85bcf449f867129af5501b1469790c3457f82f62b39d8dcad15f6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.2.96,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jan 17 15:19:46.414: INFO: pod: "test-deployment-7c7d8d58c8-cn74t":
&Pod{ObjectMeta:{test-deployment-7c7d8d58c8-cn74t test-deployment-7c7d8d58c8- deployment-4831  cb7e6cc6-ee5c-4a93-9501-dc9563feb0d9 95156 0 2023-01-17 15:19:43 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.131.0.150/23"],"mac_address":"0a:58:0a:83:00:96","gateway_ips":["10.131.0.1"],"ip_address":"10.131.0.150/23","gateway_ip":"10.131.0.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.131.0.150"
    ],
    "mac": "0a:58:0a:83:00:96",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.131.0.150"
    ],
    "mac": "0a:58:0a:83:00:96",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 46fd0bbf-910b-49db-8696-b12b21997f7e 0xc00a9dc867 0xc00a9dc868}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:19:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:19:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"46fd0bbf-910b-49db-8696-b12b21997f7e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-17 15:19:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-17 15:19:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.0.150\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bs29r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bs29r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-151-22.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c49,c24,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-rvrr4,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:19:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:19:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:19:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:19:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.151.22,PodIP:10.131.0.150,StartTime:2023-01-17 15:19:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 15:19:44 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://2f32adee8af56b8ed45a840de45144361b518c206373b36433e38c46159a878f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.131.0.150,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jan 17 15:19:46.414: INFO: ReplicaSet "test-deployment-8594bb6fdd":
&ReplicaSet{ObjectMeta:{test-deployment-8594bb6fdd  deployment-4831  cf16c3f1-89f3-457d-99c2-d358be497394 95100 3 2023-01-17 15:19:41 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment ee8d791c-9226-4d0e-ae21-2e1fc997a99e 0xc00a9ae737 0xc00a9ae738}] [] [{kube-controller-manager Update apps/v1 2023-01-17 15:19:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ee8d791c-9226-4d0e-ae21-2e1fc997a99e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 15:19:43 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 8594bb6fdd,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00a9ae7c0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan 17 15:19:46.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4831" for this suite. 01/17/23 15:19:46.421
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","completed":160,"skipped":2930,"failed":0}
------------------------------
• [SLOW TEST] [5.426 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:19:41.007
    Jan 17 15:19:41.008: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename deployment 01/17/23 15:19:41.008
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:19:41.035
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:19:41.037
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 01/17/23 15:19:41.058
    STEP: waiting for Deployment to be created 01/17/23 15:19:41.066
    STEP: waiting for all Replicas to be Ready 01/17/23 15:19:41.075
    Jan 17 15:19:41.075: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 17 15:19:41.075: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 17 15:19:41.095: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 17 15:19:41.095: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 17 15:19:41.110: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 17 15:19:41.110: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 17 15:19:41.165: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 17 15:19:41.165: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 17 15:19:42.060: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Jan 17 15:19:42.060: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Jan 17 15:19:42.326: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 01/17/23 15:19:42.326
    W0117 15:19:42.338996      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jan 17 15:19:42.340: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 01/17/23 15:19:42.34
    Jan 17 15:19:42.341: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 0
    Jan 17 15:19:42.341: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 0
    Jan 17 15:19:42.341: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 0
    Jan 17 15:19:42.341: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 0
    Jan 17 15:19:42.341: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 0
    Jan 17 15:19:42.341: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 0
    Jan 17 15:19:42.341: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 0
    Jan 17 15:19:42.341: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 0
    Jan 17 15:19:42.341: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 1
    Jan 17 15:19:42.341: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 1
    Jan 17 15:19:42.341: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 2
    Jan 17 15:19:42.341: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 2
    Jan 17 15:19:42.341: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 2
    Jan 17 15:19:42.341: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 2
    Jan 17 15:19:42.352: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 2
    Jan 17 15:19:42.352: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 2
    Jan 17 15:19:42.369: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 2
    Jan 17 15:19:42.369: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 2
    Jan 17 15:19:42.376: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 1
    Jan 17 15:19:42.376: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 1
    Jan 17 15:19:42.394: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 1
    Jan 17 15:19:42.394: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 1
    Jan 17 15:19:43.471: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 2
    Jan 17 15:19:43.471: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 2
    Jan 17 15:19:43.492: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 1
    STEP: listing Deployments 01/17/23 15:19:43.492
    Jan 17 15:19:43.500: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 01/17/23 15:19:43.5
    Jan 17 15:19:43.509: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 01/17/23 15:19:43.509
    Jan 17 15:19:43.515: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 17 15:19:43.518: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 17 15:19:43.539: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 17 15:19:43.558: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 17 15:19:43.564: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 17 15:19:45.123: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 17 15:19:45.139: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 17 15:19:45.148: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 17 15:19:45.166: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 17 15:19:45.173: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 17 15:19:46.342: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 01/17/23 15:19:46.363
    STEP: fetching the DeploymentStatus 01/17/23 15:19:46.371
    Jan 17 15:19:46.375: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 1
    Jan 17 15:19:46.375: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 1
    Jan 17 15:19:46.375: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 1
    Jan 17 15:19:46.375: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 1
    Jan 17 15:19:46.375: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 1
    Jan 17 15:19:46.376: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 2
    Jan 17 15:19:46.376: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 2
    Jan 17 15:19:46.376: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 3
    Jan 17 15:19:46.376: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 2
    Jan 17 15:19:46.376: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 2
    Jan 17 15:19:46.376: INFO: observed Deployment test-deployment in namespace deployment-4831 with ReadyReplicas 3
    STEP: deleting the Deployment 01/17/23 15:19:46.376
    Jan 17 15:19:46.388: INFO: observed event type MODIFIED
    Jan 17 15:19:46.388: INFO: observed event type MODIFIED
    Jan 17 15:19:46.388: INFO: observed event type MODIFIED
    Jan 17 15:19:46.388: INFO: observed event type MODIFIED
    Jan 17 15:19:46.388: INFO: observed event type MODIFIED
    Jan 17 15:19:46.388: INFO: observed event type MODIFIED
    Jan 17 15:19:46.388: INFO: observed event type MODIFIED
    Jan 17 15:19:46.389: INFO: observed event type MODIFIED
    Jan 17 15:19:46.389: INFO: observed event type MODIFIED
    Jan 17 15:19:46.389: INFO: observed event type MODIFIED
    Jan 17 15:19:46.389: INFO: observed event type MODIFIED
    Jan 17 15:19:46.389: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 17 15:19:46.391: INFO: Log out all the ReplicaSets if there is no deployment created
    Jan 17 15:19:46.402: INFO: ReplicaSet "test-deployment-54cc775c4b":
    &ReplicaSet{ObjectMeta:{test-deployment-54cc775c4b  deployment-4831  b5bf4998-5311-4897-abe4-32aa65a1b46c 95212 4 2023-01-17 15:19:42 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment ee8d791c-9226-4d0e-ae21-2e1fc997a99e 0xc00a9ae557 0xc00a9ae558}] [] [{kube-controller-manager Update apps/v1 2023-01-17 15:19:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ee8d791c-9226-4d0e-ae21-2e1fc997a99e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 15:19:46 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 54cc775c4b,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00a9ae5e0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    Jan 17 15:19:46.405: INFO: pod: "test-deployment-54cc775c4b-thsmp":
    &Pod{ObjectMeta:{test-deployment-54cc775c4b-thsmp test-deployment-54cc775c4b- deployment-4831  4c1b6d91-c2fe-4ef2-b27d-accfe5c16c48 95208 0 2023-01-17 15:19:42 +0000 UTC 2023-01-17 15:19:47 +0000 UTC 0xc00a9aea78 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.129.2.59/23"],"mac_address":"0a:58:0a:81:02:3b","gateway_ips":["10.129.2.1"],"ip_address":"10.129.2.59/23","gateway_ip":"10.129.2.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.129.2.59"
        ],
        "mac": "0a:58:0a:81:02:3b",
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.129.2.59"
        ],
        "mac": "0a:58:0a:81:02:3b",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-54cc775c4b b5bf4998-5311-4897-abe4-32aa65a1b46c 0xc00a9aeab7 0xc00a9aeab8}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:19:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:19:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b5bf4998-5311-4897-abe4-32aa65a1b46c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-17 15:19:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-17 15:19:43 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.129.2.59\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tpqxl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tpqxl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-139-213.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c49,c24,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-rvrr4,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:19:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:19:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:19:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:19:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.139.213,PodIP:10.129.2.59,StartTime:2023-01-17 15:19:42 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 15:19:43 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d,ContainerID:cri-o://5607549ac6a95894257e6a516ff0c8c1d2a6c5d165a2e19c64d2ca3e02ba2af5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.129.2.59,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jan 17 15:19:46.405: INFO: pod: "test-deployment-54cc775c4b-x2hlr":
    &Pod{ObjectMeta:{test-deployment-54cc775c4b-x2hlr test-deployment-54cc775c4b- deployment-4831  3a142bb4-5a23-4260-aed4-406de1c9ecfc 95166 0 2023-01-17 15:19:43 +0000 UTC 2023-01-17 15:19:46 +0000 UTC 0xc00a9aecf0 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.131.0.149/23"],"mac_address":"0a:58:0a:83:00:95","gateway_ips":["10.131.0.1"],"ip_address":"10.131.0.149/23","gateway_ip":"10.131.0.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.131.0.149"
        ],
        "mac": "0a:58:0a:83:00:95",
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.131.0.149"
        ],
        "mac": "0a:58:0a:83:00:95",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-54cc775c4b b5bf4998-5311-4897-abe4-32aa65a1b46c 0xc00a9aed37 0xc00a9aed38}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:19:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:19:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b5bf4998-5311-4897-abe4-32aa65a1b46c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-17 15:19:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-17 15:19:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.0.149\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r6sbt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r6sbt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-151-22.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c49,c24,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-rvrr4,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:19:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:19:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:19:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:19:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.151.22,PodIP:10.131.0.149,StartTime:2023-01-17 15:19:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 15:19:44 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d,ContainerID:cri-o://7d2901cf52cb3bdec6fbf18849b97f26ad1acc9ed184c4940c15d1f384e6c5f6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.131.0.149,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jan 17 15:19:46.405: INFO: ReplicaSet "test-deployment-7c7d8d58c8":
    &ReplicaSet{ObjectMeta:{test-deployment-7c7d8d58c8  deployment-4831  46fd0bbf-910b-49db-8696-b12b21997f7e 95204 2 2023-01-17 15:19:43 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment ee8d791c-9226-4d0e-ae21-2e1fc997a99e 0xc00a9ae647 0xc00a9ae648}] [] [{kube-controller-manager Update apps/v1 2023-01-17 15:19:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ee8d791c-9226-4d0e-ae21-2e1fc997a99e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 15:19:46 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7c7d8d58c8,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00a9ae6d0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

    Jan 17 15:19:46.414: INFO: pod: "test-deployment-7c7d8d58c8-7j7gp":
    &Pod{ObjectMeta:{test-deployment-7c7d8d58c8-7j7gp test-deployment-7c7d8d58c8- deployment-4831  398529aa-db5a-4251-b50e-549b894d795a 95203 0 2023-01-17 15:19:45 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.2.96/23"],"mac_address":"0a:58:0a:80:02:60","gateway_ips":["10.128.2.1"],"ip_address":"10.128.2.96/23","gateway_ip":"10.128.2.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.128.2.96"
        ],
        "mac": "0a:58:0a:80:02:60",
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.128.2.96"
        ],
        "mac": "0a:58:0a:80:02:60",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 46fd0bbf-910b-49db-8696-b12b21997f7e 0xc00a9dc567 0xc00a9dc568}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:19:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:19:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"46fd0bbf-910b-49db-8696-b12b21997f7e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-17 15:19:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-17 15:19:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.2.96\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pf86c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pf86c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-165-14.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c49,c24,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-rvrr4,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:19:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:19:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:19:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:19:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.165.14,PodIP:10.128.2.96,StartTime:2023-01-17 15:19:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 15:19:45 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://b81e020cafe85bcf449f867129af5501b1469790c3457f82f62b39d8dcad15f6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.2.96,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jan 17 15:19:46.414: INFO: pod: "test-deployment-7c7d8d58c8-cn74t":
    &Pod{ObjectMeta:{test-deployment-7c7d8d58c8-cn74t test-deployment-7c7d8d58c8- deployment-4831  cb7e6cc6-ee5c-4a93-9501-dc9563feb0d9 95156 0 2023-01-17 15:19:43 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.131.0.150/23"],"mac_address":"0a:58:0a:83:00:96","gateway_ips":["10.131.0.1"],"ip_address":"10.131.0.150/23","gateway_ip":"10.131.0.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.131.0.150"
        ],
        "mac": "0a:58:0a:83:00:96",
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.131.0.150"
        ],
        "mac": "0a:58:0a:83:00:96",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 46fd0bbf-910b-49db-8696-b12b21997f7e 0xc00a9dc867 0xc00a9dc868}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:19:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:19:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"46fd0bbf-910b-49db-8696-b12b21997f7e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-17 15:19:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-17 15:19:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.0.150\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bs29r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bs29r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-151-22.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c49,c24,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-rvrr4,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:19:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:19:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:19:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:19:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.151.22,PodIP:10.131.0.150,StartTime:2023-01-17 15:19:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 15:19:44 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://2f32adee8af56b8ed45a840de45144361b518c206373b36433e38c46159a878f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.131.0.150,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jan 17 15:19:46.414: INFO: ReplicaSet "test-deployment-8594bb6fdd":
    &ReplicaSet{ObjectMeta:{test-deployment-8594bb6fdd  deployment-4831  cf16c3f1-89f3-457d-99c2-d358be497394 95100 3 2023-01-17 15:19:41 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment ee8d791c-9226-4d0e-ae21-2e1fc997a99e 0xc00a9ae737 0xc00a9ae738}] [] [{kube-controller-manager Update apps/v1 2023-01-17 15:19:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ee8d791c-9226-4d0e-ae21-2e1fc997a99e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 15:19:43 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 8594bb6fdd,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00a9ae7c0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan 17 15:19:46.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-4831" for this suite. 01/17/23 15:19:46.421
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:19:46.434
Jan 17 15:19:46.434: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename endpointslice 01/17/23 15:19:46.435
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:19:46.466
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:19:46.468
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352
STEP: getting /apis 01/17/23 15:19:46.47
STEP: getting /apis/discovery.k8s.io 01/17/23 15:19:46.471
STEP: getting /apis/discovery.k8s.iov1 01/17/23 15:19:46.472
STEP: creating 01/17/23 15:19:46.473
STEP: getting 01/17/23 15:19:46.496
STEP: listing 01/17/23 15:19:46.499
STEP: watching 01/17/23 15:19:46.502
Jan 17 15:19:46.502: INFO: starting watch
STEP: cluster-wide listing 01/17/23 15:19:46.503
STEP: cluster-wide watching 01/17/23 15:19:46.508
Jan 17 15:19:46.508: INFO: starting watch
STEP: patching 01/17/23 15:19:46.509
STEP: updating 01/17/23 15:19:46.533
Jan 17 15:19:46.545: INFO: waiting for watch events with expected annotations
Jan 17 15:19:46.545: INFO: saw patched and updated annotations
STEP: deleting 01/17/23 15:19:46.545
STEP: deleting a collection 01/17/23 15:19:46.558
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Jan 17 15:19:46.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-5030" for this suite. 01/17/23 15:19:46.577
{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","completed":161,"skipped":2933,"failed":0}
------------------------------
• [0.149 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:19:46.434
    Jan 17 15:19:46.434: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename endpointslice 01/17/23 15:19:46.435
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:19:46.466
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:19:46.468
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:352
    STEP: getting /apis 01/17/23 15:19:46.47
    STEP: getting /apis/discovery.k8s.io 01/17/23 15:19:46.471
    STEP: getting /apis/discovery.k8s.iov1 01/17/23 15:19:46.472
    STEP: creating 01/17/23 15:19:46.473
    STEP: getting 01/17/23 15:19:46.496
    STEP: listing 01/17/23 15:19:46.499
    STEP: watching 01/17/23 15:19:46.502
    Jan 17 15:19:46.502: INFO: starting watch
    STEP: cluster-wide listing 01/17/23 15:19:46.503
    STEP: cluster-wide watching 01/17/23 15:19:46.508
    Jan 17 15:19:46.508: INFO: starting watch
    STEP: patching 01/17/23 15:19:46.509
    STEP: updating 01/17/23 15:19:46.533
    Jan 17 15:19:46.545: INFO: waiting for watch events with expected annotations
    Jan 17 15:19:46.545: INFO: saw patched and updated annotations
    STEP: deleting 01/17/23 15:19:46.545
    STEP: deleting a collection 01/17/23 15:19:46.558
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Jan 17 15:19:46.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-5030" for this suite. 01/17/23 15:19:46.577
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:19:46.583
Jan 17 15:19:46.583: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename var-expansion 01/17/23 15:19:46.584
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:19:46.604
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:19:46.606
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296
STEP: creating the pod 01/17/23 15:19:46.608
STEP: waiting for pod running 01/17/23 15:19:46.634
Jan 17 15:19:46.634: INFO: Waiting up to 2m0s for pod "var-expansion-1714de76-0b75-44d0-8eae-f7c81bf23645" in namespace "var-expansion-4926" to be "running"
Jan 17 15:19:46.637: INFO: Pod "var-expansion-1714de76-0b75-44d0-8eae-f7c81bf23645": Phase="Pending", Reason="", readiness=false. Elapsed: 2.786303ms
Jan 17 15:19:48.642: INFO: Pod "var-expansion-1714de76-0b75-44d0-8eae-f7c81bf23645": Phase="Running", Reason="", readiness=true. Elapsed: 2.008083948s
Jan 17 15:19:48.642: INFO: Pod "var-expansion-1714de76-0b75-44d0-8eae-f7c81bf23645" satisfied condition "running"
STEP: creating a file in subpath 01/17/23 15:19:48.642
Jan 17 15:19:48.646: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-4926 PodName:var-expansion-1714de76-0b75-44d0-8eae-f7c81bf23645 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 15:19:48.646: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
Jan 17 15:19:48.646: INFO: ExecWithOptions: Clientset creation
Jan 17 15:19:48.646: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/var-expansion-4926/pods/var-expansion-1714de76-0b75-44d0-8eae-f7c81bf23645/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 01/17/23 15:19:48.692
Jan 17 15:19:48.695: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-4926 PodName:var-expansion-1714de76-0b75-44d0-8eae-f7c81bf23645 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 15:19:48.696: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
Jan 17 15:19:48.696: INFO: ExecWithOptions: Clientset creation
Jan 17 15:19:48.696: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/var-expansion-4926/pods/var-expansion-1714de76-0b75-44d0-8eae-f7c81bf23645/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 01/17/23 15:19:48.746
Jan 17 15:19:49.262: INFO: Successfully updated pod "var-expansion-1714de76-0b75-44d0-8eae-f7c81bf23645"
STEP: waiting for annotated pod running 01/17/23 15:19:49.262
Jan 17 15:19:49.262: INFO: Waiting up to 2m0s for pod "var-expansion-1714de76-0b75-44d0-8eae-f7c81bf23645" in namespace "var-expansion-4926" to be "running"
Jan 17 15:19:49.265: INFO: Pod "var-expansion-1714de76-0b75-44d0-8eae-f7c81bf23645": Phase="Running", Reason="", readiness=true. Elapsed: 2.542115ms
Jan 17 15:19:49.265: INFO: Pod "var-expansion-1714de76-0b75-44d0-8eae-f7c81bf23645" satisfied condition "running"
STEP: deleting the pod gracefully 01/17/23 15:19:49.265
Jan 17 15:19:49.265: INFO: Deleting pod "var-expansion-1714de76-0b75-44d0-8eae-f7c81bf23645" in namespace "var-expansion-4926"
Jan 17 15:19:49.271: INFO: Wait up to 5m0s for pod "var-expansion-1714de76-0b75-44d0-8eae-f7c81bf23645" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan 17 15:20:23.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4926" for this suite. 01/17/23 15:20:23.291
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","completed":162,"skipped":2942,"failed":0}
------------------------------
• [SLOW TEST] [36.713 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:19:46.583
    Jan 17 15:19:46.583: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename var-expansion 01/17/23 15:19:46.584
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:19:46.604
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:19:46.606
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:296
    STEP: creating the pod 01/17/23 15:19:46.608
    STEP: waiting for pod running 01/17/23 15:19:46.634
    Jan 17 15:19:46.634: INFO: Waiting up to 2m0s for pod "var-expansion-1714de76-0b75-44d0-8eae-f7c81bf23645" in namespace "var-expansion-4926" to be "running"
    Jan 17 15:19:46.637: INFO: Pod "var-expansion-1714de76-0b75-44d0-8eae-f7c81bf23645": Phase="Pending", Reason="", readiness=false. Elapsed: 2.786303ms
    Jan 17 15:19:48.642: INFO: Pod "var-expansion-1714de76-0b75-44d0-8eae-f7c81bf23645": Phase="Running", Reason="", readiness=true. Elapsed: 2.008083948s
    Jan 17 15:19:48.642: INFO: Pod "var-expansion-1714de76-0b75-44d0-8eae-f7c81bf23645" satisfied condition "running"
    STEP: creating a file in subpath 01/17/23 15:19:48.642
    Jan 17 15:19:48.646: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-4926 PodName:var-expansion-1714de76-0b75-44d0-8eae-f7c81bf23645 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 17 15:19:48.646: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    Jan 17 15:19:48.646: INFO: ExecWithOptions: Clientset creation
    Jan 17 15:19:48.646: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/var-expansion-4926/pods/var-expansion-1714de76-0b75-44d0-8eae-f7c81bf23645/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 01/17/23 15:19:48.692
    Jan 17 15:19:48.695: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-4926 PodName:var-expansion-1714de76-0b75-44d0-8eae-f7c81bf23645 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 17 15:19:48.696: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    Jan 17 15:19:48.696: INFO: ExecWithOptions: Clientset creation
    Jan 17 15:19:48.696: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/var-expansion-4926/pods/var-expansion-1714de76-0b75-44d0-8eae-f7c81bf23645/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 01/17/23 15:19:48.746
    Jan 17 15:19:49.262: INFO: Successfully updated pod "var-expansion-1714de76-0b75-44d0-8eae-f7c81bf23645"
    STEP: waiting for annotated pod running 01/17/23 15:19:49.262
    Jan 17 15:19:49.262: INFO: Waiting up to 2m0s for pod "var-expansion-1714de76-0b75-44d0-8eae-f7c81bf23645" in namespace "var-expansion-4926" to be "running"
    Jan 17 15:19:49.265: INFO: Pod "var-expansion-1714de76-0b75-44d0-8eae-f7c81bf23645": Phase="Running", Reason="", readiness=true. Elapsed: 2.542115ms
    Jan 17 15:19:49.265: INFO: Pod "var-expansion-1714de76-0b75-44d0-8eae-f7c81bf23645" satisfied condition "running"
    STEP: deleting the pod gracefully 01/17/23 15:19:49.265
    Jan 17 15:19:49.265: INFO: Deleting pod "var-expansion-1714de76-0b75-44d0-8eae-f7c81bf23645" in namespace "var-expansion-4926"
    Jan 17 15:19:49.271: INFO: Wait up to 5m0s for pod "var-expansion-1714de76-0b75-44d0-8eae-f7c81bf23645" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan 17 15:20:23.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-4926" for this suite. 01/17/23 15:20:23.291
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:350
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:20:23.298
Jan 17 15:20:23.298: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename kubectl 01/17/23 15:20:23.299
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:20:23.315
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:20:23.317
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:324
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:350
STEP: creating a replication controller 01/17/23 15:20:23.333
Jan 17 15:20:23.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-734 create -f -'
Jan 17 15:20:24.288: INFO: stderr: ""
Jan 17 15:20:24.288: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/17/23 15:20:24.288
Jan 17 15:20:24.288: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-734 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 17 15:20:24.346: INFO: stderr: ""
Jan 17 15:20:24.346: INFO: stdout: "update-demo-nautilus-4sn7h update-demo-nautilus-qq4sd "
Jan 17 15:20:24.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-734 get pods update-demo-nautilus-4sn7h -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 17 15:20:24.402: INFO: stderr: ""
Jan 17 15:20:24.402: INFO: stdout: ""
Jan 17 15:20:24.402: INFO: update-demo-nautilus-4sn7h is created but not running
Jan 17 15:20:29.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-734 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 17 15:20:29.451: INFO: stderr: ""
Jan 17 15:20:29.451: INFO: stdout: "update-demo-nautilus-4sn7h update-demo-nautilus-qq4sd "
Jan 17 15:20:29.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-734 get pods update-demo-nautilus-4sn7h -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 17 15:20:29.497: INFO: stderr: ""
Jan 17 15:20:29.497: INFO: stdout: "true"
Jan 17 15:20:29.497: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-734 get pods update-demo-nautilus-4sn7h -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 17 15:20:29.541: INFO: stderr: ""
Jan 17 15:20:29.541: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan 17 15:20:29.541: INFO: validating pod update-demo-nautilus-4sn7h
Jan 17 15:20:29.547: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 17 15:20:29.547: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 17 15:20:29.547: INFO: update-demo-nautilus-4sn7h is verified up and running
Jan 17 15:20:29.547: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-734 get pods update-demo-nautilus-qq4sd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 17 15:20:29.591: INFO: stderr: ""
Jan 17 15:20:29.591: INFO: stdout: "true"
Jan 17 15:20:29.591: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-734 get pods update-demo-nautilus-qq4sd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 17 15:20:29.636: INFO: stderr: ""
Jan 17 15:20:29.636: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan 17 15:20:29.636: INFO: validating pod update-demo-nautilus-qq4sd
Jan 17 15:20:29.642: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 17 15:20:29.642: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 17 15:20:29.642: INFO: update-demo-nautilus-qq4sd is verified up and running
STEP: scaling down the replication controller 01/17/23 15:20:29.642
Jan 17 15:20:29.644: INFO: scanned /root for discovery docs: <nil>
Jan 17 15:20:29.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-734 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Jan 17 15:20:30.709: INFO: stderr: ""
Jan 17 15:20:30.709: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/17/23 15:20:30.709
Jan 17 15:20:30.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-734 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 17 15:20:30.762: INFO: stderr: ""
Jan 17 15:20:30.762: INFO: stdout: "update-demo-nautilus-4sn7h update-demo-nautilus-qq4sd "
STEP: Replicas for name=update-demo: expected=1 actual=2 01/17/23 15:20:30.762
Jan 17 15:20:35.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-734 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 17 15:20:35.812: INFO: stderr: ""
Jan 17 15:20:35.812: INFO: stdout: "update-demo-nautilus-qq4sd "
Jan 17 15:20:35.812: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-734 get pods update-demo-nautilus-qq4sd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 17 15:20:35.859: INFO: stderr: ""
Jan 17 15:20:35.859: INFO: stdout: "true"
Jan 17 15:20:35.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-734 get pods update-demo-nautilus-qq4sd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 17 15:20:35.906: INFO: stderr: ""
Jan 17 15:20:35.906: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan 17 15:20:35.906: INFO: validating pod update-demo-nautilus-qq4sd
Jan 17 15:20:35.910: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 17 15:20:35.910: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 17 15:20:35.910: INFO: update-demo-nautilus-qq4sd is verified up and running
STEP: scaling up the replication controller 01/17/23 15:20:35.91
Jan 17 15:20:35.912: INFO: scanned /root for discovery docs: <nil>
Jan 17 15:20:35.912: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-734 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Jan 17 15:20:36.973: INFO: stderr: ""
Jan 17 15:20:36.973: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/17/23 15:20:36.973
Jan 17 15:20:36.974: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-734 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 17 15:20:37.021: INFO: stderr: ""
Jan 17 15:20:37.021: INFO: stdout: "update-demo-nautilus-496b9 update-demo-nautilus-qq4sd "
Jan 17 15:20:37.021: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-734 get pods update-demo-nautilus-496b9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 17 15:20:37.067: INFO: stderr: ""
Jan 17 15:20:37.067: INFO: stdout: ""
Jan 17 15:20:37.067: INFO: update-demo-nautilus-496b9 is created but not running
Jan 17 15:20:42.070: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-734 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 17 15:20:42.119: INFO: stderr: ""
Jan 17 15:20:42.119: INFO: stdout: "update-demo-nautilus-496b9 update-demo-nautilus-qq4sd "
Jan 17 15:20:42.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-734 get pods update-demo-nautilus-496b9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 17 15:20:42.164: INFO: stderr: ""
Jan 17 15:20:42.164: INFO: stdout: "true"
Jan 17 15:20:42.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-734 get pods update-demo-nautilus-496b9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 17 15:20:42.209: INFO: stderr: ""
Jan 17 15:20:42.209: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan 17 15:20:42.209: INFO: validating pod update-demo-nautilus-496b9
Jan 17 15:20:42.214: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 17 15:20:42.214: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 17 15:20:42.214: INFO: update-demo-nautilus-496b9 is verified up and running
Jan 17 15:20:42.214: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-734 get pods update-demo-nautilus-qq4sd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 17 15:20:42.259: INFO: stderr: ""
Jan 17 15:20:42.259: INFO: stdout: "true"
Jan 17 15:20:42.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-734 get pods update-demo-nautilus-qq4sd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 17 15:20:42.309: INFO: stderr: ""
Jan 17 15:20:42.309: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan 17 15:20:42.309: INFO: validating pod update-demo-nautilus-qq4sd
Jan 17 15:20:42.313: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 17 15:20:42.313: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 17 15:20:42.313: INFO: update-demo-nautilus-qq4sd is verified up and running
STEP: using delete to clean up resources 01/17/23 15:20:42.313
Jan 17 15:20:42.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-734 delete --grace-period=0 --force -f -'
Jan 17 15:20:42.362: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 17 15:20:42.362: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan 17 15:20:42.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-734 get rc,svc -l name=update-demo --no-headers'
Jan 17 15:20:42.421: INFO: stderr: "No resources found in kubectl-734 namespace.\n"
Jan 17 15:20:42.421: INFO: stdout: ""
Jan 17 15:20:42.421: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-734 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 17 15:20:42.468: INFO: stderr: ""
Jan 17 15:20:42.468: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 17 15:20:42.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-734" for this suite. 01/17/23 15:20:42.472
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","completed":163,"skipped":2984,"failed":0}
------------------------------
• [SLOW TEST] [19.180 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:322
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:350

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:20:23.298
    Jan 17 15:20:23.298: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename kubectl 01/17/23 15:20:23.299
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:20:23.315
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:20:23.317
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:324
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:350
    STEP: creating a replication controller 01/17/23 15:20:23.333
    Jan 17 15:20:23.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-734 create -f -'
    Jan 17 15:20:24.288: INFO: stderr: ""
    Jan 17 15:20:24.288: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/17/23 15:20:24.288
    Jan 17 15:20:24.288: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-734 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 17 15:20:24.346: INFO: stderr: ""
    Jan 17 15:20:24.346: INFO: stdout: "update-demo-nautilus-4sn7h update-demo-nautilus-qq4sd "
    Jan 17 15:20:24.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-734 get pods update-demo-nautilus-4sn7h -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 17 15:20:24.402: INFO: stderr: ""
    Jan 17 15:20:24.402: INFO: stdout: ""
    Jan 17 15:20:24.402: INFO: update-demo-nautilus-4sn7h is created but not running
    Jan 17 15:20:29.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-734 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 17 15:20:29.451: INFO: stderr: ""
    Jan 17 15:20:29.451: INFO: stdout: "update-demo-nautilus-4sn7h update-demo-nautilus-qq4sd "
    Jan 17 15:20:29.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-734 get pods update-demo-nautilus-4sn7h -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 17 15:20:29.497: INFO: stderr: ""
    Jan 17 15:20:29.497: INFO: stdout: "true"
    Jan 17 15:20:29.497: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-734 get pods update-demo-nautilus-4sn7h -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 17 15:20:29.541: INFO: stderr: ""
    Jan 17 15:20:29.541: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan 17 15:20:29.541: INFO: validating pod update-demo-nautilus-4sn7h
    Jan 17 15:20:29.547: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 17 15:20:29.547: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 17 15:20:29.547: INFO: update-demo-nautilus-4sn7h is verified up and running
    Jan 17 15:20:29.547: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-734 get pods update-demo-nautilus-qq4sd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 17 15:20:29.591: INFO: stderr: ""
    Jan 17 15:20:29.591: INFO: stdout: "true"
    Jan 17 15:20:29.591: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-734 get pods update-demo-nautilus-qq4sd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 17 15:20:29.636: INFO: stderr: ""
    Jan 17 15:20:29.636: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan 17 15:20:29.636: INFO: validating pod update-demo-nautilus-qq4sd
    Jan 17 15:20:29.642: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 17 15:20:29.642: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 17 15:20:29.642: INFO: update-demo-nautilus-qq4sd is verified up and running
    STEP: scaling down the replication controller 01/17/23 15:20:29.642
    Jan 17 15:20:29.644: INFO: scanned /root for discovery docs: <nil>
    Jan 17 15:20:29.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-734 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    Jan 17 15:20:30.709: INFO: stderr: ""
    Jan 17 15:20:30.709: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/17/23 15:20:30.709
    Jan 17 15:20:30.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-734 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 17 15:20:30.762: INFO: stderr: ""
    Jan 17 15:20:30.762: INFO: stdout: "update-demo-nautilus-4sn7h update-demo-nautilus-qq4sd "
    STEP: Replicas for name=update-demo: expected=1 actual=2 01/17/23 15:20:30.762
    Jan 17 15:20:35.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-734 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 17 15:20:35.812: INFO: stderr: ""
    Jan 17 15:20:35.812: INFO: stdout: "update-demo-nautilus-qq4sd "
    Jan 17 15:20:35.812: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-734 get pods update-demo-nautilus-qq4sd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 17 15:20:35.859: INFO: stderr: ""
    Jan 17 15:20:35.859: INFO: stdout: "true"
    Jan 17 15:20:35.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-734 get pods update-demo-nautilus-qq4sd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 17 15:20:35.906: INFO: stderr: ""
    Jan 17 15:20:35.906: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan 17 15:20:35.906: INFO: validating pod update-demo-nautilus-qq4sd
    Jan 17 15:20:35.910: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 17 15:20:35.910: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 17 15:20:35.910: INFO: update-demo-nautilus-qq4sd is verified up and running
    STEP: scaling up the replication controller 01/17/23 15:20:35.91
    Jan 17 15:20:35.912: INFO: scanned /root for discovery docs: <nil>
    Jan 17 15:20:35.912: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-734 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    Jan 17 15:20:36.973: INFO: stderr: ""
    Jan 17 15:20:36.973: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/17/23 15:20:36.973
    Jan 17 15:20:36.974: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-734 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 17 15:20:37.021: INFO: stderr: ""
    Jan 17 15:20:37.021: INFO: stdout: "update-demo-nautilus-496b9 update-demo-nautilus-qq4sd "
    Jan 17 15:20:37.021: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-734 get pods update-demo-nautilus-496b9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 17 15:20:37.067: INFO: stderr: ""
    Jan 17 15:20:37.067: INFO: stdout: ""
    Jan 17 15:20:37.067: INFO: update-demo-nautilus-496b9 is created but not running
    Jan 17 15:20:42.070: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-734 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 17 15:20:42.119: INFO: stderr: ""
    Jan 17 15:20:42.119: INFO: stdout: "update-demo-nautilus-496b9 update-demo-nautilus-qq4sd "
    Jan 17 15:20:42.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-734 get pods update-demo-nautilus-496b9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 17 15:20:42.164: INFO: stderr: ""
    Jan 17 15:20:42.164: INFO: stdout: "true"
    Jan 17 15:20:42.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-734 get pods update-demo-nautilus-496b9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 17 15:20:42.209: INFO: stderr: ""
    Jan 17 15:20:42.209: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan 17 15:20:42.209: INFO: validating pod update-demo-nautilus-496b9
    Jan 17 15:20:42.214: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 17 15:20:42.214: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 17 15:20:42.214: INFO: update-demo-nautilus-496b9 is verified up and running
    Jan 17 15:20:42.214: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-734 get pods update-demo-nautilus-qq4sd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 17 15:20:42.259: INFO: stderr: ""
    Jan 17 15:20:42.259: INFO: stdout: "true"
    Jan 17 15:20:42.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-734 get pods update-demo-nautilus-qq4sd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 17 15:20:42.309: INFO: stderr: ""
    Jan 17 15:20:42.309: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan 17 15:20:42.309: INFO: validating pod update-demo-nautilus-qq4sd
    Jan 17 15:20:42.313: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 17 15:20:42.313: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 17 15:20:42.313: INFO: update-demo-nautilus-qq4sd is verified up and running
    STEP: using delete to clean up resources 01/17/23 15:20:42.313
    Jan 17 15:20:42.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-734 delete --grace-period=0 --force -f -'
    Jan 17 15:20:42.362: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 17 15:20:42.362: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Jan 17 15:20:42.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-734 get rc,svc -l name=update-demo --no-headers'
    Jan 17 15:20:42.421: INFO: stderr: "No resources found in kubectl-734 namespace.\n"
    Jan 17 15:20:42.421: INFO: stdout: ""
    Jan 17 15:20:42.421: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-734 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jan 17 15:20:42.468: INFO: stderr: ""
    Jan 17 15:20:42.468: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 17 15:20:42.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-734" for this suite. 01/17/23 15:20:42.472
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:20:42.479
Jan 17 15:20:42.479: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename gc 01/17/23 15:20:42.479
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:20:42.498
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:20:42.5
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 01/17/23 15:20:42.502
W0117 15:20:42.511602      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: delete the rc 01/17/23 15:20:47.515
STEP: wait for all pods to be garbage collected 01/17/23 15:20:47.521
STEP: Gathering metrics 01/17/23 15:20:52.528
W0117 15:20:52.530797      22 metrics_grabber.go:110] Can't find any pods in namespace kube-system to grab metrics from
W0117 15:20:52.530815      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan 17 15:20:52.530: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan 17 15:20:52.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7418" for this suite. 01/17/23 15:20:52.534
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","completed":164,"skipped":3010,"failed":0}
------------------------------
• [SLOW TEST] [10.062 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:20:42.479
    Jan 17 15:20:42.479: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename gc 01/17/23 15:20:42.479
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:20:42.498
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:20:42.5
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 01/17/23 15:20:42.502
    W0117 15:20:42.511602      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: delete the rc 01/17/23 15:20:47.515
    STEP: wait for all pods to be garbage collected 01/17/23 15:20:47.521
    STEP: Gathering metrics 01/17/23 15:20:52.528
    W0117 15:20:52.530797      22 metrics_grabber.go:110] Can't find any pods in namespace kube-system to grab metrics from
    W0117 15:20:52.530815      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jan 17 15:20:52.530: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan 17 15:20:52.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-7418" for this suite. 01/17/23 15:20:52.534
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:20:52.541
Jan 17 15:20:52.541: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename configmap 01/17/23 15:20:52.541
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:20:52.569
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:20:52.571
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56
STEP: Creating configMap with name configmap-test-volume-3cb297f2-d25f-4dc3-94c2-3259d1b3f0ff 01/17/23 15:20:52.574
STEP: Creating a pod to test consume configMaps 01/17/23 15:20:52.583
Jan 17 15:20:52.599: INFO: Waiting up to 5m0s for pod "pod-configmaps-a572f7d2-a379-4c2c-a1c0-4de6102a7767" in namespace "configmap-749" to be "Succeeded or Failed"
Jan 17 15:20:52.602: INFO: Pod "pod-configmaps-a572f7d2-a379-4c2c-a1c0-4de6102a7767": Phase="Pending", Reason="", readiness=false. Elapsed: 3.221008ms
Jan 17 15:20:54.606: INFO: Pod "pod-configmaps-a572f7d2-a379-4c2c-a1c0-4de6102a7767": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007641264s
Jan 17 15:20:56.605: INFO: Pod "pod-configmaps-a572f7d2-a379-4c2c-a1c0-4de6102a7767": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006276079s
STEP: Saw pod success 01/17/23 15:20:56.605
Jan 17 15:20:56.605: INFO: Pod "pod-configmaps-a572f7d2-a379-4c2c-a1c0-4de6102a7767" satisfied condition "Succeeded or Failed"
Jan 17 15:20:56.608: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-configmaps-a572f7d2-a379-4c2c-a1c0-4de6102a7767 container agnhost-container: <nil>
STEP: delete the pod 01/17/23 15:20:56.616
Jan 17 15:20:56.629: INFO: Waiting for pod pod-configmaps-a572f7d2-a379-4c2c-a1c0-4de6102a7767 to disappear
Jan 17 15:20:56.631: INFO: Pod pod-configmaps-a572f7d2-a379-4c2c-a1c0-4de6102a7767 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 17 15:20:56.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-749" for this suite. 01/17/23 15:20:56.64
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":165,"skipped":3022,"failed":0}
------------------------------
• [4.104 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:20:52.541
    Jan 17 15:20:52.541: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename configmap 01/17/23 15:20:52.541
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:20:52.569
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:20:52.571
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:56
    STEP: Creating configMap with name configmap-test-volume-3cb297f2-d25f-4dc3-94c2-3259d1b3f0ff 01/17/23 15:20:52.574
    STEP: Creating a pod to test consume configMaps 01/17/23 15:20:52.583
    Jan 17 15:20:52.599: INFO: Waiting up to 5m0s for pod "pod-configmaps-a572f7d2-a379-4c2c-a1c0-4de6102a7767" in namespace "configmap-749" to be "Succeeded or Failed"
    Jan 17 15:20:52.602: INFO: Pod "pod-configmaps-a572f7d2-a379-4c2c-a1c0-4de6102a7767": Phase="Pending", Reason="", readiness=false. Elapsed: 3.221008ms
    Jan 17 15:20:54.606: INFO: Pod "pod-configmaps-a572f7d2-a379-4c2c-a1c0-4de6102a7767": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007641264s
    Jan 17 15:20:56.605: INFO: Pod "pod-configmaps-a572f7d2-a379-4c2c-a1c0-4de6102a7767": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006276079s
    STEP: Saw pod success 01/17/23 15:20:56.605
    Jan 17 15:20:56.605: INFO: Pod "pod-configmaps-a572f7d2-a379-4c2c-a1c0-4de6102a7767" satisfied condition "Succeeded or Failed"
    Jan 17 15:20:56.608: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-configmaps-a572f7d2-a379-4c2c-a1c0-4de6102a7767 container agnhost-container: <nil>
    STEP: delete the pod 01/17/23 15:20:56.616
    Jan 17 15:20:56.629: INFO: Waiting for pod pod-configmaps-a572f7d2-a379-4c2c-a1c0-4de6102a7767 to disappear
    Jan 17 15:20:56.631: INFO: Pod pod-configmaps-a572f7d2-a379-4c2c-a1c0-4de6102a7767 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 17 15:20:56.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-749" for this suite. 01/17/23 15:20:56.64
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:20:56.647
Jan 17 15:20:56.647: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename events 01/17/23 15:20:56.647
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:20:56.667
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:20:56.669
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 01/17/23 15:20:56.671
STEP: listing events in all namespaces 01/17/23 15:20:56.696
STEP: listing events in test namespace 01/17/23 15:20:56.781
STEP: listing events with field selection filtering on source 01/17/23 15:20:56.785
STEP: listing events with field selection filtering on reportingController 01/17/23 15:20:56.789
STEP: getting the test event 01/17/23 15:20:56.791
STEP: patching the test event 01/17/23 15:20:56.793
STEP: getting the test event 01/17/23 15:20:56.802
STEP: updating the test event 01/17/23 15:20:56.805
STEP: getting the test event 01/17/23 15:20:56.81
STEP: deleting the test event 01/17/23 15:20:56.814
STEP: listing events in all namespaces 01/17/23 15:20:56.82
STEP: listing events in test namespace 01/17/23 15:20:56.882
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
Jan 17 15:20:56.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-7972" for this suite. 01/17/23 15:20:56.889
{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","completed":166,"skipped":3088,"failed":0}
------------------------------
• [0.249 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:20:56.647
    Jan 17 15:20:56.647: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename events 01/17/23 15:20:56.647
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:20:56.667
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:20:56.669
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 01/17/23 15:20:56.671
    STEP: listing events in all namespaces 01/17/23 15:20:56.696
    STEP: listing events in test namespace 01/17/23 15:20:56.781
    STEP: listing events with field selection filtering on source 01/17/23 15:20:56.785
    STEP: listing events with field selection filtering on reportingController 01/17/23 15:20:56.789
    STEP: getting the test event 01/17/23 15:20:56.791
    STEP: patching the test event 01/17/23 15:20:56.793
    STEP: getting the test event 01/17/23 15:20:56.802
    STEP: updating the test event 01/17/23 15:20:56.805
    STEP: getting the test event 01/17/23 15:20:56.81
    STEP: deleting the test event 01/17/23 15:20:56.814
    STEP: listing events in all namespaces 01/17/23 15:20:56.82
    STEP: listing events in test namespace 01/17/23 15:20:56.882
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:187
    Jan 17 15:20:56.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-7972" for this suite. 01/17/23 15:20:56.889
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:20:56.896
Jan 17 15:20:56.896: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename downward-api 01/17/23 15:20:56.897
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:20:56.916
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:20:56.918
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83
STEP: Creating a pod to test downward API volume plugin 01/17/23 15:20:56.919
Jan 17 15:20:56.947: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fc2c3548-a407-4ec3-be96-f5561c9e1afb" in namespace "downward-api-2234" to be "Succeeded or Failed"
Jan 17 15:20:56.952: INFO: Pod "downwardapi-volume-fc2c3548-a407-4ec3-be96-f5561c9e1afb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.908657ms
Jan 17 15:20:58.956: INFO: Pod "downwardapi-volume-fc2c3548-a407-4ec3-be96-f5561c9e1afb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008606387s
Jan 17 15:21:00.957: INFO: Pod "downwardapi-volume-fc2c3548-a407-4ec3-be96-f5561c9e1afb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009518345s
STEP: Saw pod success 01/17/23 15:21:00.957
Jan 17 15:21:00.957: INFO: Pod "downwardapi-volume-fc2c3548-a407-4ec3-be96-f5561c9e1afb" satisfied condition "Succeeded or Failed"
Jan 17 15:21:00.959: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod downwardapi-volume-fc2c3548-a407-4ec3-be96-f5561c9e1afb container client-container: <nil>
STEP: delete the pod 01/17/23 15:21:00.965
Jan 17 15:21:00.977: INFO: Waiting for pod downwardapi-volume-fc2c3548-a407-4ec3-be96-f5561c9e1afb to disappear
Jan 17 15:21:00.979: INFO: Pod downwardapi-volume-fc2c3548-a407-4ec3-be96-f5561c9e1afb no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 17 15:21:00.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2234" for this suite. 01/17/23 15:21:00.983
{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","completed":167,"skipped":3110,"failed":0}
------------------------------
• [4.092 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:20:56.896
    Jan 17 15:20:56.896: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename downward-api 01/17/23 15:20:56.897
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:20:56.916
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:20:56.918
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:83
    STEP: Creating a pod to test downward API volume plugin 01/17/23 15:20:56.919
    Jan 17 15:20:56.947: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fc2c3548-a407-4ec3-be96-f5561c9e1afb" in namespace "downward-api-2234" to be "Succeeded or Failed"
    Jan 17 15:20:56.952: INFO: Pod "downwardapi-volume-fc2c3548-a407-4ec3-be96-f5561c9e1afb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.908657ms
    Jan 17 15:20:58.956: INFO: Pod "downwardapi-volume-fc2c3548-a407-4ec3-be96-f5561c9e1afb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008606387s
    Jan 17 15:21:00.957: INFO: Pod "downwardapi-volume-fc2c3548-a407-4ec3-be96-f5561c9e1afb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009518345s
    STEP: Saw pod success 01/17/23 15:21:00.957
    Jan 17 15:21:00.957: INFO: Pod "downwardapi-volume-fc2c3548-a407-4ec3-be96-f5561c9e1afb" satisfied condition "Succeeded or Failed"
    Jan 17 15:21:00.959: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod downwardapi-volume-fc2c3548-a407-4ec3-be96-f5561c9e1afb container client-container: <nil>
    STEP: delete the pod 01/17/23 15:21:00.965
    Jan 17 15:21:00.977: INFO: Waiting for pod downwardapi-volume-fc2c3548-a407-4ec3-be96-f5561c9e1afb to disappear
    Jan 17 15:21:00.979: INFO: Pod downwardapi-volume-fc2c3548-a407-4ec3-be96-f5561c9e1afb no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 17 15:21:00.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-2234" for this suite. 01/17/23 15:21:00.983
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:21:00.988
Jan 17 15:21:00.988: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename dns 01/17/23 15:21:00.989
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:21:01.01
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:21:01.012
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1164.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-1164.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 01/17/23 15:21:01.014
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1164.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-1164.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 01/17/23 15:21:01.014
STEP: creating a pod to probe /etc/hosts 01/17/23 15:21:01.015
STEP: submitting the pod to kubernetes 01/17/23 15:21:01.015
Jan 17 15:21:01.060: INFO: Waiting up to 15m0s for pod "dns-test-e31b12e4-73d7-4eb6-aa79-2e8487b8132e" in namespace "dns-1164" to be "running"
Jan 17 15:21:01.071: INFO: Pod "dns-test-e31b12e4-73d7-4eb6-aa79-2e8487b8132e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.921434ms
Jan 17 15:21:03.075: INFO: Pod "dns-test-e31b12e4-73d7-4eb6-aa79-2e8487b8132e": Phase="Running", Reason="", readiness=true. Elapsed: 2.014872648s
Jan 17 15:21:03.075: INFO: Pod "dns-test-e31b12e4-73d7-4eb6-aa79-2e8487b8132e" satisfied condition "running"
STEP: retrieving the pod 01/17/23 15:21:03.075
STEP: looking for the results for each expected name from probers 01/17/23 15:21:03.078
Jan 17 15:21:03.137: INFO: DNS probes using dns-1164/dns-test-e31b12e4-73d7-4eb6-aa79-2e8487b8132e succeeded

STEP: deleting the pod 01/17/23 15:21:03.137
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan 17 15:21:03.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1164" for this suite. 01/17/23 15:21:03.156
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]","completed":168,"skipped":3110,"failed":0}
------------------------------
• [2.176 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:21:00.988
    Jan 17 15:21:00.988: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename dns 01/17/23 15:21:00.989
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:21:01.01
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:21:01.012
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1164.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-1164.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     01/17/23 15:21:01.014
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1164.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-1164.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     01/17/23 15:21:01.014
    STEP: creating a pod to probe /etc/hosts 01/17/23 15:21:01.015
    STEP: submitting the pod to kubernetes 01/17/23 15:21:01.015
    Jan 17 15:21:01.060: INFO: Waiting up to 15m0s for pod "dns-test-e31b12e4-73d7-4eb6-aa79-2e8487b8132e" in namespace "dns-1164" to be "running"
    Jan 17 15:21:01.071: INFO: Pod "dns-test-e31b12e4-73d7-4eb6-aa79-2e8487b8132e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.921434ms
    Jan 17 15:21:03.075: INFO: Pod "dns-test-e31b12e4-73d7-4eb6-aa79-2e8487b8132e": Phase="Running", Reason="", readiness=true. Elapsed: 2.014872648s
    Jan 17 15:21:03.075: INFO: Pod "dns-test-e31b12e4-73d7-4eb6-aa79-2e8487b8132e" satisfied condition "running"
    STEP: retrieving the pod 01/17/23 15:21:03.075
    STEP: looking for the results for each expected name from probers 01/17/23 15:21:03.078
    Jan 17 15:21:03.137: INFO: DNS probes using dns-1164/dns-test-e31b12e4-73d7-4eb6-aa79-2e8487b8132e succeeded

    STEP: deleting the pod 01/17/23 15:21:03.137
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan 17 15:21:03.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-1164" for this suite. 01/17/23 15:21:03.156
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:21:03.166
Jan 17 15:21:03.166: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename security-context 01/17/23 15:21:03.167
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:21:03.197
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:21:03.2
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/17/23 15:21:03.202
Jan 17 15:21:03.228: INFO: Waiting up to 5m0s for pod "security-context-7f0aeb75-ff28-4ba4-94e3-bded3a525b8b" in namespace "security-context-9617" to be "Succeeded or Failed"
Jan 17 15:21:03.235: INFO: Pod "security-context-7f0aeb75-ff28-4ba4-94e3-bded3a525b8b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.541225ms
Jan 17 15:21:05.239: INFO: Pod "security-context-7f0aeb75-ff28-4ba4-94e3-bded3a525b8b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01119946s
Jan 17 15:21:07.239: INFO: Pod "security-context-7f0aeb75-ff28-4ba4-94e3-bded3a525b8b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010802665s
STEP: Saw pod success 01/17/23 15:21:07.239
Jan 17 15:21:07.239: INFO: Pod "security-context-7f0aeb75-ff28-4ba4-94e3-bded3a525b8b" satisfied condition "Succeeded or Failed"
Jan 17 15:21:07.242: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod security-context-7f0aeb75-ff28-4ba4-94e3-bded3a525b8b container test-container: <nil>
STEP: delete the pod 01/17/23 15:21:07.248
Jan 17 15:21:07.262: INFO: Waiting for pod security-context-7f0aeb75-ff28-4ba4-94e3-bded3a525b8b to disappear
Jan 17 15:21:07.264: INFO: Pod security-context-7f0aeb75-ff28-4ba4-94e3-bded3a525b8b no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jan 17 15:21:07.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-9617" for this suite. 01/17/23 15:21:07.268
{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","completed":169,"skipped":3124,"failed":0}
------------------------------
• [4.108 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:21:03.166
    Jan 17 15:21:03.166: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename security-context 01/17/23 15:21:03.167
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:21:03.197
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:21:03.2
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:132
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/17/23 15:21:03.202
    Jan 17 15:21:03.228: INFO: Waiting up to 5m0s for pod "security-context-7f0aeb75-ff28-4ba4-94e3-bded3a525b8b" in namespace "security-context-9617" to be "Succeeded or Failed"
    Jan 17 15:21:03.235: INFO: Pod "security-context-7f0aeb75-ff28-4ba4-94e3-bded3a525b8b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.541225ms
    Jan 17 15:21:05.239: INFO: Pod "security-context-7f0aeb75-ff28-4ba4-94e3-bded3a525b8b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01119946s
    Jan 17 15:21:07.239: INFO: Pod "security-context-7f0aeb75-ff28-4ba4-94e3-bded3a525b8b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010802665s
    STEP: Saw pod success 01/17/23 15:21:07.239
    Jan 17 15:21:07.239: INFO: Pod "security-context-7f0aeb75-ff28-4ba4-94e3-bded3a525b8b" satisfied condition "Succeeded or Failed"
    Jan 17 15:21:07.242: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod security-context-7f0aeb75-ff28-4ba4-94e3-bded3a525b8b container test-container: <nil>
    STEP: delete the pod 01/17/23 15:21:07.248
    Jan 17 15:21:07.262: INFO: Waiting for pod security-context-7f0aeb75-ff28-4ba4-94e3-bded3a525b8b to disappear
    Jan 17 15:21:07.264: INFO: Pod security-context-7f0aeb75-ff28-4ba4-94e3-bded3a525b8b no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jan 17 15:21:07.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-9617" for this suite. 01/17/23 15:21:07.268
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:21:07.275
Jan 17 15:21:07.275: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename sched-preemption 01/17/23 15:21:07.276
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:21:07.298
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:21:07.3
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jan 17 15:21:07.320: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 17 15:22:07.447: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218
STEP: Create pods that use 4/5 of node resources. 01/17/23 15:22:07.452
Jan 17 15:22:07.506: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jan 17 15:22:07.525: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jan 17 15:22:07.549: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jan 17 15:22:07.571: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Jan 17 15:22:07.598: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Jan 17 15:22:07.623: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 01/17/23 15:22:07.623
Jan 17 15:22:07.623: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-6361" to be "running"
Jan 17 15:22:07.626: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.895157ms
Jan 17 15:22:09.630: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006846786s
Jan 17 15:22:11.631: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007972306s
Jan 17 15:22:13.630: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007099973s
Jan 17 15:22:15.631: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.00734531s
Jan 17 15:22:15.631: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Jan 17 15:22:15.631: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-6361" to be "running"
Jan 17 15:22:15.633: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.660738ms
Jan 17 15:22:15.633: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Jan 17 15:22:15.633: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-6361" to be "running"
Jan 17 15:22:15.636: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.378369ms
Jan 17 15:22:15.636: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Jan 17 15:22:15.636: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-6361" to be "running"
Jan 17 15:22:15.638: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.461399ms
Jan 17 15:22:15.638: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Jan 17 15:22:15.638: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-6361" to be "running"
Jan 17 15:22:15.641: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.862345ms
Jan 17 15:22:17.646: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.007368133s
Jan 17 15:22:17.646: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Jan 17 15:22:17.646: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-6361" to be "running"
Jan 17 15:22:17.648: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.548924ms
Jan 17 15:22:17.648: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 01/17/23 15:22:17.648
Jan 17 15:22:17.658: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Jan 17 15:22:17.661: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.9675ms
Jan 17 15:22:19.665: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006916686s
Jan 17 15:22:21.666: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.008065008s
Jan 17 15:22:21.666: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Jan 17 15:22:21.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-6361" for this suite. 01/17/23 15:22:21.707
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","completed":170,"skipped":3139,"failed":0}
------------------------------
• [SLOW TEST] [74.493 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:21:07.275
    Jan 17 15:21:07.275: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename sched-preemption 01/17/23 15:21:07.276
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:21:07.298
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:21:07.3
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Jan 17 15:21:07.320: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 17 15:22:07.447: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:218
    STEP: Create pods that use 4/5 of node resources. 01/17/23 15:22:07.452
    Jan 17 15:22:07.506: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Jan 17 15:22:07.525: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Jan 17 15:22:07.549: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Jan 17 15:22:07.571: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Jan 17 15:22:07.598: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Jan 17 15:22:07.623: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 01/17/23 15:22:07.623
    Jan 17 15:22:07.623: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-6361" to be "running"
    Jan 17 15:22:07.626: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.895157ms
    Jan 17 15:22:09.630: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006846786s
    Jan 17 15:22:11.631: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007972306s
    Jan 17 15:22:13.630: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007099973s
    Jan 17 15:22:15.631: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.00734531s
    Jan 17 15:22:15.631: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Jan 17 15:22:15.631: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-6361" to be "running"
    Jan 17 15:22:15.633: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.660738ms
    Jan 17 15:22:15.633: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Jan 17 15:22:15.633: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-6361" to be "running"
    Jan 17 15:22:15.636: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.378369ms
    Jan 17 15:22:15.636: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Jan 17 15:22:15.636: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-6361" to be "running"
    Jan 17 15:22:15.638: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.461399ms
    Jan 17 15:22:15.638: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Jan 17 15:22:15.638: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-6361" to be "running"
    Jan 17 15:22:15.641: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.862345ms
    Jan 17 15:22:17.646: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.007368133s
    Jan 17 15:22:17.646: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Jan 17 15:22:17.646: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-6361" to be "running"
    Jan 17 15:22:17.648: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.548924ms
    Jan 17 15:22:17.648: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 01/17/23 15:22:17.648
    Jan 17 15:22:17.658: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Jan 17 15:22:17.661: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.9675ms
    Jan 17 15:22:19.665: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006916686s
    Jan 17 15:22:21.666: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.008065008s
    Jan 17 15:22:21.666: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Jan 17 15:22:21.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-6361" for this suite. 01/17/23 15:22:21.707
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:22:21.768
Jan 17 15:22:21.768: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename taint-single-pod 01/17/23 15:22:21.769
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:22:21.801
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:22:21.803
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:166
Jan 17 15:22:21.810: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 17 15:23:21.920: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289
Jan 17 15:23:21.924: INFO: Starting informer...
STEP: Starting pod... 01/17/23 15:23:21.924
Jan 17 15:23:22.142: INFO: Pod is running on ip-10-0-151-22.ec2.internal. Tainting Node
STEP: Trying to apply a taint on the Node 01/17/23 15:23:22.142
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/17/23 15:23:22.154
STEP: Waiting short time to make sure Pod is queued for deletion 01/17/23 15:23:22.158
Jan 17 15:23:22.159: INFO: Pod wasn't evicted. Proceeding
Jan 17 15:23:22.159: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/17/23 15:23:22.175
STEP: Waiting some time to make sure that toleration time passed. 01/17/23 15:23:22.19
Jan 17 15:24:37.193: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:187
Jan 17 15:24:37.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-5991" for this suite. 01/17/23 15:24:37.199
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","completed":171,"skipped":3144,"failed":0}
------------------------------
• [SLOW TEST] [135.440 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:22:21.768
    Jan 17 15:22:21.768: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename taint-single-pod 01/17/23 15:22:21.769
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:22:21.801
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:22:21.803
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:166
    Jan 17 15:22:21.810: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 17 15:23:21.920: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:289
    Jan 17 15:23:21.924: INFO: Starting informer...
    STEP: Starting pod... 01/17/23 15:23:21.924
    Jan 17 15:23:22.142: INFO: Pod is running on ip-10-0-151-22.ec2.internal. Tainting Node
    STEP: Trying to apply a taint on the Node 01/17/23 15:23:22.142
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/17/23 15:23:22.154
    STEP: Waiting short time to make sure Pod is queued for deletion 01/17/23 15:23:22.158
    Jan 17 15:23:22.159: INFO: Pod wasn't evicted. Proceeding
    Jan 17 15:23:22.159: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/17/23 15:23:22.175
    STEP: Waiting some time to make sure that toleration time passed. 01/17/23 15:23:22.19
    Jan 17 15:24:37.193: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/framework.go:187
    Jan 17 15:24:37.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "taint-single-pod-5991" for this suite. 01/17/23 15:24:37.199
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:24:37.209
Jan 17 15:24:37.209: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename subpath 01/17/23 15:24:37.21
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:24:37.235
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:24:37.237
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/17/23 15:24:37.239
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-ps29 01/17/23 15:24:37.254
STEP: Creating a pod to test atomic-volume-subpath 01/17/23 15:24:37.254
Jan 17 15:24:37.287: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-ps29" in namespace "subpath-4105" to be "Succeeded or Failed"
Jan 17 15:24:37.294: INFO: Pod "pod-subpath-test-projected-ps29": Phase="Pending", Reason="", readiness=false. Elapsed: 6.751516ms
Jan 17 15:24:39.298: INFO: Pod "pod-subpath-test-projected-ps29": Phase="Running", Reason="", readiness=true. Elapsed: 2.011259338s
Jan 17 15:24:41.298: INFO: Pod "pod-subpath-test-projected-ps29": Phase="Running", Reason="", readiness=true. Elapsed: 4.011641365s
Jan 17 15:24:43.298: INFO: Pod "pod-subpath-test-projected-ps29": Phase="Running", Reason="", readiness=true. Elapsed: 6.010694682s
Jan 17 15:24:45.298: INFO: Pod "pod-subpath-test-projected-ps29": Phase="Running", Reason="", readiness=true. Elapsed: 8.011254735s
Jan 17 15:24:47.298: INFO: Pod "pod-subpath-test-projected-ps29": Phase="Running", Reason="", readiness=true. Elapsed: 10.010971497s
Jan 17 15:24:49.298: INFO: Pod "pod-subpath-test-projected-ps29": Phase="Running", Reason="", readiness=true. Elapsed: 12.011101934s
Jan 17 15:24:51.299: INFO: Pod "pod-subpath-test-projected-ps29": Phase="Running", Reason="", readiness=true. Elapsed: 14.011885956s
Jan 17 15:24:53.297: INFO: Pod "pod-subpath-test-projected-ps29": Phase="Running", Reason="", readiness=true. Elapsed: 16.01029569s
Jan 17 15:24:55.298: INFO: Pod "pod-subpath-test-projected-ps29": Phase="Running", Reason="", readiness=true. Elapsed: 18.011264797s
Jan 17 15:24:57.297: INFO: Pod "pod-subpath-test-projected-ps29": Phase="Running", Reason="", readiness=true. Elapsed: 20.01010345s
Jan 17 15:24:59.298: INFO: Pod "pod-subpath-test-projected-ps29": Phase="Running", Reason="", readiness=false. Elapsed: 22.010993963s
Jan 17 15:25:01.298: INFO: Pod "pod-subpath-test-projected-ps29": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.011107145s
STEP: Saw pod success 01/17/23 15:25:01.298
Jan 17 15:25:01.298: INFO: Pod "pod-subpath-test-projected-ps29" satisfied condition "Succeeded or Failed"
Jan 17 15:25:01.302: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-subpath-test-projected-ps29 container test-container-subpath-projected-ps29: <nil>
STEP: delete the pod 01/17/23 15:25:01.312
Jan 17 15:25:01.334: INFO: Waiting for pod pod-subpath-test-projected-ps29 to disappear
Jan 17 15:25:01.338: INFO: Pod pod-subpath-test-projected-ps29 no longer exists
STEP: Deleting pod pod-subpath-test-projected-ps29 01/17/23 15:25:01.338
Jan 17 15:25:01.338: INFO: Deleting pod "pod-subpath-test-projected-ps29" in namespace "subpath-4105"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Jan 17 15:25:01.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4105" for this suite. 01/17/23 15:25:01.344
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]","completed":172,"skipped":3183,"failed":0}
------------------------------
• [SLOW TEST] [24.142 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:24:37.209
    Jan 17 15:24:37.209: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename subpath 01/17/23 15:24:37.21
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:24:37.235
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:24:37.237
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/17/23 15:24:37.239
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-ps29 01/17/23 15:24:37.254
    STEP: Creating a pod to test atomic-volume-subpath 01/17/23 15:24:37.254
    Jan 17 15:24:37.287: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-ps29" in namespace "subpath-4105" to be "Succeeded or Failed"
    Jan 17 15:24:37.294: INFO: Pod "pod-subpath-test-projected-ps29": Phase="Pending", Reason="", readiness=false. Elapsed: 6.751516ms
    Jan 17 15:24:39.298: INFO: Pod "pod-subpath-test-projected-ps29": Phase="Running", Reason="", readiness=true. Elapsed: 2.011259338s
    Jan 17 15:24:41.298: INFO: Pod "pod-subpath-test-projected-ps29": Phase="Running", Reason="", readiness=true. Elapsed: 4.011641365s
    Jan 17 15:24:43.298: INFO: Pod "pod-subpath-test-projected-ps29": Phase="Running", Reason="", readiness=true. Elapsed: 6.010694682s
    Jan 17 15:24:45.298: INFO: Pod "pod-subpath-test-projected-ps29": Phase="Running", Reason="", readiness=true. Elapsed: 8.011254735s
    Jan 17 15:24:47.298: INFO: Pod "pod-subpath-test-projected-ps29": Phase="Running", Reason="", readiness=true. Elapsed: 10.010971497s
    Jan 17 15:24:49.298: INFO: Pod "pod-subpath-test-projected-ps29": Phase="Running", Reason="", readiness=true. Elapsed: 12.011101934s
    Jan 17 15:24:51.299: INFO: Pod "pod-subpath-test-projected-ps29": Phase="Running", Reason="", readiness=true. Elapsed: 14.011885956s
    Jan 17 15:24:53.297: INFO: Pod "pod-subpath-test-projected-ps29": Phase="Running", Reason="", readiness=true. Elapsed: 16.01029569s
    Jan 17 15:24:55.298: INFO: Pod "pod-subpath-test-projected-ps29": Phase="Running", Reason="", readiness=true. Elapsed: 18.011264797s
    Jan 17 15:24:57.297: INFO: Pod "pod-subpath-test-projected-ps29": Phase="Running", Reason="", readiness=true. Elapsed: 20.01010345s
    Jan 17 15:24:59.298: INFO: Pod "pod-subpath-test-projected-ps29": Phase="Running", Reason="", readiness=false. Elapsed: 22.010993963s
    Jan 17 15:25:01.298: INFO: Pod "pod-subpath-test-projected-ps29": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.011107145s
    STEP: Saw pod success 01/17/23 15:25:01.298
    Jan 17 15:25:01.298: INFO: Pod "pod-subpath-test-projected-ps29" satisfied condition "Succeeded or Failed"
    Jan 17 15:25:01.302: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-subpath-test-projected-ps29 container test-container-subpath-projected-ps29: <nil>
    STEP: delete the pod 01/17/23 15:25:01.312
    Jan 17 15:25:01.334: INFO: Waiting for pod pod-subpath-test-projected-ps29 to disappear
    Jan 17 15:25:01.338: INFO: Pod pod-subpath-test-projected-ps29 no longer exists
    STEP: Deleting pod pod-subpath-test-projected-ps29 01/17/23 15:25:01.338
    Jan 17 15:25:01.338: INFO: Deleting pod "pod-subpath-test-projected-ps29" in namespace "subpath-4105"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Jan 17 15:25:01.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-4105" for this suite. 01/17/23 15:25:01.344
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:25:01.352
Jan 17 15:25:01.352: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename gc 01/17/23 15:25:01.352
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:25:01.379
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:25:01.381
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 01/17/23 15:25:01.393
STEP: delete the rc 01/17/23 15:25:06.417
STEP: wait for the rc to be deleted 01/17/23 15:25:06.438
STEP: Gathering metrics 01/17/23 15:25:07.514
W0117 15:25:07.525210      22 metrics_grabber.go:110] Can't find any pods in namespace kube-system to grab metrics from
W0117 15:25:07.525228      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan 17 15:25:07.525: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan 17 15:25:07.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1094" for this suite. 01/17/23 15:25:07.537
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","completed":173,"skipped":3186,"failed":0}
------------------------------
• [SLOW TEST] [6.201 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:25:01.352
    Jan 17 15:25:01.352: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename gc 01/17/23 15:25:01.352
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:25:01.379
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:25:01.381
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 01/17/23 15:25:01.393
    STEP: delete the rc 01/17/23 15:25:06.417
    STEP: wait for the rc to be deleted 01/17/23 15:25:06.438
    STEP: Gathering metrics 01/17/23 15:25:07.514
    W0117 15:25:07.525210      22 metrics_grabber.go:110] Can't find any pods in namespace kube-system to grab metrics from
    W0117 15:25:07.525228      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jan 17 15:25:07.525: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan 17 15:25:07.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-1094" for this suite. 01/17/23 15:25:07.537
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:25:07.554
Jan 17 15:25:07.554: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename runtimeclass 01/17/23 15:25:07.554
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:25:07.576
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:25:07.58
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
Jan 17 15:25:07.617: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-1314 to be scheduled
Jan 17 15:25:07.627: INFO: 1 pods are not scheduled: [runtimeclass-1314/test-runtimeclass-runtimeclass-1314-preconfigured-handler-7wvp4(11a4b218-b28e-4084-9072-c6915014e89e)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Jan 17 15:25:09.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-1314" for this suite. 01/17/23 15:25:09.641
{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]","completed":174,"skipped":3214,"failed":0}
------------------------------
• [2.092 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:25:07.554
    Jan 17 15:25:07.554: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename runtimeclass 01/17/23 15:25:07.554
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:25:07.576
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:25:07.58
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    Jan 17 15:25:07.617: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-1314 to be scheduled
    Jan 17 15:25:07.627: INFO: 1 pods are not scheduled: [runtimeclass-1314/test-runtimeclass-runtimeclass-1314-preconfigured-handler-7wvp4(11a4b218-b28e-4084-9072-c6915014e89e)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Jan 17 15:25:09.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-1314" for this suite. 01/17/23 15:25:09.641
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:25:09.647
Jan 17 15:25:09.647: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename deployment 01/17/23 15:25:09.647
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:25:09.668
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:25:09.671
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
Jan 17 15:25:09.673: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
W0117 15:25:09.685364      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 17 15:25:09.689: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 17 15:25:14.693: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/17/23 15:25:14.693
Jan 17 15:25:14.693: INFO: Creating deployment "test-rolling-update-deployment"
Jan 17 15:25:14.698: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jan 17 15:25:14.704: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jan 17 15:25:16.712: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jan 17 15:25:16.714: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 17 15:25:16.722: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-5207  ed7c33b7-12d4-43da-b8e9-4b8c15c9a8bc 100300 1 2023-01-17 15:25:14 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-01-17 15:25:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 15:25:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00811e1b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-17 15:25:14 +0000 UTC,LastTransitionTime:2023-01-17 15:25:14 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-78f575d8ff" has successfully progressed.,LastUpdateTime:2023-01-17 15:25:15 +0000 UTC,LastTransitionTime:2023-01-17 15:25:14 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 17 15:25:16.724: INFO: New ReplicaSet "test-rolling-update-deployment-78f575d8ff" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-78f575d8ff  deployment-5207  960192e9-36fa-4bd1-bedb-653470bd648b 100291 1 2023-01-17 15:25:14 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment ed7c33b7-12d4-43da-b8e9-4b8c15c9a8bc 0xc008b03ad7 0xc008b03ad8}] [] [{kube-controller-manager Update apps/v1 2023-01-17 15:25:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ed7c33b7-12d4-43da-b8e9-4b8c15c9a8bc\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 15:25:15 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 78f575d8ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc008b03b88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 17 15:25:16.724: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jan 17 15:25:16.724: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-5207  4661c0e0-f4e1-4ba5-8fc2-516fc5865581 100299 2 2023-01-17 15:25:09 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment ed7c33b7-12d4-43da-b8e9-4b8c15c9a8bc 0xc008b039a7 0xc008b039a8}] [] [{e2e.test Update apps/v1 2023-01-17 15:25:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 15:25:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ed7c33b7-12d4-43da-b8e9-4b8c15c9a8bc\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-17 15:25:15 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc008b03a68 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 17 15:25:16.727: INFO: Pod "test-rolling-update-deployment-78f575d8ff-l4757" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-78f575d8ff-l4757 test-rolling-update-deployment-78f575d8ff- deployment-5207  7adad92e-2aa6-4ae5-87ad-b603b470f33f 100290 0 2023-01-17 15:25:14 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.131.0.202/23"],"mac_address":"0a:58:0a:83:00:ca","gateway_ips":["10.131.0.1"],"ip_address":"10.131.0.202/23","gateway_ip":"10.131.0.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.131.0.202"
    ],
    "mac": "0a:58:0a:83:00:ca",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.131.0.202"
    ],
    "mac": "0a:58:0a:83:00:ca",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-rolling-update-deployment-78f575d8ff 960192e9-36fa-4bd1-bedb-653470bd648b 0xc000902037 0xc000902038}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:25:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:25:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"960192e9-36fa-4bd1-bedb-653470bd648b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 15:25:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.0.202\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-01-17 15:25:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hds65,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hds65,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-151-22.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c51,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-d8mlv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:25:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:25:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:25:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:25:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.151.22,PodIP:10.131.0.202,StartTime:2023-01-17 15:25:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 15:25:15 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:37dd9ad84b34913b4bc57e00b0a28e8b00f18ca6a5ecec69461aa57402e5c787,ContainerID:cri-o://31a7c12d2751f275698d2c2b62f2371963adc3204fbc512285be482068f7698b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.131.0.202,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan 17 15:25:16.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5207" for this suite. 01/17/23 15:25:16.731
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","completed":175,"skipped":3236,"failed":0}
------------------------------
• [SLOW TEST] [7.089 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:25:09.647
    Jan 17 15:25:09.647: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename deployment 01/17/23 15:25:09.647
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:25:09.668
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:25:09.671
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    Jan 17 15:25:09.673: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    W0117 15:25:09.685364      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jan 17 15:25:09.689: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan 17 15:25:14.693: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/17/23 15:25:14.693
    Jan 17 15:25:14.693: INFO: Creating deployment "test-rolling-update-deployment"
    Jan 17 15:25:14.698: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    Jan 17 15:25:14.704: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    Jan 17 15:25:16.712: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    Jan 17 15:25:16.714: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 17 15:25:16.722: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-5207  ed7c33b7-12d4-43da-b8e9-4b8c15c9a8bc 100300 1 2023-01-17 15:25:14 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-01-17 15:25:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 15:25:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00811e1b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-17 15:25:14 +0000 UTC,LastTransitionTime:2023-01-17 15:25:14 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-78f575d8ff" has successfully progressed.,LastUpdateTime:2023-01-17 15:25:15 +0000 UTC,LastTransitionTime:2023-01-17 15:25:14 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan 17 15:25:16.724: INFO: New ReplicaSet "test-rolling-update-deployment-78f575d8ff" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-78f575d8ff  deployment-5207  960192e9-36fa-4bd1-bedb-653470bd648b 100291 1 2023-01-17 15:25:14 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment ed7c33b7-12d4-43da-b8e9-4b8c15c9a8bc 0xc008b03ad7 0xc008b03ad8}] [] [{kube-controller-manager Update apps/v1 2023-01-17 15:25:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ed7c33b7-12d4-43da-b8e9-4b8c15c9a8bc\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 15:25:15 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 78f575d8ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc008b03b88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 17 15:25:16.724: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    Jan 17 15:25:16.724: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-5207  4661c0e0-f4e1-4ba5-8fc2-516fc5865581 100299 2 2023-01-17 15:25:09 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment ed7c33b7-12d4-43da-b8e9-4b8c15c9a8bc 0xc008b039a7 0xc008b039a8}] [] [{e2e.test Update apps/v1 2023-01-17 15:25:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 15:25:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ed7c33b7-12d4-43da-b8e9-4b8c15c9a8bc\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-17 15:25:15 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc008b03a68 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 17 15:25:16.727: INFO: Pod "test-rolling-update-deployment-78f575d8ff-l4757" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-78f575d8ff-l4757 test-rolling-update-deployment-78f575d8ff- deployment-5207  7adad92e-2aa6-4ae5-87ad-b603b470f33f 100290 0 2023-01-17 15:25:14 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.131.0.202/23"],"mac_address":"0a:58:0a:83:00:ca","gateway_ips":["10.131.0.1"],"ip_address":"10.131.0.202/23","gateway_ip":"10.131.0.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.131.0.202"
        ],
        "mac": "0a:58:0a:83:00:ca",
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.131.0.202"
        ],
        "mac": "0a:58:0a:83:00:ca",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-rolling-update-deployment-78f575d8ff 960192e9-36fa-4bd1-bedb-653470bd648b 0xc000902037 0xc000902038}] [] [{ip-10-0-135-246 Update v1 2023-01-17 15:25:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-17 15:25:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"960192e9-36fa-4bd1-bedb-653470bd648b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 15:25:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.0.202\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-01-17 15:25:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hds65,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hds65,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-151-22.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c51,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-d8mlv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:25:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:25:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:25:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 15:25:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.151.22,PodIP:10.131.0.202,StartTime:2023-01-17 15:25:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 15:25:15 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:37dd9ad84b34913b4bc57e00b0a28e8b00f18ca6a5ecec69461aa57402e5c787,ContainerID:cri-o://31a7c12d2751f275698d2c2b62f2371963adc3204fbc512285be482068f7698b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.131.0.202,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan 17 15:25:16.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-5207" for this suite. 01/17/23 15:25:16.731
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:25:16.736
Jan 17 15:25:16.736: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename kubelet-test 01/17/23 15:25:16.737
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:25:16.763
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:25:16.764
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Jan 17 15:25:16.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7654" for this suite. 01/17/23 15:25:16.811
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","completed":176,"skipped":3257,"failed":0}
------------------------------
• [0.089 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:25:16.736
    Jan 17 15:25:16.736: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename kubelet-test 01/17/23 15:25:16.737
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:25:16.763
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:25:16.764
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Jan 17 15:25:16.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-7654" for this suite. 01/17/23 15:25:16.811
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:737
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:25:16.826
Jan 17 15:25:16.826: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename statefulset 01/17/23 15:25:16.826
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:25:16.849
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:25:16.852
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-5942 01/17/23 15:25:16.854
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:737
STEP: Looking for a node to schedule stateful set and pod 01/17/23 15:25:16.862
STEP: Creating pod with conflicting port in namespace statefulset-5942 01/17/23 15:25:16.87
STEP: Waiting until pod test-pod will start running in namespace statefulset-5942 01/17/23 15:25:16.909
Jan 17 15:25:16.909: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-5942" to be "running"
Jan 17 15:25:16.913: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.120091ms
Jan 17 15:25:18.917: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=false. Elapsed: 2.007769127s
Jan 17 15:25:18.917: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-5942 01/17/23 15:25:18.917
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-5942 01/17/23 15:25:18.922
Jan 17 15:25:18.943: INFO: Observed stateful pod in namespace: statefulset-5942, name: ss-0, uid: c6546d1a-6b8e-4040-bcda-1e69205d5b74, status phase: Pending. Waiting for statefulset controller to delete.
Jan 17 15:25:18.954: INFO: Observed stateful pod in namespace: statefulset-5942, name: ss-0, uid: c6546d1a-6b8e-4040-bcda-1e69205d5b74, status phase: Failed. Waiting for statefulset controller to delete.
Jan 17 15:25:18.963: INFO: Observed stateful pod in namespace: statefulset-5942, name: ss-0, uid: c6546d1a-6b8e-4040-bcda-1e69205d5b74, status phase: Failed. Waiting for statefulset controller to delete.
Jan 17 15:25:18.966: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-5942
STEP: Removing pod with conflicting port in namespace statefulset-5942 01/17/23 15:25:18.966
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-5942 and will be in running state 01/17/23 15:25:18.983
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 17 15:25:22.994: INFO: Deleting all statefulset in ns statefulset-5942
Jan 17 15:25:22.997: INFO: Scaling statefulset ss to 0
Jan 17 15:25:33.009: INFO: Waiting for statefulset status.replicas updated to 0
Jan 17 15:25:33.012: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan 17 15:25:33.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5942" for this suite. 01/17/23 15:25:33.029
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","completed":177,"skipped":3263,"failed":0}
------------------------------
• [SLOW TEST] [16.209 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:737

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:25:16.826
    Jan 17 15:25:16.826: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename statefulset 01/17/23 15:25:16.826
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:25:16.849
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:25:16.852
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-5942 01/17/23 15:25:16.854
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:737
    STEP: Looking for a node to schedule stateful set and pod 01/17/23 15:25:16.862
    STEP: Creating pod with conflicting port in namespace statefulset-5942 01/17/23 15:25:16.87
    STEP: Waiting until pod test-pod will start running in namespace statefulset-5942 01/17/23 15:25:16.909
    Jan 17 15:25:16.909: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-5942" to be "running"
    Jan 17 15:25:16.913: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.120091ms
    Jan 17 15:25:18.917: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=false. Elapsed: 2.007769127s
    Jan 17 15:25:18.917: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-5942 01/17/23 15:25:18.917
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-5942 01/17/23 15:25:18.922
    Jan 17 15:25:18.943: INFO: Observed stateful pod in namespace: statefulset-5942, name: ss-0, uid: c6546d1a-6b8e-4040-bcda-1e69205d5b74, status phase: Pending. Waiting for statefulset controller to delete.
    Jan 17 15:25:18.954: INFO: Observed stateful pod in namespace: statefulset-5942, name: ss-0, uid: c6546d1a-6b8e-4040-bcda-1e69205d5b74, status phase: Failed. Waiting for statefulset controller to delete.
    Jan 17 15:25:18.963: INFO: Observed stateful pod in namespace: statefulset-5942, name: ss-0, uid: c6546d1a-6b8e-4040-bcda-1e69205d5b74, status phase: Failed. Waiting for statefulset controller to delete.
    Jan 17 15:25:18.966: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-5942
    STEP: Removing pod with conflicting port in namespace statefulset-5942 01/17/23 15:25:18.966
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-5942 and will be in running state 01/17/23 15:25:18.983
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan 17 15:25:22.994: INFO: Deleting all statefulset in ns statefulset-5942
    Jan 17 15:25:22.997: INFO: Scaling statefulset ss to 0
    Jan 17 15:25:33.009: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 17 15:25:33.012: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan 17 15:25:33.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-5942" for this suite. 01/17/23 15:25:33.029
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:25:33.035
Jan 17 15:25:33.035: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename services 01/17/23 15:25:33.036
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:25:33.069
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:25:33.071
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523
STEP: creating a service nodeport-service with the type=NodePort in namespace services-875 01/17/23 15:25:33.073
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/17/23 15:25:33.095
STEP: creating service externalsvc in namespace services-875 01/17/23 15:25:33.096
STEP: creating replication controller externalsvc in namespace services-875 01/17/23 15:25:33.117
I0117 15:25:33.133334      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-875, replica count: 2
I0117 15:25:36.184949      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 01/17/23 15:25:36.188
Jan 17 15:25:36.206: INFO: Creating new exec pod
Jan 17 15:25:36.221: INFO: Waiting up to 5m0s for pod "execpodkmjxs" in namespace "services-875" to be "running"
Jan 17 15:25:36.224: INFO: Pod "execpodkmjxs": Phase="Pending", Reason="", readiness=false. Elapsed: 2.743648ms
Jan 17 15:25:38.229: INFO: Pod "execpodkmjxs": Phase="Running", Reason="", readiness=true. Elapsed: 2.007102437s
Jan 17 15:25:38.229: INFO: Pod "execpodkmjxs" satisfied condition "running"
Jan 17 15:25:38.229: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-875 exec execpodkmjxs -- /bin/sh -x -c nslookup nodeport-service.services-875.svc.cluster.local'
Jan 17 15:25:38.373: INFO: stderr: "+ nslookup nodeport-service.services-875.svc.cluster.local\n"
Jan 17 15:25:38.373: INFO: stdout: "Server:\t\t172.30.0.10\nAddress:\t172.30.0.10#53\n\nnodeport-service.services-875.svc.cluster.local\tcanonical name = externalsvc.services-875.svc.cluster.local.\nName:\texternalsvc.services-875.svc.cluster.local\nAddress: 172.30.118.196\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-875, will wait for the garbage collector to delete the pods 01/17/23 15:25:38.373
Jan 17 15:25:38.435: INFO: Deleting ReplicationController externalsvc took: 8.041823ms
Jan 17 15:25:38.536: INFO: Terminating ReplicationController externalsvc pods took: 100.691654ms
Jan 17 15:25:40.960: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 17 15:25:40.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-875" for this suite. 01/17/23 15:25:40.99
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","completed":178,"skipped":3264,"failed":0}
------------------------------
• [SLOW TEST] [7.962 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:25:33.035
    Jan 17 15:25:33.035: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename services 01/17/23 15:25:33.036
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:25:33.069
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:25:33.071
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1523
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-875 01/17/23 15:25:33.073
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/17/23 15:25:33.095
    STEP: creating service externalsvc in namespace services-875 01/17/23 15:25:33.096
    STEP: creating replication controller externalsvc in namespace services-875 01/17/23 15:25:33.117
    I0117 15:25:33.133334      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-875, replica count: 2
    I0117 15:25:36.184949      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 01/17/23 15:25:36.188
    Jan 17 15:25:36.206: INFO: Creating new exec pod
    Jan 17 15:25:36.221: INFO: Waiting up to 5m0s for pod "execpodkmjxs" in namespace "services-875" to be "running"
    Jan 17 15:25:36.224: INFO: Pod "execpodkmjxs": Phase="Pending", Reason="", readiness=false. Elapsed: 2.743648ms
    Jan 17 15:25:38.229: INFO: Pod "execpodkmjxs": Phase="Running", Reason="", readiness=true. Elapsed: 2.007102437s
    Jan 17 15:25:38.229: INFO: Pod "execpodkmjxs" satisfied condition "running"
    Jan 17 15:25:38.229: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-875 exec execpodkmjxs -- /bin/sh -x -c nslookup nodeport-service.services-875.svc.cluster.local'
    Jan 17 15:25:38.373: INFO: stderr: "+ nslookup nodeport-service.services-875.svc.cluster.local\n"
    Jan 17 15:25:38.373: INFO: stdout: "Server:\t\t172.30.0.10\nAddress:\t172.30.0.10#53\n\nnodeport-service.services-875.svc.cluster.local\tcanonical name = externalsvc.services-875.svc.cluster.local.\nName:\texternalsvc.services-875.svc.cluster.local\nAddress: 172.30.118.196\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-875, will wait for the garbage collector to delete the pods 01/17/23 15:25:38.373
    Jan 17 15:25:38.435: INFO: Deleting ReplicationController externalsvc took: 8.041823ms
    Jan 17 15:25:38.536: INFO: Terminating ReplicationController externalsvc pods took: 100.691654ms
    Jan 17 15:25:40.960: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 17 15:25:40.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-875" for this suite. 01/17/23 15:25:40.99
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:25:40.999
Jan 17 15:25:40.999: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename proxy 01/17/23 15:25:40.999
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:25:41.023
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:25:41.025
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
Jan 17 15:25:41.027: INFO: Creating pod...
Jan 17 15:25:41.068: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-2555" to be "running"
Jan 17 15:25:41.070: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.404481ms
Jan 17 15:25:43.073: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.005419902s
Jan 17 15:25:43.073: INFO: Pod "agnhost" satisfied condition "running"
Jan 17 15:25:43.073: INFO: Creating service...
Jan 17 15:25:43.083: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-2555/pods/agnhost/proxy/some/path/with/DELETE
Jan 17 15:25:43.088: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 17 15:25:43.088: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-2555/pods/agnhost/proxy/some/path/with/GET
Jan 17 15:25:43.091: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jan 17 15:25:43.091: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-2555/pods/agnhost/proxy/some/path/with/HEAD
Jan 17 15:25:43.094: INFO: http.Client request:HEAD | StatusCode:200
Jan 17 15:25:43.094: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-2555/pods/agnhost/proxy/some/path/with/OPTIONS
Jan 17 15:25:43.098: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 17 15:25:43.098: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-2555/pods/agnhost/proxy/some/path/with/PATCH
Jan 17 15:25:43.101: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 17 15:25:43.101: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-2555/pods/agnhost/proxy/some/path/with/POST
Jan 17 15:25:43.104: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 17 15:25:43.104: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-2555/pods/agnhost/proxy/some/path/with/PUT
Jan 17 15:25:43.108: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan 17 15:25:43.108: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-2555/services/test-service/proxy/some/path/with/DELETE
Jan 17 15:25:43.114: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 17 15:25:43.114: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-2555/services/test-service/proxy/some/path/with/GET
Jan 17 15:25:43.119: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jan 17 15:25:43.119: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-2555/services/test-service/proxy/some/path/with/HEAD
Jan 17 15:25:43.130: INFO: http.Client request:HEAD | StatusCode:200
Jan 17 15:25:43.130: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-2555/services/test-service/proxy/some/path/with/OPTIONS
Jan 17 15:25:43.135: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 17 15:25:43.135: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-2555/services/test-service/proxy/some/path/with/PATCH
Jan 17 15:25:43.140: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 17 15:25:43.140: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-2555/services/test-service/proxy/some/path/with/POST
Jan 17 15:25:43.144: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 17 15:25:43.144: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-2555/services/test-service/proxy/some/path/with/PUT
Jan 17 15:25:43.150: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Jan 17 15:25:43.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-2555" for this suite. 01/17/23 15:25:43.154
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","completed":179,"skipped":3313,"failed":0}
------------------------------
• [2.161 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:25:40.999
    Jan 17 15:25:40.999: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename proxy 01/17/23 15:25:40.999
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:25:41.023
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:25:41.025
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    Jan 17 15:25:41.027: INFO: Creating pod...
    Jan 17 15:25:41.068: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-2555" to be "running"
    Jan 17 15:25:41.070: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.404481ms
    Jan 17 15:25:43.073: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.005419902s
    Jan 17 15:25:43.073: INFO: Pod "agnhost" satisfied condition "running"
    Jan 17 15:25:43.073: INFO: Creating service...
    Jan 17 15:25:43.083: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-2555/pods/agnhost/proxy/some/path/with/DELETE
    Jan 17 15:25:43.088: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan 17 15:25:43.088: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-2555/pods/agnhost/proxy/some/path/with/GET
    Jan 17 15:25:43.091: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Jan 17 15:25:43.091: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-2555/pods/agnhost/proxy/some/path/with/HEAD
    Jan 17 15:25:43.094: INFO: http.Client request:HEAD | StatusCode:200
    Jan 17 15:25:43.094: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-2555/pods/agnhost/proxy/some/path/with/OPTIONS
    Jan 17 15:25:43.098: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan 17 15:25:43.098: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-2555/pods/agnhost/proxy/some/path/with/PATCH
    Jan 17 15:25:43.101: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan 17 15:25:43.101: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-2555/pods/agnhost/proxy/some/path/with/POST
    Jan 17 15:25:43.104: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan 17 15:25:43.104: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-2555/pods/agnhost/proxy/some/path/with/PUT
    Jan 17 15:25:43.108: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jan 17 15:25:43.108: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-2555/services/test-service/proxy/some/path/with/DELETE
    Jan 17 15:25:43.114: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan 17 15:25:43.114: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-2555/services/test-service/proxy/some/path/with/GET
    Jan 17 15:25:43.119: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Jan 17 15:25:43.119: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-2555/services/test-service/proxy/some/path/with/HEAD
    Jan 17 15:25:43.130: INFO: http.Client request:HEAD | StatusCode:200
    Jan 17 15:25:43.130: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-2555/services/test-service/proxy/some/path/with/OPTIONS
    Jan 17 15:25:43.135: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan 17 15:25:43.135: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-2555/services/test-service/proxy/some/path/with/PATCH
    Jan 17 15:25:43.140: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan 17 15:25:43.140: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-2555/services/test-service/proxy/some/path/with/POST
    Jan 17 15:25:43.144: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan 17 15:25:43.144: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-2555/services/test-service/proxy/some/path/with/PUT
    Jan 17 15:25:43.150: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Jan 17 15:25:43.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-2555" for this suite. 01/17/23 15:25:43.154
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:25:43.16
Jan 17 15:25:43.160: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename services 01/17/23 15:25:43.16
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:25:43.182
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:25:43.184
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206
STEP: fetching services 01/17/23 15:25:43.186
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 17 15:25:43.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8488" for this suite. 01/17/23 15:25:43.202
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","completed":180,"skipped":3321,"failed":0}
------------------------------
• [0.051 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:25:43.16
    Jan 17 15:25:43.160: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename services 01/17/23 15:25:43.16
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:25:43.182
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:25:43.184
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3206
    STEP: fetching services 01/17/23 15:25:43.186
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 17 15:25:43.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-8488" for this suite. 01/17/23 15:25:43.202
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:543
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:25:43.211
Jan 17 15:25:43.211: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename sched-preemption 01/17/23 15:25:43.212
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:25:43.25
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:25:43.252
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jan 17 15:25:43.277: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 17 15:26:43.395: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:26:43.399
Jan 17 15:26:43.399: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename sched-preemption-path 01/17/23 15:26:43.4
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:26:43.419
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:26:43.421
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:496
STEP: Finding an available node 01/17/23 15:26:43.423
STEP: Trying to launch a pod without a label to get a node which can launch it. 01/17/23 15:26:43.423
Jan 17 15:26:43.453: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-2229" to be "running"
Jan 17 15:26:43.456: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.965619ms
Jan 17 15:26:45.461: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.007776317s
Jan 17 15:26:45.461: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 01/17/23 15:26:45.463
Jan 17 15:26:45.475: INFO: found a healthy node: ip-10-0-151-22.ec2.internal
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:543
Jan 17 15:26:53.551: INFO: pods created so far: [1 1 1]
Jan 17 15:26:53.551: INFO: length of pods created so far: 3
Jan 17 15:26:55.573: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:187
Jan 17 15:27:02.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-2229" for this suite. 01/17/23 15:27:02.577
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:470
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Jan 17 15:27:02.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-5845" for this suite. 01/17/23 15:27:02.616
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","completed":181,"skipped":3326,"failed":0}
------------------------------
• [SLOW TEST] [79.463 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:458
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:543

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:25:43.211
    Jan 17 15:25:43.211: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename sched-preemption 01/17/23 15:25:43.212
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:25:43.25
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:25:43.252
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Jan 17 15:25:43.277: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 17 15:26:43.395: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:26:43.399
    Jan 17 15:26:43.399: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename sched-preemption-path 01/17/23 15:26:43.4
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:26:43.419
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:26:43.421
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:496
    STEP: Finding an available node 01/17/23 15:26:43.423
    STEP: Trying to launch a pod without a label to get a node which can launch it. 01/17/23 15:26:43.423
    Jan 17 15:26:43.453: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-2229" to be "running"
    Jan 17 15:26:43.456: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.965619ms
    Jan 17 15:26:45.461: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.007776317s
    Jan 17 15:26:45.461: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 01/17/23 15:26:45.463
    Jan 17 15:26:45.475: INFO: found a healthy node: ip-10-0-151-22.ec2.internal
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:543
    Jan 17 15:26:53.551: INFO: pods created so far: [1 1 1]
    Jan 17 15:26:53.551: INFO: length of pods created so far: 3
    Jan 17 15:26:55.573: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/framework.go:187
    Jan 17 15:27:02.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-path-2229" for this suite. 01/17/23 15:27:02.577
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:470
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Jan 17 15:27:02.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-5845" for this suite. 01/17/23 15:27:02.616
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:27:02.674
Jan 17 15:27:02.674: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename projected 01/17/23 15:27:02.675
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:27:02.719
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:27:02.721
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129
STEP: Creating the pod 01/17/23 15:27:02.723
Jan 17 15:27:02.760: INFO: Waiting up to 5m0s for pod "labelsupdate267f1792-bec8-4116-a8d0-d352be470142" in namespace "projected-3072" to be "running and ready"
Jan 17 15:27:02.772: INFO: Pod "labelsupdate267f1792-bec8-4116-a8d0-d352be470142": Phase="Pending", Reason="", readiness=false. Elapsed: 12.036262ms
Jan 17 15:27:02.772: INFO: The phase of Pod labelsupdate267f1792-bec8-4116-a8d0-d352be470142 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 15:27:04.776: INFO: Pod "labelsupdate267f1792-bec8-4116-a8d0-d352be470142": Phase="Running", Reason="", readiness=true. Elapsed: 2.015706953s
Jan 17 15:27:04.776: INFO: The phase of Pod labelsupdate267f1792-bec8-4116-a8d0-d352be470142 is Running (Ready = true)
Jan 17 15:27:04.776: INFO: Pod "labelsupdate267f1792-bec8-4116-a8d0-d352be470142" satisfied condition "running and ready"
Jan 17 15:27:05.377: INFO: Successfully updated pod "labelsupdate267f1792-bec8-4116-a8d0-d352be470142"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 17 15:27:09.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3072" for this suite. 01/17/23 15:27:09.405
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","completed":182,"skipped":3333,"failed":0}
------------------------------
• [SLOW TEST] [6.738 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:27:02.674
    Jan 17 15:27:02.674: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename projected 01/17/23 15:27:02.675
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:27:02.719
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:27:02.721
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:129
    STEP: Creating the pod 01/17/23 15:27:02.723
    Jan 17 15:27:02.760: INFO: Waiting up to 5m0s for pod "labelsupdate267f1792-bec8-4116-a8d0-d352be470142" in namespace "projected-3072" to be "running and ready"
    Jan 17 15:27:02.772: INFO: Pod "labelsupdate267f1792-bec8-4116-a8d0-d352be470142": Phase="Pending", Reason="", readiness=false. Elapsed: 12.036262ms
    Jan 17 15:27:02.772: INFO: The phase of Pod labelsupdate267f1792-bec8-4116-a8d0-d352be470142 is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 15:27:04.776: INFO: Pod "labelsupdate267f1792-bec8-4116-a8d0-d352be470142": Phase="Running", Reason="", readiness=true. Elapsed: 2.015706953s
    Jan 17 15:27:04.776: INFO: The phase of Pod labelsupdate267f1792-bec8-4116-a8d0-d352be470142 is Running (Ready = true)
    Jan 17 15:27:04.776: INFO: Pod "labelsupdate267f1792-bec8-4116-a8d0-d352be470142" satisfied condition "running and ready"
    Jan 17 15:27:05.377: INFO: Successfully updated pod "labelsupdate267f1792-bec8-4116-a8d0-d352be470142"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 17 15:27:09.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3072" for this suite. 01/17/23 15:27:09.405
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:27:09.413
Jan 17 15:27:09.413: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename crd-publish-openapi 01/17/23 15:27:09.414
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:27:09.44
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:27:09.442
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 01/17/23 15:27:09.444
Jan 17 15:27:09.444: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
Jan 17 15:27:15.833: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 17 15:27:39.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-199" for this suite. 01/17/23 15:27:39.277
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","completed":183,"skipped":3379,"failed":0}
------------------------------
• [SLOW TEST] [29.876 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:27:09.413
    Jan 17 15:27:09.413: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename crd-publish-openapi 01/17/23 15:27:09.414
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:27:09.44
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:27:09.442
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:356
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 01/17/23 15:27:09.444
    Jan 17 15:27:09.444: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    Jan 17 15:27:15.833: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 17 15:27:39.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-199" for this suite. 01/17/23 15:27:39.277
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1507
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:27:39.29
Jan 17 15:27:39.290: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename kubectl 01/17/23 15:27:39.291
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:27:39.319
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:27:39.322
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1492
STEP: creating the pod 01/17/23 15:27:39.325
Jan 17 15:27:39.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-7074 create -f -'
Jan 17 15:27:40.483: INFO: stderr: ""
Jan 17 15:27:40.483: INFO: stdout: "pod/pause created\n"
Jan 17 15:27:40.483: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jan 17 15:27:40.483: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-7074" to be "running and ready"
Jan 17 15:27:40.486: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.786956ms
Jan 17 15:27:40.486: INFO: Error evaluating pod condition running and ready: want pod 'pause' on '' to be 'Running' but was 'Pending'
Jan 17 15:27:42.490: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.007138059s
Jan 17 15:27:42.490: INFO: Pod "pause" satisfied condition "running and ready"
Jan 17 15:27:42.490: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1507
STEP: adding the label testing-label with value testing-label-value to a pod 01/17/23 15:27:42.49
Jan 17 15:27:42.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-7074 label pods pause testing-label=testing-label-value'
Jan 17 15:27:42.551: INFO: stderr: ""
Jan 17 15:27:42.551: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 01/17/23 15:27:42.551
Jan 17 15:27:42.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-7074 get pod pause -L testing-label'
Jan 17 15:27:42.597: INFO: stderr: ""
Jan 17 15:27:42.597: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod 01/17/23 15:27:42.597
Jan 17 15:27:42.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-7074 label pods pause testing-label-'
Jan 17 15:27:42.654: INFO: stderr: ""
Jan 17 15:27:42.654: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 01/17/23 15:27:42.654
Jan 17 15:27:42.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-7074 get pod pause -L testing-label'
Jan 17 15:27:42.700: INFO: stderr: ""
Jan 17 15:27:42.700: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1498
STEP: using delete to clean up resources 01/17/23 15:27:42.7
Jan 17 15:27:42.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-7074 delete --grace-period=0 --force -f -'
Jan 17 15:27:42.756: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 17 15:27:42.756: INFO: stdout: "pod \"pause\" force deleted\n"
Jan 17 15:27:42.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-7074 get rc,svc -l name=pause --no-headers'
Jan 17 15:27:42.811: INFO: stderr: "No resources found in kubectl-7074 namespace.\n"
Jan 17 15:27:42.811: INFO: stdout: ""
Jan 17 15:27:42.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-7074 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 17 15:27:42.859: INFO: stderr: ""
Jan 17 15:27:42.859: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 17 15:27:42.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7074" for this suite. 01/17/23 15:27:42.863
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","completed":184,"skipped":3384,"failed":0}
------------------------------
• [3.579 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1490
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:27:39.29
    Jan 17 15:27:39.290: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename kubectl 01/17/23 15:27:39.291
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:27:39.319
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:27:39.322
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1492
    STEP: creating the pod 01/17/23 15:27:39.325
    Jan 17 15:27:39.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-7074 create -f -'
    Jan 17 15:27:40.483: INFO: stderr: ""
    Jan 17 15:27:40.483: INFO: stdout: "pod/pause created\n"
    Jan 17 15:27:40.483: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    Jan 17 15:27:40.483: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-7074" to be "running and ready"
    Jan 17 15:27:40.486: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.786956ms
    Jan 17 15:27:40.486: INFO: Error evaluating pod condition running and ready: want pod 'pause' on '' to be 'Running' but was 'Pending'
    Jan 17 15:27:42.490: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.007138059s
    Jan 17 15:27:42.490: INFO: Pod "pause" satisfied condition "running and ready"
    Jan 17 15:27:42.490: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1507
    STEP: adding the label testing-label with value testing-label-value to a pod 01/17/23 15:27:42.49
    Jan 17 15:27:42.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-7074 label pods pause testing-label=testing-label-value'
    Jan 17 15:27:42.551: INFO: stderr: ""
    Jan 17 15:27:42.551: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 01/17/23 15:27:42.551
    Jan 17 15:27:42.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-7074 get pod pause -L testing-label'
    Jan 17 15:27:42.597: INFO: stderr: ""
    Jan 17 15:27:42.597: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 01/17/23 15:27:42.597
    Jan 17 15:27:42.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-7074 label pods pause testing-label-'
    Jan 17 15:27:42.654: INFO: stderr: ""
    Jan 17 15:27:42.654: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 01/17/23 15:27:42.654
    Jan 17 15:27:42.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-7074 get pod pause -L testing-label'
    Jan 17 15:27:42.700: INFO: stderr: ""
    Jan 17 15:27:42.700: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1498
    STEP: using delete to clean up resources 01/17/23 15:27:42.7
    Jan 17 15:27:42.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-7074 delete --grace-period=0 --force -f -'
    Jan 17 15:27:42.756: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 17 15:27:42.756: INFO: stdout: "pod \"pause\" force deleted\n"
    Jan 17 15:27:42.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-7074 get rc,svc -l name=pause --no-headers'
    Jan 17 15:27:42.811: INFO: stderr: "No resources found in kubectl-7074 namespace.\n"
    Jan 17 15:27:42.811: INFO: stdout: ""
    Jan 17 15:27:42.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-7074 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jan 17 15:27:42.859: INFO: stderr: ""
    Jan 17 15:27:42.859: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 17 15:27:42.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-7074" for this suite. 01/17/23 15:27:42.863
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:27:42.869
Jan 17 15:27:42.869: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename emptydir 01/17/23 15:27:42.87
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:27:42.898
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:27:42.901
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96
STEP: Creating a pod to test emptydir 0644 on tmpfs 01/17/23 15:27:42.904
Jan 17 15:27:42.935: INFO: Waiting up to 5m0s for pod "pod-55b9ef6b-5149-41d2-be3d-31426e2c10d9" in namespace "emptydir-9777" to be "Succeeded or Failed"
Jan 17 15:27:42.944: INFO: Pod "pod-55b9ef6b-5149-41d2-be3d-31426e2c10d9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.589412ms
Jan 17 15:27:44.948: INFO: Pod "pod-55b9ef6b-5149-41d2-be3d-31426e2c10d9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01239054s
Jan 17 15:27:46.948: INFO: Pod "pod-55b9ef6b-5149-41d2-be3d-31426e2c10d9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012694436s
STEP: Saw pod success 01/17/23 15:27:46.948
Jan 17 15:27:46.948: INFO: Pod "pod-55b9ef6b-5149-41d2-be3d-31426e2c10d9" satisfied condition "Succeeded or Failed"
Jan 17 15:27:46.951: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-55b9ef6b-5149-41d2-be3d-31426e2c10d9 container test-container: <nil>
STEP: delete the pod 01/17/23 15:27:46.961
Jan 17 15:27:46.974: INFO: Waiting for pod pod-55b9ef6b-5149-41d2-be3d-31426e2c10d9 to disappear
Jan 17 15:27:46.977: INFO: Pod pod-55b9ef6b-5149-41d2-be3d-31426e2c10d9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 17 15:27:46.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9777" for this suite. 01/17/23 15:27:46.983
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":185,"skipped":3386,"failed":0}
------------------------------
• [4.122 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:27:42.869
    Jan 17 15:27:42.869: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename emptydir 01/17/23 15:27:42.87
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:27:42.898
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:27:42.901
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:96
    STEP: Creating a pod to test emptydir 0644 on tmpfs 01/17/23 15:27:42.904
    Jan 17 15:27:42.935: INFO: Waiting up to 5m0s for pod "pod-55b9ef6b-5149-41d2-be3d-31426e2c10d9" in namespace "emptydir-9777" to be "Succeeded or Failed"
    Jan 17 15:27:42.944: INFO: Pod "pod-55b9ef6b-5149-41d2-be3d-31426e2c10d9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.589412ms
    Jan 17 15:27:44.948: INFO: Pod "pod-55b9ef6b-5149-41d2-be3d-31426e2c10d9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01239054s
    Jan 17 15:27:46.948: INFO: Pod "pod-55b9ef6b-5149-41d2-be3d-31426e2c10d9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012694436s
    STEP: Saw pod success 01/17/23 15:27:46.948
    Jan 17 15:27:46.948: INFO: Pod "pod-55b9ef6b-5149-41d2-be3d-31426e2c10d9" satisfied condition "Succeeded or Failed"
    Jan 17 15:27:46.951: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-55b9ef6b-5149-41d2-be3d-31426e2c10d9 container test-container: <nil>
    STEP: delete the pod 01/17/23 15:27:46.961
    Jan 17 15:27:46.974: INFO: Waiting for pod pod-55b9ef6b-5149-41d2-be3d-31426e2c10d9 to disappear
    Jan 17 15:27:46.977: INFO: Pod pod-55b9ef6b-5149-41d2-be3d-31426e2c10d9 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 17 15:27:46.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-9777" for this suite. 01/17/23 15:27:46.983
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:27:46.992
Jan 17 15:27:46.992: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename downward-api 01/17/23 15:27:46.993
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:27:47.011
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:27:47.016
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89
STEP: Creating a pod to test downward api env vars 01/17/23 15:27:47.018
Jan 17 15:27:47.050: INFO: Waiting up to 5m0s for pod "downward-api-d76636a3-7ba4-4722-ad8b-2607d33a4b4b" in namespace "downward-api-7797" to be "Succeeded or Failed"
Jan 17 15:27:47.061: INFO: Pod "downward-api-d76636a3-7ba4-4722-ad8b-2607d33a4b4b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.421964ms
Jan 17 15:27:49.066: INFO: Pod "downward-api-d76636a3-7ba4-4722-ad8b-2607d33a4b4b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015239341s
Jan 17 15:27:51.064: INFO: Pod "downward-api-d76636a3-7ba4-4722-ad8b-2607d33a4b4b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014020228s
STEP: Saw pod success 01/17/23 15:27:51.064
Jan 17 15:27:51.064: INFO: Pod "downward-api-d76636a3-7ba4-4722-ad8b-2607d33a4b4b" satisfied condition "Succeeded or Failed"
Jan 17 15:27:51.067: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod downward-api-d76636a3-7ba4-4722-ad8b-2607d33a4b4b container dapi-container: <nil>
STEP: delete the pod 01/17/23 15:27:51.073
Jan 17 15:27:51.086: INFO: Waiting for pod downward-api-d76636a3-7ba4-4722-ad8b-2607d33a4b4b to disappear
Jan 17 15:27:51.089: INFO: Pod downward-api-d76636a3-7ba4-4722-ad8b-2607d33a4b4b no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Jan 17 15:27:51.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7797" for this suite. 01/17/23 15:27:51.093
{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","completed":186,"skipped":3423,"failed":0}
------------------------------
• [4.108 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:27:46.992
    Jan 17 15:27:46.992: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename downward-api 01/17/23 15:27:46.993
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:27:47.011
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:27:47.016
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:89
    STEP: Creating a pod to test downward api env vars 01/17/23 15:27:47.018
    Jan 17 15:27:47.050: INFO: Waiting up to 5m0s for pod "downward-api-d76636a3-7ba4-4722-ad8b-2607d33a4b4b" in namespace "downward-api-7797" to be "Succeeded or Failed"
    Jan 17 15:27:47.061: INFO: Pod "downward-api-d76636a3-7ba4-4722-ad8b-2607d33a4b4b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.421964ms
    Jan 17 15:27:49.066: INFO: Pod "downward-api-d76636a3-7ba4-4722-ad8b-2607d33a4b4b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015239341s
    Jan 17 15:27:51.064: INFO: Pod "downward-api-d76636a3-7ba4-4722-ad8b-2607d33a4b4b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014020228s
    STEP: Saw pod success 01/17/23 15:27:51.064
    Jan 17 15:27:51.064: INFO: Pod "downward-api-d76636a3-7ba4-4722-ad8b-2607d33a4b4b" satisfied condition "Succeeded or Failed"
    Jan 17 15:27:51.067: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod downward-api-d76636a3-7ba4-4722-ad8b-2607d33a4b4b container dapi-container: <nil>
    STEP: delete the pod 01/17/23 15:27:51.073
    Jan 17 15:27:51.086: INFO: Waiting for pod downward-api-d76636a3-7ba4-4722-ad8b-2607d33a4b4b to disappear
    Jan 17 15:27:51.089: INFO: Pod downward-api-d76636a3-7ba4-4722-ad8b-2607d33a4b4b no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Jan 17 15:27:51.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-7797" for this suite. 01/17/23 15:27:51.093
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:27:51.101
Jan 17 15:27:51.101: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename services 01/17/23 15:27:51.101
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:27:51.124
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:27:51.127
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415
STEP: creating a Service 01/17/23 15:27:51.138
STEP: watching for the Service to be added 01/17/23 15:27:51.162
Jan 17 15:27:51.164: INFO: Found Service test-service-9zxtx in namespace services-7455 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Jan 17 15:27:51.164: INFO: Service test-service-9zxtx created
STEP: Getting /status 01/17/23 15:27:51.164
Jan 17 15:27:51.167: INFO: Service test-service-9zxtx has LoadBalancer: {[]}
STEP: patching the ServiceStatus 01/17/23 15:27:51.167
STEP: watching for the Service to be patched 01/17/23 15:27:51.172
Jan 17 15:27:51.174: INFO: observed Service test-service-9zxtx in namespace services-7455 with annotations: map[] & LoadBalancer: {[]}
Jan 17 15:27:51.174: INFO: Found Service test-service-9zxtx in namespace services-7455 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Jan 17 15:27:51.174: INFO: Service test-service-9zxtx has service status patched
STEP: updating the ServiceStatus 01/17/23 15:27:51.174
Jan 17 15:27:51.194: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 01/17/23 15:27:51.194
Jan 17 15:27:51.195: INFO: Observed Service test-service-9zxtx in namespace services-7455 with annotations: map[] & Conditions: {[]}
Jan 17 15:27:51.195: INFO: Observed event: &Service{ObjectMeta:{test-service-9zxtx  services-7455  838cb562-09ee-481b-b91e-c4d5a1f7d3b2 102318 0 2023-01-17 15:27:51 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-01-17 15:27:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-01-17 15:27:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:172.30.129.128,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[172.30.129.128],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Jan 17 15:27:51.195: INFO: Found Service test-service-9zxtx in namespace services-7455 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 17 15:27:51.195: INFO: Service test-service-9zxtx has service status updated
STEP: patching the service 01/17/23 15:27:51.195
STEP: watching for the Service to be patched 01/17/23 15:27:51.223
Jan 17 15:27:51.224: INFO: observed Service test-service-9zxtx in namespace services-7455 with labels: map[test-service-static:true]
Jan 17 15:27:51.224: INFO: observed Service test-service-9zxtx in namespace services-7455 with labels: map[test-service-static:true]
Jan 17 15:27:51.224: INFO: observed Service test-service-9zxtx in namespace services-7455 with labels: map[test-service-static:true]
Jan 17 15:27:51.224: INFO: Found Service test-service-9zxtx in namespace services-7455 with labels: map[test-service:patched test-service-static:true]
Jan 17 15:27:51.224: INFO: Service test-service-9zxtx patched
STEP: deleting the service 01/17/23 15:27:51.224
STEP: watching for the Service to be deleted 01/17/23 15:27:51.249
Jan 17 15:27:51.252: INFO: Observed event: ADDED
Jan 17 15:27:51.252: INFO: Observed event: MODIFIED
Jan 17 15:27:51.252: INFO: Observed event: MODIFIED
Jan 17 15:27:51.253: INFO: Observed event: MODIFIED
Jan 17 15:27:51.253: INFO: Found Service test-service-9zxtx in namespace services-7455 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Jan 17 15:27:51.253: INFO: Service test-service-9zxtx deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 17 15:27:51.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7455" for this suite. 01/17/23 15:27:51.258
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","completed":187,"skipped":3424,"failed":0}
------------------------------
• [0.179 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:27:51.101
    Jan 17 15:27:51.101: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename services 01/17/23 15:27:51.101
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:27:51.124
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:27:51.127
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3415
    STEP: creating a Service 01/17/23 15:27:51.138
    STEP: watching for the Service to be added 01/17/23 15:27:51.162
    Jan 17 15:27:51.164: INFO: Found Service test-service-9zxtx in namespace services-7455 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    Jan 17 15:27:51.164: INFO: Service test-service-9zxtx created
    STEP: Getting /status 01/17/23 15:27:51.164
    Jan 17 15:27:51.167: INFO: Service test-service-9zxtx has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 01/17/23 15:27:51.167
    STEP: watching for the Service to be patched 01/17/23 15:27:51.172
    Jan 17 15:27:51.174: INFO: observed Service test-service-9zxtx in namespace services-7455 with annotations: map[] & LoadBalancer: {[]}
    Jan 17 15:27:51.174: INFO: Found Service test-service-9zxtx in namespace services-7455 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    Jan 17 15:27:51.174: INFO: Service test-service-9zxtx has service status patched
    STEP: updating the ServiceStatus 01/17/23 15:27:51.174
    Jan 17 15:27:51.194: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 01/17/23 15:27:51.194
    Jan 17 15:27:51.195: INFO: Observed Service test-service-9zxtx in namespace services-7455 with annotations: map[] & Conditions: {[]}
    Jan 17 15:27:51.195: INFO: Observed event: &Service{ObjectMeta:{test-service-9zxtx  services-7455  838cb562-09ee-481b-b91e-c4d5a1f7d3b2 102318 0 2023-01-17 15:27:51 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-01-17 15:27:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-01-17 15:27:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:172.30.129.128,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[172.30.129.128],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    Jan 17 15:27:51.195: INFO: Found Service test-service-9zxtx in namespace services-7455 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan 17 15:27:51.195: INFO: Service test-service-9zxtx has service status updated
    STEP: patching the service 01/17/23 15:27:51.195
    STEP: watching for the Service to be patched 01/17/23 15:27:51.223
    Jan 17 15:27:51.224: INFO: observed Service test-service-9zxtx in namespace services-7455 with labels: map[test-service-static:true]
    Jan 17 15:27:51.224: INFO: observed Service test-service-9zxtx in namespace services-7455 with labels: map[test-service-static:true]
    Jan 17 15:27:51.224: INFO: observed Service test-service-9zxtx in namespace services-7455 with labels: map[test-service-static:true]
    Jan 17 15:27:51.224: INFO: Found Service test-service-9zxtx in namespace services-7455 with labels: map[test-service:patched test-service-static:true]
    Jan 17 15:27:51.224: INFO: Service test-service-9zxtx patched
    STEP: deleting the service 01/17/23 15:27:51.224
    STEP: watching for the Service to be deleted 01/17/23 15:27:51.249
    Jan 17 15:27:51.252: INFO: Observed event: ADDED
    Jan 17 15:27:51.252: INFO: Observed event: MODIFIED
    Jan 17 15:27:51.252: INFO: Observed event: MODIFIED
    Jan 17 15:27:51.253: INFO: Observed event: MODIFIED
    Jan 17 15:27:51.253: INFO: Found Service test-service-9zxtx in namespace services-7455 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    Jan 17 15:27:51.253: INFO: Service test-service-9zxtx deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 17 15:27:51.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-7455" for this suite. 01/17/23 15:27:51.258
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:27:51.28
Jan 17 15:27:51.280: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename pods 01/17/23 15:27:51.281
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:27:51.311
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:27:51.322
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895
STEP: creating a Pod with a static label 01/17/23 15:27:51.348
STEP: watching for Pod to be ready 01/17/23 15:27:51.386
Jan 17 15:27:51.388: INFO: observed Pod pod-test in namespace pods-3440 in phase Pending with labels: map[test-pod-static:true] & conditions []
Jan 17 15:27:51.394: INFO: observed Pod pod-test in namespace pods-3440 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 15:27:51 +0000 UTC  }]
Jan 17 15:27:51.407: INFO: observed Pod pod-test in namespace pods-3440 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 15:27:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 15:27:51 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 15:27:51 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 15:27:51 +0000 UTC  }]
Jan 17 15:27:51.423: INFO: observed Pod pod-test in namespace pods-3440 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 15:27:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 15:27:51 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 15:27:51 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 15:27:51 +0000 UTC  }]
Jan 17 15:27:51.926: INFO: observed Pod pod-test in namespace pods-3440 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 15:27:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 15:27:51 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 15:27:51 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 15:27:51 +0000 UTC  }]
Jan 17 15:27:52.163: INFO: Found Pod pod-test in namespace pods-3440 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 15:27:51 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 15:27:52 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 15:27:52 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 15:27:51 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 01/17/23 15:27:52.166
STEP: getting the Pod and ensuring that it's patched 01/17/23 15:27:52.18
STEP: replacing the Pod's status Ready condition to False 01/17/23 15:27:52.183
STEP: check the Pod again to ensure its Ready conditions are False 01/17/23 15:27:52.191
STEP: deleting the Pod via a Collection with a LabelSelector 01/17/23 15:27:52.192
STEP: watching for the Pod to be deleted 01/17/23 15:27:52.199
Jan 17 15:27:52.200: INFO: observed event type MODIFIED
Jan 17 15:27:54.168: INFO: observed event type MODIFIED
Jan 17 15:27:55.169: INFO: observed event type MODIFIED
Jan 17 15:27:55.177: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 17 15:27:55.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3440" for this suite. 01/17/23 15:27:55.197
{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","completed":188,"skipped":3432,"failed":0}
------------------------------
• [3.925 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:27:51.28
    Jan 17 15:27:51.280: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename pods 01/17/23 15:27:51.281
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:27:51.311
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:27:51.322
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:895
    STEP: creating a Pod with a static label 01/17/23 15:27:51.348
    STEP: watching for Pod to be ready 01/17/23 15:27:51.386
    Jan 17 15:27:51.388: INFO: observed Pod pod-test in namespace pods-3440 in phase Pending with labels: map[test-pod-static:true] & conditions []
    Jan 17 15:27:51.394: INFO: observed Pod pod-test in namespace pods-3440 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 15:27:51 +0000 UTC  }]
    Jan 17 15:27:51.407: INFO: observed Pod pod-test in namespace pods-3440 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 15:27:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 15:27:51 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 15:27:51 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 15:27:51 +0000 UTC  }]
    Jan 17 15:27:51.423: INFO: observed Pod pod-test in namespace pods-3440 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 15:27:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 15:27:51 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 15:27:51 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 15:27:51 +0000 UTC  }]
    Jan 17 15:27:51.926: INFO: observed Pod pod-test in namespace pods-3440 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 15:27:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 15:27:51 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 15:27:51 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 15:27:51 +0000 UTC  }]
    Jan 17 15:27:52.163: INFO: Found Pod pod-test in namespace pods-3440 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 15:27:51 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 15:27:52 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 15:27:52 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 15:27:51 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 01/17/23 15:27:52.166
    STEP: getting the Pod and ensuring that it's patched 01/17/23 15:27:52.18
    STEP: replacing the Pod's status Ready condition to False 01/17/23 15:27:52.183
    STEP: check the Pod again to ensure its Ready conditions are False 01/17/23 15:27:52.191
    STEP: deleting the Pod via a Collection with a LabelSelector 01/17/23 15:27:52.192
    STEP: watching for the Pod to be deleted 01/17/23 15:27:52.199
    Jan 17 15:27:52.200: INFO: observed event type MODIFIED
    Jan 17 15:27:54.168: INFO: observed event type MODIFIED
    Jan 17 15:27:55.169: INFO: observed event type MODIFIED
    Jan 17 15:27:55.177: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 17 15:27:55.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-3440" for this suite. 01/17/23 15:27:55.197
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:27:55.205
Jan 17 15:27:55.205: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename job 01/17/23 15:27:55.206
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:27:55.238
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:27:55.241
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335
STEP: Creating a job 01/17/23 15:27:55.243
W0117 15:27:55.249463      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring active pods == parallelism 01/17/23 15:27:55.249
STEP: Orphaning one of the Job's Pods 01/17/23 15:27:57.268
Jan 17 15:27:57.783: INFO: Successfully updated pod "adopt-release-msxdc"
STEP: Checking that the Job readopts the Pod 01/17/23 15:27:57.783
Jan 17 15:27:57.783: INFO: Waiting up to 15m0s for pod "adopt-release-msxdc" in namespace "job-8725" to be "adopted"
Jan 17 15:27:57.787: INFO: Pod "adopt-release-msxdc": Phase="Running", Reason="", readiness=true. Elapsed: 3.744906ms
Jan 17 15:27:59.791: INFO: Pod "adopt-release-msxdc": Phase="Running", Reason="", readiness=true. Elapsed: 2.007735457s
Jan 17 15:27:59.791: INFO: Pod "adopt-release-msxdc" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 01/17/23 15:27:59.791
Jan 17 15:28:00.308: INFO: Successfully updated pod "adopt-release-msxdc"
STEP: Checking that the Job releases the Pod 01/17/23 15:28:00.308
Jan 17 15:28:00.309: INFO: Waiting up to 15m0s for pod "adopt-release-msxdc" in namespace "job-8725" to be "released"
Jan 17 15:28:00.311: INFO: Pod "adopt-release-msxdc": Phase="Running", Reason="", readiness=true. Elapsed: 2.681365ms
Jan 17 15:28:02.315: INFO: Pod "adopt-release-msxdc": Phase="Running", Reason="", readiness=true. Elapsed: 2.006399301s
Jan 17 15:28:02.315: INFO: Pod "adopt-release-msxdc" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jan 17 15:28:02.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-8725" for this suite. 01/17/23 15:28:02.319
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","completed":189,"skipped":3439,"failed":0}
------------------------------
• [SLOW TEST] [7.123 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:27:55.205
    Jan 17 15:27:55.205: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename job 01/17/23 15:27:55.206
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:27:55.238
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:27:55.241
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:335
    STEP: Creating a job 01/17/23 15:27:55.243
    W0117 15:27:55.249463      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Ensuring active pods == parallelism 01/17/23 15:27:55.249
    STEP: Orphaning one of the Job's Pods 01/17/23 15:27:57.268
    Jan 17 15:27:57.783: INFO: Successfully updated pod "adopt-release-msxdc"
    STEP: Checking that the Job readopts the Pod 01/17/23 15:27:57.783
    Jan 17 15:27:57.783: INFO: Waiting up to 15m0s for pod "adopt-release-msxdc" in namespace "job-8725" to be "adopted"
    Jan 17 15:27:57.787: INFO: Pod "adopt-release-msxdc": Phase="Running", Reason="", readiness=true. Elapsed: 3.744906ms
    Jan 17 15:27:59.791: INFO: Pod "adopt-release-msxdc": Phase="Running", Reason="", readiness=true. Elapsed: 2.007735457s
    Jan 17 15:27:59.791: INFO: Pod "adopt-release-msxdc" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 01/17/23 15:27:59.791
    Jan 17 15:28:00.308: INFO: Successfully updated pod "adopt-release-msxdc"
    STEP: Checking that the Job releases the Pod 01/17/23 15:28:00.308
    Jan 17 15:28:00.309: INFO: Waiting up to 15m0s for pod "adopt-release-msxdc" in namespace "job-8725" to be "released"
    Jan 17 15:28:00.311: INFO: Pod "adopt-release-msxdc": Phase="Running", Reason="", readiness=true. Elapsed: 2.681365ms
    Jan 17 15:28:02.315: INFO: Pod "adopt-release-msxdc": Phase="Running", Reason="", readiness=true. Elapsed: 2.006399301s
    Jan 17 15:28:02.315: INFO: Pod "adopt-release-msxdc" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jan 17 15:28:02.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-8725" for this suite. 01/17/23 15:28:02.319
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:28:02.328
Jan 17 15:28:02.328: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename server-version 01/17/23 15:28:02.329
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:28:02.355
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:28:02.358
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 01/17/23 15:28:02.36
STEP: Confirm major version 01/17/23 15:28:02.361
Jan 17 15:28:02.361: INFO: Major version: 1
STEP: Confirm minor version 01/17/23 15:28:02.361
Jan 17 15:28:02.361: INFO: cleanMinorVersion: 25
Jan 17 15:28:02.361: INFO: Minor version: 25
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:187
Jan 17 15:28:02.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-6077" for this suite. 01/17/23 15:28:02.377
{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","completed":190,"skipped":3441,"failed":0}
------------------------------
• [0.060 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:28:02.328
    Jan 17 15:28:02.328: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename server-version 01/17/23 15:28:02.329
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:28:02.355
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:28:02.358
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 01/17/23 15:28:02.36
    STEP: Confirm major version 01/17/23 15:28:02.361
    Jan 17 15:28:02.361: INFO: Major version: 1
    STEP: Confirm minor version 01/17/23 15:28:02.361
    Jan 17 15:28:02.361: INFO: cleanMinorVersion: 25
    Jan 17 15:28:02.361: INFO: Minor version: 25
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/framework.go:187
    Jan 17 15:28:02.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "server-version-6077" for this suite. 01/17/23 15:28:02.377
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:28:02.388
Jan 17 15:28:02.389: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename namespaces 01/17/23 15:28:02.389
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:28:02.428
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:28:02.431
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250
STEP: Creating a test namespace 01/17/23 15:28:02.434
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:28:02.476
STEP: Creating a service in the namespace 01/17/23 15:28:02.479
STEP: Deleting the namespace 01/17/23 15:28:02.516
STEP: Waiting for the namespace to be removed. 01/17/23 15:28:02.528
STEP: Recreating the namespace 01/17/23 15:28:08.537
STEP: Verifying there is no service in the namespace 01/17/23 15:28:08.557
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Jan 17 15:28:08.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-1770" for this suite. 01/17/23 15:28:08.574
STEP: Destroying namespace "nsdeletetest-3490" for this suite. 01/17/23 15:28:08.584
Jan 17 15:28:08.586: INFO: Namespace nsdeletetest-3490 was already deleted
STEP: Destroying namespace "nsdeletetest-8425" for this suite. 01/17/23 15:28:08.586
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","completed":191,"skipped":3450,"failed":0}
------------------------------
• [SLOW TEST] [6.207 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:28:02.388
    Jan 17 15:28:02.389: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename namespaces 01/17/23 15:28:02.389
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:28:02.428
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:28:02.431
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:250
    STEP: Creating a test namespace 01/17/23 15:28:02.434
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:28:02.476
    STEP: Creating a service in the namespace 01/17/23 15:28:02.479
    STEP: Deleting the namespace 01/17/23 15:28:02.516
    STEP: Waiting for the namespace to be removed. 01/17/23 15:28:02.528
    STEP: Recreating the namespace 01/17/23 15:28:08.537
    STEP: Verifying there is no service in the namespace 01/17/23 15:28:08.557
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Jan 17 15:28:08.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-1770" for this suite. 01/17/23 15:28:08.574
    STEP: Destroying namespace "nsdeletetest-3490" for this suite. 01/17/23 15:28:08.584
    Jan 17 15:28:08.586: INFO: Namespace nsdeletetest-3490 was already deleted
    STEP: Destroying namespace "nsdeletetest-8425" for this suite. 01/17/23 15:28:08.586
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:28:08.595
Jan 17 15:28:08.595: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename services 01/17/23 15:28:08.596
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:28:08.645
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:28:08.65
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791
STEP: creating service endpoint-test2 in namespace services-9134 01/17/23 15:28:08.653
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9134 to expose endpoints map[] 01/17/23 15:28:08.682
Jan 17 15:28:08.700: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Jan 17 15:28:09.707: INFO: successfully validated that service endpoint-test2 in namespace services-9134 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-9134 01/17/23 15:28:09.707
Jan 17 15:28:09.720: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-9134" to be "running and ready"
Jan 17 15:28:09.723: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.778456ms
Jan 17 15:28:09.723: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 15:28:11.727: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.007433666s
Jan 17 15:28:11.727: INFO: The phase of Pod pod1 is Running (Ready = true)
Jan 17 15:28:11.727: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9134 to expose endpoints map[pod1:[80]] 01/17/23 15:28:11.73
Jan 17 15:28:11.741: INFO: successfully validated that service endpoint-test2 in namespace services-9134 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 01/17/23 15:28:11.741
Jan 17 15:28:11.741: INFO: Creating new exec pod
Jan 17 15:28:11.752: INFO: Waiting up to 5m0s for pod "execpodvws66" in namespace "services-9134" to be "running"
Jan 17 15:28:11.755: INFO: Pod "execpodvws66": Phase="Pending", Reason="", readiness=false. Elapsed: 2.83667ms
Jan 17 15:28:13.758: INFO: Pod "execpodvws66": Phase="Running", Reason="", readiness=true. Elapsed: 2.006160914s
Jan 17 15:28:13.758: INFO: Pod "execpodvws66" satisfied condition "running"
Jan 17 15:28:14.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-9134 exec execpodvws66 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jan 17 15:28:14.868: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan 17 15:28:14.868: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 17 15:28:14.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-9134 exec execpodvws66 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.110.17 80'
Jan 17 15:28:14.969: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.110.17 80\nConnection to 172.30.110.17 80 port [tcp/http] succeeded!\n"
Jan 17 15:28:14.969: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Creating pod pod2 in namespace services-9134 01/17/23 15:28:14.97
Jan 17 15:28:14.982: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-9134" to be "running and ready"
Jan 17 15:28:14.985: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.092373ms
Jan 17 15:28:14.985: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 15:28:16.989: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.007132924s
Jan 17 15:28:16.989: INFO: The phase of Pod pod2 is Running (Ready = true)
Jan 17 15:28:16.989: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9134 to expose endpoints map[pod1:[80] pod2:[80]] 01/17/23 15:28:16.992
Jan 17 15:28:17.004: INFO: successfully validated that service endpoint-test2 in namespace services-9134 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 01/17/23 15:28:17.004
Jan 17 15:28:18.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-9134 exec execpodvws66 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jan 17 15:28:18.113: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan 17 15:28:18.113: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 17 15:28:18.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-9134 exec execpodvws66 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.110.17 80'
Jan 17 15:28:18.215: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.110.17 80\nConnection to 172.30.110.17 80 port [tcp/http] succeeded!\n"
Jan 17 15:28:18.215: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-9134 01/17/23 15:28:18.215
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9134 to expose endpoints map[pod2:[80]] 01/17/23 15:28:18.23
Jan 17 15:28:19.249: INFO: successfully validated that service endpoint-test2 in namespace services-9134 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 01/17/23 15:28:19.249
Jan 17 15:28:20.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-9134 exec execpodvws66 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jan 17 15:28:20.369: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan 17 15:28:20.369: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 17 15:28:20.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-9134 exec execpodvws66 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.110.17 80'
Jan 17 15:28:20.473: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.110.17 80\nConnection to 172.30.110.17 80 port [tcp/http] succeeded!\n"
Jan 17 15:28:20.473: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod2 in namespace services-9134 01/17/23 15:28:20.473
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9134 to expose endpoints map[] 01/17/23 15:28:20.487
Jan 17 15:28:20.497: INFO: successfully validated that service endpoint-test2 in namespace services-9134 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 17 15:28:20.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9134" for this suite. 01/17/23 15:28:20.533
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","completed":192,"skipped":3452,"failed":0}
------------------------------
• [SLOW TEST] [11.946 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:28:08.595
    Jan 17 15:28:08.595: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename services 01/17/23 15:28:08.596
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:28:08.645
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:28:08.65
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:791
    STEP: creating service endpoint-test2 in namespace services-9134 01/17/23 15:28:08.653
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9134 to expose endpoints map[] 01/17/23 15:28:08.682
    Jan 17 15:28:08.700: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
    Jan 17 15:28:09.707: INFO: successfully validated that service endpoint-test2 in namespace services-9134 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-9134 01/17/23 15:28:09.707
    Jan 17 15:28:09.720: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-9134" to be "running and ready"
    Jan 17 15:28:09.723: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.778456ms
    Jan 17 15:28:09.723: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 15:28:11.727: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.007433666s
    Jan 17 15:28:11.727: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jan 17 15:28:11.727: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9134 to expose endpoints map[pod1:[80]] 01/17/23 15:28:11.73
    Jan 17 15:28:11.741: INFO: successfully validated that service endpoint-test2 in namespace services-9134 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 01/17/23 15:28:11.741
    Jan 17 15:28:11.741: INFO: Creating new exec pod
    Jan 17 15:28:11.752: INFO: Waiting up to 5m0s for pod "execpodvws66" in namespace "services-9134" to be "running"
    Jan 17 15:28:11.755: INFO: Pod "execpodvws66": Phase="Pending", Reason="", readiness=false. Elapsed: 2.83667ms
    Jan 17 15:28:13.758: INFO: Pod "execpodvws66": Phase="Running", Reason="", readiness=true. Elapsed: 2.006160914s
    Jan 17 15:28:13.758: INFO: Pod "execpodvws66" satisfied condition "running"
    Jan 17 15:28:14.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-9134 exec execpodvws66 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Jan 17 15:28:14.868: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jan 17 15:28:14.868: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 17 15:28:14.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-9134 exec execpodvws66 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.110.17 80'
    Jan 17 15:28:14.969: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.110.17 80\nConnection to 172.30.110.17 80 port [tcp/http] succeeded!\n"
    Jan 17 15:28:14.969: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Creating pod pod2 in namespace services-9134 01/17/23 15:28:14.97
    Jan 17 15:28:14.982: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-9134" to be "running and ready"
    Jan 17 15:28:14.985: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.092373ms
    Jan 17 15:28:14.985: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 15:28:16.989: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.007132924s
    Jan 17 15:28:16.989: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jan 17 15:28:16.989: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9134 to expose endpoints map[pod1:[80] pod2:[80]] 01/17/23 15:28:16.992
    Jan 17 15:28:17.004: INFO: successfully validated that service endpoint-test2 in namespace services-9134 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 01/17/23 15:28:17.004
    Jan 17 15:28:18.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-9134 exec execpodvws66 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Jan 17 15:28:18.113: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jan 17 15:28:18.113: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 17 15:28:18.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-9134 exec execpodvws66 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.110.17 80'
    Jan 17 15:28:18.215: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.110.17 80\nConnection to 172.30.110.17 80 port [tcp/http] succeeded!\n"
    Jan 17 15:28:18.215: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod1 in namespace services-9134 01/17/23 15:28:18.215
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9134 to expose endpoints map[pod2:[80]] 01/17/23 15:28:18.23
    Jan 17 15:28:19.249: INFO: successfully validated that service endpoint-test2 in namespace services-9134 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 01/17/23 15:28:19.249
    Jan 17 15:28:20.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-9134 exec execpodvws66 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Jan 17 15:28:20.369: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jan 17 15:28:20.369: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 17 15:28:20.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-9134 exec execpodvws66 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.110.17 80'
    Jan 17 15:28:20.473: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.110.17 80\nConnection to 172.30.110.17 80 port [tcp/http] succeeded!\n"
    Jan 17 15:28:20.473: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod2 in namespace services-9134 01/17/23 15:28:20.473
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9134 to expose endpoints map[] 01/17/23 15:28:20.487
    Jan 17 15:28:20.497: INFO: successfully validated that service endpoint-test2 in namespace services-9134 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 17 15:28:20.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-9134" for this suite. 01/17/23 15:28:20.533
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:337
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:28:20.541
Jan 17 15:28:20.541: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename kubectl 01/17/23 15:28:20.542
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:28:20.571
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:28:20.575
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:324
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:337
STEP: creating a replication controller 01/17/23 15:28:20.578
Jan 17 15:28:20.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-5430 create -f -'
Jan 17 15:28:21.496: INFO: stderr: ""
Jan 17 15:28:21.496: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/17/23 15:28:21.496
Jan 17 15:28:21.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-5430 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 17 15:28:21.546: INFO: stderr: ""
Jan 17 15:28:21.546: INFO: stdout: "update-demo-nautilus-lnmgk update-demo-nautilus-wk54m "
Jan 17 15:28:21.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-5430 get pods update-demo-nautilus-lnmgk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 17 15:28:21.596: INFO: stderr: ""
Jan 17 15:28:21.596: INFO: stdout: ""
Jan 17 15:28:21.596: INFO: update-demo-nautilus-lnmgk is created but not running
Jan 17 15:28:26.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-5430 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 17 15:28:26.646: INFO: stderr: ""
Jan 17 15:28:26.646: INFO: stdout: "update-demo-nautilus-lnmgk update-demo-nautilus-wk54m "
Jan 17 15:28:26.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-5430 get pods update-demo-nautilus-lnmgk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 17 15:28:26.693: INFO: stderr: ""
Jan 17 15:28:26.693: INFO: stdout: "true"
Jan 17 15:28:26.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-5430 get pods update-demo-nautilus-lnmgk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 17 15:28:26.740: INFO: stderr: ""
Jan 17 15:28:26.740: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan 17 15:28:26.740: INFO: validating pod update-demo-nautilus-lnmgk
Jan 17 15:28:26.747: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 17 15:28:26.747: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 17 15:28:26.747: INFO: update-demo-nautilus-lnmgk is verified up and running
Jan 17 15:28:26.747: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-5430 get pods update-demo-nautilus-wk54m -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 17 15:28:26.794: INFO: stderr: ""
Jan 17 15:28:26.794: INFO: stdout: "true"
Jan 17 15:28:26.794: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-5430 get pods update-demo-nautilus-wk54m -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 17 15:28:26.838: INFO: stderr: ""
Jan 17 15:28:26.838: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan 17 15:28:26.838: INFO: validating pod update-demo-nautilus-wk54m
Jan 17 15:28:26.847: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 17 15:28:26.847: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 17 15:28:26.847: INFO: update-demo-nautilus-wk54m is verified up and running
STEP: using delete to clean up resources 01/17/23 15:28:26.847
Jan 17 15:28:26.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-5430 delete --grace-period=0 --force -f -'
Jan 17 15:28:26.895: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 17 15:28:26.895: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan 17 15:28:26.895: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-5430 get rc,svc -l name=update-demo --no-headers'
Jan 17 15:28:26.954: INFO: stderr: "No resources found in kubectl-5430 namespace.\n"
Jan 17 15:28:26.954: INFO: stdout: ""
Jan 17 15:28:26.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-5430 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 17 15:28:27.002: INFO: stderr: ""
Jan 17 15:28:27.002: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 17 15:28:27.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5430" for this suite. 01/17/23 15:28:27.006
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","completed":193,"skipped":3455,"failed":0}
------------------------------
• [SLOW TEST] [6.471 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:322
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:337

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:28:20.541
    Jan 17 15:28:20.541: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename kubectl 01/17/23 15:28:20.542
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:28:20.571
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:28:20.575
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:324
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:337
    STEP: creating a replication controller 01/17/23 15:28:20.578
    Jan 17 15:28:20.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-5430 create -f -'
    Jan 17 15:28:21.496: INFO: stderr: ""
    Jan 17 15:28:21.496: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/17/23 15:28:21.496
    Jan 17 15:28:21.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-5430 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 17 15:28:21.546: INFO: stderr: ""
    Jan 17 15:28:21.546: INFO: stdout: "update-demo-nautilus-lnmgk update-demo-nautilus-wk54m "
    Jan 17 15:28:21.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-5430 get pods update-demo-nautilus-lnmgk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 17 15:28:21.596: INFO: stderr: ""
    Jan 17 15:28:21.596: INFO: stdout: ""
    Jan 17 15:28:21.596: INFO: update-demo-nautilus-lnmgk is created but not running
    Jan 17 15:28:26.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-5430 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 17 15:28:26.646: INFO: stderr: ""
    Jan 17 15:28:26.646: INFO: stdout: "update-demo-nautilus-lnmgk update-demo-nautilus-wk54m "
    Jan 17 15:28:26.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-5430 get pods update-demo-nautilus-lnmgk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 17 15:28:26.693: INFO: stderr: ""
    Jan 17 15:28:26.693: INFO: stdout: "true"
    Jan 17 15:28:26.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-5430 get pods update-demo-nautilus-lnmgk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 17 15:28:26.740: INFO: stderr: ""
    Jan 17 15:28:26.740: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan 17 15:28:26.740: INFO: validating pod update-demo-nautilus-lnmgk
    Jan 17 15:28:26.747: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 17 15:28:26.747: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 17 15:28:26.747: INFO: update-demo-nautilus-lnmgk is verified up and running
    Jan 17 15:28:26.747: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-5430 get pods update-demo-nautilus-wk54m -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 17 15:28:26.794: INFO: stderr: ""
    Jan 17 15:28:26.794: INFO: stdout: "true"
    Jan 17 15:28:26.794: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-5430 get pods update-demo-nautilus-wk54m -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 17 15:28:26.838: INFO: stderr: ""
    Jan 17 15:28:26.838: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan 17 15:28:26.838: INFO: validating pod update-demo-nautilus-wk54m
    Jan 17 15:28:26.847: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 17 15:28:26.847: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 17 15:28:26.847: INFO: update-demo-nautilus-wk54m is verified up and running
    STEP: using delete to clean up resources 01/17/23 15:28:26.847
    Jan 17 15:28:26.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-5430 delete --grace-period=0 --force -f -'
    Jan 17 15:28:26.895: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 17 15:28:26.895: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Jan 17 15:28:26.895: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-5430 get rc,svc -l name=update-demo --no-headers'
    Jan 17 15:28:26.954: INFO: stderr: "No resources found in kubectl-5430 namespace.\n"
    Jan 17 15:28:26.954: INFO: stdout: ""
    Jan 17 15:28:26.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-5430 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jan 17 15:28:27.002: INFO: stderr: ""
    Jan 17 15:28:27.002: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 17 15:28:27.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-5430" for this suite. 01/17/23 15:28:27.006
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:28:27.013
Jan 17 15:28:27.013: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename cronjob 01/17/23 15:28:27.014
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:28:27.041
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:28:27.045
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 01/17/23 15:28:27.047
W0117 15:28:27.056714      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring no jobs are scheduled 01/17/23 15:28:27.056
STEP: Ensuring no job exists by listing jobs explicitly 01/17/23 15:33:27.064
STEP: Removing cronjob 01/17/23 15:33:27.068
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Jan 17 15:33:27.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-8640" for this suite. 01/17/23 15:33:27.08
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","completed":194,"skipped":3475,"failed":0}
------------------------------
• [SLOW TEST] [300.073 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:28:27.013
    Jan 17 15:28:27.013: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename cronjob 01/17/23 15:28:27.014
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:28:27.041
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:28:27.045
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 01/17/23 15:28:27.047
    W0117 15:28:27.056714      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Ensuring no jobs are scheduled 01/17/23 15:28:27.056
    STEP: Ensuring no job exists by listing jobs explicitly 01/17/23 15:33:27.064
    STEP: Removing cronjob 01/17/23 15:33:27.068
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Jan 17 15:33:27.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-8640" for this suite. 01/17/23 15:33:27.08
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:822
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:33:27.089
Jan 17 15:33:27.089: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename kubectl 01/17/23 15:33:27.09
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:33:27.147
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:33:27.15
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:822
STEP: validating api versions 01/17/23 15:33:27.153
Jan 17 15:33:27.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-7694 api-versions'
Jan 17 15:33:27.193: INFO: stderr: ""
Jan 17 15:33:27.193: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napiserver.openshift.io/v1\napps.openshift.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nauthorization.openshift.io/v1\nautoscaling.openshift.io/v1\nautoscaling.openshift.io/v1beta1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta2\nbatch/v1\nbuild.openshift.io/v1\ncertificates.k8s.io/v1\ncloud.network.openshift.io/v1\ncloudcredential.openshift.io/v1\nconfig.openshift.io/v1\nconsole.openshift.io/v1\nconsole.openshift.io/v1alpha1\ncontrolplane.operator.openshift.io/v1alpha1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nhelm.openshift.io/v1beta1\nimage.openshift.io/v1\nimageregistry.operator.openshift.io/v1\ningress.operator.openshift.io/v1\nk8s.cni.cncf.io/v1\nk8s.ovn.org/v1\nmachine.openshift.io/v1\nmachine.openshift.io/v1beta1\nmachineconfiguration.openshift.io/v1\nmetal3.io/v1alpha1\nmetrics.k8s.io/v1beta1\nmigration.k8s.io/v1alpha1\nmonitoring.coreos.com/v1\nmonitoring.coreos.com/v1alpha1\nmonitoring.coreos.com/v1beta1\nnetwork.operator.openshift.io/v1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\noauth.openshift.io/v1\noperator.openshift.io/v1\noperator.openshift.io/v1alpha1\noperators.coreos.com/v1\noperators.coreos.com/v1alpha1\noperators.coreos.com/v1alpha2\noperators.coreos.com/v2\npackages.operators.coreos.com/v1\nperformance.openshift.io/v1\nperformance.openshift.io/v1alpha1\nperformance.openshift.io/v2\npolicy/v1\nproject.openshift.io/v1\nquota.openshift.io/v1\nrbac.authorization.k8s.io/v1\nroute.openshift.io/v1\nsamples.operator.openshift.io/v1\nscheduling.k8s.io/v1\nsecurity.internal.openshift.io/v1\nsecurity.openshift.io/v1\nsnapshot.storage.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\ntemplate.openshift.io/v1\ntuned.openshift.io/v1\nuser.openshift.io/v1\nv1\nwhereabouts.cni.cncf.io/v1alpha1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 17 15:33:27.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7694" for this suite. 01/17/23 15:33:27.198
{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","completed":195,"skipped":3573,"failed":0}
------------------------------
• [0.118 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:816
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:822

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:33:27.089
    Jan 17 15:33:27.089: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename kubectl 01/17/23 15:33:27.09
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:33:27.147
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:33:27.15
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:822
    STEP: validating api versions 01/17/23 15:33:27.153
    Jan 17 15:33:27.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-7694 api-versions'
    Jan 17 15:33:27.193: INFO: stderr: ""
    Jan 17 15:33:27.193: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napiserver.openshift.io/v1\napps.openshift.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nauthorization.openshift.io/v1\nautoscaling.openshift.io/v1\nautoscaling.openshift.io/v1beta1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta2\nbatch/v1\nbuild.openshift.io/v1\ncertificates.k8s.io/v1\ncloud.network.openshift.io/v1\ncloudcredential.openshift.io/v1\nconfig.openshift.io/v1\nconsole.openshift.io/v1\nconsole.openshift.io/v1alpha1\ncontrolplane.operator.openshift.io/v1alpha1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nhelm.openshift.io/v1beta1\nimage.openshift.io/v1\nimageregistry.operator.openshift.io/v1\ningress.operator.openshift.io/v1\nk8s.cni.cncf.io/v1\nk8s.ovn.org/v1\nmachine.openshift.io/v1\nmachine.openshift.io/v1beta1\nmachineconfiguration.openshift.io/v1\nmetal3.io/v1alpha1\nmetrics.k8s.io/v1beta1\nmigration.k8s.io/v1alpha1\nmonitoring.coreos.com/v1\nmonitoring.coreos.com/v1alpha1\nmonitoring.coreos.com/v1beta1\nnetwork.operator.openshift.io/v1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\noauth.openshift.io/v1\noperator.openshift.io/v1\noperator.openshift.io/v1alpha1\noperators.coreos.com/v1\noperators.coreos.com/v1alpha1\noperators.coreos.com/v1alpha2\noperators.coreos.com/v2\npackages.operators.coreos.com/v1\nperformance.openshift.io/v1\nperformance.openshift.io/v1alpha1\nperformance.openshift.io/v2\npolicy/v1\nproject.openshift.io/v1\nquota.openshift.io/v1\nrbac.authorization.k8s.io/v1\nroute.openshift.io/v1\nsamples.operator.openshift.io/v1\nscheduling.k8s.io/v1\nsecurity.internal.openshift.io/v1\nsecurity.openshift.io/v1\nsnapshot.storage.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\ntemplate.openshift.io/v1\ntuned.openshift.io/v1\nuser.openshift.io/v1\nv1\nwhereabouts.cni.cncf.io/v1alpha1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 17 15:33:27.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-7694" for this suite. 01/17/23 15:33:27.198
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:33:27.208
Jan 17 15:33:27.209: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename svcaccounts 01/17/23 15:33:27.209
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:33:27.231
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:33:27.234
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646
STEP: creating a ServiceAccount 01/17/23 15:33:27.236
STEP: watching for the ServiceAccount to be added 01/17/23 15:33:27.248
STEP: patching the ServiceAccount 01/17/23 15:33:27.249
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 01/17/23 15:33:27.254
STEP: deleting the ServiceAccount 01/17/23 15:33:27.27
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jan 17 15:33:27.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5682" for this suite. 01/17/23 15:33:27.344
{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","completed":196,"skipped":3618,"failed":0}
------------------------------
• [0.148 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:33:27.208
    Jan 17 15:33:27.209: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename svcaccounts 01/17/23 15:33:27.209
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:33:27.231
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:33:27.234
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:646
    STEP: creating a ServiceAccount 01/17/23 15:33:27.236
    STEP: watching for the ServiceAccount to be added 01/17/23 15:33:27.248
    STEP: patching the ServiceAccount 01/17/23 15:33:27.249
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 01/17/23 15:33:27.254
    STEP: deleting the ServiceAccount 01/17/23 15:33:27.27
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jan 17 15:33:27.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-5682" for this suite. 01/17/23 15:33:27.344
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:846
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:33:27.357
Jan 17 15:33:27.357: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename statefulset 01/17/23 15:33:27.358
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:33:27.387
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:33:27.39
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-635 01/17/23 15:33:27.392
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:846
STEP: Creating statefulset ss in namespace statefulset-635 01/17/23 15:33:27.4
Jan 17 15:33:27.418: INFO: Found 0 stateful pods, waiting for 1
Jan 17 15:33:37.423: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 01/17/23 15:33:37.429
STEP: updating a scale subresource 01/17/23 15:33:37.432
STEP: verifying the statefulset Spec.Replicas was modified 01/17/23 15:33:37.437
STEP: Patch a scale subresource 01/17/23 15:33:37.44
STEP: verifying the statefulset Spec.Replicas was modified 01/17/23 15:33:37.444
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 17 15:33:37.448: INFO: Deleting all statefulset in ns statefulset-635
Jan 17 15:33:37.450: INFO: Scaling statefulset ss to 0
Jan 17 15:33:47.469: INFO: Waiting for statefulset status.replicas updated to 0
Jan 17 15:33:47.472: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan 17 15:33:47.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-635" for this suite. 01/17/23 15:33:47.486
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","completed":197,"skipped":3630,"failed":0}
------------------------------
• [SLOW TEST] [20.136 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:846

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:33:27.357
    Jan 17 15:33:27.357: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename statefulset 01/17/23 15:33:27.358
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:33:27.387
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:33:27.39
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-635 01/17/23 15:33:27.392
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:846
    STEP: Creating statefulset ss in namespace statefulset-635 01/17/23 15:33:27.4
    Jan 17 15:33:27.418: INFO: Found 0 stateful pods, waiting for 1
    Jan 17 15:33:37.423: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 01/17/23 15:33:37.429
    STEP: updating a scale subresource 01/17/23 15:33:37.432
    STEP: verifying the statefulset Spec.Replicas was modified 01/17/23 15:33:37.437
    STEP: Patch a scale subresource 01/17/23 15:33:37.44
    STEP: verifying the statefulset Spec.Replicas was modified 01/17/23 15:33:37.444
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan 17 15:33:37.448: INFO: Deleting all statefulset in ns statefulset-635
    Jan 17 15:33:37.450: INFO: Scaling statefulset ss to 0
    Jan 17 15:33:47.469: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 17 15:33:47.472: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan 17 15:33:47.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-635" for this suite. 01/17/23 15:33:47.486
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:33:47.494
Jan 17 15:33:47.494: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename pod-network-test 01/17/23 15:33:47.495
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:33:47.526
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:33:47.529
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-1588 01/17/23 15:33:47.531
STEP: creating a selector 01/17/23 15:33:47.531
STEP: Creating the service pods in kubernetes 01/17/23 15:33:47.531
Jan 17 15:33:47.531: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 17 15:33:47.598: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-1588" to be "running and ready"
Jan 17 15:33:47.601: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.924738ms
Jan 17 15:33:47.601: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 15:33:49.605: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.006864146s
Jan 17 15:33:49.605: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 17 15:33:51.606: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.007802949s
Jan 17 15:33:51.606: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 17 15:33:53.605: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.006478991s
Jan 17 15:33:53.605: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 17 15:33:55.606: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.007268224s
Jan 17 15:33:55.606: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 17 15:33:57.606: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.007816615s
Jan 17 15:33:57.606: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 17 15:33:59.605: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.006998038s
Jan 17 15:33:59.605: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 17 15:34:01.607: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.008381393s
Jan 17 15:34:01.607: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 17 15:34:03.605: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.006205402s
Jan 17 15:34:03.605: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 17 15:34:05.606: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.007119615s
Jan 17 15:34:05.606: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 17 15:34:07.607: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.008939303s
Jan 17 15:34:07.607: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 17 15:34:09.606: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.007110098s
Jan 17 15:34:09.606: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan 17 15:34:09.606: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan 17 15:34:09.609: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-1588" to be "running and ready"
Jan 17 15:34:09.612: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.139075ms
Jan 17 15:34:09.612: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan 17 15:34:09.612: INFO: Pod "netserver-1" satisfied condition "running and ready"
Jan 17 15:34:09.615: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-1588" to be "running and ready"
Jan 17 15:34:09.620: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 5.259339ms
Jan 17 15:34:09.620: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Jan 17 15:34:09.620: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 01/17/23 15:34:09.628
Jan 17 15:34:09.639: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-1588" to be "running"
Jan 17 15:34:09.642: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.856624ms
Jan 17 15:34:11.646: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007080289s
Jan 17 15:34:11.646: INFO: Pod "test-container-pod" satisfied condition "running"
Jan 17 15:34:11.650: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Jan 17 15:34:11.650: INFO: Breadth first check of 10.129.2.102 on host 10.0.139.213...
Jan 17 15:34:11.652: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.131.0.224:9080/dial?request=hostname&protocol=http&host=10.129.2.102&port=8083&tries=1'] Namespace:pod-network-test-1588 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 15:34:11.652: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
Jan 17 15:34:11.653: INFO: ExecWithOptions: Clientset creation
Jan 17 15:34:11.653: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-1588/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.131.0.224%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.129.2.102%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 17 15:34:11.723: INFO: Waiting for responses: map[]
Jan 17 15:34:11.723: INFO: reached 10.129.2.102 after 0/1 tries
Jan 17 15:34:11.723: INFO: Breadth first check of 10.131.0.223 on host 10.0.151.22...
Jan 17 15:34:11.726: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.131.0.224:9080/dial?request=hostname&protocol=http&host=10.131.0.223&port=8083&tries=1'] Namespace:pod-network-test-1588 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 15:34:11.726: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
Jan 17 15:34:11.727: INFO: ExecWithOptions: Clientset creation
Jan 17 15:34:11.727: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-1588/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.131.0.224%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.131.0.223%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 17 15:34:11.791: INFO: Waiting for responses: map[]
Jan 17 15:34:11.791: INFO: reached 10.131.0.223 after 0/1 tries
Jan 17 15:34:11.791: INFO: Breadth first check of 10.128.2.142 on host 10.0.165.14...
Jan 17 15:34:11.794: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.131.0.224:9080/dial?request=hostname&protocol=http&host=10.128.2.142&port=8083&tries=1'] Namespace:pod-network-test-1588 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 15:34:11.794: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
Jan 17 15:34:11.795: INFO: ExecWithOptions: Clientset creation
Jan 17 15:34:11.795: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-1588/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.131.0.224%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.128.2.142%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 17 15:34:11.862: INFO: Waiting for responses: map[]
Jan 17 15:34:11.862: INFO: reached 10.128.2.142 after 0/1 tries
Jan 17 15:34:11.862: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Jan 17 15:34:11.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1588" for this suite. 01/17/23 15:34:11.867
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","completed":198,"skipped":3673,"failed":0}
------------------------------
• [SLOW TEST] [24.380 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:33:47.494
    Jan 17 15:33:47.494: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename pod-network-test 01/17/23 15:33:47.495
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:33:47.526
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:33:47.529
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-1588 01/17/23 15:33:47.531
    STEP: creating a selector 01/17/23 15:33:47.531
    STEP: Creating the service pods in kubernetes 01/17/23 15:33:47.531
    Jan 17 15:33:47.531: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan 17 15:33:47.598: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-1588" to be "running and ready"
    Jan 17 15:33:47.601: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.924738ms
    Jan 17 15:33:47.601: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 15:33:49.605: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.006864146s
    Jan 17 15:33:49.605: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 17 15:33:51.606: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.007802949s
    Jan 17 15:33:51.606: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 17 15:33:53.605: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.006478991s
    Jan 17 15:33:53.605: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 17 15:33:55.606: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.007268224s
    Jan 17 15:33:55.606: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 17 15:33:57.606: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.007816615s
    Jan 17 15:33:57.606: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 17 15:33:59.605: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.006998038s
    Jan 17 15:33:59.605: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 17 15:34:01.607: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.008381393s
    Jan 17 15:34:01.607: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 17 15:34:03.605: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.006205402s
    Jan 17 15:34:03.605: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 17 15:34:05.606: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.007119615s
    Jan 17 15:34:05.606: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 17 15:34:07.607: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.008939303s
    Jan 17 15:34:07.607: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 17 15:34:09.606: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.007110098s
    Jan 17 15:34:09.606: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan 17 15:34:09.606: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan 17 15:34:09.609: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-1588" to be "running and ready"
    Jan 17 15:34:09.612: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.139075ms
    Jan 17 15:34:09.612: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan 17 15:34:09.612: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Jan 17 15:34:09.615: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-1588" to be "running and ready"
    Jan 17 15:34:09.620: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 5.259339ms
    Jan 17 15:34:09.620: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Jan 17 15:34:09.620: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 01/17/23 15:34:09.628
    Jan 17 15:34:09.639: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-1588" to be "running"
    Jan 17 15:34:09.642: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.856624ms
    Jan 17 15:34:11.646: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007080289s
    Jan 17 15:34:11.646: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan 17 15:34:11.650: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Jan 17 15:34:11.650: INFO: Breadth first check of 10.129.2.102 on host 10.0.139.213...
    Jan 17 15:34:11.652: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.131.0.224:9080/dial?request=hostname&protocol=http&host=10.129.2.102&port=8083&tries=1'] Namespace:pod-network-test-1588 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 17 15:34:11.652: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    Jan 17 15:34:11.653: INFO: ExecWithOptions: Clientset creation
    Jan 17 15:34:11.653: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-1588/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.131.0.224%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.129.2.102%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 17 15:34:11.723: INFO: Waiting for responses: map[]
    Jan 17 15:34:11.723: INFO: reached 10.129.2.102 after 0/1 tries
    Jan 17 15:34:11.723: INFO: Breadth first check of 10.131.0.223 on host 10.0.151.22...
    Jan 17 15:34:11.726: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.131.0.224:9080/dial?request=hostname&protocol=http&host=10.131.0.223&port=8083&tries=1'] Namespace:pod-network-test-1588 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 17 15:34:11.726: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    Jan 17 15:34:11.727: INFO: ExecWithOptions: Clientset creation
    Jan 17 15:34:11.727: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-1588/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.131.0.224%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.131.0.223%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 17 15:34:11.791: INFO: Waiting for responses: map[]
    Jan 17 15:34:11.791: INFO: reached 10.131.0.223 after 0/1 tries
    Jan 17 15:34:11.791: INFO: Breadth first check of 10.128.2.142 on host 10.0.165.14...
    Jan 17 15:34:11.794: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.131.0.224:9080/dial?request=hostname&protocol=http&host=10.128.2.142&port=8083&tries=1'] Namespace:pod-network-test-1588 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 17 15:34:11.794: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    Jan 17 15:34:11.795: INFO: ExecWithOptions: Clientset creation
    Jan 17 15:34:11.795: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-1588/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.131.0.224%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.128.2.142%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 17 15:34:11.862: INFO: Waiting for responses: map[]
    Jan 17 15:34:11.862: INFO: reached 10.128.2.142 after 0/1 tries
    Jan 17 15:34:11.862: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Jan 17 15:34:11.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-1588" for this suite. 01/17/23 15:34:11.867
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:34:11.874
Jan 17 15:34:11.874: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename services 01/17/23 15:34:11.874
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:34:11.901
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:34:11.904
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157
STEP: creating service in namespace services-1423 01/17/23 15:34:11.907
STEP: creating service affinity-clusterip in namespace services-1423 01/17/23 15:34:11.907
STEP: creating replication controller affinity-clusterip in namespace services-1423 01/17/23 15:34:11.935
I0117 15:34:11.945922      22 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-1423, replica count: 3
I0117 15:34:14.997363      22 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 17 15:34:15.004: INFO: Creating new exec pod
Jan 17 15:34:15.019: INFO: Waiting up to 5m0s for pod "execpod-affinity9qcsn" in namespace "services-1423" to be "running"
Jan 17 15:34:15.022: INFO: Pod "execpod-affinity9qcsn": Phase="Pending", Reason="", readiness=false. Elapsed: 2.929612ms
Jan 17 15:34:17.027: INFO: Pod "execpod-affinity9qcsn": Phase="Running", Reason="", readiness=true. Elapsed: 2.007681601s
Jan 17 15:34:17.027: INFO: Pod "execpod-affinity9qcsn" satisfied condition "running"
Jan 17 15:34:18.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-1423 exec execpod-affinity9qcsn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
Jan 17 15:34:19.151: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Jan 17 15:34:19.151: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 17 15:34:19.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-1423 exec execpod-affinity9qcsn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.249.184 80'
Jan 17 15:34:19.268: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.249.184 80\nConnection to 172.30.249.184 80 port [tcp/http] succeeded!\n"
Jan 17 15:34:19.268: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 17 15:34:19.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-1423 exec execpod-affinity9qcsn -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.30.249.184:80/ ; done'
Jan 17 15:34:19.415: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.249.184:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.249.184:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.249.184:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.249.184:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.249.184:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.249.184:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.249.184:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.249.184:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.249.184:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.249.184:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.249.184:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.249.184:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.249.184:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.249.184:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.249.184:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.249.184:80/\n"
Jan 17 15:34:19.415: INFO: stdout: "\naffinity-clusterip-bdb9c\naffinity-clusterip-bdb9c\naffinity-clusterip-bdb9c\naffinity-clusterip-bdb9c\naffinity-clusterip-bdb9c\naffinity-clusterip-bdb9c\naffinity-clusterip-bdb9c\naffinity-clusterip-bdb9c\naffinity-clusterip-bdb9c\naffinity-clusterip-bdb9c\naffinity-clusterip-bdb9c\naffinity-clusterip-bdb9c\naffinity-clusterip-bdb9c\naffinity-clusterip-bdb9c\naffinity-clusterip-bdb9c\naffinity-clusterip-bdb9c"
Jan 17 15:34:19.415: INFO: Received response from host: affinity-clusterip-bdb9c
Jan 17 15:34:19.415: INFO: Received response from host: affinity-clusterip-bdb9c
Jan 17 15:34:19.415: INFO: Received response from host: affinity-clusterip-bdb9c
Jan 17 15:34:19.415: INFO: Received response from host: affinity-clusterip-bdb9c
Jan 17 15:34:19.415: INFO: Received response from host: affinity-clusterip-bdb9c
Jan 17 15:34:19.415: INFO: Received response from host: affinity-clusterip-bdb9c
Jan 17 15:34:19.415: INFO: Received response from host: affinity-clusterip-bdb9c
Jan 17 15:34:19.415: INFO: Received response from host: affinity-clusterip-bdb9c
Jan 17 15:34:19.415: INFO: Received response from host: affinity-clusterip-bdb9c
Jan 17 15:34:19.415: INFO: Received response from host: affinity-clusterip-bdb9c
Jan 17 15:34:19.415: INFO: Received response from host: affinity-clusterip-bdb9c
Jan 17 15:34:19.415: INFO: Received response from host: affinity-clusterip-bdb9c
Jan 17 15:34:19.415: INFO: Received response from host: affinity-clusterip-bdb9c
Jan 17 15:34:19.415: INFO: Received response from host: affinity-clusterip-bdb9c
Jan 17 15:34:19.415: INFO: Received response from host: affinity-clusterip-bdb9c
Jan 17 15:34:19.415: INFO: Received response from host: affinity-clusterip-bdb9c
Jan 17 15:34:19.415: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-1423, will wait for the garbage collector to delete the pods 01/17/23 15:34:19.429
Jan 17 15:34:19.490: INFO: Deleting ReplicationController affinity-clusterip took: 7.919289ms
Jan 17 15:34:19.591: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.810233ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 17 15:34:22.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1423" for this suite. 01/17/23 15:34:22.024
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","completed":199,"skipped":3678,"failed":0}
------------------------------
• [SLOW TEST] [10.160 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:34:11.874
    Jan 17 15:34:11.874: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename services 01/17/23 15:34:11.874
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:34:11.901
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:34:11.904
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2157
    STEP: creating service in namespace services-1423 01/17/23 15:34:11.907
    STEP: creating service affinity-clusterip in namespace services-1423 01/17/23 15:34:11.907
    STEP: creating replication controller affinity-clusterip in namespace services-1423 01/17/23 15:34:11.935
    I0117 15:34:11.945922      22 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-1423, replica count: 3
    I0117 15:34:14.997363      22 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 17 15:34:15.004: INFO: Creating new exec pod
    Jan 17 15:34:15.019: INFO: Waiting up to 5m0s for pod "execpod-affinity9qcsn" in namespace "services-1423" to be "running"
    Jan 17 15:34:15.022: INFO: Pod "execpod-affinity9qcsn": Phase="Pending", Reason="", readiness=false. Elapsed: 2.929612ms
    Jan 17 15:34:17.027: INFO: Pod "execpod-affinity9qcsn": Phase="Running", Reason="", readiness=true. Elapsed: 2.007681601s
    Jan 17 15:34:17.027: INFO: Pod "execpod-affinity9qcsn" satisfied condition "running"
    Jan 17 15:34:18.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-1423 exec execpod-affinity9qcsn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
    Jan 17 15:34:19.151: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    Jan 17 15:34:19.151: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 17 15:34:19.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-1423 exec execpod-affinity9qcsn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.249.184 80'
    Jan 17 15:34:19.268: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.249.184 80\nConnection to 172.30.249.184 80 port [tcp/http] succeeded!\n"
    Jan 17 15:34:19.268: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 17 15:34:19.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-1423 exec execpod-affinity9qcsn -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.30.249.184:80/ ; done'
    Jan 17 15:34:19.415: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.249.184:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.249.184:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.249.184:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.249.184:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.249.184:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.249.184:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.249.184:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.249.184:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.249.184:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.249.184:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.249.184:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.249.184:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.249.184:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.249.184:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.249.184:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.249.184:80/\n"
    Jan 17 15:34:19.415: INFO: stdout: "\naffinity-clusterip-bdb9c\naffinity-clusterip-bdb9c\naffinity-clusterip-bdb9c\naffinity-clusterip-bdb9c\naffinity-clusterip-bdb9c\naffinity-clusterip-bdb9c\naffinity-clusterip-bdb9c\naffinity-clusterip-bdb9c\naffinity-clusterip-bdb9c\naffinity-clusterip-bdb9c\naffinity-clusterip-bdb9c\naffinity-clusterip-bdb9c\naffinity-clusterip-bdb9c\naffinity-clusterip-bdb9c\naffinity-clusterip-bdb9c\naffinity-clusterip-bdb9c"
    Jan 17 15:34:19.415: INFO: Received response from host: affinity-clusterip-bdb9c
    Jan 17 15:34:19.415: INFO: Received response from host: affinity-clusterip-bdb9c
    Jan 17 15:34:19.415: INFO: Received response from host: affinity-clusterip-bdb9c
    Jan 17 15:34:19.415: INFO: Received response from host: affinity-clusterip-bdb9c
    Jan 17 15:34:19.415: INFO: Received response from host: affinity-clusterip-bdb9c
    Jan 17 15:34:19.415: INFO: Received response from host: affinity-clusterip-bdb9c
    Jan 17 15:34:19.415: INFO: Received response from host: affinity-clusterip-bdb9c
    Jan 17 15:34:19.415: INFO: Received response from host: affinity-clusterip-bdb9c
    Jan 17 15:34:19.415: INFO: Received response from host: affinity-clusterip-bdb9c
    Jan 17 15:34:19.415: INFO: Received response from host: affinity-clusterip-bdb9c
    Jan 17 15:34:19.415: INFO: Received response from host: affinity-clusterip-bdb9c
    Jan 17 15:34:19.415: INFO: Received response from host: affinity-clusterip-bdb9c
    Jan 17 15:34:19.415: INFO: Received response from host: affinity-clusterip-bdb9c
    Jan 17 15:34:19.415: INFO: Received response from host: affinity-clusterip-bdb9c
    Jan 17 15:34:19.415: INFO: Received response from host: affinity-clusterip-bdb9c
    Jan 17 15:34:19.415: INFO: Received response from host: affinity-clusterip-bdb9c
    Jan 17 15:34:19.415: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-1423, will wait for the garbage collector to delete the pods 01/17/23 15:34:19.429
    Jan 17 15:34:19.490: INFO: Deleting ReplicationController affinity-clusterip took: 7.919289ms
    Jan 17 15:34:19.591: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.810233ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 17 15:34:22.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-1423" for this suite. 01/17/23 15:34:22.024
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:34:22.034
Jan 17 15:34:22.034: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename services 01/17/23 15:34:22.035
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:34:22.057
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:34:22.06
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-1099 01/17/23 15:34:22.063
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/17/23 15:34:22.081
STEP: creating service externalsvc in namespace services-1099 01/17/23 15:34:22.082
STEP: creating replication controller externalsvc in namespace services-1099 01/17/23 15:34:22.12
I0117 15:34:22.131548      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-1099, replica count: 2
I0117 15:34:25.182946      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 01/17/23 15:34:25.186
Jan 17 15:34:25.199: INFO: Creating new exec pod
Jan 17 15:34:25.216: INFO: Waiting up to 5m0s for pod "execpod9p9kw" in namespace "services-1099" to be "running"
Jan 17 15:34:25.219: INFO: Pod "execpod9p9kw": Phase="Pending", Reason="", readiness=false. Elapsed: 3.517542ms
Jan 17 15:34:27.223: INFO: Pod "execpod9p9kw": Phase="Running", Reason="", readiness=true. Elapsed: 2.007232203s
Jan 17 15:34:27.223: INFO: Pod "execpod9p9kw" satisfied condition "running"
Jan 17 15:34:27.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-1099 exec execpod9p9kw -- /bin/sh -x -c nslookup clusterip-service.services-1099.svc.cluster.local'
Jan 17 15:34:27.337: INFO: stderr: "+ nslookup clusterip-service.services-1099.svc.cluster.local\n"
Jan 17 15:34:27.337: INFO: stdout: "Server:\t\t172.30.0.10\nAddress:\t172.30.0.10#53\n\nclusterip-service.services-1099.svc.cluster.local\tcanonical name = externalsvc.services-1099.svc.cluster.local.\nName:\texternalsvc.services-1099.svc.cluster.local\nAddress: 172.30.69.199\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-1099, will wait for the garbage collector to delete the pods 01/17/23 15:34:27.337
Jan 17 15:34:27.397: INFO: Deleting ReplicationController externalsvc took: 6.633316ms
Jan 17 15:34:27.498: INFO: Terminating ReplicationController externalsvc pods took: 101.061893ms
Jan 17 15:34:30.026: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 17 15:34:30.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1099" for this suite. 01/17/23 15:34:30.053
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","completed":200,"skipped":3680,"failed":0}
------------------------------
• [SLOW TEST] [8.027 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:34:22.034
    Jan 17 15:34:22.034: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename services 01/17/23 15:34:22.035
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:34:22.057
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:34:22.06
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1481
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-1099 01/17/23 15:34:22.063
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/17/23 15:34:22.081
    STEP: creating service externalsvc in namespace services-1099 01/17/23 15:34:22.082
    STEP: creating replication controller externalsvc in namespace services-1099 01/17/23 15:34:22.12
    I0117 15:34:22.131548      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-1099, replica count: 2
    I0117 15:34:25.182946      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 01/17/23 15:34:25.186
    Jan 17 15:34:25.199: INFO: Creating new exec pod
    Jan 17 15:34:25.216: INFO: Waiting up to 5m0s for pod "execpod9p9kw" in namespace "services-1099" to be "running"
    Jan 17 15:34:25.219: INFO: Pod "execpod9p9kw": Phase="Pending", Reason="", readiness=false. Elapsed: 3.517542ms
    Jan 17 15:34:27.223: INFO: Pod "execpod9p9kw": Phase="Running", Reason="", readiness=true. Elapsed: 2.007232203s
    Jan 17 15:34:27.223: INFO: Pod "execpod9p9kw" satisfied condition "running"
    Jan 17 15:34:27.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-1099 exec execpod9p9kw -- /bin/sh -x -c nslookup clusterip-service.services-1099.svc.cluster.local'
    Jan 17 15:34:27.337: INFO: stderr: "+ nslookup clusterip-service.services-1099.svc.cluster.local\n"
    Jan 17 15:34:27.337: INFO: stdout: "Server:\t\t172.30.0.10\nAddress:\t172.30.0.10#53\n\nclusterip-service.services-1099.svc.cluster.local\tcanonical name = externalsvc.services-1099.svc.cluster.local.\nName:\texternalsvc.services-1099.svc.cluster.local\nAddress: 172.30.69.199\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-1099, will wait for the garbage collector to delete the pods 01/17/23 15:34:27.337
    Jan 17 15:34:27.397: INFO: Deleting ReplicationController externalsvc took: 6.633316ms
    Jan 17 15:34:27.498: INFO: Terminating ReplicationController externalsvc pods took: 101.061893ms
    Jan 17 15:34:30.026: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 17 15:34:30.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-1099" for this suite. 01/17/23 15:34:30.053
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:346
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:34:30.062
Jan 17 15:34:30.062: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename security-context-test 01/17/23 15:34:30.063
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:34:30.14
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:34:30.155
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:346
Jan 17 15:34:30.189: INFO: Waiting up to 5m0s for pod "busybox-user-65534-5727316d-3a2c-4a58-b764-8ab75ac9da6f" in namespace "security-context-test-3194" to be "Succeeded or Failed"
Jan 17 15:34:30.223: INFO: Pod "busybox-user-65534-5727316d-3a2c-4a58-b764-8ab75ac9da6f": Phase="Pending", Reason="", readiness=false. Elapsed: 33.698187ms
Jan 17 15:34:32.228: INFO: Pod "busybox-user-65534-5727316d-3a2c-4a58-b764-8ab75ac9da6f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039081483s
Jan 17 15:34:34.227: INFO: Pod "busybox-user-65534-5727316d-3a2c-4a58-b764-8ab75ac9da6f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038079468s
Jan 17 15:34:34.227: INFO: Pod "busybox-user-65534-5727316d-3a2c-4a58-b764-8ab75ac9da6f" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jan 17 15:34:34.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3194" for this suite. 01/17/23 15:34:34.231
{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","completed":201,"skipped":3712,"failed":0}
------------------------------
• [4.175 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:308
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:346

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:34:30.062
    Jan 17 15:34:30.062: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename security-context-test 01/17/23 15:34:30.063
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:34:30.14
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:34:30.155
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:346
    Jan 17 15:34:30.189: INFO: Waiting up to 5m0s for pod "busybox-user-65534-5727316d-3a2c-4a58-b764-8ab75ac9da6f" in namespace "security-context-test-3194" to be "Succeeded or Failed"
    Jan 17 15:34:30.223: INFO: Pod "busybox-user-65534-5727316d-3a2c-4a58-b764-8ab75ac9da6f": Phase="Pending", Reason="", readiness=false. Elapsed: 33.698187ms
    Jan 17 15:34:32.228: INFO: Pod "busybox-user-65534-5727316d-3a2c-4a58-b764-8ab75ac9da6f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039081483s
    Jan 17 15:34:34.227: INFO: Pod "busybox-user-65534-5727316d-3a2c-4a58-b764-8ab75ac9da6f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038079468s
    Jan 17 15:34:34.227: INFO: Pod "busybox-user-65534-5727316d-3a2c-4a58-b764-8ab75ac9da6f" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jan 17 15:34:34.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-3194" for this suite. 01/17/23 15:34:34.231
  << End Captured GinkgoWriter Output
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:34:34.238
Jan 17 15:34:34.238: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename conformance-tests 01/17/23 15:34:34.238
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:34:34.261
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:34:34.264
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 01/17/23 15:34:34.268
Jan 17 15:34:34.268: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:187
Jan 17 15:34:34.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "conformance-tests-3875" for this suite. 01/17/23 15:34:34.284
{"msg":"PASSED [sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]","completed":202,"skipped":3712,"failed":0}
------------------------------
• [0.059 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:34:34.238
    Jan 17 15:34:34.238: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename conformance-tests 01/17/23 15:34:34.238
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:34:34.261
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:34:34.264
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 01/17/23 15:34:34.268
    Jan 17 15:34:34.268: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/framework.go:187
    Jan 17 15:34:34.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "conformance-tests-3875" for this suite. 01/17/23 15:34:34.284
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:34:34.297
Jan 17 15:34:34.297: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename replication-controller 01/17/23 15:34:34.298
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:34:34.329
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:34:34.333
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109
STEP: creating a ReplicationController 01/17/23 15:34:34.341
STEP: waiting for RC to be added 01/17/23 15:34:34.348
STEP: waiting for available Replicas 01/17/23 15:34:34.349
STEP: patching ReplicationController 01/17/23 15:34:35.989
STEP: waiting for RC to be modified 01/17/23 15:34:35.997
STEP: patching ReplicationController status 01/17/23 15:34:35.997
STEP: waiting for RC to be modified 01/17/23 15:34:36.004
STEP: waiting for available Replicas 01/17/23 15:34:36.005
STEP: fetching ReplicationController status 01/17/23 15:34:36.01
STEP: patching ReplicationController scale 01/17/23 15:34:36.013
STEP: waiting for RC to be modified 01/17/23 15:34:36.018
STEP: waiting for ReplicationController's scale to be the max amount 01/17/23 15:34:36.018
STEP: fetching ReplicationController; ensuring that it's patched 01/17/23 15:34:37.586
STEP: updating ReplicationController status 01/17/23 15:34:37.589
STEP: waiting for RC to be modified 01/17/23 15:34:37.594
STEP: listing all ReplicationControllers 01/17/23 15:34:37.594
STEP: checking that ReplicationController has expected values 01/17/23 15:34:37.597
STEP: deleting ReplicationControllers by collection 01/17/23 15:34:37.597
STEP: waiting for ReplicationController to have a DELETED watchEvent 01/17/23 15:34:37.605
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Jan 17 15:34:37.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7810" for this suite. 01/17/23 15:34:37.658
{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","completed":203,"skipped":3732,"failed":0}
------------------------------
• [3.368 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:34:34.297
    Jan 17 15:34:34.297: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename replication-controller 01/17/23 15:34:34.298
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:34:34.329
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:34:34.333
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:109
    STEP: creating a ReplicationController 01/17/23 15:34:34.341
    STEP: waiting for RC to be added 01/17/23 15:34:34.348
    STEP: waiting for available Replicas 01/17/23 15:34:34.349
    STEP: patching ReplicationController 01/17/23 15:34:35.989
    STEP: waiting for RC to be modified 01/17/23 15:34:35.997
    STEP: patching ReplicationController status 01/17/23 15:34:35.997
    STEP: waiting for RC to be modified 01/17/23 15:34:36.004
    STEP: waiting for available Replicas 01/17/23 15:34:36.005
    STEP: fetching ReplicationController status 01/17/23 15:34:36.01
    STEP: patching ReplicationController scale 01/17/23 15:34:36.013
    STEP: waiting for RC to be modified 01/17/23 15:34:36.018
    STEP: waiting for ReplicationController's scale to be the max amount 01/17/23 15:34:36.018
    STEP: fetching ReplicationController; ensuring that it's patched 01/17/23 15:34:37.586
    STEP: updating ReplicationController status 01/17/23 15:34:37.589
    STEP: waiting for RC to be modified 01/17/23 15:34:37.594
    STEP: listing all ReplicationControllers 01/17/23 15:34:37.594
    STEP: checking that ReplicationController has expected values 01/17/23 15:34:37.597
    STEP: deleting ReplicationControllers by collection 01/17/23 15:34:37.597
    STEP: waiting for ReplicationController to have a DELETED watchEvent 01/17/23 15:34:37.605
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Jan 17 15:34:37.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-7810" for this suite. 01/17/23 15:34:37.658
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:34:37.665
Jan 17 15:34:37.665: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename projected 01/17/23 15:34:37.666
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:34:37.692
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:34:37.696
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46
STEP: Creating configMap with name projected-configmap-test-volume-5aa4b7a5-2b64-43f6-9ae8-bcb3257b89ab 01/17/23 15:34:37.698
STEP: Creating a pod to test consume configMaps 01/17/23 15:34:37.706
Jan 17 15:34:37.742: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-71b70009-b022-45a3-bce2-ba20b016403a" in namespace "projected-3458" to be "Succeeded or Failed"
Jan 17 15:34:37.761: INFO: Pod "pod-projected-configmaps-71b70009-b022-45a3-bce2-ba20b016403a": Phase="Pending", Reason="", readiness=false. Elapsed: 18.756122ms
Jan 17 15:34:39.765: INFO: Pod "pod-projected-configmaps-71b70009-b022-45a3-bce2-ba20b016403a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022767158s
Jan 17 15:34:41.765: INFO: Pod "pod-projected-configmaps-71b70009-b022-45a3-bce2-ba20b016403a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023302011s
STEP: Saw pod success 01/17/23 15:34:41.765
Jan 17 15:34:41.765: INFO: Pod "pod-projected-configmaps-71b70009-b022-45a3-bce2-ba20b016403a" satisfied condition "Succeeded or Failed"
Jan 17 15:34:41.770: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-projected-configmaps-71b70009-b022-45a3-bce2-ba20b016403a container agnhost-container: <nil>
STEP: delete the pod 01/17/23 15:34:41.782
Jan 17 15:34:41.800: INFO: Waiting for pod pod-projected-configmaps-71b70009-b022-45a3-bce2-ba20b016403a to disappear
Jan 17 15:34:41.803: INFO: Pod pod-projected-configmaps-71b70009-b022-45a3-bce2-ba20b016403a no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan 17 15:34:41.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3458" for this suite. 01/17/23 15:34:41.807
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","completed":204,"skipped":3739,"failed":0}
------------------------------
• [4.148 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:34:37.665
    Jan 17 15:34:37.665: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename projected 01/17/23 15:34:37.666
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:34:37.692
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:34:37.696
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:46
    STEP: Creating configMap with name projected-configmap-test-volume-5aa4b7a5-2b64-43f6-9ae8-bcb3257b89ab 01/17/23 15:34:37.698
    STEP: Creating a pod to test consume configMaps 01/17/23 15:34:37.706
    Jan 17 15:34:37.742: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-71b70009-b022-45a3-bce2-ba20b016403a" in namespace "projected-3458" to be "Succeeded or Failed"
    Jan 17 15:34:37.761: INFO: Pod "pod-projected-configmaps-71b70009-b022-45a3-bce2-ba20b016403a": Phase="Pending", Reason="", readiness=false. Elapsed: 18.756122ms
    Jan 17 15:34:39.765: INFO: Pod "pod-projected-configmaps-71b70009-b022-45a3-bce2-ba20b016403a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022767158s
    Jan 17 15:34:41.765: INFO: Pod "pod-projected-configmaps-71b70009-b022-45a3-bce2-ba20b016403a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023302011s
    STEP: Saw pod success 01/17/23 15:34:41.765
    Jan 17 15:34:41.765: INFO: Pod "pod-projected-configmaps-71b70009-b022-45a3-bce2-ba20b016403a" satisfied condition "Succeeded or Failed"
    Jan 17 15:34:41.770: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-projected-configmaps-71b70009-b022-45a3-bce2-ba20b016403a container agnhost-container: <nil>
    STEP: delete the pod 01/17/23 15:34:41.782
    Jan 17 15:34:41.800: INFO: Waiting for pod pod-projected-configmaps-71b70009-b022-45a3-bce2-ba20b016403a to disappear
    Jan 17 15:34:41.803: INFO: Pod pod-projected-configmaps-71b70009-b022-45a3-bce2-ba20b016403a no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan 17 15:34:41.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3458" for this suite. 01/17/23 15:34:41.807
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:34:41.813
Jan 17 15:34:41.813: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename replicaset 01/17/23 15:34:41.814
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:34:41.834
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:34:41.836
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
Jan 17 15:34:41.839: INFO: Creating ReplicaSet my-hostname-basic-4b397c9d-533f-48b5-9955-5c844f3a2259
W0117 15:34:41.846824      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "my-hostname-basic-4b397c9d-533f-48b5-9955-5c844f3a2259" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "my-hostname-basic-4b397c9d-533f-48b5-9955-5c844f3a2259" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "my-hostname-basic-4b397c9d-533f-48b5-9955-5c844f3a2259" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "my-hostname-basic-4b397c9d-533f-48b5-9955-5c844f3a2259" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 17 15:34:41.851: INFO: Pod name my-hostname-basic-4b397c9d-533f-48b5-9955-5c844f3a2259: Found 0 pods out of 1
Jan 17 15:34:46.857: INFO: Pod name my-hostname-basic-4b397c9d-533f-48b5-9955-5c844f3a2259: Found 1 pods out of 1
Jan 17 15:34:46.857: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-4b397c9d-533f-48b5-9955-5c844f3a2259" is running
Jan 17 15:34:46.857: INFO: Waiting up to 5m0s for pod "my-hostname-basic-4b397c9d-533f-48b5-9955-5c844f3a2259-46ncb" in namespace "replicaset-4388" to be "running"
Jan 17 15:34:46.860: INFO: Pod "my-hostname-basic-4b397c9d-533f-48b5-9955-5c844f3a2259-46ncb": Phase="Running", Reason="", readiness=true. Elapsed: 2.836536ms
Jan 17 15:34:46.860: INFO: Pod "my-hostname-basic-4b397c9d-533f-48b5-9955-5c844f3a2259-46ncb" satisfied condition "running"
Jan 17 15:34:46.860: INFO: Pod "my-hostname-basic-4b397c9d-533f-48b5-9955-5c844f3a2259-46ncb" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-17 15:34:41 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-17 15:34:42 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-17 15:34:42 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-17 15:34:41 +0000 UTC Reason: Message:}])
Jan 17 15:34:46.860: INFO: Trying to dial the pod
Jan 17 15:34:51.873: INFO: Controller my-hostname-basic-4b397c9d-533f-48b5-9955-5c844f3a2259: Got expected result from replica 1 [my-hostname-basic-4b397c9d-533f-48b5-9955-5c844f3a2259-46ncb]: "my-hostname-basic-4b397c9d-533f-48b5-9955-5c844f3a2259-46ncb", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jan 17 15:34:51.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4388" for this suite. 01/17/23 15:34:51.877
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","completed":205,"skipped":3745,"failed":0}
------------------------------
• [SLOW TEST] [10.070 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:34:41.813
    Jan 17 15:34:41.813: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename replicaset 01/17/23 15:34:41.814
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:34:41.834
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:34:41.836
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    Jan 17 15:34:41.839: INFO: Creating ReplicaSet my-hostname-basic-4b397c9d-533f-48b5-9955-5c844f3a2259
    W0117 15:34:41.846824      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "my-hostname-basic-4b397c9d-533f-48b5-9955-5c844f3a2259" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "my-hostname-basic-4b397c9d-533f-48b5-9955-5c844f3a2259" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "my-hostname-basic-4b397c9d-533f-48b5-9955-5c844f3a2259" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "my-hostname-basic-4b397c9d-533f-48b5-9955-5c844f3a2259" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jan 17 15:34:41.851: INFO: Pod name my-hostname-basic-4b397c9d-533f-48b5-9955-5c844f3a2259: Found 0 pods out of 1
    Jan 17 15:34:46.857: INFO: Pod name my-hostname-basic-4b397c9d-533f-48b5-9955-5c844f3a2259: Found 1 pods out of 1
    Jan 17 15:34:46.857: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-4b397c9d-533f-48b5-9955-5c844f3a2259" is running
    Jan 17 15:34:46.857: INFO: Waiting up to 5m0s for pod "my-hostname-basic-4b397c9d-533f-48b5-9955-5c844f3a2259-46ncb" in namespace "replicaset-4388" to be "running"
    Jan 17 15:34:46.860: INFO: Pod "my-hostname-basic-4b397c9d-533f-48b5-9955-5c844f3a2259-46ncb": Phase="Running", Reason="", readiness=true. Elapsed: 2.836536ms
    Jan 17 15:34:46.860: INFO: Pod "my-hostname-basic-4b397c9d-533f-48b5-9955-5c844f3a2259-46ncb" satisfied condition "running"
    Jan 17 15:34:46.860: INFO: Pod "my-hostname-basic-4b397c9d-533f-48b5-9955-5c844f3a2259-46ncb" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-17 15:34:41 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-17 15:34:42 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-17 15:34:42 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-17 15:34:41 +0000 UTC Reason: Message:}])
    Jan 17 15:34:46.860: INFO: Trying to dial the pod
    Jan 17 15:34:51.873: INFO: Controller my-hostname-basic-4b397c9d-533f-48b5-9955-5c844f3a2259: Got expected result from replica 1 [my-hostname-basic-4b397c9d-533f-48b5-9955-5c844f3a2259-46ncb]: "my-hostname-basic-4b397c9d-533f-48b5-9955-5c844f3a2259-46ncb", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jan 17 15:34:51.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-4388" for this suite. 01/17/23 15:34:51.877
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:34:51.884
Jan 17 15:34:51.884: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename secrets 01/17/23 15:34:51.885
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:34:51.906
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:34:51.908
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124
STEP: Creating secret with name secret-test-3023f3d0-5bba-4f9d-b773-51d7fd401916 01/17/23 15:34:51.911
STEP: Creating a pod to test consume secrets 01/17/23 15:34:51.92
Jan 17 15:34:51.954: INFO: Waiting up to 5m0s for pod "pod-secrets-fecceba1-b6cc-4800-98f9-8d892f895b01" in namespace "secrets-4035" to be "Succeeded or Failed"
Jan 17 15:34:51.958: INFO: Pod "pod-secrets-fecceba1-b6cc-4800-98f9-8d892f895b01": Phase="Pending", Reason="", readiness=false. Elapsed: 3.652969ms
Jan 17 15:34:53.962: INFO: Pod "pod-secrets-fecceba1-b6cc-4800-98f9-8d892f895b01": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00747814s
Jan 17 15:34:55.961: INFO: Pod "pod-secrets-fecceba1-b6cc-4800-98f9-8d892f895b01": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00698154s
STEP: Saw pod success 01/17/23 15:34:55.961
Jan 17 15:34:55.962: INFO: Pod "pod-secrets-fecceba1-b6cc-4800-98f9-8d892f895b01" satisfied condition "Succeeded or Failed"
Jan 17 15:34:55.965: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-secrets-fecceba1-b6cc-4800-98f9-8d892f895b01 container secret-volume-test: <nil>
STEP: delete the pod 01/17/23 15:34:55.971
Jan 17 15:34:55.982: INFO: Waiting for pod pod-secrets-fecceba1-b6cc-4800-98f9-8d892f895b01 to disappear
Jan 17 15:34:55.985: INFO: Pod pod-secrets-fecceba1-b6cc-4800-98f9-8d892f895b01 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan 17 15:34:55.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4035" for this suite. 01/17/23 15:34:55.989
{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","completed":206,"skipped":3750,"failed":0}
------------------------------
• [4.112 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:34:51.884
    Jan 17 15:34:51.884: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename secrets 01/17/23 15:34:51.885
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:34:51.906
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:34:51.908
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:124
    STEP: Creating secret with name secret-test-3023f3d0-5bba-4f9d-b773-51d7fd401916 01/17/23 15:34:51.911
    STEP: Creating a pod to test consume secrets 01/17/23 15:34:51.92
    Jan 17 15:34:51.954: INFO: Waiting up to 5m0s for pod "pod-secrets-fecceba1-b6cc-4800-98f9-8d892f895b01" in namespace "secrets-4035" to be "Succeeded or Failed"
    Jan 17 15:34:51.958: INFO: Pod "pod-secrets-fecceba1-b6cc-4800-98f9-8d892f895b01": Phase="Pending", Reason="", readiness=false. Elapsed: 3.652969ms
    Jan 17 15:34:53.962: INFO: Pod "pod-secrets-fecceba1-b6cc-4800-98f9-8d892f895b01": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00747814s
    Jan 17 15:34:55.961: INFO: Pod "pod-secrets-fecceba1-b6cc-4800-98f9-8d892f895b01": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00698154s
    STEP: Saw pod success 01/17/23 15:34:55.961
    Jan 17 15:34:55.962: INFO: Pod "pod-secrets-fecceba1-b6cc-4800-98f9-8d892f895b01" satisfied condition "Succeeded or Failed"
    Jan 17 15:34:55.965: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-secrets-fecceba1-b6cc-4800-98f9-8d892f895b01 container secret-volume-test: <nil>
    STEP: delete the pod 01/17/23 15:34:55.971
    Jan 17 15:34:55.982: INFO: Waiting for pod pod-secrets-fecceba1-b6cc-4800-98f9-8d892f895b01 to disappear
    Jan 17 15:34:55.985: INFO: Pod pod-secrets-fecceba1-b6cc-4800-98f9-8d892f895b01 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan 17 15:34:55.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-4035" for this suite. 01/17/23 15:34:55.989
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:34:55.997
Jan 17 15:34:55.997: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename projected 01/17/23 15:34:55.998
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:34:56.022
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:34:56.025
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45
STEP: Creating projection with secret that has name projected-secret-test-ec0e4409-1b54-43ee-8450-0e66614562c9 01/17/23 15:34:56.027
STEP: Creating a pod to test consume secrets 01/17/23 15:34:56.035
Jan 17 15:34:56.067: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a1c0c01c-6e26-4e5f-ae97-91f9bc5e3643" in namespace "projected-5682" to be "Succeeded or Failed"
Jan 17 15:34:56.074: INFO: Pod "pod-projected-secrets-a1c0c01c-6e26-4e5f-ae97-91f9bc5e3643": Phase="Pending", Reason="", readiness=false. Elapsed: 6.808737ms
Jan 17 15:34:58.079: INFO: Pod "pod-projected-secrets-a1c0c01c-6e26-4e5f-ae97-91f9bc5e3643": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011480201s
Jan 17 15:35:00.079: INFO: Pod "pod-projected-secrets-a1c0c01c-6e26-4e5f-ae97-91f9bc5e3643": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012054731s
STEP: Saw pod success 01/17/23 15:35:00.079
Jan 17 15:35:00.079: INFO: Pod "pod-projected-secrets-a1c0c01c-6e26-4e5f-ae97-91f9bc5e3643" satisfied condition "Succeeded or Failed"
Jan 17 15:35:00.083: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-projected-secrets-a1c0c01c-6e26-4e5f-ae97-91f9bc5e3643 container projected-secret-volume-test: <nil>
STEP: delete the pod 01/17/23 15:35:00.089
Jan 17 15:35:00.101: INFO: Waiting for pod pod-projected-secrets-a1c0c01c-6e26-4e5f-ae97-91f9bc5e3643 to disappear
Jan 17 15:35:00.105: INFO: Pod pod-projected-secrets-a1c0c01c-6e26-4e5f-ae97-91f9bc5e3643 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan 17 15:35:00.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5682" for this suite. 01/17/23 15:35:00.109
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","completed":207,"skipped":3786,"failed":0}
------------------------------
• [4.129 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:34:55.997
    Jan 17 15:34:55.997: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename projected 01/17/23 15:34:55.998
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:34:56.022
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:34:56.025
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:45
    STEP: Creating projection with secret that has name projected-secret-test-ec0e4409-1b54-43ee-8450-0e66614562c9 01/17/23 15:34:56.027
    STEP: Creating a pod to test consume secrets 01/17/23 15:34:56.035
    Jan 17 15:34:56.067: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a1c0c01c-6e26-4e5f-ae97-91f9bc5e3643" in namespace "projected-5682" to be "Succeeded or Failed"
    Jan 17 15:34:56.074: INFO: Pod "pod-projected-secrets-a1c0c01c-6e26-4e5f-ae97-91f9bc5e3643": Phase="Pending", Reason="", readiness=false. Elapsed: 6.808737ms
    Jan 17 15:34:58.079: INFO: Pod "pod-projected-secrets-a1c0c01c-6e26-4e5f-ae97-91f9bc5e3643": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011480201s
    Jan 17 15:35:00.079: INFO: Pod "pod-projected-secrets-a1c0c01c-6e26-4e5f-ae97-91f9bc5e3643": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012054731s
    STEP: Saw pod success 01/17/23 15:35:00.079
    Jan 17 15:35:00.079: INFO: Pod "pod-projected-secrets-a1c0c01c-6e26-4e5f-ae97-91f9bc5e3643" satisfied condition "Succeeded or Failed"
    Jan 17 15:35:00.083: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-projected-secrets-a1c0c01c-6e26-4e5f-ae97-91f9bc5e3643 container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/17/23 15:35:00.089
    Jan 17 15:35:00.101: INFO: Waiting for pod pod-projected-secrets-a1c0c01c-6e26-4e5f-ae97-91f9bc5e3643 to disappear
    Jan 17 15:35:00.105: INFO: Pod pod-projected-secrets-a1c0c01c-6e26-4e5f-ae97-91f9bc5e3643 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan 17 15:35:00.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5682" for this suite. 01/17/23 15:35:00.109
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:35:00.126
Jan 17 15:35:00.126: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename projected 01/17/23 15:35:00.127
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:35:00.145
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:35:00.147
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73
STEP: Creating configMap with name projected-configmap-test-volume-cbfdc9a3-bd3d-4577-b146-d100dfa60bf1 01/17/23 15:35:00.152
STEP: Creating a pod to test consume configMaps 01/17/23 15:35:00.163
Jan 17 15:35:00.214: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-42099730-5126-4ae2-85ee-caacd3c210e1" in namespace "projected-6105" to be "Succeeded or Failed"
Jan 17 15:35:00.217: INFO: Pod "pod-projected-configmaps-42099730-5126-4ae2-85ee-caacd3c210e1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.895947ms
Jan 17 15:35:02.222: INFO: Pod "pod-projected-configmaps-42099730-5126-4ae2-85ee-caacd3c210e1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007719871s
Jan 17 15:35:04.221: INFO: Pod "pod-projected-configmaps-42099730-5126-4ae2-85ee-caacd3c210e1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006960663s
STEP: Saw pod success 01/17/23 15:35:04.221
Jan 17 15:35:04.221: INFO: Pod "pod-projected-configmaps-42099730-5126-4ae2-85ee-caacd3c210e1" satisfied condition "Succeeded or Failed"
Jan 17 15:35:04.224: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-projected-configmaps-42099730-5126-4ae2-85ee-caacd3c210e1 container agnhost-container: <nil>
STEP: delete the pod 01/17/23 15:35:04.23
Jan 17 15:35:04.242: INFO: Waiting for pod pod-projected-configmaps-42099730-5126-4ae2-85ee-caacd3c210e1 to disappear
Jan 17 15:35:04.245: INFO: Pod pod-projected-configmaps-42099730-5126-4ae2-85ee-caacd3c210e1 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan 17 15:35:04.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6105" for this suite. 01/17/23 15:35:04.249
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","completed":208,"skipped":3796,"failed":0}
------------------------------
• [4.129 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:35:00.126
    Jan 17 15:35:00.126: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename projected 01/17/23 15:35:00.127
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:35:00.145
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:35:00.147
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:73
    STEP: Creating configMap with name projected-configmap-test-volume-cbfdc9a3-bd3d-4577-b146-d100dfa60bf1 01/17/23 15:35:00.152
    STEP: Creating a pod to test consume configMaps 01/17/23 15:35:00.163
    Jan 17 15:35:00.214: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-42099730-5126-4ae2-85ee-caacd3c210e1" in namespace "projected-6105" to be "Succeeded or Failed"
    Jan 17 15:35:00.217: INFO: Pod "pod-projected-configmaps-42099730-5126-4ae2-85ee-caacd3c210e1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.895947ms
    Jan 17 15:35:02.222: INFO: Pod "pod-projected-configmaps-42099730-5126-4ae2-85ee-caacd3c210e1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007719871s
    Jan 17 15:35:04.221: INFO: Pod "pod-projected-configmaps-42099730-5126-4ae2-85ee-caacd3c210e1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006960663s
    STEP: Saw pod success 01/17/23 15:35:04.221
    Jan 17 15:35:04.221: INFO: Pod "pod-projected-configmaps-42099730-5126-4ae2-85ee-caacd3c210e1" satisfied condition "Succeeded or Failed"
    Jan 17 15:35:04.224: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-projected-configmaps-42099730-5126-4ae2-85ee-caacd3c210e1 container agnhost-container: <nil>
    STEP: delete the pod 01/17/23 15:35:04.23
    Jan 17 15:35:04.242: INFO: Waiting for pod pod-projected-configmaps-42099730-5126-4ae2-85ee-caacd3c210e1 to disappear
    Jan 17 15:35:04.245: INFO: Pod pod-projected-configmaps-42099730-5126-4ae2-85ee-caacd3c210e1 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan 17 15:35:04.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6105" for this suite. 01/17/23 15:35:04.249
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:35:04.257
Jan 17 15:35:04.257: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename projected 01/17/23 15:35:04.257
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:35:04.278
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:35:04.282
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118
STEP: Creating secret with name projected-secret-test-ae368ebf-b9e5-4d28-8693-8433e27b37fb 01/17/23 15:35:04.285
STEP: Creating a pod to test consume secrets 01/17/23 15:35:04.295
Jan 17 15:35:04.330: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-95ceb967-65e2-49a5-aa76-95f18016bca7" in namespace "projected-5836" to be "Succeeded or Failed"
Jan 17 15:35:04.339: INFO: Pod "pod-projected-secrets-95ceb967-65e2-49a5-aa76-95f18016bca7": Phase="Pending", Reason="", readiness=false. Elapsed: 9.058417ms
Jan 17 15:35:06.344: INFO: Pod "pod-projected-secrets-95ceb967-65e2-49a5-aa76-95f18016bca7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013962983s
Jan 17 15:35:08.344: INFO: Pod "pod-projected-secrets-95ceb967-65e2-49a5-aa76-95f18016bca7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013594747s
STEP: Saw pod success 01/17/23 15:35:08.344
Jan 17 15:35:08.344: INFO: Pod "pod-projected-secrets-95ceb967-65e2-49a5-aa76-95f18016bca7" satisfied condition "Succeeded or Failed"
Jan 17 15:35:08.348: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-projected-secrets-95ceb967-65e2-49a5-aa76-95f18016bca7 container secret-volume-test: <nil>
STEP: delete the pod 01/17/23 15:35:08.354
Jan 17 15:35:08.366: INFO: Waiting for pod pod-projected-secrets-95ceb967-65e2-49a5-aa76-95f18016bca7 to disappear
Jan 17 15:35:08.369: INFO: Pod pod-projected-secrets-95ceb967-65e2-49a5-aa76-95f18016bca7 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan 17 15:35:08.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5836" for this suite. 01/17/23 15:35:08.373
{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","completed":209,"skipped":3873,"failed":0}
------------------------------
• [4.122 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:35:04.257
    Jan 17 15:35:04.257: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename projected 01/17/23 15:35:04.257
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:35:04.278
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:35:04.282
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:118
    STEP: Creating secret with name projected-secret-test-ae368ebf-b9e5-4d28-8693-8433e27b37fb 01/17/23 15:35:04.285
    STEP: Creating a pod to test consume secrets 01/17/23 15:35:04.295
    Jan 17 15:35:04.330: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-95ceb967-65e2-49a5-aa76-95f18016bca7" in namespace "projected-5836" to be "Succeeded or Failed"
    Jan 17 15:35:04.339: INFO: Pod "pod-projected-secrets-95ceb967-65e2-49a5-aa76-95f18016bca7": Phase="Pending", Reason="", readiness=false. Elapsed: 9.058417ms
    Jan 17 15:35:06.344: INFO: Pod "pod-projected-secrets-95ceb967-65e2-49a5-aa76-95f18016bca7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013962983s
    Jan 17 15:35:08.344: INFO: Pod "pod-projected-secrets-95ceb967-65e2-49a5-aa76-95f18016bca7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013594747s
    STEP: Saw pod success 01/17/23 15:35:08.344
    Jan 17 15:35:08.344: INFO: Pod "pod-projected-secrets-95ceb967-65e2-49a5-aa76-95f18016bca7" satisfied condition "Succeeded or Failed"
    Jan 17 15:35:08.348: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-projected-secrets-95ceb967-65e2-49a5-aa76-95f18016bca7 container secret-volume-test: <nil>
    STEP: delete the pod 01/17/23 15:35:08.354
    Jan 17 15:35:08.366: INFO: Waiting for pod pod-projected-secrets-95ceb967-65e2-49a5-aa76-95f18016bca7 to disappear
    Jan 17 15:35:08.369: INFO: Pod pod-projected-secrets-95ceb967-65e2-49a5-aa76-95f18016bca7 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan 17 15:35:08.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5836" for this suite. 01/17/23 15:35:08.373
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:35:08.379
Jan 17 15:35:08.379: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename endpointslice 01/17/23 15:35:08.38
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:35:08.406
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:35:08.409
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204
STEP: referencing a single matching pod 01/17/23 15:35:13.553
STEP: referencing matching pods with named port 01/17/23 15:35:18.56
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 01/17/23 15:35:23.568
STEP: recreating EndpointSlices after they've been deleted 01/17/23 15:35:28.574
Jan 17 15:35:28.594: INFO: EndpointSlice for Service endpointslice-4846/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Jan 17 15:35:38.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-4846" for this suite. 01/17/23 15:35:38.606
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","completed":210,"skipped":3906,"failed":0}
------------------------------
• [SLOW TEST] [30.232 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:35:08.379
    Jan 17 15:35:08.379: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename endpointslice 01/17/23 15:35:08.38
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:35:08.406
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:35:08.409
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:204
    STEP: referencing a single matching pod 01/17/23 15:35:13.553
    STEP: referencing matching pods with named port 01/17/23 15:35:18.56
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 01/17/23 15:35:23.568
    STEP: recreating EndpointSlices after they've been deleted 01/17/23 15:35:28.574
    Jan 17 15:35:28.594: INFO: EndpointSlice for Service endpointslice-4846/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Jan 17 15:35:38.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-4846" for this suite. 01/17/23 15:35:38.606
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:35:38.612
Jan 17 15:35:38.612: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename container-probe 01/17/23 15:35:38.613
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:35:38.659
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:35:38.664
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195
STEP: Creating pod liveness-59a40493-b658-4f01-b4c0-a72374e75b06 in namespace container-probe-4900 01/17/23 15:35:38.67
W0117 15:35:38.698002      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 17 15:35:38.698: INFO: Waiting up to 5m0s for pod "liveness-59a40493-b658-4f01-b4c0-a72374e75b06" in namespace "container-probe-4900" to be "not pending"
Jan 17 15:35:38.708: INFO: Pod "liveness-59a40493-b658-4f01-b4c0-a72374e75b06": Phase="Pending", Reason="", readiness=false. Elapsed: 10.403302ms
Jan 17 15:35:40.712: INFO: Pod "liveness-59a40493-b658-4f01-b4c0-a72374e75b06": Phase="Running", Reason="", readiness=true. Elapsed: 2.014375693s
Jan 17 15:35:40.712: INFO: Pod "liveness-59a40493-b658-4f01-b4c0-a72374e75b06" satisfied condition "not pending"
Jan 17 15:35:40.712: INFO: Started pod liveness-59a40493-b658-4f01-b4c0-a72374e75b06 in namespace container-probe-4900
STEP: checking the pod's current state and verifying that restartCount is present 01/17/23 15:35:40.712
Jan 17 15:35:40.715: INFO: Initial restart count of pod liveness-59a40493-b658-4f01-b4c0-a72374e75b06 is 0
Jan 17 15:36:00.761: INFO: Restart count of pod container-probe-4900/liveness-59a40493-b658-4f01-b4c0-a72374e75b06 is now 1 (20.045057281s elapsed)
Jan 17 15:36:20.805: INFO: Restart count of pod container-probe-4900/liveness-59a40493-b658-4f01-b4c0-a72374e75b06 is now 2 (40.089632337s elapsed)
Jan 17 15:36:40.867: INFO: Restart count of pod container-probe-4900/liveness-59a40493-b658-4f01-b4c0-a72374e75b06 is now 3 (1m0.151641868s elapsed)
Jan 17 15:37:00.910: INFO: Restart count of pod container-probe-4900/liveness-59a40493-b658-4f01-b4c0-a72374e75b06 is now 4 (1m20.194751744s elapsed)
Jan 17 15:38:13.067: INFO: Restart count of pod container-probe-4900/liveness-59a40493-b658-4f01-b4c0-a72374e75b06 is now 5 (2m32.351806792s elapsed)
STEP: deleting the pod 01/17/23 15:38:13.067
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan 17 15:38:13.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4900" for this suite. 01/17/23 15:38:13.085
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","completed":211,"skipped":3921,"failed":0}
------------------------------
• [SLOW TEST] [154.481 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:35:38.612
    Jan 17 15:35:38.612: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename container-probe 01/17/23 15:35:38.613
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:35:38.659
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:35:38.664
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:195
    STEP: Creating pod liveness-59a40493-b658-4f01-b4c0-a72374e75b06 in namespace container-probe-4900 01/17/23 15:35:38.67
    W0117 15:35:38.698002      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jan 17 15:35:38.698: INFO: Waiting up to 5m0s for pod "liveness-59a40493-b658-4f01-b4c0-a72374e75b06" in namespace "container-probe-4900" to be "not pending"
    Jan 17 15:35:38.708: INFO: Pod "liveness-59a40493-b658-4f01-b4c0-a72374e75b06": Phase="Pending", Reason="", readiness=false. Elapsed: 10.403302ms
    Jan 17 15:35:40.712: INFO: Pod "liveness-59a40493-b658-4f01-b4c0-a72374e75b06": Phase="Running", Reason="", readiness=true. Elapsed: 2.014375693s
    Jan 17 15:35:40.712: INFO: Pod "liveness-59a40493-b658-4f01-b4c0-a72374e75b06" satisfied condition "not pending"
    Jan 17 15:35:40.712: INFO: Started pod liveness-59a40493-b658-4f01-b4c0-a72374e75b06 in namespace container-probe-4900
    STEP: checking the pod's current state and verifying that restartCount is present 01/17/23 15:35:40.712
    Jan 17 15:35:40.715: INFO: Initial restart count of pod liveness-59a40493-b658-4f01-b4c0-a72374e75b06 is 0
    Jan 17 15:36:00.761: INFO: Restart count of pod container-probe-4900/liveness-59a40493-b658-4f01-b4c0-a72374e75b06 is now 1 (20.045057281s elapsed)
    Jan 17 15:36:20.805: INFO: Restart count of pod container-probe-4900/liveness-59a40493-b658-4f01-b4c0-a72374e75b06 is now 2 (40.089632337s elapsed)
    Jan 17 15:36:40.867: INFO: Restart count of pod container-probe-4900/liveness-59a40493-b658-4f01-b4c0-a72374e75b06 is now 3 (1m0.151641868s elapsed)
    Jan 17 15:37:00.910: INFO: Restart count of pod container-probe-4900/liveness-59a40493-b658-4f01-b4c0-a72374e75b06 is now 4 (1m20.194751744s elapsed)
    Jan 17 15:38:13.067: INFO: Restart count of pod container-probe-4900/liveness-59a40493-b658-4f01-b4c0-a72374e75b06 is now 5 (2m32.351806792s elapsed)
    STEP: deleting the pod 01/17/23 15:38:13.067
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan 17 15:38:13.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-4900" for this suite. 01/17/23 15:38:13.085
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:38:13.094
Jan 17 15:38:13.094: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename resourcequota 01/17/23 15:38:13.095
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:38:13.119
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:38:13.122
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933
STEP: Creating a ResourceQuota 01/17/23 15:38:13.124
STEP: Getting a ResourceQuota 01/17/23 15:38:13.142
STEP: Listing all ResourceQuotas with LabelSelector 01/17/23 15:38:13.151
STEP: Patching the ResourceQuota 01/17/23 15:38:13.155
STEP: Deleting a Collection of ResourceQuotas 01/17/23 15:38:13.162
STEP: Verifying the deleted ResourceQuota 01/17/23 15:38:13.181
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 17 15:38:13.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3528" for this suite. 01/17/23 15:38:13.192
{"msg":"PASSED [sig-api-machinery] ResourceQuota should manage the lifecycle of a ResourceQuota [Conformance]","completed":212,"skipped":3922,"failed":0}
------------------------------
• [0.107 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:38:13.094
    Jan 17 15:38:13.094: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename resourcequota 01/17/23 15:38:13.095
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:38:13.119
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:38:13.122
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:933
    STEP: Creating a ResourceQuota 01/17/23 15:38:13.124
    STEP: Getting a ResourceQuota 01/17/23 15:38:13.142
    STEP: Listing all ResourceQuotas with LabelSelector 01/17/23 15:38:13.151
    STEP: Patching the ResourceQuota 01/17/23 15:38:13.155
    STEP: Deleting a Collection of ResourceQuotas 01/17/23 15:38:13.162
    STEP: Verifying the deleted ResourceQuota 01/17/23 15:38:13.181
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 17 15:38:13.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-3528" for this suite. 01/17/23 15:38:13.192
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:38:13.201
Jan 17 15:38:13.201: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename emptydir 01/17/23 15:38:13.202
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:38:13.222
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:38:13.224
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176
STEP: Creating a pod to test emptydir 0666 on node default medium 01/17/23 15:38:13.227
Jan 17 15:38:13.263: INFO: Waiting up to 5m0s for pod "pod-444bc9d3-af02-46fb-ad93-7cc0985fe66b" in namespace "emptydir-8185" to be "Succeeded or Failed"
Jan 17 15:38:13.271: INFO: Pod "pod-444bc9d3-af02-46fb-ad93-7cc0985fe66b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.718291ms
Jan 17 15:38:15.276: INFO: Pod "pod-444bc9d3-af02-46fb-ad93-7cc0985fe66b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013842925s
Jan 17 15:38:17.275: INFO: Pod "pod-444bc9d3-af02-46fb-ad93-7cc0985fe66b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012464351s
STEP: Saw pod success 01/17/23 15:38:17.275
Jan 17 15:38:17.275: INFO: Pod "pod-444bc9d3-af02-46fb-ad93-7cc0985fe66b" satisfied condition "Succeeded or Failed"
Jan 17 15:38:17.278: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-444bc9d3-af02-46fb-ad93-7cc0985fe66b container test-container: <nil>
STEP: delete the pod 01/17/23 15:38:17.287
Jan 17 15:38:17.297: INFO: Waiting for pod pod-444bc9d3-af02-46fb-ad93-7cc0985fe66b to disappear
Jan 17 15:38:17.302: INFO: Pod pod-444bc9d3-af02-46fb-ad93-7cc0985fe66b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 17 15:38:17.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8185" for this suite. 01/17/23 15:38:17.306
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":213,"skipped":3922,"failed":0}
------------------------------
• [4.110 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:38:13.201
    Jan 17 15:38:13.201: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename emptydir 01/17/23 15:38:13.202
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:38:13.222
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:38:13.224
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:176
    STEP: Creating a pod to test emptydir 0666 on node default medium 01/17/23 15:38:13.227
    Jan 17 15:38:13.263: INFO: Waiting up to 5m0s for pod "pod-444bc9d3-af02-46fb-ad93-7cc0985fe66b" in namespace "emptydir-8185" to be "Succeeded or Failed"
    Jan 17 15:38:13.271: INFO: Pod "pod-444bc9d3-af02-46fb-ad93-7cc0985fe66b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.718291ms
    Jan 17 15:38:15.276: INFO: Pod "pod-444bc9d3-af02-46fb-ad93-7cc0985fe66b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013842925s
    Jan 17 15:38:17.275: INFO: Pod "pod-444bc9d3-af02-46fb-ad93-7cc0985fe66b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012464351s
    STEP: Saw pod success 01/17/23 15:38:17.275
    Jan 17 15:38:17.275: INFO: Pod "pod-444bc9d3-af02-46fb-ad93-7cc0985fe66b" satisfied condition "Succeeded or Failed"
    Jan 17 15:38:17.278: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-444bc9d3-af02-46fb-ad93-7cc0985fe66b container test-container: <nil>
    STEP: delete the pod 01/17/23 15:38:17.287
    Jan 17 15:38:17.297: INFO: Waiting for pod pod-444bc9d3-af02-46fb-ad93-7cc0985fe66b to disappear
    Jan 17 15:38:17.302: INFO: Pod pod-444bc9d3-af02-46fb-ad93-7cc0985fe66b no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 17 15:38:17.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-8185" for this suite. 01/17/23 15:38:17.306
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:38:17.312
Jan 17 15:38:17.312: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename downward-api 01/17/23 15:38:17.313
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:38:17.344
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:38:17.347
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43
STEP: Creating a pod to test downward api env vars 01/17/23 15:38:17.349
Jan 17 15:38:17.376: INFO: Waiting up to 5m0s for pod "downward-api-47e7cf22-eee9-4606-93b6-04b248c7af00" in namespace "downward-api-6798" to be "Succeeded or Failed"
Jan 17 15:38:17.400: INFO: Pod "downward-api-47e7cf22-eee9-4606-93b6-04b248c7af00": Phase="Pending", Reason="", readiness=false. Elapsed: 24.33939ms
Jan 17 15:38:19.407: INFO: Pod "downward-api-47e7cf22-eee9-4606-93b6-04b248c7af00": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031030756s
Jan 17 15:38:21.405: INFO: Pod "downward-api-47e7cf22-eee9-4606-93b6-04b248c7af00": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029007153s
STEP: Saw pod success 01/17/23 15:38:21.405
Jan 17 15:38:21.405: INFO: Pod "downward-api-47e7cf22-eee9-4606-93b6-04b248c7af00" satisfied condition "Succeeded or Failed"
Jan 17 15:38:21.408: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod downward-api-47e7cf22-eee9-4606-93b6-04b248c7af00 container dapi-container: <nil>
STEP: delete the pod 01/17/23 15:38:21.415
Jan 17 15:38:21.429: INFO: Waiting for pod downward-api-47e7cf22-eee9-4606-93b6-04b248c7af00 to disappear
Jan 17 15:38:21.432: INFO: Pod downward-api-47e7cf22-eee9-4606-93b6-04b248c7af00 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Jan 17 15:38:21.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6798" for this suite. 01/17/23 15:38:21.436
{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","completed":214,"skipped":3928,"failed":0}
------------------------------
• [4.129 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:38:17.312
    Jan 17 15:38:17.312: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename downward-api 01/17/23 15:38:17.313
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:38:17.344
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:38:17.347
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:43
    STEP: Creating a pod to test downward api env vars 01/17/23 15:38:17.349
    Jan 17 15:38:17.376: INFO: Waiting up to 5m0s for pod "downward-api-47e7cf22-eee9-4606-93b6-04b248c7af00" in namespace "downward-api-6798" to be "Succeeded or Failed"
    Jan 17 15:38:17.400: INFO: Pod "downward-api-47e7cf22-eee9-4606-93b6-04b248c7af00": Phase="Pending", Reason="", readiness=false. Elapsed: 24.33939ms
    Jan 17 15:38:19.407: INFO: Pod "downward-api-47e7cf22-eee9-4606-93b6-04b248c7af00": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031030756s
    Jan 17 15:38:21.405: INFO: Pod "downward-api-47e7cf22-eee9-4606-93b6-04b248c7af00": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029007153s
    STEP: Saw pod success 01/17/23 15:38:21.405
    Jan 17 15:38:21.405: INFO: Pod "downward-api-47e7cf22-eee9-4606-93b6-04b248c7af00" satisfied condition "Succeeded or Failed"
    Jan 17 15:38:21.408: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod downward-api-47e7cf22-eee9-4606-93b6-04b248c7af00 container dapi-container: <nil>
    STEP: delete the pod 01/17/23 15:38:21.415
    Jan 17 15:38:21.429: INFO: Waiting for pod downward-api-47e7cf22-eee9-4606-93b6-04b248c7af00 to disappear
    Jan 17 15:38:21.432: INFO: Pod downward-api-47e7cf22-eee9-4606-93b6-04b248c7af00 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Jan 17 15:38:21.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-6798" for this suite. 01/17/23 15:38:21.436
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:38:21.442
Jan 17 15:38:21.442: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename sched-pred 01/17/23 15:38:21.443
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:38:21.465
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:38:21.467
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jan 17 15:38:21.469: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 17 15:38:21.488: INFO: Waiting for terminating namespaces to be deleted...
Jan 17 15:38:21.494: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-139-213.ec2.internal before test
Jan 17 15:38:21.522: INFO: aws-ebs-csi-driver-node-5tmvr from openshift-cluster-csi-drivers started at 2023-01-17 12:55:50 +0000 UTC (3 container statuses recorded)
Jan 17 15:38:21.522: INFO: 	Container csi-driver ready: true, restart count 0
Jan 17 15:38:21.522: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Jan 17 15:38:21.522: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
Jan 17 15:38:21.522: INFO: tuned-jzlnj from openshift-cluster-node-tuning-operator started at 2023-01-17 12:55:50 +0000 UTC (1 container statuses recorded)
Jan 17 15:38:21.522: INFO: 	Container tuned ready: true, restart count 0
Jan 17 15:38:21.522: INFO: downloads-8d695cd69-ltk55 from openshift-console started at 2023-01-17 12:57:39 +0000 UTC (1 container statuses recorded)
Jan 17 15:38:21.522: INFO: 	Container download-server ready: true, restart count 0
Jan 17 15:38:21.522: INFO: dns-default-jgzr9 from openshift-dns started at 2023-01-17 12:56:45 +0000 UTC (2 container statuses recorded)
Jan 17 15:38:21.522: INFO: 	Container dns ready: true, restart count 0
Jan 17 15:38:21.522: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 17 15:38:21.522: INFO: node-resolver-gkxnp from openshift-dns started at 2023-01-17 12:55:50 +0000 UTC (1 container statuses recorded)
Jan 17 15:38:21.522: INFO: 	Container dns-node-resolver ready: true, restart count 0
Jan 17 15:38:21.522: INFO: image-registry-6d685bd45d-mk7wb from openshift-image-registry started at 2023-01-17 12:56:45 +0000 UTC (1 container statuses recorded)
Jan 17 15:38:21.522: INFO: 	Container registry ready: true, restart count 0
Jan 17 15:38:21.522: INFO: node-ca-4l49x from openshift-image-registry started at 2023-01-17 12:55:50 +0000 UTC (1 container statuses recorded)
Jan 17 15:38:21.522: INFO: 	Container node-ca ready: true, restart count 0
Jan 17 15:38:21.522: INFO: ingress-canary-xqzwm from openshift-ingress-canary started at 2023-01-17 12:56:45 +0000 UTC (1 container statuses recorded)
Jan 17 15:38:21.522: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Jan 17 15:38:21.522: INFO: router-default-c95cc587f-9clvn from openshift-ingress started at 2023-01-17 12:56:45 +0000 UTC (1 container statuses recorded)
Jan 17 15:38:21.522: INFO: 	Container router ready: true, restart count 0
Jan 17 15:38:21.522: INFO: machine-config-daemon-6hjqr from openshift-machine-config-operator started at 2023-01-17 12:55:50 +0000 UTC (2 container statuses recorded)
Jan 17 15:38:21.522: INFO: 	Container machine-config-daemon ready: true, restart count 0
Jan 17 15:38:21.522: INFO: 	Container oauth-proxy ready: true, restart count 0
Jan 17 15:38:21.522: INFO: alertmanager-main-0 from openshift-monitoring started at 2023-01-17 12:59:05 +0000 UTC (6 container statuses recorded)
Jan 17 15:38:21.522: INFO: 	Container alertmanager ready: true, restart count 1
Jan 17 15:38:21.522: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jan 17 15:38:21.522: INFO: 	Container config-reloader ready: true, restart count 0
Jan 17 15:38:21.522: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 17 15:38:21.522: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Jan 17 15:38:21.522: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jan 17 15:38:21.522: INFO: node-exporter-lwbc8 from openshift-monitoring started at 2023-01-17 12:56:55 +0000 UTC (2 container statuses recorded)
Jan 17 15:38:21.522: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 17 15:38:21.522: INFO: 	Container node-exporter ready: true, restart count 0
Jan 17 15:38:21.522: INFO: prometheus-adapter-cd9bc68fc-dstxm from openshift-monitoring started at 2023-01-17 12:57:45 +0000 UTC (1 container statuses recorded)
Jan 17 15:38:21.522: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jan 17 15:38:21.522: INFO: prometheus-k8s-0 from openshift-monitoring started at 2023-01-17 12:59:05 +0000 UTC (6 container statuses recorded)
Jan 17 15:38:21.522: INFO: 	Container config-reloader ready: true, restart count 0
Jan 17 15:38:21.522: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 17 15:38:21.522: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Jan 17 15:38:21.522: INFO: 	Container prometheus ready: true, restart count 0
Jan 17 15:38:21.522: INFO: 	Container prometheus-proxy ready: true, restart count 0
Jan 17 15:38:21.522: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jan 17 15:38:21.522: INFO: prometheus-operator-admission-webhook-7d4759d465-lgsbt from openshift-monitoring started at 2023-01-17 15:23:22 +0000 UTC (1 container statuses recorded)
Jan 17 15:38:21.522: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Jan 17 15:38:21.522: INFO: thanos-querier-5fccfc4877-prbhx from openshift-monitoring started at 2023-01-17 12:57:02 +0000 UTC (6 container statuses recorded)
Jan 17 15:38:21.522: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 17 15:38:21.522: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Jan 17 15:38:21.522: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Jan 17 15:38:21.522: INFO: 	Container oauth-proxy ready: true, restart count 0
Jan 17 15:38:21.522: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jan 17 15:38:21.522: INFO: 	Container thanos-query ready: true, restart count 0
Jan 17 15:38:21.522: INFO: multus-additional-cni-plugins-bpc6w from openshift-multus started at 2023-01-17 12:55:50 +0000 UTC (1 container statuses recorded)
Jan 17 15:38:21.522: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Jan 17 15:38:21.522: INFO: multus-tt5fs from openshift-multus started at 2023-01-17 12:55:50 +0000 UTC (1 container statuses recorded)
Jan 17 15:38:21.522: INFO: 	Container kube-multus ready: true, restart count 0
Jan 17 15:38:21.522: INFO: network-metrics-daemon-22p8j from openshift-multus started at 2023-01-17 12:55:50 +0000 UTC (2 container statuses recorded)
Jan 17 15:38:21.522: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 17 15:38:21.522: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Jan 17 15:38:21.522: INFO: network-check-target-k685r from openshift-network-diagnostics started at 2023-01-17 12:55:50 +0000 UTC (1 container statuses recorded)
Jan 17 15:38:21.522: INFO: 	Container network-check-target-container ready: true, restart count 0
Jan 17 15:38:21.522: INFO: ovnkube-node-7n8jx from openshift-ovn-kubernetes started at 2023-01-17 12:55:50 +0000 UTC (5 container statuses recorded)
Jan 17 15:38:21.522: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 17 15:38:21.522: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 0
Jan 17 15:38:21.522: INFO: 	Container ovn-acl-logging ready: true, restart count 0
Jan 17 15:38:21.522: INFO: 	Container ovn-controller ready: true, restart count 0
Jan 17 15:38:21.522: INFO: 	Container ovnkube-node ready: true, restart count 0
Jan 17 15:38:21.522: INFO: sonobuoy-systemd-logs-daemon-set-329d7d7181d04e86-mhx5d from sonobuoy started at 2023-01-17 14:47:25 +0000 UTC (2 container statuses recorded)
Jan 17 15:38:21.522: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 17 15:38:21.522: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 17 15:38:21.522: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-151-22.ec2.internal before test
Jan 17 15:38:21.549: INFO: aws-ebs-csi-driver-node-mx922 from openshift-cluster-csi-drivers started at 2023-01-17 12:51:50 +0000 UTC (3 container statuses recorded)
Jan 17 15:38:21.549: INFO: 	Container csi-driver ready: true, restart count 0
Jan 17 15:38:21.549: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Jan 17 15:38:21.549: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Jan 17 15:38:21.549: INFO: tuned-9hhcs from openshift-cluster-node-tuning-operator started at 2023-01-17 12:51:50 +0000 UTC (1 container statuses recorded)
Jan 17 15:38:21.549: INFO: 	Container tuned ready: true, restart count 0
Jan 17 15:38:21.549: INFO: downloads-8d695cd69-gbsf4 from openshift-console started at 2023-01-17 15:23:22 +0000 UTC (1 container statuses recorded)
Jan 17 15:38:21.549: INFO: 	Container download-server ready: true, restart count 0
Jan 17 15:38:21.549: INFO: dns-default-mdfm8 from openshift-dns started at 2023-01-17 15:23:43 +0000 UTC (2 container statuses recorded)
Jan 17 15:38:21.549: INFO: 	Container dns ready: true, restart count 0
Jan 17 15:38:21.549: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 17 15:38:21.549: INFO: node-resolver-fxksv from openshift-dns started at 2023-01-17 12:51:50 +0000 UTC (1 container statuses recorded)
Jan 17 15:38:21.549: INFO: 	Container dns-node-resolver ready: true, restart count 0
Jan 17 15:38:21.549: INFO: node-ca-tpp6r from openshift-image-registry started at 2023-01-17 12:54:40 +0000 UTC (1 container statuses recorded)
Jan 17 15:38:21.549: INFO: 	Container node-ca ready: true, restart count 0
Jan 17 15:38:21.549: INFO: ingress-canary-9tpfk from openshift-ingress-canary started at 2023-01-17 15:23:23 +0000 UTC (1 container statuses recorded)
Jan 17 15:38:21.549: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Jan 17 15:38:21.549: INFO: machine-config-daemon-bgrjb from openshift-machine-config-operator started at 2023-01-17 12:51:50 +0000 UTC (2 container statuses recorded)
Jan 17 15:38:21.549: INFO: 	Container machine-config-daemon ready: true, restart count 0
Jan 17 15:38:21.549: INFO: 	Container oauth-proxy ready: true, restart count 0
Jan 17 15:38:21.549: INFO: alertmanager-main-1 from openshift-monitoring started at 2023-01-17 15:23:24 +0000 UTC (6 container statuses recorded)
Jan 17 15:38:21.549: INFO: 	Container alertmanager ready: true, restart count 1
Jan 17 15:38:21.549: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jan 17 15:38:21.549: INFO: 	Container config-reloader ready: true, restart count 0
Jan 17 15:38:21.549: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 17 15:38:21.549: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Jan 17 15:38:21.549: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jan 17 15:38:21.549: INFO: node-exporter-snggg from openshift-monitoring started at 2023-01-17 12:56:55 +0000 UTC (2 container statuses recorded)
Jan 17 15:38:21.549: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 17 15:38:21.549: INFO: 	Container node-exporter ready: true, restart count 0
Jan 17 15:38:21.549: INFO: multus-additional-cni-plugins-grzml from openshift-multus started at 2023-01-17 12:51:50 +0000 UTC (1 container statuses recorded)
Jan 17 15:38:21.549: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Jan 17 15:38:21.549: INFO: multus-qspfd from openshift-multus started at 2023-01-17 12:51:50 +0000 UTC (1 container statuses recorded)
Jan 17 15:38:21.549: INFO: 	Container kube-multus ready: true, restart count 0
Jan 17 15:38:21.549: INFO: network-metrics-daemon-x4zrh from openshift-multus started at 2023-01-17 12:51:50 +0000 UTC (2 container statuses recorded)
Jan 17 15:38:21.549: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 17 15:38:21.549: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Jan 17 15:38:21.549: INFO: network-check-target-f794b from openshift-network-diagnostics started at 2023-01-17 12:51:50 +0000 UTC (1 container statuses recorded)
Jan 17 15:38:21.549: INFO: 	Container network-check-target-container ready: true, restart count 0
Jan 17 15:38:21.549: INFO: collect-profiles-27899490-z525c from openshift-operator-lifecycle-manager started at 2023-01-17 15:30:00 +0000 UTC (1 container statuses recorded)
Jan 17 15:38:21.549: INFO: 	Container collect-profiles ready: false, restart count 0
Jan 17 15:38:21.549: INFO: ovnkube-node-bks72 from openshift-ovn-kubernetes started at 2023-01-17 12:51:50 +0000 UTC (5 container statuses recorded)
Jan 17 15:38:21.549: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 17 15:38:21.549: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 0
Jan 17 15:38:21.549: INFO: 	Container ovn-acl-logging ready: true, restart count 0
Jan 17 15:38:21.549: INFO: 	Container ovn-controller ready: true, restart count 0
Jan 17 15:38:21.549: INFO: 	Container ovnkube-node ready: true, restart count 0
Jan 17 15:38:21.549: INFO: sonobuoy from sonobuoy started at 2023-01-17 14:47:22 +0000 UTC (1 container statuses recorded)
Jan 17 15:38:21.549: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 17 15:38:21.549: INFO: sonobuoy-e2e-job-0a9b408aa2f34bad from sonobuoy started at 2023-01-17 14:47:25 +0000 UTC (2 container statuses recorded)
Jan 17 15:38:21.549: INFO: 	Container e2e ready: true, restart count 0
Jan 17 15:38:21.549: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 17 15:38:21.549: INFO: sonobuoy-systemd-logs-daemon-set-329d7d7181d04e86-8knzd from sonobuoy started at 2023-01-17 14:47:25 +0000 UTC (2 container statuses recorded)
Jan 17 15:38:21.549: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 17 15:38:21.549: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 17 15:38:21.549: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-165-14.ec2.internal before test
Jan 17 15:38:21.570: INFO: aws-ebs-csi-driver-node-x9fxp from openshift-cluster-csi-drivers started at 2023-01-17 12:55:53 +0000 UTC (3 container statuses recorded)
Jan 17 15:38:21.570: INFO: 	Container csi-driver ready: true, restart count 0
Jan 17 15:38:21.570: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Jan 17 15:38:21.570: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Jan 17 15:38:21.570: INFO: tuned-cgskr from openshift-cluster-node-tuning-operator started at 2023-01-17 12:55:53 +0000 UTC (1 container statuses recorded)
Jan 17 15:38:21.570: INFO: 	Container tuned ready: true, restart count 0
Jan 17 15:38:21.570: INFO: dns-default-8q4m7 from openshift-dns started at 2023-01-17 12:56:45 +0000 UTC (2 container statuses recorded)
Jan 17 15:38:21.570: INFO: 	Container dns ready: true, restart count 0
Jan 17 15:38:21.570: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 17 15:38:21.570: INFO: node-resolver-dvw44 from openshift-dns started at 2023-01-17 12:55:53 +0000 UTC (1 container statuses recorded)
Jan 17 15:38:21.570: INFO: 	Container dns-node-resolver ready: true, restart count 0
Jan 17 15:38:21.570: INFO: image-registry-6d685bd45d-g2mxt from openshift-image-registry started at 2023-01-17 12:56:45 +0000 UTC (1 container statuses recorded)
Jan 17 15:38:21.570: INFO: 	Container registry ready: true, restart count 0
Jan 17 15:38:21.570: INFO: node-ca-45nfl from openshift-image-registry started at 2023-01-17 12:55:53 +0000 UTC (1 container statuses recorded)
Jan 17 15:38:21.570: INFO: 	Container node-ca ready: true, restart count 0
Jan 17 15:38:21.570: INFO: ingress-canary-q6p6t from openshift-ingress-canary started at 2023-01-17 12:56:45 +0000 UTC (1 container statuses recorded)
Jan 17 15:38:21.570: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Jan 17 15:38:21.570: INFO: router-default-c95cc587f-pj9sc from openshift-ingress started at 2023-01-17 15:23:22 +0000 UTC (1 container statuses recorded)
Jan 17 15:38:21.570: INFO: 	Container router ready: true, restart count 0
Jan 17 15:38:21.570: INFO: machine-config-daemon-v7v4j from openshift-machine-config-operator started at 2023-01-17 12:55:53 +0000 UTC (2 container statuses recorded)
Jan 17 15:38:21.570: INFO: 	Container machine-config-daemon ready: true, restart count 0
Jan 17 15:38:21.570: INFO: 	Container oauth-proxy ready: true, restart count 0
Jan 17 15:38:21.570: INFO: kube-state-metrics-75455b796c-rgbdl from openshift-monitoring started at 2023-01-17 12:56:55 +0000 UTC (3 container statuses recorded)
Jan 17 15:38:21.570: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jan 17 15:38:21.570: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jan 17 15:38:21.570: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jan 17 15:38:21.570: INFO: node-exporter-pdxfc from openshift-monitoring started at 2023-01-17 12:56:55 +0000 UTC (2 container statuses recorded)
Jan 17 15:38:21.570: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 17 15:38:21.570: INFO: 	Container node-exporter ready: true, restart count 0
Jan 17 15:38:21.570: INFO: openshift-state-metrics-5ff95d844f-dl27q from openshift-monitoring started at 2023-01-17 12:56:55 +0000 UTC (3 container statuses recorded)
Jan 17 15:38:21.570: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jan 17 15:38:21.570: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jan 17 15:38:21.570: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Jan 17 15:38:21.570: INFO: prometheus-adapter-cd9bc68fc-ptnx8 from openshift-monitoring started at 2023-01-17 15:23:22 +0000 UTC (1 container statuses recorded)
Jan 17 15:38:21.570: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jan 17 15:38:21.570: INFO: prometheus-k8s-1 from openshift-monitoring started at 2023-01-17 12:58:37 +0000 UTC (6 container statuses recorded)
Jan 17 15:38:21.570: INFO: 	Container config-reloader ready: true, restart count 0
Jan 17 15:38:21.570: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 17 15:38:21.570: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Jan 17 15:38:21.570: INFO: 	Container prometheus ready: true, restart count 0
Jan 17 15:38:21.570: INFO: 	Container prometheus-proxy ready: true, restart count 0
Jan 17 15:38:21.570: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jan 17 15:38:21.570: INFO: prometheus-operator-admission-webhook-7d4759d465-bw8tr from openshift-monitoring started at 2023-01-17 12:56:45 +0000 UTC (1 container statuses recorded)
Jan 17 15:38:21.570: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Jan 17 15:38:21.570: INFO: telemeter-client-585b5cf5bf-zlltk from openshift-monitoring started at 2023-01-17 12:57:00 +0000 UTC (3 container statuses recorded)
Jan 17 15:38:21.570: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 17 15:38:21.570: INFO: 	Container reload ready: true, restart count 0
Jan 17 15:38:21.570: INFO: 	Container telemeter-client ready: true, restart count 0
Jan 17 15:38:21.570: INFO: thanos-querier-5fccfc4877-lc5nk from openshift-monitoring started at 2023-01-17 12:57:02 +0000 UTC (6 container statuses recorded)
Jan 17 15:38:21.570: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 17 15:38:21.570: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Jan 17 15:38:21.570: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Jan 17 15:38:21.570: INFO: 	Container oauth-proxy ready: true, restart count 0
Jan 17 15:38:21.570: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jan 17 15:38:21.570: INFO: 	Container thanos-query ready: true, restart count 0
Jan 17 15:38:21.570: INFO: multus-additional-cni-plugins-zg84j from openshift-multus started at 2023-01-17 12:55:53 +0000 UTC (1 container statuses recorded)
Jan 17 15:38:21.570: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Jan 17 15:38:21.570: INFO: multus-jq6qc from openshift-multus started at 2023-01-17 12:55:53 +0000 UTC (1 container statuses recorded)
Jan 17 15:38:21.570: INFO: 	Container kube-multus ready: true, restart count 0
Jan 17 15:38:21.570: INFO: network-metrics-daemon-gngg5 from openshift-multus started at 2023-01-17 12:55:53 +0000 UTC (2 container statuses recorded)
Jan 17 15:38:21.570: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 17 15:38:21.570: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Jan 17 15:38:21.570: INFO: network-check-source-746dd6c885-gl46q from openshift-network-diagnostics started at 2023-01-17 15:23:22 +0000 UTC (1 container statuses recorded)
Jan 17 15:38:21.570: INFO: 	Container check-endpoints ready: true, restart count 0
Jan 17 15:38:21.570: INFO: network-check-target-v9g9x from openshift-network-diagnostics started at 2023-01-17 12:55:53 +0000 UTC (1 container statuses recorded)
Jan 17 15:38:21.570: INFO: 	Container network-check-target-container ready: true, restart count 0
Jan 17 15:38:21.570: INFO: ovnkube-node-q54mr from openshift-ovn-kubernetes started at 2023-01-17 12:55:53 +0000 UTC (5 container statuses recorded)
Jan 17 15:38:21.570: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 17 15:38:21.570: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 0
Jan 17 15:38:21.570: INFO: 	Container ovn-acl-logging ready: true, restart count 0
Jan 17 15:38:21.570: INFO: 	Container ovn-controller ready: true, restart count 0
Jan 17 15:38:21.570: INFO: 	Container ovnkube-node ready: true, restart count 0
Jan 17 15:38:21.570: INFO: sonobuoy-systemd-logs-daemon-set-329d7d7181d04e86-qcrqh from sonobuoy started at 2023-01-17 14:47:25 +0000 UTC (2 container statuses recorded)
Jan 17 15:38:21.570: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 17 15:38:21.570: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438
STEP: Trying to schedule Pod with nonempty NodeSelector. 01/17/23 15:38:21.57
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.173b22c1a7c295ab], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 6 node(s) didn't match Pod's node affinity/selector. preemption: 0/6 nodes are available: 6 Preemption is not helpful for scheduling.] 01/17/23 15:38:21.642
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Jan 17 15:38:22.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2482" for this suite. 01/17/23 15:38:22.644
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","completed":215,"skipped":3940,"failed":0}
------------------------------
• [1.210 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:38:21.442
    Jan 17 15:38:21.442: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename sched-pred 01/17/23 15:38:21.443
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:38:21.465
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:38:21.467
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Jan 17 15:38:21.469: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan 17 15:38:21.488: INFO: Waiting for terminating namespaces to be deleted...
    Jan 17 15:38:21.494: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-139-213.ec2.internal before test
    Jan 17 15:38:21.522: INFO: aws-ebs-csi-driver-node-5tmvr from openshift-cluster-csi-drivers started at 2023-01-17 12:55:50 +0000 UTC (3 container statuses recorded)
    Jan 17 15:38:21.522: INFO: 	Container csi-driver ready: true, restart count 0
    Jan 17 15:38:21.522: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    Jan 17 15:38:21.522: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
    Jan 17 15:38:21.522: INFO: tuned-jzlnj from openshift-cluster-node-tuning-operator started at 2023-01-17 12:55:50 +0000 UTC (1 container statuses recorded)
    Jan 17 15:38:21.522: INFO: 	Container tuned ready: true, restart count 0
    Jan 17 15:38:21.522: INFO: downloads-8d695cd69-ltk55 from openshift-console started at 2023-01-17 12:57:39 +0000 UTC (1 container statuses recorded)
    Jan 17 15:38:21.522: INFO: 	Container download-server ready: true, restart count 0
    Jan 17 15:38:21.522: INFO: dns-default-jgzr9 from openshift-dns started at 2023-01-17 12:56:45 +0000 UTC (2 container statuses recorded)
    Jan 17 15:38:21.522: INFO: 	Container dns ready: true, restart count 0
    Jan 17 15:38:21.522: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 17 15:38:21.522: INFO: node-resolver-gkxnp from openshift-dns started at 2023-01-17 12:55:50 +0000 UTC (1 container statuses recorded)
    Jan 17 15:38:21.522: INFO: 	Container dns-node-resolver ready: true, restart count 0
    Jan 17 15:38:21.522: INFO: image-registry-6d685bd45d-mk7wb from openshift-image-registry started at 2023-01-17 12:56:45 +0000 UTC (1 container statuses recorded)
    Jan 17 15:38:21.522: INFO: 	Container registry ready: true, restart count 0
    Jan 17 15:38:21.522: INFO: node-ca-4l49x from openshift-image-registry started at 2023-01-17 12:55:50 +0000 UTC (1 container statuses recorded)
    Jan 17 15:38:21.522: INFO: 	Container node-ca ready: true, restart count 0
    Jan 17 15:38:21.522: INFO: ingress-canary-xqzwm from openshift-ingress-canary started at 2023-01-17 12:56:45 +0000 UTC (1 container statuses recorded)
    Jan 17 15:38:21.522: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    Jan 17 15:38:21.522: INFO: router-default-c95cc587f-9clvn from openshift-ingress started at 2023-01-17 12:56:45 +0000 UTC (1 container statuses recorded)
    Jan 17 15:38:21.522: INFO: 	Container router ready: true, restart count 0
    Jan 17 15:38:21.522: INFO: machine-config-daemon-6hjqr from openshift-machine-config-operator started at 2023-01-17 12:55:50 +0000 UTC (2 container statuses recorded)
    Jan 17 15:38:21.522: INFO: 	Container machine-config-daemon ready: true, restart count 0
    Jan 17 15:38:21.522: INFO: 	Container oauth-proxy ready: true, restart count 0
    Jan 17 15:38:21.522: INFO: alertmanager-main-0 from openshift-monitoring started at 2023-01-17 12:59:05 +0000 UTC (6 container statuses recorded)
    Jan 17 15:38:21.522: INFO: 	Container alertmanager ready: true, restart count 1
    Jan 17 15:38:21.522: INFO: 	Container alertmanager-proxy ready: true, restart count 0
    Jan 17 15:38:21.522: INFO: 	Container config-reloader ready: true, restart count 0
    Jan 17 15:38:21.522: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 17 15:38:21.522: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
    Jan 17 15:38:21.522: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Jan 17 15:38:21.522: INFO: node-exporter-lwbc8 from openshift-monitoring started at 2023-01-17 12:56:55 +0000 UTC (2 container statuses recorded)
    Jan 17 15:38:21.522: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 17 15:38:21.522: INFO: 	Container node-exporter ready: true, restart count 0
    Jan 17 15:38:21.522: INFO: prometheus-adapter-cd9bc68fc-dstxm from openshift-monitoring started at 2023-01-17 12:57:45 +0000 UTC (1 container statuses recorded)
    Jan 17 15:38:21.522: INFO: 	Container prometheus-adapter ready: true, restart count 0
    Jan 17 15:38:21.522: INFO: prometheus-k8s-0 from openshift-monitoring started at 2023-01-17 12:59:05 +0000 UTC (6 container statuses recorded)
    Jan 17 15:38:21.522: INFO: 	Container config-reloader ready: true, restart count 0
    Jan 17 15:38:21.522: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 17 15:38:21.522: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
    Jan 17 15:38:21.522: INFO: 	Container prometheus ready: true, restart count 0
    Jan 17 15:38:21.522: INFO: 	Container prometheus-proxy ready: true, restart count 0
    Jan 17 15:38:21.522: INFO: 	Container thanos-sidecar ready: true, restart count 0
    Jan 17 15:38:21.522: INFO: prometheus-operator-admission-webhook-7d4759d465-lgsbt from openshift-monitoring started at 2023-01-17 15:23:22 +0000 UTC (1 container statuses recorded)
    Jan 17 15:38:21.522: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
    Jan 17 15:38:21.522: INFO: thanos-querier-5fccfc4877-prbhx from openshift-monitoring started at 2023-01-17 12:57:02 +0000 UTC (6 container statuses recorded)
    Jan 17 15:38:21.522: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 17 15:38:21.522: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
    Jan 17 15:38:21.522: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
    Jan 17 15:38:21.522: INFO: 	Container oauth-proxy ready: true, restart count 0
    Jan 17 15:38:21.522: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Jan 17 15:38:21.522: INFO: 	Container thanos-query ready: true, restart count 0
    Jan 17 15:38:21.522: INFO: multus-additional-cni-plugins-bpc6w from openshift-multus started at 2023-01-17 12:55:50 +0000 UTC (1 container statuses recorded)
    Jan 17 15:38:21.522: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    Jan 17 15:38:21.522: INFO: multus-tt5fs from openshift-multus started at 2023-01-17 12:55:50 +0000 UTC (1 container statuses recorded)
    Jan 17 15:38:21.522: INFO: 	Container kube-multus ready: true, restart count 0
    Jan 17 15:38:21.522: INFO: network-metrics-daemon-22p8j from openshift-multus started at 2023-01-17 12:55:50 +0000 UTC (2 container statuses recorded)
    Jan 17 15:38:21.522: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 17 15:38:21.522: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    Jan 17 15:38:21.522: INFO: network-check-target-k685r from openshift-network-diagnostics started at 2023-01-17 12:55:50 +0000 UTC (1 container statuses recorded)
    Jan 17 15:38:21.522: INFO: 	Container network-check-target-container ready: true, restart count 0
    Jan 17 15:38:21.522: INFO: ovnkube-node-7n8jx from openshift-ovn-kubernetes started at 2023-01-17 12:55:50 +0000 UTC (5 container statuses recorded)
    Jan 17 15:38:21.522: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 17 15:38:21.522: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 0
    Jan 17 15:38:21.522: INFO: 	Container ovn-acl-logging ready: true, restart count 0
    Jan 17 15:38:21.522: INFO: 	Container ovn-controller ready: true, restart count 0
    Jan 17 15:38:21.522: INFO: 	Container ovnkube-node ready: true, restart count 0
    Jan 17 15:38:21.522: INFO: sonobuoy-systemd-logs-daemon-set-329d7d7181d04e86-mhx5d from sonobuoy started at 2023-01-17 14:47:25 +0000 UTC (2 container statuses recorded)
    Jan 17 15:38:21.522: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 17 15:38:21.522: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 17 15:38:21.522: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-151-22.ec2.internal before test
    Jan 17 15:38:21.549: INFO: aws-ebs-csi-driver-node-mx922 from openshift-cluster-csi-drivers started at 2023-01-17 12:51:50 +0000 UTC (3 container statuses recorded)
    Jan 17 15:38:21.549: INFO: 	Container csi-driver ready: true, restart count 0
    Jan 17 15:38:21.549: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    Jan 17 15:38:21.549: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Jan 17 15:38:21.549: INFO: tuned-9hhcs from openshift-cluster-node-tuning-operator started at 2023-01-17 12:51:50 +0000 UTC (1 container statuses recorded)
    Jan 17 15:38:21.549: INFO: 	Container tuned ready: true, restart count 0
    Jan 17 15:38:21.549: INFO: downloads-8d695cd69-gbsf4 from openshift-console started at 2023-01-17 15:23:22 +0000 UTC (1 container statuses recorded)
    Jan 17 15:38:21.549: INFO: 	Container download-server ready: true, restart count 0
    Jan 17 15:38:21.549: INFO: dns-default-mdfm8 from openshift-dns started at 2023-01-17 15:23:43 +0000 UTC (2 container statuses recorded)
    Jan 17 15:38:21.549: INFO: 	Container dns ready: true, restart count 0
    Jan 17 15:38:21.549: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 17 15:38:21.549: INFO: node-resolver-fxksv from openshift-dns started at 2023-01-17 12:51:50 +0000 UTC (1 container statuses recorded)
    Jan 17 15:38:21.549: INFO: 	Container dns-node-resolver ready: true, restart count 0
    Jan 17 15:38:21.549: INFO: node-ca-tpp6r from openshift-image-registry started at 2023-01-17 12:54:40 +0000 UTC (1 container statuses recorded)
    Jan 17 15:38:21.549: INFO: 	Container node-ca ready: true, restart count 0
    Jan 17 15:38:21.549: INFO: ingress-canary-9tpfk from openshift-ingress-canary started at 2023-01-17 15:23:23 +0000 UTC (1 container statuses recorded)
    Jan 17 15:38:21.549: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    Jan 17 15:38:21.549: INFO: machine-config-daemon-bgrjb from openshift-machine-config-operator started at 2023-01-17 12:51:50 +0000 UTC (2 container statuses recorded)
    Jan 17 15:38:21.549: INFO: 	Container machine-config-daemon ready: true, restart count 0
    Jan 17 15:38:21.549: INFO: 	Container oauth-proxy ready: true, restart count 0
    Jan 17 15:38:21.549: INFO: alertmanager-main-1 from openshift-monitoring started at 2023-01-17 15:23:24 +0000 UTC (6 container statuses recorded)
    Jan 17 15:38:21.549: INFO: 	Container alertmanager ready: true, restart count 1
    Jan 17 15:38:21.549: INFO: 	Container alertmanager-proxy ready: true, restart count 0
    Jan 17 15:38:21.549: INFO: 	Container config-reloader ready: true, restart count 0
    Jan 17 15:38:21.549: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 17 15:38:21.549: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
    Jan 17 15:38:21.549: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Jan 17 15:38:21.549: INFO: node-exporter-snggg from openshift-monitoring started at 2023-01-17 12:56:55 +0000 UTC (2 container statuses recorded)
    Jan 17 15:38:21.549: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 17 15:38:21.549: INFO: 	Container node-exporter ready: true, restart count 0
    Jan 17 15:38:21.549: INFO: multus-additional-cni-plugins-grzml from openshift-multus started at 2023-01-17 12:51:50 +0000 UTC (1 container statuses recorded)
    Jan 17 15:38:21.549: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    Jan 17 15:38:21.549: INFO: multus-qspfd from openshift-multus started at 2023-01-17 12:51:50 +0000 UTC (1 container statuses recorded)
    Jan 17 15:38:21.549: INFO: 	Container kube-multus ready: true, restart count 0
    Jan 17 15:38:21.549: INFO: network-metrics-daemon-x4zrh from openshift-multus started at 2023-01-17 12:51:50 +0000 UTC (2 container statuses recorded)
    Jan 17 15:38:21.549: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 17 15:38:21.549: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    Jan 17 15:38:21.549: INFO: network-check-target-f794b from openshift-network-diagnostics started at 2023-01-17 12:51:50 +0000 UTC (1 container statuses recorded)
    Jan 17 15:38:21.549: INFO: 	Container network-check-target-container ready: true, restart count 0
    Jan 17 15:38:21.549: INFO: collect-profiles-27899490-z525c from openshift-operator-lifecycle-manager started at 2023-01-17 15:30:00 +0000 UTC (1 container statuses recorded)
    Jan 17 15:38:21.549: INFO: 	Container collect-profiles ready: false, restart count 0
    Jan 17 15:38:21.549: INFO: ovnkube-node-bks72 from openshift-ovn-kubernetes started at 2023-01-17 12:51:50 +0000 UTC (5 container statuses recorded)
    Jan 17 15:38:21.549: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 17 15:38:21.549: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 0
    Jan 17 15:38:21.549: INFO: 	Container ovn-acl-logging ready: true, restart count 0
    Jan 17 15:38:21.549: INFO: 	Container ovn-controller ready: true, restart count 0
    Jan 17 15:38:21.549: INFO: 	Container ovnkube-node ready: true, restart count 0
    Jan 17 15:38:21.549: INFO: sonobuoy from sonobuoy started at 2023-01-17 14:47:22 +0000 UTC (1 container statuses recorded)
    Jan 17 15:38:21.549: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan 17 15:38:21.549: INFO: sonobuoy-e2e-job-0a9b408aa2f34bad from sonobuoy started at 2023-01-17 14:47:25 +0000 UTC (2 container statuses recorded)
    Jan 17 15:38:21.549: INFO: 	Container e2e ready: true, restart count 0
    Jan 17 15:38:21.549: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 17 15:38:21.549: INFO: sonobuoy-systemd-logs-daemon-set-329d7d7181d04e86-8knzd from sonobuoy started at 2023-01-17 14:47:25 +0000 UTC (2 container statuses recorded)
    Jan 17 15:38:21.549: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 17 15:38:21.549: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 17 15:38:21.549: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-165-14.ec2.internal before test
    Jan 17 15:38:21.570: INFO: aws-ebs-csi-driver-node-x9fxp from openshift-cluster-csi-drivers started at 2023-01-17 12:55:53 +0000 UTC (3 container statuses recorded)
    Jan 17 15:38:21.570: INFO: 	Container csi-driver ready: true, restart count 0
    Jan 17 15:38:21.570: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    Jan 17 15:38:21.570: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Jan 17 15:38:21.570: INFO: tuned-cgskr from openshift-cluster-node-tuning-operator started at 2023-01-17 12:55:53 +0000 UTC (1 container statuses recorded)
    Jan 17 15:38:21.570: INFO: 	Container tuned ready: true, restart count 0
    Jan 17 15:38:21.570: INFO: dns-default-8q4m7 from openshift-dns started at 2023-01-17 12:56:45 +0000 UTC (2 container statuses recorded)
    Jan 17 15:38:21.570: INFO: 	Container dns ready: true, restart count 0
    Jan 17 15:38:21.570: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 17 15:38:21.570: INFO: node-resolver-dvw44 from openshift-dns started at 2023-01-17 12:55:53 +0000 UTC (1 container statuses recorded)
    Jan 17 15:38:21.570: INFO: 	Container dns-node-resolver ready: true, restart count 0
    Jan 17 15:38:21.570: INFO: image-registry-6d685bd45d-g2mxt from openshift-image-registry started at 2023-01-17 12:56:45 +0000 UTC (1 container statuses recorded)
    Jan 17 15:38:21.570: INFO: 	Container registry ready: true, restart count 0
    Jan 17 15:38:21.570: INFO: node-ca-45nfl from openshift-image-registry started at 2023-01-17 12:55:53 +0000 UTC (1 container statuses recorded)
    Jan 17 15:38:21.570: INFO: 	Container node-ca ready: true, restart count 0
    Jan 17 15:38:21.570: INFO: ingress-canary-q6p6t from openshift-ingress-canary started at 2023-01-17 12:56:45 +0000 UTC (1 container statuses recorded)
    Jan 17 15:38:21.570: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    Jan 17 15:38:21.570: INFO: router-default-c95cc587f-pj9sc from openshift-ingress started at 2023-01-17 15:23:22 +0000 UTC (1 container statuses recorded)
    Jan 17 15:38:21.570: INFO: 	Container router ready: true, restart count 0
    Jan 17 15:38:21.570: INFO: machine-config-daemon-v7v4j from openshift-machine-config-operator started at 2023-01-17 12:55:53 +0000 UTC (2 container statuses recorded)
    Jan 17 15:38:21.570: INFO: 	Container machine-config-daemon ready: true, restart count 0
    Jan 17 15:38:21.570: INFO: 	Container oauth-proxy ready: true, restart count 0
    Jan 17 15:38:21.570: INFO: kube-state-metrics-75455b796c-rgbdl from openshift-monitoring started at 2023-01-17 12:56:55 +0000 UTC (3 container statuses recorded)
    Jan 17 15:38:21.570: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
    Jan 17 15:38:21.570: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
    Jan 17 15:38:21.570: INFO: 	Container kube-state-metrics ready: true, restart count 0
    Jan 17 15:38:21.570: INFO: node-exporter-pdxfc from openshift-monitoring started at 2023-01-17 12:56:55 +0000 UTC (2 container statuses recorded)
    Jan 17 15:38:21.570: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 17 15:38:21.570: INFO: 	Container node-exporter ready: true, restart count 0
    Jan 17 15:38:21.570: INFO: openshift-state-metrics-5ff95d844f-dl27q from openshift-monitoring started at 2023-01-17 12:56:55 +0000 UTC (3 container statuses recorded)
    Jan 17 15:38:21.570: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
    Jan 17 15:38:21.570: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
    Jan 17 15:38:21.570: INFO: 	Container openshift-state-metrics ready: true, restart count 0
    Jan 17 15:38:21.570: INFO: prometheus-adapter-cd9bc68fc-ptnx8 from openshift-monitoring started at 2023-01-17 15:23:22 +0000 UTC (1 container statuses recorded)
    Jan 17 15:38:21.570: INFO: 	Container prometheus-adapter ready: true, restart count 0
    Jan 17 15:38:21.570: INFO: prometheus-k8s-1 from openshift-monitoring started at 2023-01-17 12:58:37 +0000 UTC (6 container statuses recorded)
    Jan 17 15:38:21.570: INFO: 	Container config-reloader ready: true, restart count 0
    Jan 17 15:38:21.570: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 17 15:38:21.570: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
    Jan 17 15:38:21.570: INFO: 	Container prometheus ready: true, restart count 0
    Jan 17 15:38:21.570: INFO: 	Container prometheus-proxy ready: true, restart count 0
    Jan 17 15:38:21.570: INFO: 	Container thanos-sidecar ready: true, restart count 0
    Jan 17 15:38:21.570: INFO: prometheus-operator-admission-webhook-7d4759d465-bw8tr from openshift-monitoring started at 2023-01-17 12:56:45 +0000 UTC (1 container statuses recorded)
    Jan 17 15:38:21.570: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
    Jan 17 15:38:21.570: INFO: telemeter-client-585b5cf5bf-zlltk from openshift-monitoring started at 2023-01-17 12:57:00 +0000 UTC (3 container statuses recorded)
    Jan 17 15:38:21.570: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 17 15:38:21.570: INFO: 	Container reload ready: true, restart count 0
    Jan 17 15:38:21.570: INFO: 	Container telemeter-client ready: true, restart count 0
    Jan 17 15:38:21.570: INFO: thanos-querier-5fccfc4877-lc5nk from openshift-monitoring started at 2023-01-17 12:57:02 +0000 UTC (6 container statuses recorded)
    Jan 17 15:38:21.570: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 17 15:38:21.570: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
    Jan 17 15:38:21.570: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
    Jan 17 15:38:21.570: INFO: 	Container oauth-proxy ready: true, restart count 0
    Jan 17 15:38:21.570: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Jan 17 15:38:21.570: INFO: 	Container thanos-query ready: true, restart count 0
    Jan 17 15:38:21.570: INFO: multus-additional-cni-plugins-zg84j from openshift-multus started at 2023-01-17 12:55:53 +0000 UTC (1 container statuses recorded)
    Jan 17 15:38:21.570: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    Jan 17 15:38:21.570: INFO: multus-jq6qc from openshift-multus started at 2023-01-17 12:55:53 +0000 UTC (1 container statuses recorded)
    Jan 17 15:38:21.570: INFO: 	Container kube-multus ready: true, restart count 0
    Jan 17 15:38:21.570: INFO: network-metrics-daemon-gngg5 from openshift-multus started at 2023-01-17 12:55:53 +0000 UTC (2 container statuses recorded)
    Jan 17 15:38:21.570: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 17 15:38:21.570: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    Jan 17 15:38:21.570: INFO: network-check-source-746dd6c885-gl46q from openshift-network-diagnostics started at 2023-01-17 15:23:22 +0000 UTC (1 container statuses recorded)
    Jan 17 15:38:21.570: INFO: 	Container check-endpoints ready: true, restart count 0
    Jan 17 15:38:21.570: INFO: network-check-target-v9g9x from openshift-network-diagnostics started at 2023-01-17 12:55:53 +0000 UTC (1 container statuses recorded)
    Jan 17 15:38:21.570: INFO: 	Container network-check-target-container ready: true, restart count 0
    Jan 17 15:38:21.570: INFO: ovnkube-node-q54mr from openshift-ovn-kubernetes started at 2023-01-17 12:55:53 +0000 UTC (5 container statuses recorded)
    Jan 17 15:38:21.570: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 17 15:38:21.570: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 0
    Jan 17 15:38:21.570: INFO: 	Container ovn-acl-logging ready: true, restart count 0
    Jan 17 15:38:21.570: INFO: 	Container ovn-controller ready: true, restart count 0
    Jan 17 15:38:21.570: INFO: 	Container ovnkube-node ready: true, restart count 0
    Jan 17 15:38:21.570: INFO: sonobuoy-systemd-logs-daemon-set-329d7d7181d04e86-qcrqh from sonobuoy started at 2023-01-17 14:47:25 +0000 UTC (2 container statuses recorded)
    Jan 17 15:38:21.570: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 17 15:38:21.570: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:438
    STEP: Trying to schedule Pod with nonempty NodeSelector. 01/17/23 15:38:21.57
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.173b22c1a7c295ab], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 6 node(s) didn't match Pod's node affinity/selector. preemption: 0/6 nodes are available: 6 Preemption is not helpful for scheduling.] 01/17/23 15:38:21.642
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Jan 17 15:38:22.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-2482" for this suite. 01/17/23 15:38:22.644
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:38:22.653
Jan 17 15:38:22.653: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename sysctl 01/17/23 15:38:22.653
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:38:22.678
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:38:22.682
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 01/17/23 15:38:22.693
STEP: Watching for error events or started pod 01/17/23 15:38:22.724
STEP: Waiting for pod completion 01/17/23 15:38:24.729
Jan 17 15:38:24.729: INFO: Waiting up to 3m0s for pod "sysctl-ba836aec-8d7f-424c-a44e-9bb2cf43f6bc" in namespace "sysctl-487" to be "completed"
Jan 17 15:38:24.732: INFO: Pod "sysctl-ba836aec-8d7f-424c-a44e-9bb2cf43f6bc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.195777ms
Jan 17 15:38:26.736: INFO: Pod "sysctl-ba836aec-8d7f-424c-a44e-9bb2cf43f6bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007423783s
Jan 17 15:38:26.736: INFO: Pod "sysctl-ba836aec-8d7f-424c-a44e-9bb2cf43f6bc" satisfied condition "completed"
STEP: Checking that the pod succeeded 01/17/23 15:38:26.739
STEP: Getting logs from the pod 01/17/23 15:38:26.739
STEP: Checking that the sysctl is actually updated 01/17/23 15:38:26.745
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
Jan 17 15:38:26.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-487" for this suite. 01/17/23 15:38:26.75
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","completed":216,"skipped":3964,"failed":0}
------------------------------
• [4.104 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:38:22.653
    Jan 17 15:38:22.653: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename sysctl 01/17/23 15:38:22.653
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:38:22.678
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:38:22.682
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 01/17/23 15:38:22.693
    STEP: Watching for error events or started pod 01/17/23 15:38:22.724
    STEP: Waiting for pod completion 01/17/23 15:38:24.729
    Jan 17 15:38:24.729: INFO: Waiting up to 3m0s for pod "sysctl-ba836aec-8d7f-424c-a44e-9bb2cf43f6bc" in namespace "sysctl-487" to be "completed"
    Jan 17 15:38:24.732: INFO: Pod "sysctl-ba836aec-8d7f-424c-a44e-9bb2cf43f6bc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.195777ms
    Jan 17 15:38:26.736: INFO: Pod "sysctl-ba836aec-8d7f-424c-a44e-9bb2cf43f6bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007423783s
    Jan 17 15:38:26.736: INFO: Pod "sysctl-ba836aec-8d7f-424c-a44e-9bb2cf43f6bc" satisfied condition "completed"
    STEP: Checking that the pod succeeded 01/17/23 15:38:26.739
    STEP: Getting logs from the pod 01/17/23 15:38:26.739
    STEP: Checking that the sysctl is actually updated 01/17/23 15:38:26.745
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan 17 15:38:26.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sysctl-487" for this suite. 01/17/23 15:38:26.75
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:38:26.757
Jan 17 15:38:26.757: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename secrets 01/17/23 15:38:26.758
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:38:26.789
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:38:26.797
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45
STEP: Creating secret with name secret-test-2154f0be-13e9-4001-ac46-052470f51eb5 01/17/23 15:38:26.8
STEP: Creating a pod to test consume secrets 01/17/23 15:38:26.809
Jan 17 15:38:26.834: INFO: Waiting up to 5m0s for pod "pod-secrets-5ea85c84-81dc-43df-8559-ce40f243e99b" in namespace "secrets-6433" to be "Succeeded or Failed"
Jan 17 15:38:26.839: INFO: Pod "pod-secrets-5ea85c84-81dc-43df-8559-ce40f243e99b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.204385ms
Jan 17 15:38:28.844: INFO: Pod "pod-secrets-5ea85c84-81dc-43df-8559-ce40f243e99b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009768985s
Jan 17 15:38:30.845: INFO: Pod "pod-secrets-5ea85c84-81dc-43df-8559-ce40f243e99b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010804528s
STEP: Saw pod success 01/17/23 15:38:30.845
Jan 17 15:38:30.845: INFO: Pod "pod-secrets-5ea85c84-81dc-43df-8559-ce40f243e99b" satisfied condition "Succeeded or Failed"
Jan 17 15:38:30.848: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-secrets-5ea85c84-81dc-43df-8559-ce40f243e99b container secret-env-test: <nil>
STEP: delete the pod 01/17/23 15:38:30.854
Jan 17 15:38:30.863: INFO: Waiting for pod pod-secrets-5ea85c84-81dc-43df-8559-ce40f243e99b to disappear
Jan 17 15:38:30.866: INFO: Pod pod-secrets-5ea85c84-81dc-43df-8559-ce40f243e99b no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Jan 17 15:38:30.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6433" for this suite. 01/17/23 15:38:30.87
{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","completed":217,"skipped":3979,"failed":0}
------------------------------
• [4.118 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:38:26.757
    Jan 17 15:38:26.757: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename secrets 01/17/23 15:38:26.758
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:38:26.789
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:38:26.797
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:45
    STEP: Creating secret with name secret-test-2154f0be-13e9-4001-ac46-052470f51eb5 01/17/23 15:38:26.8
    STEP: Creating a pod to test consume secrets 01/17/23 15:38:26.809
    Jan 17 15:38:26.834: INFO: Waiting up to 5m0s for pod "pod-secrets-5ea85c84-81dc-43df-8559-ce40f243e99b" in namespace "secrets-6433" to be "Succeeded or Failed"
    Jan 17 15:38:26.839: INFO: Pod "pod-secrets-5ea85c84-81dc-43df-8559-ce40f243e99b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.204385ms
    Jan 17 15:38:28.844: INFO: Pod "pod-secrets-5ea85c84-81dc-43df-8559-ce40f243e99b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009768985s
    Jan 17 15:38:30.845: INFO: Pod "pod-secrets-5ea85c84-81dc-43df-8559-ce40f243e99b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010804528s
    STEP: Saw pod success 01/17/23 15:38:30.845
    Jan 17 15:38:30.845: INFO: Pod "pod-secrets-5ea85c84-81dc-43df-8559-ce40f243e99b" satisfied condition "Succeeded or Failed"
    Jan 17 15:38:30.848: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-secrets-5ea85c84-81dc-43df-8559-ce40f243e99b container secret-env-test: <nil>
    STEP: delete the pod 01/17/23 15:38:30.854
    Jan 17 15:38:30.863: INFO: Waiting for pod pod-secrets-5ea85c84-81dc-43df-8559-ce40f243e99b to disappear
    Jan 17 15:38:30.866: INFO: Pod pod-secrets-5ea85c84-81dc-43df-8559-ce40f243e99b no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Jan 17 15:38:30.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-6433" for this suite. 01/17/23 15:38:30.87
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:38:30.876
Jan 17 15:38:30.877: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename secrets 01/17/23 15:38:30.877
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:38:30.907
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:38:30.91
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153
STEP: creating a secret 01/17/23 15:38:30.912
STEP: listing secrets in all namespaces to ensure that there are more than zero 01/17/23 15:38:30.923
STEP: patching the secret 01/17/23 15:38:30.99
STEP: deleting the secret using a LabelSelector 01/17/23 15:38:31
STEP: listing secrets in all namespaces, searching for label name and value in patch 01/17/23 15:38:31.008
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Jan 17 15:38:31.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8002" for this suite. 01/17/23 15:38:31.066
{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","completed":218,"skipped":4008,"failed":0}
------------------------------
• [0.194 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:38:30.876
    Jan 17 15:38:30.877: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename secrets 01/17/23 15:38:30.877
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:38:30.907
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:38:30.91
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:153
    STEP: creating a secret 01/17/23 15:38:30.912
    STEP: listing secrets in all namespaces to ensure that there are more than zero 01/17/23 15:38:30.923
    STEP: patching the secret 01/17/23 15:38:30.99
    STEP: deleting the secret using a LabelSelector 01/17/23 15:38:31
    STEP: listing secrets in all namespaces, searching for label name and value in patch 01/17/23 15:38:31.008
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Jan 17 15:38:31.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-8002" for this suite. 01/17/23 15:38:31.066
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:38:31.071
Jan 17 15:38:31.071: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename gc 01/17/23 15:38:31.072
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:38:31.094
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:38:31.097
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 01/17/23 15:38:31.106
STEP: delete the rc 01/17/23 15:38:36.127
STEP: wait for the rc to be deleted 01/17/23 15:38:36.133
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 01/17/23 15:38:41.138
STEP: Gathering metrics 01/17/23 15:39:11.151
W0117 15:39:11.154010      22 metrics_grabber.go:110] Can't find any pods in namespace kube-system to grab metrics from
W0117 15:39:11.154024      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan 17 15:39:11.154: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jan 17 15:39:11.154: INFO: Deleting pod "simpletest.rc-2dq2h" in namespace "gc-2890"
Jan 17 15:39:11.165: INFO: Deleting pod "simpletest.rc-2j29v" in namespace "gc-2890"
Jan 17 15:39:11.178: INFO: Deleting pod "simpletest.rc-2q8rp" in namespace "gc-2890"
Jan 17 15:39:11.202: INFO: Deleting pod "simpletest.rc-2q9tp" in namespace "gc-2890"
Jan 17 15:39:11.212: INFO: Deleting pod "simpletest.rc-4gxvd" in namespace "gc-2890"
Jan 17 15:39:11.246: INFO: Deleting pod "simpletest.rc-5595v" in namespace "gc-2890"
Jan 17 15:39:11.258: INFO: Deleting pod "simpletest.rc-58ncs" in namespace "gc-2890"
Jan 17 15:39:11.314: INFO: Deleting pod "simpletest.rc-5927k" in namespace "gc-2890"
Jan 17 15:39:11.335: INFO: Deleting pod "simpletest.rc-5fsd8" in namespace "gc-2890"
Jan 17 15:39:11.346: INFO: Deleting pod "simpletest.rc-5kr55" in namespace "gc-2890"
Jan 17 15:39:11.359: INFO: Deleting pod "simpletest.rc-5r287" in namespace "gc-2890"
Jan 17 15:39:11.369: INFO: Deleting pod "simpletest.rc-5vqxd" in namespace "gc-2890"
Jan 17 15:39:11.386: INFO: Deleting pod "simpletest.rc-5xkm4" in namespace "gc-2890"
Jan 17 15:39:11.408: INFO: Deleting pod "simpletest.rc-64bpc" in namespace "gc-2890"
Jan 17 15:39:11.427: INFO: Deleting pod "simpletest.rc-69m4q" in namespace "gc-2890"
Jan 17 15:39:11.442: INFO: Deleting pod "simpletest.rc-6nk2k" in namespace "gc-2890"
Jan 17 15:39:11.466: INFO: Deleting pod "simpletest.rc-7q4f2" in namespace "gc-2890"
Jan 17 15:39:11.496: INFO: Deleting pod "simpletest.rc-7zcsl" in namespace "gc-2890"
Jan 17 15:39:11.518: INFO: Deleting pod "simpletest.rc-85c5g" in namespace "gc-2890"
Jan 17 15:39:11.535: INFO: Deleting pod "simpletest.rc-8ghzk" in namespace "gc-2890"
Jan 17 15:39:11.548: INFO: Deleting pod "simpletest.rc-8h2fw" in namespace "gc-2890"
Jan 17 15:39:11.563: INFO: Deleting pod "simpletest.rc-8hnwn" in namespace "gc-2890"
Jan 17 15:39:11.586: INFO: Deleting pod "simpletest.rc-9gpbf" in namespace "gc-2890"
Jan 17 15:39:11.602: INFO: Deleting pod "simpletest.rc-9hr4t" in namespace "gc-2890"
Jan 17 15:39:11.620: INFO: Deleting pod "simpletest.rc-9jq84" in namespace "gc-2890"
Jan 17 15:39:11.644: INFO: Deleting pod "simpletest.rc-9qfqv" in namespace "gc-2890"
Jan 17 15:39:11.666: INFO: Deleting pod "simpletest.rc-b2tt4" in namespace "gc-2890"
Jan 17 15:39:11.683: INFO: Deleting pod "simpletest.rc-bwldp" in namespace "gc-2890"
Jan 17 15:39:11.695: INFO: Deleting pod "simpletest.rc-cfqpd" in namespace "gc-2890"
Jan 17 15:39:11.728: INFO: Deleting pod "simpletest.rc-d46xx" in namespace "gc-2890"
Jan 17 15:39:11.745: INFO: Deleting pod "simpletest.rc-d5b8v" in namespace "gc-2890"
Jan 17 15:39:11.794: INFO: Deleting pod "simpletest.rc-d5lmk" in namespace "gc-2890"
Jan 17 15:39:11.810: INFO: Deleting pod "simpletest.rc-dl64b" in namespace "gc-2890"
Jan 17 15:39:11.823: INFO: Deleting pod "simpletest.rc-dsdwv" in namespace "gc-2890"
Jan 17 15:39:11.838: INFO: Deleting pod "simpletest.rc-f2q6b" in namespace "gc-2890"
Jan 17 15:39:11.851: INFO: Deleting pod "simpletest.rc-fhfs7" in namespace "gc-2890"
Jan 17 15:39:11.868: INFO: Deleting pod "simpletest.rc-fqn6m" in namespace "gc-2890"
Jan 17 15:39:11.882: INFO: Deleting pod "simpletest.rc-gbmn5" in namespace "gc-2890"
Jan 17 15:39:11.896: INFO: Deleting pod "simpletest.rc-ggj48" in namespace "gc-2890"
Jan 17 15:39:11.911: INFO: Deleting pod "simpletest.rc-ghqcz" in namespace "gc-2890"
Jan 17 15:39:11.936: INFO: Deleting pod "simpletest.rc-gmrdp" in namespace "gc-2890"
Jan 17 15:39:11.950: INFO: Deleting pod "simpletest.rc-gxnm9" in namespace "gc-2890"
Jan 17 15:39:11.965: INFO: Deleting pod "simpletest.rc-h4c9p" in namespace "gc-2890"
Jan 17 15:39:12.050: INFO: Deleting pod "simpletest.rc-hnmvw" in namespace "gc-2890"
Jan 17 15:39:12.066: INFO: Deleting pod "simpletest.rc-j5plm" in namespace "gc-2890"
Jan 17 15:39:12.082: INFO: Deleting pod "simpletest.rc-jcpk4" in namespace "gc-2890"
Jan 17 15:39:12.100: INFO: Deleting pod "simpletest.rc-jmzqw" in namespace "gc-2890"
Jan 17 15:39:12.116: INFO: Deleting pod "simpletest.rc-jqffn" in namespace "gc-2890"
Jan 17 15:39:12.158: INFO: Deleting pod "simpletest.rc-jr7x5" in namespace "gc-2890"
Jan 17 15:39:12.173: INFO: Deleting pod "simpletest.rc-jrzwq" in namespace "gc-2890"
Jan 17 15:39:12.253: INFO: Deleting pod "simpletest.rc-jt64r" in namespace "gc-2890"
Jan 17 15:39:12.270: INFO: Deleting pod "simpletest.rc-k8rzc" in namespace "gc-2890"
Jan 17 15:39:12.285: INFO: Deleting pod "simpletest.rc-k9llk" in namespace "gc-2890"
Jan 17 15:39:12.300: INFO: Deleting pod "simpletest.rc-kc6jn" in namespace "gc-2890"
Jan 17 15:39:12.314: INFO: Deleting pod "simpletest.rc-l2zg7" in namespace "gc-2890"
Jan 17 15:39:12.336: INFO: Deleting pod "simpletest.rc-ld6dz" in namespace "gc-2890"
Jan 17 15:39:12.352: INFO: Deleting pod "simpletest.rc-lhv2r" in namespace "gc-2890"
Jan 17 15:39:12.378: INFO: Deleting pod "simpletest.rc-lxn9c" in namespace "gc-2890"
Jan 17 15:39:12.389: INFO: Deleting pod "simpletest.rc-m8tst" in namespace "gc-2890"
Jan 17 15:39:12.407: INFO: Deleting pod "simpletest.rc-mj8hx" in namespace "gc-2890"
Jan 17 15:39:12.419: INFO: Deleting pod "simpletest.rc-mxfqx" in namespace "gc-2890"
Jan 17 15:39:12.438: INFO: Deleting pod "simpletest.rc-n62wc" in namespace "gc-2890"
Jan 17 15:39:12.459: INFO: Deleting pod "simpletest.rc-n6gb5" in namespace "gc-2890"
Jan 17 15:39:12.480: INFO: Deleting pod "simpletest.rc-nnms4" in namespace "gc-2890"
Jan 17 15:39:12.491: INFO: Deleting pod "simpletest.rc-npj2p" in namespace "gc-2890"
Jan 17 15:39:12.502: INFO: Deleting pod "simpletest.rc-nz526" in namespace "gc-2890"
Jan 17 15:39:12.514: INFO: Deleting pod "simpletest.rc-pr9x8" in namespace "gc-2890"
Jan 17 15:39:12.537: INFO: Deleting pod "simpletest.rc-pwx25" in namespace "gc-2890"
Jan 17 15:39:12.550: INFO: Deleting pod "simpletest.rc-q26j4" in namespace "gc-2890"
Jan 17 15:39:12.567: INFO: Deleting pod "simpletest.rc-qlkqq" in namespace "gc-2890"
Jan 17 15:39:12.586: INFO: Deleting pod "simpletest.rc-qlqvh" in namespace "gc-2890"
Jan 17 15:39:12.612: INFO: Deleting pod "simpletest.rc-qpjkl" in namespace "gc-2890"
Jan 17 15:39:12.627: INFO: Deleting pod "simpletest.rc-qt5x2" in namespace "gc-2890"
Jan 17 15:39:12.641: INFO: Deleting pod "simpletest.rc-r5dxm" in namespace "gc-2890"
Jan 17 15:39:12.653: INFO: Deleting pod "simpletest.rc-rjshg" in namespace "gc-2890"
Jan 17 15:39:12.671: INFO: Deleting pod "simpletest.rc-rl826" in namespace "gc-2890"
Jan 17 15:39:12.686: INFO: Deleting pod "simpletest.rc-rxh4q" in namespace "gc-2890"
Jan 17 15:39:12.711: INFO: Deleting pod "simpletest.rc-s689z" in namespace "gc-2890"
Jan 17 15:39:12.726: INFO: Deleting pod "simpletest.rc-s7f2g" in namespace "gc-2890"
Jan 17 15:39:12.739: INFO: Deleting pod "simpletest.rc-s7j22" in namespace "gc-2890"
Jan 17 15:39:12.766: INFO: Deleting pod "simpletest.rc-sdcgs" in namespace "gc-2890"
Jan 17 15:39:12.820: INFO: Deleting pod "simpletest.rc-sqtck" in namespace "gc-2890"
Jan 17 15:39:12.861: INFO: Deleting pod "simpletest.rc-srwph" in namespace "gc-2890"
Jan 17 15:39:12.910: INFO: Deleting pod "simpletest.rc-t8r2h" in namespace "gc-2890"
Jan 17 15:39:12.994: INFO: Deleting pod "simpletest.rc-tlmgt" in namespace "gc-2890"
Jan 17 15:39:13.049: INFO: Deleting pod "simpletest.rc-tqqrg" in namespace "gc-2890"
Jan 17 15:39:13.061: INFO: Deleting pod "simpletest.rc-v9jb4" in namespace "gc-2890"
Jan 17 15:39:13.107: INFO: Deleting pod "simpletest.rc-vd4zj" in namespace "gc-2890"
Jan 17 15:39:13.199: INFO: Deleting pod "simpletest.rc-vf9w4" in namespace "gc-2890"
Jan 17 15:39:13.212: INFO: Deleting pod "simpletest.rc-vjrj9" in namespace "gc-2890"
Jan 17 15:39:13.263: INFO: Deleting pod "simpletest.rc-w5r6s" in namespace "gc-2890"
Jan 17 15:39:13.314: INFO: Deleting pod "simpletest.rc-w8lcc" in namespace "gc-2890"
Jan 17 15:39:13.404: INFO: Deleting pod "simpletest.rc-wbtz6" in namespace "gc-2890"
Jan 17 15:39:13.439: INFO: Deleting pod "simpletest.rc-wrm4w" in namespace "gc-2890"
Jan 17 15:39:13.470: INFO: Deleting pod "simpletest.rc-ww8sb" in namespace "gc-2890"
Jan 17 15:39:13.505: INFO: Deleting pod "simpletest.rc-x586x" in namespace "gc-2890"
Jan 17 15:39:13.598: INFO: Deleting pod "simpletest.rc-x726n" in namespace "gc-2890"
Jan 17 15:39:13.613: INFO: Deleting pod "simpletest.rc-xn7lx" in namespace "gc-2890"
Jan 17 15:39:13.654: INFO: Deleting pod "simpletest.rc-zwd8q" in namespace "gc-2890"
Jan 17 15:39:13.709: INFO: Deleting pod "simpletest.rc-zxzl4" in namespace "gc-2890"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan 17 15:39:13.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2890" for this suite. 01/17/23 15:39:13.795
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","completed":219,"skipped":4013,"failed":0}
------------------------------
• [SLOW TEST] [42.777 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:38:31.071
    Jan 17 15:38:31.071: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename gc 01/17/23 15:38:31.072
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:38:31.094
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:38:31.097
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 01/17/23 15:38:31.106
    STEP: delete the rc 01/17/23 15:38:36.127
    STEP: wait for the rc to be deleted 01/17/23 15:38:36.133
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 01/17/23 15:38:41.138
    STEP: Gathering metrics 01/17/23 15:39:11.151
    W0117 15:39:11.154010      22 metrics_grabber.go:110] Can't find any pods in namespace kube-system to grab metrics from
    W0117 15:39:11.154024      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jan 17 15:39:11.154: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Jan 17 15:39:11.154: INFO: Deleting pod "simpletest.rc-2dq2h" in namespace "gc-2890"
    Jan 17 15:39:11.165: INFO: Deleting pod "simpletest.rc-2j29v" in namespace "gc-2890"
    Jan 17 15:39:11.178: INFO: Deleting pod "simpletest.rc-2q8rp" in namespace "gc-2890"
    Jan 17 15:39:11.202: INFO: Deleting pod "simpletest.rc-2q9tp" in namespace "gc-2890"
    Jan 17 15:39:11.212: INFO: Deleting pod "simpletest.rc-4gxvd" in namespace "gc-2890"
    Jan 17 15:39:11.246: INFO: Deleting pod "simpletest.rc-5595v" in namespace "gc-2890"
    Jan 17 15:39:11.258: INFO: Deleting pod "simpletest.rc-58ncs" in namespace "gc-2890"
    Jan 17 15:39:11.314: INFO: Deleting pod "simpletest.rc-5927k" in namespace "gc-2890"
    Jan 17 15:39:11.335: INFO: Deleting pod "simpletest.rc-5fsd8" in namespace "gc-2890"
    Jan 17 15:39:11.346: INFO: Deleting pod "simpletest.rc-5kr55" in namespace "gc-2890"
    Jan 17 15:39:11.359: INFO: Deleting pod "simpletest.rc-5r287" in namespace "gc-2890"
    Jan 17 15:39:11.369: INFO: Deleting pod "simpletest.rc-5vqxd" in namespace "gc-2890"
    Jan 17 15:39:11.386: INFO: Deleting pod "simpletest.rc-5xkm4" in namespace "gc-2890"
    Jan 17 15:39:11.408: INFO: Deleting pod "simpletest.rc-64bpc" in namespace "gc-2890"
    Jan 17 15:39:11.427: INFO: Deleting pod "simpletest.rc-69m4q" in namespace "gc-2890"
    Jan 17 15:39:11.442: INFO: Deleting pod "simpletest.rc-6nk2k" in namespace "gc-2890"
    Jan 17 15:39:11.466: INFO: Deleting pod "simpletest.rc-7q4f2" in namespace "gc-2890"
    Jan 17 15:39:11.496: INFO: Deleting pod "simpletest.rc-7zcsl" in namespace "gc-2890"
    Jan 17 15:39:11.518: INFO: Deleting pod "simpletest.rc-85c5g" in namespace "gc-2890"
    Jan 17 15:39:11.535: INFO: Deleting pod "simpletest.rc-8ghzk" in namespace "gc-2890"
    Jan 17 15:39:11.548: INFO: Deleting pod "simpletest.rc-8h2fw" in namespace "gc-2890"
    Jan 17 15:39:11.563: INFO: Deleting pod "simpletest.rc-8hnwn" in namespace "gc-2890"
    Jan 17 15:39:11.586: INFO: Deleting pod "simpletest.rc-9gpbf" in namespace "gc-2890"
    Jan 17 15:39:11.602: INFO: Deleting pod "simpletest.rc-9hr4t" in namespace "gc-2890"
    Jan 17 15:39:11.620: INFO: Deleting pod "simpletest.rc-9jq84" in namespace "gc-2890"
    Jan 17 15:39:11.644: INFO: Deleting pod "simpletest.rc-9qfqv" in namespace "gc-2890"
    Jan 17 15:39:11.666: INFO: Deleting pod "simpletest.rc-b2tt4" in namespace "gc-2890"
    Jan 17 15:39:11.683: INFO: Deleting pod "simpletest.rc-bwldp" in namespace "gc-2890"
    Jan 17 15:39:11.695: INFO: Deleting pod "simpletest.rc-cfqpd" in namespace "gc-2890"
    Jan 17 15:39:11.728: INFO: Deleting pod "simpletest.rc-d46xx" in namespace "gc-2890"
    Jan 17 15:39:11.745: INFO: Deleting pod "simpletest.rc-d5b8v" in namespace "gc-2890"
    Jan 17 15:39:11.794: INFO: Deleting pod "simpletest.rc-d5lmk" in namespace "gc-2890"
    Jan 17 15:39:11.810: INFO: Deleting pod "simpletest.rc-dl64b" in namespace "gc-2890"
    Jan 17 15:39:11.823: INFO: Deleting pod "simpletest.rc-dsdwv" in namespace "gc-2890"
    Jan 17 15:39:11.838: INFO: Deleting pod "simpletest.rc-f2q6b" in namespace "gc-2890"
    Jan 17 15:39:11.851: INFO: Deleting pod "simpletest.rc-fhfs7" in namespace "gc-2890"
    Jan 17 15:39:11.868: INFO: Deleting pod "simpletest.rc-fqn6m" in namespace "gc-2890"
    Jan 17 15:39:11.882: INFO: Deleting pod "simpletest.rc-gbmn5" in namespace "gc-2890"
    Jan 17 15:39:11.896: INFO: Deleting pod "simpletest.rc-ggj48" in namespace "gc-2890"
    Jan 17 15:39:11.911: INFO: Deleting pod "simpletest.rc-ghqcz" in namespace "gc-2890"
    Jan 17 15:39:11.936: INFO: Deleting pod "simpletest.rc-gmrdp" in namespace "gc-2890"
    Jan 17 15:39:11.950: INFO: Deleting pod "simpletest.rc-gxnm9" in namespace "gc-2890"
    Jan 17 15:39:11.965: INFO: Deleting pod "simpletest.rc-h4c9p" in namespace "gc-2890"
    Jan 17 15:39:12.050: INFO: Deleting pod "simpletest.rc-hnmvw" in namespace "gc-2890"
    Jan 17 15:39:12.066: INFO: Deleting pod "simpletest.rc-j5plm" in namespace "gc-2890"
    Jan 17 15:39:12.082: INFO: Deleting pod "simpletest.rc-jcpk4" in namespace "gc-2890"
    Jan 17 15:39:12.100: INFO: Deleting pod "simpletest.rc-jmzqw" in namespace "gc-2890"
    Jan 17 15:39:12.116: INFO: Deleting pod "simpletest.rc-jqffn" in namespace "gc-2890"
    Jan 17 15:39:12.158: INFO: Deleting pod "simpletest.rc-jr7x5" in namespace "gc-2890"
    Jan 17 15:39:12.173: INFO: Deleting pod "simpletest.rc-jrzwq" in namespace "gc-2890"
    Jan 17 15:39:12.253: INFO: Deleting pod "simpletest.rc-jt64r" in namespace "gc-2890"
    Jan 17 15:39:12.270: INFO: Deleting pod "simpletest.rc-k8rzc" in namespace "gc-2890"
    Jan 17 15:39:12.285: INFO: Deleting pod "simpletest.rc-k9llk" in namespace "gc-2890"
    Jan 17 15:39:12.300: INFO: Deleting pod "simpletest.rc-kc6jn" in namespace "gc-2890"
    Jan 17 15:39:12.314: INFO: Deleting pod "simpletest.rc-l2zg7" in namespace "gc-2890"
    Jan 17 15:39:12.336: INFO: Deleting pod "simpletest.rc-ld6dz" in namespace "gc-2890"
    Jan 17 15:39:12.352: INFO: Deleting pod "simpletest.rc-lhv2r" in namespace "gc-2890"
    Jan 17 15:39:12.378: INFO: Deleting pod "simpletest.rc-lxn9c" in namespace "gc-2890"
    Jan 17 15:39:12.389: INFO: Deleting pod "simpletest.rc-m8tst" in namespace "gc-2890"
    Jan 17 15:39:12.407: INFO: Deleting pod "simpletest.rc-mj8hx" in namespace "gc-2890"
    Jan 17 15:39:12.419: INFO: Deleting pod "simpletest.rc-mxfqx" in namespace "gc-2890"
    Jan 17 15:39:12.438: INFO: Deleting pod "simpletest.rc-n62wc" in namespace "gc-2890"
    Jan 17 15:39:12.459: INFO: Deleting pod "simpletest.rc-n6gb5" in namespace "gc-2890"
    Jan 17 15:39:12.480: INFO: Deleting pod "simpletest.rc-nnms4" in namespace "gc-2890"
    Jan 17 15:39:12.491: INFO: Deleting pod "simpletest.rc-npj2p" in namespace "gc-2890"
    Jan 17 15:39:12.502: INFO: Deleting pod "simpletest.rc-nz526" in namespace "gc-2890"
    Jan 17 15:39:12.514: INFO: Deleting pod "simpletest.rc-pr9x8" in namespace "gc-2890"
    Jan 17 15:39:12.537: INFO: Deleting pod "simpletest.rc-pwx25" in namespace "gc-2890"
    Jan 17 15:39:12.550: INFO: Deleting pod "simpletest.rc-q26j4" in namespace "gc-2890"
    Jan 17 15:39:12.567: INFO: Deleting pod "simpletest.rc-qlkqq" in namespace "gc-2890"
    Jan 17 15:39:12.586: INFO: Deleting pod "simpletest.rc-qlqvh" in namespace "gc-2890"
    Jan 17 15:39:12.612: INFO: Deleting pod "simpletest.rc-qpjkl" in namespace "gc-2890"
    Jan 17 15:39:12.627: INFO: Deleting pod "simpletest.rc-qt5x2" in namespace "gc-2890"
    Jan 17 15:39:12.641: INFO: Deleting pod "simpletest.rc-r5dxm" in namespace "gc-2890"
    Jan 17 15:39:12.653: INFO: Deleting pod "simpletest.rc-rjshg" in namespace "gc-2890"
    Jan 17 15:39:12.671: INFO: Deleting pod "simpletest.rc-rl826" in namespace "gc-2890"
    Jan 17 15:39:12.686: INFO: Deleting pod "simpletest.rc-rxh4q" in namespace "gc-2890"
    Jan 17 15:39:12.711: INFO: Deleting pod "simpletest.rc-s689z" in namespace "gc-2890"
    Jan 17 15:39:12.726: INFO: Deleting pod "simpletest.rc-s7f2g" in namespace "gc-2890"
    Jan 17 15:39:12.739: INFO: Deleting pod "simpletest.rc-s7j22" in namespace "gc-2890"
    Jan 17 15:39:12.766: INFO: Deleting pod "simpletest.rc-sdcgs" in namespace "gc-2890"
    Jan 17 15:39:12.820: INFO: Deleting pod "simpletest.rc-sqtck" in namespace "gc-2890"
    Jan 17 15:39:12.861: INFO: Deleting pod "simpletest.rc-srwph" in namespace "gc-2890"
    Jan 17 15:39:12.910: INFO: Deleting pod "simpletest.rc-t8r2h" in namespace "gc-2890"
    Jan 17 15:39:12.994: INFO: Deleting pod "simpletest.rc-tlmgt" in namespace "gc-2890"
    Jan 17 15:39:13.049: INFO: Deleting pod "simpletest.rc-tqqrg" in namespace "gc-2890"
    Jan 17 15:39:13.061: INFO: Deleting pod "simpletest.rc-v9jb4" in namespace "gc-2890"
    Jan 17 15:39:13.107: INFO: Deleting pod "simpletest.rc-vd4zj" in namespace "gc-2890"
    Jan 17 15:39:13.199: INFO: Deleting pod "simpletest.rc-vf9w4" in namespace "gc-2890"
    Jan 17 15:39:13.212: INFO: Deleting pod "simpletest.rc-vjrj9" in namespace "gc-2890"
    Jan 17 15:39:13.263: INFO: Deleting pod "simpletest.rc-w5r6s" in namespace "gc-2890"
    Jan 17 15:39:13.314: INFO: Deleting pod "simpletest.rc-w8lcc" in namespace "gc-2890"
    Jan 17 15:39:13.404: INFO: Deleting pod "simpletest.rc-wbtz6" in namespace "gc-2890"
    Jan 17 15:39:13.439: INFO: Deleting pod "simpletest.rc-wrm4w" in namespace "gc-2890"
    Jan 17 15:39:13.470: INFO: Deleting pod "simpletest.rc-ww8sb" in namespace "gc-2890"
    Jan 17 15:39:13.505: INFO: Deleting pod "simpletest.rc-x586x" in namespace "gc-2890"
    Jan 17 15:39:13.598: INFO: Deleting pod "simpletest.rc-x726n" in namespace "gc-2890"
    Jan 17 15:39:13.613: INFO: Deleting pod "simpletest.rc-xn7lx" in namespace "gc-2890"
    Jan 17 15:39:13.654: INFO: Deleting pod "simpletest.rc-zwd8q" in namespace "gc-2890"
    Jan 17 15:39:13.709: INFO: Deleting pod "simpletest.rc-zxzl4" in namespace "gc-2890"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan 17 15:39:13.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-2890" for this suite. 01/17/23 15:39:13.795
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:39:13.849
Jan 17 15:39:13.849: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename pod-network-test 01/17/23 15:39:13.85
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:39:13.891
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:39:13.894
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-370 01/17/23 15:39:13.901
STEP: creating a selector 01/17/23 15:39:13.901
STEP: Creating the service pods in kubernetes 01/17/23 15:39:13.901
Jan 17 15:39:13.901: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 17 15:39:14.035: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-370" to be "running and ready"
Jan 17 15:39:14.045: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.242317ms
Jan 17 15:39:14.045: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 15:39:16.097: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.062250884s
Jan 17 15:39:16.097: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 15:39:18.049: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014249035s
Jan 17 15:39:18.049: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 15:39:20.050: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.014742981s
Jan 17 15:39:20.050: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 17 15:39:22.050: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.015091382s
Jan 17 15:39:22.050: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 17 15:39:24.050: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.01503193s
Jan 17 15:39:24.050: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 17 15:39:26.050: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.01528726s
Jan 17 15:39:26.050: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 17 15:39:28.050: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.01459133s
Jan 17 15:39:28.050: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 17 15:39:30.049: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.014460749s
Jan 17 15:39:30.049: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 17 15:39:32.050: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.01512275s
Jan 17 15:39:32.050: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 17 15:39:34.049: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.014102895s
Jan 17 15:39:34.049: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 17 15:39:36.050: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.015334905s
Jan 17 15:39:36.050: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan 17 15:39:36.050: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan 17 15:39:36.053: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-370" to be "running and ready"
Jan 17 15:39:36.056: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.145319ms
Jan 17 15:39:36.056: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan 17 15:39:36.056: INFO: Pod "netserver-1" satisfied condition "running and ready"
Jan 17 15:39:36.059: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-370" to be "running and ready"
Jan 17 15:39:36.062: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.635338ms
Jan 17 15:39:36.062: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Jan 17 15:39:36.062: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 01/17/23 15:39:36.064
Jan 17 15:39:36.076: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-370" to be "running"
Jan 17 15:39:36.078: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.744897ms
Jan 17 15:39:38.081: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.005654995s
Jan 17 15:39:38.081: INFO: Pod "test-container-pod" satisfied condition "running"
Jan 17 15:39:38.084: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Jan 17 15:39:38.084: INFO: Breadth first check of 10.129.2.139 on host 10.0.139.213...
Jan 17 15:39:38.087: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.131.1.23:9080/dial?request=hostname&protocol=udp&host=10.129.2.139&port=8081&tries=1'] Namespace:pod-network-test-370 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 15:39:38.087: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
Jan 17 15:39:38.088: INFO: ExecWithOptions: Clientset creation
Jan 17 15:39:38.088: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-370/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.131.1.23%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.129.2.139%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 17 15:39:38.164: INFO: Waiting for responses: map[]
Jan 17 15:39:38.164: INFO: reached 10.129.2.139 after 0/1 tries
Jan 17 15:39:38.164: INFO: Breadth first check of 10.131.1.22 on host 10.0.151.22...
Jan 17 15:39:38.168: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.131.1.23:9080/dial?request=hostname&protocol=udp&host=10.131.1.22&port=8081&tries=1'] Namespace:pod-network-test-370 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 15:39:38.168: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
Jan 17 15:39:38.168: INFO: ExecWithOptions: Clientset creation
Jan 17 15:39:38.168: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-370/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.131.1.23%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.131.1.22%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 17 15:39:38.225: INFO: Waiting for responses: map[]
Jan 17 15:39:38.225: INFO: reached 10.131.1.22 after 0/1 tries
Jan 17 15:39:38.225: INFO: Breadth first check of 10.128.2.177 on host 10.0.165.14...
Jan 17 15:39:38.228: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.131.1.23:9080/dial?request=hostname&protocol=udp&host=10.128.2.177&port=8081&tries=1'] Namespace:pod-network-test-370 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 15:39:38.228: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
Jan 17 15:39:38.228: INFO: ExecWithOptions: Clientset creation
Jan 17 15:39:38.228: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-370/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.131.1.23%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.128.2.177%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 17 15:39:38.289: INFO: Waiting for responses: map[]
Jan 17 15:39:38.290: INFO: reached 10.128.2.177 after 0/1 tries
Jan 17 15:39:38.290: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Jan 17 15:39:38.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-370" for this suite. 01/17/23 15:39:38.294
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","completed":220,"skipped":4019,"failed":0}
------------------------------
• [SLOW TEST] [24.451 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:39:13.849
    Jan 17 15:39:13.849: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename pod-network-test 01/17/23 15:39:13.85
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:39:13.891
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:39:13.894
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-370 01/17/23 15:39:13.901
    STEP: creating a selector 01/17/23 15:39:13.901
    STEP: Creating the service pods in kubernetes 01/17/23 15:39:13.901
    Jan 17 15:39:13.901: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan 17 15:39:14.035: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-370" to be "running and ready"
    Jan 17 15:39:14.045: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.242317ms
    Jan 17 15:39:14.045: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 15:39:16.097: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.062250884s
    Jan 17 15:39:16.097: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 15:39:18.049: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014249035s
    Jan 17 15:39:18.049: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 15:39:20.050: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.014742981s
    Jan 17 15:39:20.050: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 17 15:39:22.050: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.015091382s
    Jan 17 15:39:22.050: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 17 15:39:24.050: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.01503193s
    Jan 17 15:39:24.050: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 17 15:39:26.050: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.01528726s
    Jan 17 15:39:26.050: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 17 15:39:28.050: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.01459133s
    Jan 17 15:39:28.050: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 17 15:39:30.049: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.014460749s
    Jan 17 15:39:30.049: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 17 15:39:32.050: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.01512275s
    Jan 17 15:39:32.050: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 17 15:39:34.049: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.014102895s
    Jan 17 15:39:34.049: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 17 15:39:36.050: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.015334905s
    Jan 17 15:39:36.050: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan 17 15:39:36.050: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan 17 15:39:36.053: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-370" to be "running and ready"
    Jan 17 15:39:36.056: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.145319ms
    Jan 17 15:39:36.056: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan 17 15:39:36.056: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Jan 17 15:39:36.059: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-370" to be "running and ready"
    Jan 17 15:39:36.062: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.635338ms
    Jan 17 15:39:36.062: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Jan 17 15:39:36.062: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 01/17/23 15:39:36.064
    Jan 17 15:39:36.076: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-370" to be "running"
    Jan 17 15:39:36.078: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.744897ms
    Jan 17 15:39:38.081: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.005654995s
    Jan 17 15:39:38.081: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan 17 15:39:38.084: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Jan 17 15:39:38.084: INFO: Breadth first check of 10.129.2.139 on host 10.0.139.213...
    Jan 17 15:39:38.087: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.131.1.23:9080/dial?request=hostname&protocol=udp&host=10.129.2.139&port=8081&tries=1'] Namespace:pod-network-test-370 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 17 15:39:38.087: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    Jan 17 15:39:38.088: INFO: ExecWithOptions: Clientset creation
    Jan 17 15:39:38.088: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-370/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.131.1.23%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.129.2.139%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 17 15:39:38.164: INFO: Waiting for responses: map[]
    Jan 17 15:39:38.164: INFO: reached 10.129.2.139 after 0/1 tries
    Jan 17 15:39:38.164: INFO: Breadth first check of 10.131.1.22 on host 10.0.151.22...
    Jan 17 15:39:38.168: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.131.1.23:9080/dial?request=hostname&protocol=udp&host=10.131.1.22&port=8081&tries=1'] Namespace:pod-network-test-370 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 17 15:39:38.168: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    Jan 17 15:39:38.168: INFO: ExecWithOptions: Clientset creation
    Jan 17 15:39:38.168: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-370/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.131.1.23%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.131.1.22%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 17 15:39:38.225: INFO: Waiting for responses: map[]
    Jan 17 15:39:38.225: INFO: reached 10.131.1.22 after 0/1 tries
    Jan 17 15:39:38.225: INFO: Breadth first check of 10.128.2.177 on host 10.0.165.14...
    Jan 17 15:39:38.228: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.131.1.23:9080/dial?request=hostname&protocol=udp&host=10.128.2.177&port=8081&tries=1'] Namespace:pod-network-test-370 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 17 15:39:38.228: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    Jan 17 15:39:38.228: INFO: ExecWithOptions: Clientset creation
    Jan 17 15:39:38.228: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-370/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.131.1.23%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.128.2.177%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 17 15:39:38.289: INFO: Waiting for responses: map[]
    Jan 17 15:39:38.290: INFO: reached 10.128.2.177 after 0/1 tries
    Jan 17 15:39:38.290: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Jan 17 15:39:38.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-370" for this suite. 01/17/23 15:39:38.294
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:39:38.301
Jan 17 15:39:38.301: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename replicaset 01/17/23 15:39:38.301
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:39:38.325
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:39:38.327
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 01/17/23 15:39:38.329
W0117 15:39:38.335337      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Verify that the required pods have come up 01/17/23 15:39:38.335
Jan 17 15:39:38.346: INFO: Pod name sample-pod: Found 0 pods out of 3
Jan 17 15:39:43.350: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 01/17/23 15:39:43.35
Jan 17 15:39:43.355: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 01/17/23 15:39:43.355
STEP: DeleteCollection of the ReplicaSets 01/17/23 15:39:43.362
STEP: After DeleteCollection verify that ReplicaSets have been deleted 01/17/23 15:39:43.377
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jan 17 15:39:43.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-7600" for this suite. 01/17/23 15:39:43.396
{"msg":"PASSED [sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]","completed":221,"skipped":4035,"failed":0}
------------------------------
• [SLOW TEST] [5.102 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:39:38.301
    Jan 17 15:39:38.301: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename replicaset 01/17/23 15:39:38.301
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:39:38.325
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:39:38.327
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 01/17/23 15:39:38.329
    W0117 15:39:38.335337      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Verify that the required pods have come up 01/17/23 15:39:38.335
    Jan 17 15:39:38.346: INFO: Pod name sample-pod: Found 0 pods out of 3
    Jan 17 15:39:43.350: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 01/17/23 15:39:43.35
    Jan 17 15:39:43.355: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 01/17/23 15:39:43.355
    STEP: DeleteCollection of the ReplicaSets 01/17/23 15:39:43.362
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 01/17/23 15:39:43.377
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jan 17 15:39:43.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-7600" for this suite. 01/17/23 15:39:43.396
  << End Captured GinkgoWriter Output
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:39:43.402
Jan 17 15:39:43.403: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename limitrange 01/17/23 15:39:43.403
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:39:43.43
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:39:43.432
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57
STEP: Creating a LimitRange 01/17/23 15:39:43.441
STEP: Setting up watch 01/17/23 15:39:43.441
STEP: Submitting a LimitRange 01/17/23 15:39:43.549
STEP: Verifying LimitRange creation was observed 01/17/23 15:39:43.554
STEP: Fetching the LimitRange to ensure it has proper values 01/17/23 15:39:43.559
Jan 17 15:39:43.563: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jan 17 15:39:43.563: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 01/17/23 15:39:43.563
STEP: Ensuring Pod has resource requirements applied from LimitRange 01/17/23 15:39:43.575
Jan 17 15:39:43.578: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jan 17 15:39:43.579: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 01/17/23 15:39:43.579
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 01/17/23 15:39:43.591
Jan 17 15:39:43.595: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Jan 17 15:39:43.595: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 01/17/23 15:39:43.595
STEP: Failing to create a Pod with more than max resources 01/17/23 15:39:43.601
STEP: Updating a LimitRange 01/17/23 15:39:43.607
STEP: Verifying LimitRange updating is effective 01/17/23 15:39:43.621
STEP: Creating a Pod with less than former min resources 01/17/23 15:39:45.625
STEP: Failing to create a Pod with more than max resources 01/17/23 15:39:45.636
STEP: Deleting a LimitRange 01/17/23 15:39:45.642
STEP: Verifying the LimitRange was deleted 01/17/23 15:39:45.648
Jan 17 15:39:50.654: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 01/17/23 15:39:50.654
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:187
Jan 17 15:39:50.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-9115" for this suite. 01/17/23 15:39:50.672
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","completed":222,"skipped":4035,"failed":0}
------------------------------
• [SLOW TEST] [7.277 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:39:43.402
    Jan 17 15:39:43.403: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename limitrange 01/17/23 15:39:43.403
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:39:43.43
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:39:43.432
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:57
    STEP: Creating a LimitRange 01/17/23 15:39:43.441
    STEP: Setting up watch 01/17/23 15:39:43.441
    STEP: Submitting a LimitRange 01/17/23 15:39:43.549
    STEP: Verifying LimitRange creation was observed 01/17/23 15:39:43.554
    STEP: Fetching the LimitRange to ensure it has proper values 01/17/23 15:39:43.559
    Jan 17 15:39:43.563: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Jan 17 15:39:43.563: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 01/17/23 15:39:43.563
    STEP: Ensuring Pod has resource requirements applied from LimitRange 01/17/23 15:39:43.575
    Jan 17 15:39:43.578: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Jan 17 15:39:43.579: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 01/17/23 15:39:43.579
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 01/17/23 15:39:43.591
    Jan 17 15:39:43.595: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    Jan 17 15:39:43.595: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 01/17/23 15:39:43.595
    STEP: Failing to create a Pod with more than max resources 01/17/23 15:39:43.601
    STEP: Updating a LimitRange 01/17/23 15:39:43.607
    STEP: Verifying LimitRange updating is effective 01/17/23 15:39:43.621
    STEP: Creating a Pod with less than former min resources 01/17/23 15:39:45.625
    STEP: Failing to create a Pod with more than max resources 01/17/23 15:39:45.636
    STEP: Deleting a LimitRange 01/17/23 15:39:45.642
    STEP: Verifying the LimitRange was deleted 01/17/23 15:39:45.648
    Jan 17 15:39:50.654: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 01/17/23 15:39:50.654
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/framework.go:187
    Jan 17 15:39:50.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "limitrange-9115" for this suite. 01/17/23 15:39:50.672
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:304
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:39:50.68
Jan 17 15:39:50.680: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename statefulset 01/17/23 15:39:50.681
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:39:50.7
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:39:50.702
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-1891 01/17/23 15:39:50.704
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:304
STEP: Creating a new StatefulSet 01/17/23 15:39:50.712
W0117 15:39:50.727187      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 17 15:39:50.732: INFO: Found 0 stateful pods, waiting for 3
Jan 17 15:40:00.738: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 17 15:40:00.738: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 17 15:40:00.738: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jan 17 15:40:00.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=statefulset-1891 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 17 15:40:00.871: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 17 15:40:00.871: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 17 15:40:00.871: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 01/17/23 15:40:10.886
Jan 17 15:40:10.905: INFO: Updating stateful set ss2
STEP: Creating a new revision 01/17/23 15:40:10.905
STEP: Updating Pods in reverse ordinal order 01/17/23 15:40:20.919
Jan 17 15:40:20.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=statefulset-1891 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 17 15:40:21.042: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 17 15:40:21.042: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 17 15:40:21.042: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision 01/17/23 15:40:31.064
Jan 17 15:40:31.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=statefulset-1891 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 17 15:40:31.173: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 17 15:40:31.173: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 17 15:40:31.173: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 17 15:40:41.209: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 01/17/23 15:40:51.223
Jan 17 15:40:51.226: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=statefulset-1891 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 17 15:40:51.322: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 17 15:40:51.322: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 17 15:40:51.322: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 17 15:41:01.346: INFO: Deleting all statefulset in ns statefulset-1891
Jan 17 15:41:01.349: INFO: Scaling statefulset ss2 to 0
Jan 17 15:41:11.368: INFO: Waiting for statefulset status.replicas updated to 0
Jan 17 15:41:11.371: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan 17 15:41:11.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1891" for this suite. 01/17/23 15:41:11.396
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","completed":223,"skipped":4059,"failed":0}
------------------------------
• [SLOW TEST] [80.723 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:304

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:39:50.68
    Jan 17 15:39:50.680: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename statefulset 01/17/23 15:39:50.681
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:39:50.7
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:39:50.702
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-1891 01/17/23 15:39:50.704
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:304
    STEP: Creating a new StatefulSet 01/17/23 15:39:50.712
    W0117 15:39:50.727187      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jan 17 15:39:50.732: INFO: Found 0 stateful pods, waiting for 3
    Jan 17 15:40:00.738: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 17 15:40:00.738: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 17 15:40:00.738: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    Jan 17 15:40:00.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=statefulset-1891 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 17 15:40:00.871: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 17 15:40:00.871: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 17 15:40:00.871: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 01/17/23 15:40:10.886
    Jan 17 15:40:10.905: INFO: Updating stateful set ss2
    STEP: Creating a new revision 01/17/23 15:40:10.905
    STEP: Updating Pods in reverse ordinal order 01/17/23 15:40:20.919
    Jan 17 15:40:20.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=statefulset-1891 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 17 15:40:21.042: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 17 15:40:21.042: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 17 15:40:21.042: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    STEP: Rolling back to a previous revision 01/17/23 15:40:31.064
    Jan 17 15:40:31.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=statefulset-1891 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 17 15:40:31.173: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 17 15:40:31.173: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 17 15:40:31.173: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 17 15:40:41.209: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 01/17/23 15:40:51.223
    Jan 17 15:40:51.226: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=statefulset-1891 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 17 15:40:51.322: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 17 15:40:51.322: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 17 15:40:51.322: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan 17 15:41:01.346: INFO: Deleting all statefulset in ns statefulset-1891
    Jan 17 15:41:01.349: INFO: Scaling statefulset ss2 to 0
    Jan 17 15:41:11.368: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 17 15:41:11.371: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan 17 15:41:11.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-1891" for this suite. 01/17/23 15:41:11.396
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:41:11.403
Jan 17 15:41:11.403: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename runtimeclass 01/17/23 15:41:11.404
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:41:11.431
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:41:11.435
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-5763-delete-me 01/17/23 15:41:11.453
STEP: Waiting for the RuntimeClass to disappear 01/17/23 15:41:11.476
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Jan 17 15:41:11.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-5763" for this suite. 01/17/23 15:41:11.504
{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]","completed":224,"skipped":4061,"failed":0}
------------------------------
• [0.107 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:41:11.403
    Jan 17 15:41:11.403: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename runtimeclass 01/17/23 15:41:11.404
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:41:11.431
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:41:11.435
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-5763-delete-me 01/17/23 15:41:11.453
    STEP: Waiting for the RuntimeClass to disappear 01/17/23 15:41:11.476
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Jan 17 15:41:11.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-5763" for this suite. 01/17/23 15:41:11.504
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:41:11.51
Jan 17 15:41:11.510: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename projected 01/17/23 15:41:11.511
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:41:11.532
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:41:11.535
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56
STEP: Creating configMap with name projected-configmap-test-volume-f0b4d74f-12c2-47b3-a936-d709360e1673 01/17/23 15:41:11.537
STEP: Creating a pod to test consume configMaps 01/17/23 15:41:11.547
Jan 17 15:41:11.578: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f0783928-0a14-4702-8b12-c5e68b1cf2d8" in namespace "projected-9937" to be "Succeeded or Failed"
Jan 17 15:41:11.582: INFO: Pod "pod-projected-configmaps-f0783928-0a14-4702-8b12-c5e68b1cf2d8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.378373ms
Jan 17 15:41:13.586: INFO: Pod "pod-projected-configmaps-f0783928-0a14-4702-8b12-c5e68b1cf2d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007990012s
Jan 17 15:41:15.585: INFO: Pod "pod-projected-configmaps-f0783928-0a14-4702-8b12-c5e68b1cf2d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00778096s
STEP: Saw pod success 01/17/23 15:41:15.585
Jan 17 15:41:15.585: INFO: Pod "pod-projected-configmaps-f0783928-0a14-4702-8b12-c5e68b1cf2d8" satisfied condition "Succeeded or Failed"
Jan 17 15:41:15.588: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-projected-configmaps-f0783928-0a14-4702-8b12-c5e68b1cf2d8 container agnhost-container: <nil>
STEP: delete the pod 01/17/23 15:41:15.6
Jan 17 15:41:15.613: INFO: Waiting for pod pod-projected-configmaps-f0783928-0a14-4702-8b12-c5e68b1cf2d8 to disappear
Jan 17 15:41:15.615: INFO: Pod pod-projected-configmaps-f0783928-0a14-4702-8b12-c5e68b1cf2d8 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan 17 15:41:15.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9937" for this suite. 01/17/23 15:41:15.619
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":225,"skipped":4065,"failed":0}
------------------------------
• [4.114 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:41:11.51
    Jan 17 15:41:11.510: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename projected 01/17/23 15:41:11.511
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:41:11.532
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:41:11.535
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:56
    STEP: Creating configMap with name projected-configmap-test-volume-f0b4d74f-12c2-47b3-a936-d709360e1673 01/17/23 15:41:11.537
    STEP: Creating a pod to test consume configMaps 01/17/23 15:41:11.547
    Jan 17 15:41:11.578: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f0783928-0a14-4702-8b12-c5e68b1cf2d8" in namespace "projected-9937" to be "Succeeded or Failed"
    Jan 17 15:41:11.582: INFO: Pod "pod-projected-configmaps-f0783928-0a14-4702-8b12-c5e68b1cf2d8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.378373ms
    Jan 17 15:41:13.586: INFO: Pod "pod-projected-configmaps-f0783928-0a14-4702-8b12-c5e68b1cf2d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007990012s
    Jan 17 15:41:15.585: INFO: Pod "pod-projected-configmaps-f0783928-0a14-4702-8b12-c5e68b1cf2d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00778096s
    STEP: Saw pod success 01/17/23 15:41:15.585
    Jan 17 15:41:15.585: INFO: Pod "pod-projected-configmaps-f0783928-0a14-4702-8b12-c5e68b1cf2d8" satisfied condition "Succeeded or Failed"
    Jan 17 15:41:15.588: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-projected-configmaps-f0783928-0a14-4702-8b12-c5e68b1cf2d8 container agnhost-container: <nil>
    STEP: delete the pod 01/17/23 15:41:15.6
    Jan 17 15:41:15.613: INFO: Waiting for pod pod-projected-configmaps-f0783928-0a14-4702-8b12-c5e68b1cf2d8 to disappear
    Jan 17 15:41:15.615: INFO: Pod pod-projected-configmaps-f0783928-0a14-4702-8b12-c5e68b1cf2d8 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan 17 15:41:15.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9937" for this suite. 01/17/23 15:41:15.619
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:41:15.624
Jan 17 15:41:15.624: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename csistoragecapacity 01/17/23 15:41:15.625
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:41:15.645
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:41:15.647
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 01/17/23 15:41:15.649
STEP: getting /apis/storage.k8s.io 01/17/23 15:41:15.651
STEP: getting /apis/storage.k8s.io/v1 01/17/23 15:41:15.653
STEP: creating 01/17/23 15:41:15.654
STEP: watching 01/17/23 15:41:15.686
Jan 17 15:41:15.686: INFO: starting watch
STEP: getting 01/17/23 15:41:15.698
STEP: listing in namespace 01/17/23 15:41:15.704
STEP: listing across namespaces 01/17/23 15:41:15.715
STEP: patching 01/17/23 15:41:15.721
STEP: updating 01/17/23 15:41:15.726
Jan 17 15:41:15.731: INFO: waiting for watch events with expected annotations in namespace
Jan 17 15:41:15.731: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 01/17/23 15:41:15.731
STEP: deleting a collection 01/17/23 15:41:15.742
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:187
Jan 17 15:41:15.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "csistoragecapacity-7416" for this suite. 01/17/23 15:41:15.773
{"msg":"PASSED [sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]","completed":226,"skipped":4065,"failed":0}
------------------------------
• [0.156 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:41:15.624
    Jan 17 15:41:15.624: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename csistoragecapacity 01/17/23 15:41:15.625
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:41:15.645
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:41:15.647
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 01/17/23 15:41:15.649
    STEP: getting /apis/storage.k8s.io 01/17/23 15:41:15.651
    STEP: getting /apis/storage.k8s.io/v1 01/17/23 15:41:15.653
    STEP: creating 01/17/23 15:41:15.654
    STEP: watching 01/17/23 15:41:15.686
    Jan 17 15:41:15.686: INFO: starting watch
    STEP: getting 01/17/23 15:41:15.698
    STEP: listing in namespace 01/17/23 15:41:15.704
    STEP: listing across namespaces 01/17/23 15:41:15.715
    STEP: patching 01/17/23 15:41:15.721
    STEP: updating 01/17/23 15:41:15.726
    Jan 17 15:41:15.731: INFO: waiting for watch events with expected annotations in namespace
    Jan 17 15:41:15.731: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 01/17/23 15:41:15.731
    STEP: deleting a collection 01/17/23 15:41:15.742
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/framework.go:187
    Jan 17 15:41:15.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "csistoragecapacity-7416" for this suite. 01/17/23 15:41:15.773
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:41:15.784
Jan 17 15:41:15.784: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename downward-api 01/17/23 15:41:15.785
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:41:15.808
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:41:15.813
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260
STEP: Creating a pod to test downward API volume plugin 01/17/23 15:41:15.815
Jan 17 15:41:15.835: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2c740e10-fb1f-48c5-8670-a03f86681635" in namespace "downward-api-3210" to be "Succeeded or Failed"
Jan 17 15:41:15.839: INFO: Pod "downwardapi-volume-2c740e10-fb1f-48c5-8670-a03f86681635": Phase="Pending", Reason="", readiness=false. Elapsed: 3.426816ms
Jan 17 15:41:17.843: INFO: Pod "downwardapi-volume-2c740e10-fb1f-48c5-8670-a03f86681635": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007543111s
Jan 17 15:41:19.844: INFO: Pod "downwardapi-volume-2c740e10-fb1f-48c5-8670-a03f86681635": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008828573s
STEP: Saw pod success 01/17/23 15:41:19.844
Jan 17 15:41:19.844: INFO: Pod "downwardapi-volume-2c740e10-fb1f-48c5-8670-a03f86681635" satisfied condition "Succeeded or Failed"
Jan 17 15:41:19.849: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod downwardapi-volume-2c740e10-fb1f-48c5-8670-a03f86681635 container client-container: <nil>
STEP: delete the pod 01/17/23 15:41:19.855
Jan 17 15:41:19.868: INFO: Waiting for pod downwardapi-volume-2c740e10-fb1f-48c5-8670-a03f86681635 to disappear
Jan 17 15:41:19.870: INFO: Pod downwardapi-volume-2c740e10-fb1f-48c5-8670-a03f86681635 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 17 15:41:19.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3210" for this suite. 01/17/23 15:41:19.874
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","completed":227,"skipped":4124,"failed":0}
------------------------------
• [4.097 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:41:15.784
    Jan 17 15:41:15.784: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename downward-api 01/17/23 15:41:15.785
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:41:15.808
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:41:15.813
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:260
    STEP: Creating a pod to test downward API volume plugin 01/17/23 15:41:15.815
    Jan 17 15:41:15.835: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2c740e10-fb1f-48c5-8670-a03f86681635" in namespace "downward-api-3210" to be "Succeeded or Failed"
    Jan 17 15:41:15.839: INFO: Pod "downwardapi-volume-2c740e10-fb1f-48c5-8670-a03f86681635": Phase="Pending", Reason="", readiness=false. Elapsed: 3.426816ms
    Jan 17 15:41:17.843: INFO: Pod "downwardapi-volume-2c740e10-fb1f-48c5-8670-a03f86681635": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007543111s
    Jan 17 15:41:19.844: INFO: Pod "downwardapi-volume-2c740e10-fb1f-48c5-8670-a03f86681635": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008828573s
    STEP: Saw pod success 01/17/23 15:41:19.844
    Jan 17 15:41:19.844: INFO: Pod "downwardapi-volume-2c740e10-fb1f-48c5-8670-a03f86681635" satisfied condition "Succeeded or Failed"
    Jan 17 15:41:19.849: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod downwardapi-volume-2c740e10-fb1f-48c5-8670-a03f86681635 container client-container: <nil>
    STEP: delete the pod 01/17/23 15:41:19.855
    Jan 17 15:41:19.868: INFO: Waiting for pod downwardapi-volume-2c740e10-fb1f-48c5-8670-a03f86681635 to disappear
    Jan 17 15:41:19.870: INFO: Pod downwardapi-volume-2c740e10-fb1f-48c5-8670-a03f86681635 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 17 15:41:19.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-3210" for this suite. 01/17/23 15:41:19.874
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:41:19.882
Jan 17 15:41:19.882: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename configmap 01/17/23 15:41:19.882
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:41:19.908
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:41:19.911
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123
Jan 17 15:41:19.919: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-77ed2cdd-528a-499d-a9d3-e63091388d3b 01/17/23 15:41:19.919
STEP: Creating the pod 01/17/23 15:41:19.926
Jan 17 15:41:19.961: INFO: Waiting up to 5m0s for pod "pod-configmaps-46007609-2046-4c5f-93a1-5cf85ae5073e" in namespace "configmap-7569" to be "running and ready"
Jan 17 15:41:19.966: INFO: Pod "pod-configmaps-46007609-2046-4c5f-93a1-5cf85ae5073e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.41746ms
Jan 17 15:41:19.966: INFO: The phase of Pod pod-configmaps-46007609-2046-4c5f-93a1-5cf85ae5073e is Pending, waiting for it to be Running (with Ready = true)
Jan 17 15:41:21.972: INFO: Pod "pod-configmaps-46007609-2046-4c5f-93a1-5cf85ae5073e": Phase="Running", Reason="", readiness=true. Elapsed: 2.010796827s
Jan 17 15:41:21.972: INFO: The phase of Pod pod-configmaps-46007609-2046-4c5f-93a1-5cf85ae5073e is Running (Ready = true)
Jan 17 15:41:21.972: INFO: Pod "pod-configmaps-46007609-2046-4c5f-93a1-5cf85ae5073e" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-77ed2cdd-528a-499d-a9d3-e63091388d3b 01/17/23 15:41:21.982
STEP: waiting to observe update in volume 01/17/23 15:41:21.986
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 17 15:41:26.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7569" for this suite. 01/17/23 15:41:26.008
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","completed":228,"skipped":4125,"failed":0}
------------------------------
• [SLOW TEST] [6.133 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:41:19.882
    Jan 17 15:41:19.882: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename configmap 01/17/23 15:41:19.882
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:41:19.908
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:41:19.911
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:123
    Jan 17 15:41:19.919: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
    STEP: Creating configMap with name configmap-test-upd-77ed2cdd-528a-499d-a9d3-e63091388d3b 01/17/23 15:41:19.919
    STEP: Creating the pod 01/17/23 15:41:19.926
    Jan 17 15:41:19.961: INFO: Waiting up to 5m0s for pod "pod-configmaps-46007609-2046-4c5f-93a1-5cf85ae5073e" in namespace "configmap-7569" to be "running and ready"
    Jan 17 15:41:19.966: INFO: Pod "pod-configmaps-46007609-2046-4c5f-93a1-5cf85ae5073e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.41746ms
    Jan 17 15:41:19.966: INFO: The phase of Pod pod-configmaps-46007609-2046-4c5f-93a1-5cf85ae5073e is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 15:41:21.972: INFO: Pod "pod-configmaps-46007609-2046-4c5f-93a1-5cf85ae5073e": Phase="Running", Reason="", readiness=true. Elapsed: 2.010796827s
    Jan 17 15:41:21.972: INFO: The phase of Pod pod-configmaps-46007609-2046-4c5f-93a1-5cf85ae5073e is Running (Ready = true)
    Jan 17 15:41:21.972: INFO: Pod "pod-configmaps-46007609-2046-4c5f-93a1-5cf85ae5073e" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-77ed2cdd-528a-499d-a9d3-e63091388d3b 01/17/23 15:41:21.982
    STEP: waiting to observe update in volume 01/17/23 15:41:21.986
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 17 15:41:26.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-7569" for this suite. 01/17/23 15:41:26.008
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:41:26.016
Jan 17 15:41:26.016: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename cronjob 01/17/23 15:41:26.017
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:41:26.046
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:41:26.048
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 01/17/23 15:41:26.053
W0117 15:41:26.061762      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring a job is scheduled 01/17/23 15:41:26.061
STEP: Ensuring exactly one is scheduled 01/17/23 15:42:02.066
STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/17/23 15:42:02.069
STEP: Ensuring no more jobs are scheduled 01/17/23 15:42:02.072
STEP: Removing cronjob 01/17/23 15:47:02.079
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Jan 17 15:47:02.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-1464" for this suite. 01/17/23 15:47:02.09
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","completed":229,"skipped":4206,"failed":0}
------------------------------
• [SLOW TEST] [336.081 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:41:26.016
    Jan 17 15:41:26.016: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename cronjob 01/17/23 15:41:26.017
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:41:26.046
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:41:26.048
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 01/17/23 15:41:26.053
    W0117 15:41:26.061762      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Ensuring a job is scheduled 01/17/23 15:41:26.061
    STEP: Ensuring exactly one is scheduled 01/17/23 15:42:02.066
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/17/23 15:42:02.069
    STEP: Ensuring no more jobs are scheduled 01/17/23 15:42:02.072
    STEP: Removing cronjob 01/17/23 15:47:02.079
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Jan 17 15:47:02.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-1464" for this suite. 01/17/23 15:47:02.09
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:47:02.098
Jan 17 15:47:02.098: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename emptydir 01/17/23 15:47:02.099
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:47:02.133
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:47:02.138
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206
STEP: Creating a pod to test emptydir 0666 on node default medium 01/17/23 15:47:02.14
Jan 17 15:47:02.177: INFO: Waiting up to 5m0s for pod "pod-f80ac725-3f02-48df-8c06-bc8c91eeb4ae" in namespace "emptydir-4571" to be "Succeeded or Failed"
Jan 17 15:47:02.182: INFO: Pod "pod-f80ac725-3f02-48df-8c06-bc8c91eeb4ae": Phase="Pending", Reason="", readiness=false. Elapsed: 5.13181ms
Jan 17 15:47:04.187: INFO: Pod "pod-f80ac725-3f02-48df-8c06-bc8c91eeb4ae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009597925s
Jan 17 15:47:06.186: INFO: Pod "pod-f80ac725-3f02-48df-8c06-bc8c91eeb4ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009465935s
STEP: Saw pod success 01/17/23 15:47:06.186
Jan 17 15:47:06.187: INFO: Pod "pod-f80ac725-3f02-48df-8c06-bc8c91eeb4ae" satisfied condition "Succeeded or Failed"
Jan 17 15:47:06.190: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-f80ac725-3f02-48df-8c06-bc8c91eeb4ae container test-container: <nil>
STEP: delete the pod 01/17/23 15:47:06.2
Jan 17 15:47:06.212: INFO: Waiting for pod pod-f80ac725-3f02-48df-8c06-bc8c91eeb4ae to disappear
Jan 17 15:47:06.215: INFO: Pod pod-f80ac725-3f02-48df-8c06-bc8c91eeb4ae no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 17 15:47:06.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4571" for this suite. 01/17/23 15:47:06.219
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":230,"skipped":4239,"failed":0}
------------------------------
• [4.128 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:47:02.098
    Jan 17 15:47:02.098: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename emptydir 01/17/23 15:47:02.099
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:47:02.133
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:47:02.138
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:206
    STEP: Creating a pod to test emptydir 0666 on node default medium 01/17/23 15:47:02.14
    Jan 17 15:47:02.177: INFO: Waiting up to 5m0s for pod "pod-f80ac725-3f02-48df-8c06-bc8c91eeb4ae" in namespace "emptydir-4571" to be "Succeeded or Failed"
    Jan 17 15:47:02.182: INFO: Pod "pod-f80ac725-3f02-48df-8c06-bc8c91eeb4ae": Phase="Pending", Reason="", readiness=false. Elapsed: 5.13181ms
    Jan 17 15:47:04.187: INFO: Pod "pod-f80ac725-3f02-48df-8c06-bc8c91eeb4ae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009597925s
    Jan 17 15:47:06.186: INFO: Pod "pod-f80ac725-3f02-48df-8c06-bc8c91eeb4ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009465935s
    STEP: Saw pod success 01/17/23 15:47:06.186
    Jan 17 15:47:06.187: INFO: Pod "pod-f80ac725-3f02-48df-8c06-bc8c91eeb4ae" satisfied condition "Succeeded or Failed"
    Jan 17 15:47:06.190: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-f80ac725-3f02-48df-8c06-bc8c91eeb4ae container test-container: <nil>
    STEP: delete the pod 01/17/23 15:47:06.2
    Jan 17 15:47:06.212: INFO: Waiting for pod pod-f80ac725-3f02-48df-8c06-bc8c91eeb4ae to disappear
    Jan 17 15:47:06.215: INFO: Pod pod-f80ac725-3f02-48df-8c06-bc8c91eeb4ae no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 17 15:47:06.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-4571" for this suite. 01/17/23 15:47:06.219
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:47:06.228
Jan 17 15:47:06.228: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename var-expansion 01/17/23 15:47:06.228
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:47:06.256
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:47:06.261
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72
STEP: Creating a pod to test substitution in container's command 01/17/23 15:47:06.264
Jan 17 15:47:06.290: INFO: Waiting up to 5m0s for pod "var-expansion-50586eaf-84f1-43a0-8667-7ecf2f1f12a3" in namespace "var-expansion-5094" to be "Succeeded or Failed"
Jan 17 15:47:06.293: INFO: Pod "var-expansion-50586eaf-84f1-43a0-8667-7ecf2f1f12a3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.042717ms
Jan 17 15:47:08.355: INFO: Pod "var-expansion-50586eaf-84f1-43a0-8667-7ecf2f1f12a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.064504815s
Jan 17 15:47:10.296: INFO: Pod "var-expansion-50586eaf-84f1-43a0-8667-7ecf2f1f12a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006310815s
STEP: Saw pod success 01/17/23 15:47:10.296
Jan 17 15:47:10.297: INFO: Pod "var-expansion-50586eaf-84f1-43a0-8667-7ecf2f1f12a3" satisfied condition "Succeeded or Failed"
Jan 17 15:47:10.299: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod var-expansion-50586eaf-84f1-43a0-8667-7ecf2f1f12a3 container dapi-container: <nil>
STEP: delete the pod 01/17/23 15:47:10.305
Jan 17 15:47:10.325: INFO: Waiting for pod var-expansion-50586eaf-84f1-43a0-8667-7ecf2f1f12a3 to disappear
Jan 17 15:47:10.328: INFO: Pod var-expansion-50586eaf-84f1-43a0-8667-7ecf2f1f12a3 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan 17 15:47:10.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5094" for this suite. 01/17/23 15:47:10.333
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","completed":231,"skipped":4295,"failed":0}
------------------------------
• [4.111 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:47:06.228
    Jan 17 15:47:06.228: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename var-expansion 01/17/23 15:47:06.228
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:47:06.256
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:47:06.261
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:72
    STEP: Creating a pod to test substitution in container's command 01/17/23 15:47:06.264
    Jan 17 15:47:06.290: INFO: Waiting up to 5m0s for pod "var-expansion-50586eaf-84f1-43a0-8667-7ecf2f1f12a3" in namespace "var-expansion-5094" to be "Succeeded or Failed"
    Jan 17 15:47:06.293: INFO: Pod "var-expansion-50586eaf-84f1-43a0-8667-7ecf2f1f12a3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.042717ms
    Jan 17 15:47:08.355: INFO: Pod "var-expansion-50586eaf-84f1-43a0-8667-7ecf2f1f12a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.064504815s
    Jan 17 15:47:10.296: INFO: Pod "var-expansion-50586eaf-84f1-43a0-8667-7ecf2f1f12a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006310815s
    STEP: Saw pod success 01/17/23 15:47:10.296
    Jan 17 15:47:10.297: INFO: Pod "var-expansion-50586eaf-84f1-43a0-8667-7ecf2f1f12a3" satisfied condition "Succeeded or Failed"
    Jan 17 15:47:10.299: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod var-expansion-50586eaf-84f1-43a0-8667-7ecf2f1f12a3 container dapi-container: <nil>
    STEP: delete the pod 01/17/23 15:47:10.305
    Jan 17 15:47:10.325: INFO: Waiting for pod var-expansion-50586eaf-84f1-43a0-8667-7ecf2f1f12a3 to disappear
    Jan 17 15:47:10.328: INFO: Pod var-expansion-50586eaf-84f1-43a0-8667-7ecf2f1f12a3 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan 17 15:47:10.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-5094" for this suite. 01/17/23 15:47:10.333
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:47:10.34
Jan 17 15:47:10.340: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename runtimeclass 01/17/23 15:47:10.34
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:47:10.365
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:47:10.367
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Jan 17 15:47:10.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-1479" for this suite. 01/17/23 15:47:10.384
{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]","completed":232,"skipped":4319,"failed":0}
------------------------------
• [0.063 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:47:10.34
    Jan 17 15:47:10.340: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename runtimeclass 01/17/23 15:47:10.34
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:47:10.365
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:47:10.367
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Jan 17 15:47:10.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-1479" for this suite. 01/17/23 15:47:10.384
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:247
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:47:10.403
Jan 17 15:47:10.403: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename container-runtime 01/17/23 15:47:10.404
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:47:10.428
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:47:10.431
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:247
STEP: create the container 01/17/23 15:47:10.433
STEP: wait for the container to reach Succeeded 01/17/23 15:47:10.456
STEP: get the container status 01/17/23 15:47:14.481
STEP: the container should be terminated 01/17/23 15:47:14.485
STEP: the termination message should be set 01/17/23 15:47:14.485
Jan 17 15:47:14.485: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 01/17/23 15:47:14.485
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Jan 17 15:47:14.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6513" for this suite. 01/17/23 15:47:14.504
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":233,"skipped":4340,"failed":0}
------------------------------
• [4.107 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:247

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:47:10.403
    Jan 17 15:47:10.403: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename container-runtime 01/17/23 15:47:10.404
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:47:10.428
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:47:10.431
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:247
    STEP: create the container 01/17/23 15:47:10.433
    STEP: wait for the container to reach Succeeded 01/17/23 15:47:10.456
    STEP: get the container status 01/17/23 15:47:14.481
    STEP: the container should be terminated 01/17/23 15:47:14.485
    STEP: the termination message should be set 01/17/23 15:47:14.485
    Jan 17 15:47:14.485: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 01/17/23 15:47:14.485
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Jan 17 15:47:14.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-6513" for this suite. 01/17/23 15:47:14.504
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:47:14.51
Jan 17 15:47:14.510: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename secrets 01/17/23 15:47:14.511
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:47:14.533
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:47:14.537
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67
STEP: Creating secret with name secret-test-e54f4f59-a377-430a-acbd-81f7f0d6822a 01/17/23 15:47:14.539
STEP: Creating a pod to test consume secrets 01/17/23 15:47:14.547
Jan 17 15:47:14.581: INFO: Waiting up to 5m0s for pod "pod-secrets-f3fee734-a1f4-4660-9cbd-cc81489be1ef" in namespace "secrets-3911" to be "Succeeded or Failed"
Jan 17 15:47:14.586: INFO: Pod "pod-secrets-f3fee734-a1f4-4660-9cbd-cc81489be1ef": Phase="Pending", Reason="", readiness=false. Elapsed: 5.295752ms
Jan 17 15:47:16.591: INFO: Pod "pod-secrets-f3fee734-a1f4-4660-9cbd-cc81489be1ef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010027419s
Jan 17 15:47:18.591: INFO: Pod "pod-secrets-f3fee734-a1f4-4660-9cbd-cc81489be1ef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009817553s
STEP: Saw pod success 01/17/23 15:47:18.591
Jan 17 15:47:18.591: INFO: Pod "pod-secrets-f3fee734-a1f4-4660-9cbd-cc81489be1ef" satisfied condition "Succeeded or Failed"
Jan 17 15:47:18.594: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-secrets-f3fee734-a1f4-4660-9cbd-cc81489be1ef container secret-volume-test: <nil>
STEP: delete the pod 01/17/23 15:47:18.6
Jan 17 15:47:18.610: INFO: Waiting for pod pod-secrets-f3fee734-a1f4-4660-9cbd-cc81489be1ef to disappear
Jan 17 15:47:18.621: INFO: Pod pod-secrets-f3fee734-a1f4-4660-9cbd-cc81489be1ef no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan 17 15:47:18.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3911" for this suite. 01/17/23 15:47:18.63
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","completed":234,"skipped":4349,"failed":0}
------------------------------
• [4.125 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:47:14.51
    Jan 17 15:47:14.510: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename secrets 01/17/23 15:47:14.511
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:47:14.533
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:47:14.537
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:67
    STEP: Creating secret with name secret-test-e54f4f59-a377-430a-acbd-81f7f0d6822a 01/17/23 15:47:14.539
    STEP: Creating a pod to test consume secrets 01/17/23 15:47:14.547
    Jan 17 15:47:14.581: INFO: Waiting up to 5m0s for pod "pod-secrets-f3fee734-a1f4-4660-9cbd-cc81489be1ef" in namespace "secrets-3911" to be "Succeeded or Failed"
    Jan 17 15:47:14.586: INFO: Pod "pod-secrets-f3fee734-a1f4-4660-9cbd-cc81489be1ef": Phase="Pending", Reason="", readiness=false. Elapsed: 5.295752ms
    Jan 17 15:47:16.591: INFO: Pod "pod-secrets-f3fee734-a1f4-4660-9cbd-cc81489be1ef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010027419s
    Jan 17 15:47:18.591: INFO: Pod "pod-secrets-f3fee734-a1f4-4660-9cbd-cc81489be1ef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009817553s
    STEP: Saw pod success 01/17/23 15:47:18.591
    Jan 17 15:47:18.591: INFO: Pod "pod-secrets-f3fee734-a1f4-4660-9cbd-cc81489be1ef" satisfied condition "Succeeded or Failed"
    Jan 17 15:47:18.594: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-secrets-f3fee734-a1f4-4660-9cbd-cc81489be1ef container secret-volume-test: <nil>
    STEP: delete the pod 01/17/23 15:47:18.6
    Jan 17 15:47:18.610: INFO: Waiting for pod pod-secrets-f3fee734-a1f4-4660-9cbd-cc81489be1ef to disappear
    Jan 17 15:47:18.621: INFO: Pod pod-secrets-f3fee734-a1f4-4660-9cbd-cc81489be1ef no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan 17 15:47:18.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-3911" for this suite. 01/17/23 15:47:18.63
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:47:18.636
Jan 17 15:47:18.636: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename replicaset 01/17/23 15:47:18.637
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:47:18.667
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:47:18.673
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 01/17/23 15:47:18.675
Jan 17 15:47:18.718: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-474" to be "running and ready"
Jan 17 15:47:18.724: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 6.43151ms
Jan 17 15:47:18.724: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Jan 17 15:47:20.728: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.010622016s
Jan 17 15:47:20.728: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
Jan 17 15:47:20.728: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 01/17/23 15:47:20.731
STEP: Then the orphan pod is adopted 01/17/23 15:47:20.737
STEP: When the matched label of one of its pods change 01/17/23 15:47:21.744
Jan 17 15:47:21.747: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 01/17/23 15:47:21.762
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jan 17 15:47:22.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-474" for this suite. 01/17/23 15:47:22.774
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","completed":235,"skipped":4378,"failed":0}
------------------------------
• [4.145 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:47:18.636
    Jan 17 15:47:18.636: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename replicaset 01/17/23 15:47:18.637
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:47:18.667
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:47:18.673
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 01/17/23 15:47:18.675
    Jan 17 15:47:18.718: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-474" to be "running and ready"
    Jan 17 15:47:18.724: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 6.43151ms
    Jan 17 15:47:18.724: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 15:47:20.728: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.010622016s
    Jan 17 15:47:20.728: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    Jan 17 15:47:20.728: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 01/17/23 15:47:20.731
    STEP: Then the orphan pod is adopted 01/17/23 15:47:20.737
    STEP: When the matched label of one of its pods change 01/17/23 15:47:21.744
    Jan 17 15:47:21.747: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 01/17/23 15:47:21.762
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jan 17 15:47:22.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-474" for this suite. 01/17/23 15:47:22.774
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:47:22.781
Jan 17 15:47:22.781: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename projected 01/17/23 15:47:22.782
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:47:22.807
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:47:22.812
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88
STEP: Creating configMap with name projected-configmap-test-volume-map-dd9974f7-9fc6-442e-a989-9fc19a92f7d6 01/17/23 15:47:22.814
STEP: Creating a pod to test consume configMaps 01/17/23 15:47:22.823
Jan 17 15:47:22.845: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-65ac0075-e0e8-488f-a48e-5c3cda580746" in namespace "projected-7060" to be "Succeeded or Failed"
Jan 17 15:47:22.851: INFO: Pod "pod-projected-configmaps-65ac0075-e0e8-488f-a48e-5c3cda580746": Phase="Pending", Reason="", readiness=false. Elapsed: 6.173021ms
Jan 17 15:47:24.855: INFO: Pod "pod-projected-configmaps-65ac0075-e0e8-488f-a48e-5c3cda580746": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010584146s
Jan 17 15:47:26.854: INFO: Pod "pod-projected-configmaps-65ac0075-e0e8-488f-a48e-5c3cda580746": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009361187s
STEP: Saw pod success 01/17/23 15:47:26.854
Jan 17 15:47:26.854: INFO: Pod "pod-projected-configmaps-65ac0075-e0e8-488f-a48e-5c3cda580746" satisfied condition "Succeeded or Failed"
Jan 17 15:47:26.857: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-projected-configmaps-65ac0075-e0e8-488f-a48e-5c3cda580746 container agnhost-container: <nil>
STEP: delete the pod 01/17/23 15:47:26.863
Jan 17 15:47:26.875: INFO: Waiting for pod pod-projected-configmaps-65ac0075-e0e8-488f-a48e-5c3cda580746 to disappear
Jan 17 15:47:26.878: INFO: Pod pod-projected-configmaps-65ac0075-e0e8-488f-a48e-5c3cda580746 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan 17 15:47:26.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7060" for this suite. 01/17/23 15:47:26.882
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":236,"skipped":4378,"failed":0}
------------------------------
• [4.105 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:47:22.781
    Jan 17 15:47:22.781: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename projected 01/17/23 15:47:22.782
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:47:22.807
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:47:22.812
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:88
    STEP: Creating configMap with name projected-configmap-test-volume-map-dd9974f7-9fc6-442e-a989-9fc19a92f7d6 01/17/23 15:47:22.814
    STEP: Creating a pod to test consume configMaps 01/17/23 15:47:22.823
    Jan 17 15:47:22.845: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-65ac0075-e0e8-488f-a48e-5c3cda580746" in namespace "projected-7060" to be "Succeeded or Failed"
    Jan 17 15:47:22.851: INFO: Pod "pod-projected-configmaps-65ac0075-e0e8-488f-a48e-5c3cda580746": Phase="Pending", Reason="", readiness=false. Elapsed: 6.173021ms
    Jan 17 15:47:24.855: INFO: Pod "pod-projected-configmaps-65ac0075-e0e8-488f-a48e-5c3cda580746": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010584146s
    Jan 17 15:47:26.854: INFO: Pod "pod-projected-configmaps-65ac0075-e0e8-488f-a48e-5c3cda580746": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009361187s
    STEP: Saw pod success 01/17/23 15:47:26.854
    Jan 17 15:47:26.854: INFO: Pod "pod-projected-configmaps-65ac0075-e0e8-488f-a48e-5c3cda580746" satisfied condition "Succeeded or Failed"
    Jan 17 15:47:26.857: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-projected-configmaps-65ac0075-e0e8-488f-a48e-5c3cda580746 container agnhost-container: <nil>
    STEP: delete the pod 01/17/23 15:47:26.863
    Jan 17 15:47:26.875: INFO: Waiting for pod pod-projected-configmaps-65ac0075-e0e8-488f-a48e-5c3cda580746 to disappear
    Jan 17 15:47:26.878: INFO: Pod pod-projected-configmaps-65ac0075-e0e8-488f-a48e-5c3cda580746 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan 17 15:47:26.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7060" for this suite. 01/17/23 15:47:26.882
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:47:26.887
Jan 17 15:47:26.887: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename projected 01/17/23 15:47:26.888
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:47:26.909
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:47:26.914
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173
Jan 17 15:47:26.922: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-a6fcacdc-f2bc-4ffa-81dd-ddf8ce9ee50c 01/17/23 15:47:26.922
STEP: Creating configMap with name cm-test-opt-upd-51b826e4-f404-493e-9ebe-9c4f66699437 01/17/23 15:47:26.928
STEP: Creating the pod 01/17/23 15:47:26.945
Jan 17 15:47:26.969: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-80c3eb4a-3090-448b-a2ea-34d4157fc1df" in namespace "projected-8355" to be "running and ready"
Jan 17 15:47:26.984: INFO: Pod "pod-projected-configmaps-80c3eb4a-3090-448b-a2ea-34d4157fc1df": Phase="Pending", Reason="", readiness=false. Elapsed: 14.676602ms
Jan 17 15:47:26.984: INFO: The phase of Pod pod-projected-configmaps-80c3eb4a-3090-448b-a2ea-34d4157fc1df is Pending, waiting for it to be Running (with Ready = true)
Jan 17 15:47:28.988: INFO: Pod "pod-projected-configmaps-80c3eb4a-3090-448b-a2ea-34d4157fc1df": Phase="Running", Reason="", readiness=true. Elapsed: 2.018705214s
Jan 17 15:47:28.988: INFO: The phase of Pod pod-projected-configmaps-80c3eb4a-3090-448b-a2ea-34d4157fc1df is Running (Ready = true)
Jan 17 15:47:28.988: INFO: Pod "pod-projected-configmaps-80c3eb4a-3090-448b-a2ea-34d4157fc1df" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-a6fcacdc-f2bc-4ffa-81dd-ddf8ce9ee50c 01/17/23 15:47:29.011
STEP: Updating configmap cm-test-opt-upd-51b826e4-f404-493e-9ebe-9c4f66699437 01/17/23 15:47:29.017
STEP: Creating configMap with name cm-test-opt-create-12da70b1-d809-45e4-bfe5-04479c9615ab 01/17/23 15:47:29.022
STEP: waiting to observe update in volume 01/17/23 15:47:29.026
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan 17 15:47:31.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8355" for this suite. 01/17/23 15:47:31.053
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":237,"skipped":4390,"failed":0}
------------------------------
• [4.172 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:47:26.887
    Jan 17 15:47:26.887: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename projected 01/17/23 15:47:26.888
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:47:26.909
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:47:26.914
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:173
    Jan 17 15:47:26.922: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
    STEP: Creating configMap with name cm-test-opt-del-a6fcacdc-f2bc-4ffa-81dd-ddf8ce9ee50c 01/17/23 15:47:26.922
    STEP: Creating configMap with name cm-test-opt-upd-51b826e4-f404-493e-9ebe-9c4f66699437 01/17/23 15:47:26.928
    STEP: Creating the pod 01/17/23 15:47:26.945
    Jan 17 15:47:26.969: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-80c3eb4a-3090-448b-a2ea-34d4157fc1df" in namespace "projected-8355" to be "running and ready"
    Jan 17 15:47:26.984: INFO: Pod "pod-projected-configmaps-80c3eb4a-3090-448b-a2ea-34d4157fc1df": Phase="Pending", Reason="", readiness=false. Elapsed: 14.676602ms
    Jan 17 15:47:26.984: INFO: The phase of Pod pod-projected-configmaps-80c3eb4a-3090-448b-a2ea-34d4157fc1df is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 15:47:28.988: INFO: Pod "pod-projected-configmaps-80c3eb4a-3090-448b-a2ea-34d4157fc1df": Phase="Running", Reason="", readiness=true. Elapsed: 2.018705214s
    Jan 17 15:47:28.988: INFO: The phase of Pod pod-projected-configmaps-80c3eb4a-3090-448b-a2ea-34d4157fc1df is Running (Ready = true)
    Jan 17 15:47:28.988: INFO: Pod "pod-projected-configmaps-80c3eb4a-3090-448b-a2ea-34d4157fc1df" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-a6fcacdc-f2bc-4ffa-81dd-ddf8ce9ee50c 01/17/23 15:47:29.011
    STEP: Updating configmap cm-test-opt-upd-51b826e4-f404-493e-9ebe-9c4f66699437 01/17/23 15:47:29.017
    STEP: Creating configMap with name cm-test-opt-create-12da70b1-d809-45e4-bfe5-04479c9615ab 01/17/23 15:47:29.022
    STEP: waiting to observe update in volume 01/17/23 15:47:29.026
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan 17 15:47:31.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8355" for this suite. 01/17/23 15:47:31.053
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:47:31.059
Jan 17 15:47:31.059: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename configmap 01/17/23 15:47:31.06
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:47:31.092
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:47:31.094
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422
STEP: Creating configMap with name configmap-test-volume-94a59e78-2ec2-402c-b7eb-77f6e6288df8 01/17/23 15:47:31.096
STEP: Creating a pod to test consume configMaps 01/17/23 15:47:31.105
Jan 17 15:47:31.130: INFO: Waiting up to 5m0s for pod "pod-configmaps-05e259e7-a41d-4499-b7ca-d696accbae41" in namespace "configmap-8827" to be "Succeeded or Failed"
Jan 17 15:47:31.134: INFO: Pod "pod-configmaps-05e259e7-a41d-4499-b7ca-d696accbae41": Phase="Pending", Reason="", readiness=false. Elapsed: 4.001017ms
Jan 17 15:47:33.138: INFO: Pod "pod-configmaps-05e259e7-a41d-4499-b7ca-d696accbae41": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00806035s
Jan 17 15:47:35.139: INFO: Pod "pod-configmaps-05e259e7-a41d-4499-b7ca-d696accbae41": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009522676s
STEP: Saw pod success 01/17/23 15:47:35.139
Jan 17 15:47:35.139: INFO: Pod "pod-configmaps-05e259e7-a41d-4499-b7ca-d696accbae41" satisfied condition "Succeeded or Failed"
Jan 17 15:47:35.142: INFO: Trying to get logs from node ip-10-0-139-213.ec2.internal pod pod-configmaps-05e259e7-a41d-4499-b7ca-d696accbae41 container configmap-volume-test: <nil>
STEP: delete the pod 01/17/23 15:47:35.153
Jan 17 15:47:35.166: INFO: Waiting for pod pod-configmaps-05e259e7-a41d-4499-b7ca-d696accbae41 to disappear
Jan 17 15:47:35.168: INFO: Pod pod-configmaps-05e259e7-a41d-4499-b7ca-d696accbae41 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 17 15:47:35.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8827" for this suite. 01/17/23 15:47:35.173
{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","completed":238,"skipped":4393,"failed":0}
------------------------------
• [4.119 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:47:31.059
    Jan 17 15:47:31.059: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename configmap 01/17/23 15:47:31.06
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:47:31.092
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:47:31.094
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:422
    STEP: Creating configMap with name configmap-test-volume-94a59e78-2ec2-402c-b7eb-77f6e6288df8 01/17/23 15:47:31.096
    STEP: Creating a pod to test consume configMaps 01/17/23 15:47:31.105
    Jan 17 15:47:31.130: INFO: Waiting up to 5m0s for pod "pod-configmaps-05e259e7-a41d-4499-b7ca-d696accbae41" in namespace "configmap-8827" to be "Succeeded or Failed"
    Jan 17 15:47:31.134: INFO: Pod "pod-configmaps-05e259e7-a41d-4499-b7ca-d696accbae41": Phase="Pending", Reason="", readiness=false. Elapsed: 4.001017ms
    Jan 17 15:47:33.138: INFO: Pod "pod-configmaps-05e259e7-a41d-4499-b7ca-d696accbae41": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00806035s
    Jan 17 15:47:35.139: INFO: Pod "pod-configmaps-05e259e7-a41d-4499-b7ca-d696accbae41": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009522676s
    STEP: Saw pod success 01/17/23 15:47:35.139
    Jan 17 15:47:35.139: INFO: Pod "pod-configmaps-05e259e7-a41d-4499-b7ca-d696accbae41" satisfied condition "Succeeded or Failed"
    Jan 17 15:47:35.142: INFO: Trying to get logs from node ip-10-0-139-213.ec2.internal pod pod-configmaps-05e259e7-a41d-4499-b7ca-d696accbae41 container configmap-volume-test: <nil>
    STEP: delete the pod 01/17/23 15:47:35.153
    Jan 17 15:47:35.166: INFO: Waiting for pod pod-configmaps-05e259e7-a41d-4499-b7ca-d696accbae41 to disappear
    Jan 17 15:47:35.168: INFO: Pod pod-configmaps-05e259e7-a41d-4499-b7ca-d696accbae41 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 17 15:47:35.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-8827" for this suite. 01/17/23 15:47:35.173
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:47:35.179
Jan 17 15:47:35.179: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename controllerrevisions 01/17/23 15:47:35.18
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:47:35.203
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:47:35.205
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-45ds8-daemon-set" 01/17/23 15:47:35.251
STEP: Check that daemon pods launch on every node of the cluster. 01/17/23 15:47:35.257
Jan 17 15:47:35.261: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 15:47:35.261: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 15:47:35.261: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 15:47:35.266: INFO: Number of nodes with available pods controlled by daemonset e2e-45ds8-daemon-set: 0
Jan 17 15:47:35.266: INFO: Node ip-10-0-139-213.ec2.internal is running 0 daemon pod, expected 1
Jan 17 15:47:36.271: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 15:47:36.271: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 15:47:36.271: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 15:47:36.275: INFO: Number of nodes with available pods controlled by daemonset e2e-45ds8-daemon-set: 1
Jan 17 15:47:36.275: INFO: Node ip-10-0-139-213.ec2.internal is running 0 daemon pod, expected 1
Jan 17 15:47:37.271: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 15:47:37.271: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 15:47:37.271: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 15:47:37.274: INFO: Number of nodes with available pods controlled by daemonset e2e-45ds8-daemon-set: 3
Jan 17 15:47:37.274: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-45ds8-daemon-set
STEP: Confirm DaemonSet "e2e-45ds8-daemon-set" successfully created with "daemonset-name=e2e-45ds8-daemon-set" label 01/17/23 15:47:37.277
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-45ds8-daemon-set" 01/17/23 15:47:37.284
Jan 17 15:47:37.288: INFO: Located ControllerRevision: "e2e-45ds8-daemon-set-67877468c7"
STEP: Patching ControllerRevision "e2e-45ds8-daemon-set-67877468c7" 01/17/23 15:47:37.291
Jan 17 15:47:37.297: INFO: e2e-45ds8-daemon-set-67877468c7 has been patched
STEP: Create a new ControllerRevision 01/17/23 15:47:37.297
Jan 17 15:47:37.301: INFO: Created ControllerRevision: e2e-45ds8-daemon-set-77c96bcb85
STEP: Confirm that there are two ControllerRevisions 01/17/23 15:47:37.302
Jan 17 15:47:37.302: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 17 15:47:37.304: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-45ds8-daemon-set-67877468c7" 01/17/23 15:47:37.304
STEP: Confirm that there is only one ControllerRevision 01/17/23 15:47:37.309
Jan 17 15:47:37.310: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 17 15:47:37.312: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-45ds8-daemon-set-77c96bcb85" 01/17/23 15:47:37.314
Jan 17 15:47:37.322: INFO: e2e-45ds8-daemon-set-77c96bcb85 has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 01/17/23 15:47:37.322
W0117 15:47:37.327383      22 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 01/17/23 15:47:37.327
Jan 17 15:47:37.327: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 17 15:47:38.330: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 17 15:47:38.334: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-45ds8-daemon-set-77c96bcb85=updated" 01/17/23 15:47:38.334
STEP: Confirm that there is only one ControllerRevision 01/17/23 15:47:38.341
Jan 17 15:47:38.341: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 17 15:47:38.344: INFO: Found 1 ControllerRevisions
Jan 17 15:47:38.347: INFO: ControllerRevision "e2e-45ds8-daemon-set-75b564b76d" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-45ds8-daemon-set" 01/17/23 15:47:38.35
STEP: deleting DaemonSet.extensions e2e-45ds8-daemon-set in namespace controllerrevisions-6661, will wait for the garbage collector to delete the pods 01/17/23 15:47:38.35
Jan 17 15:47:38.410: INFO: Deleting DaemonSet.extensions e2e-45ds8-daemon-set took: 6.857022ms
Jan 17 15:47:38.511: INFO: Terminating DaemonSet.extensions e2e-45ds8-daemon-set pods took: 101.035399ms
Jan 17 15:47:39.914: INFO: Number of nodes with available pods controlled by daemonset e2e-45ds8-daemon-set: 0
Jan 17 15:47:39.914: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-45ds8-daemon-set
Jan 17 15:47:39.916: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"116304"},"items":null}

Jan 17 15:47:39.919: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"116304"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/framework.go:187
Jan 17 15:47:39.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "controllerrevisions-6661" for this suite. 01/17/23 15:47:39.936
{"msg":"PASSED [sig-apps] ControllerRevision [Serial] should manage the lifecycle of a ControllerRevision [Conformance]","completed":239,"skipped":4418,"failed":0}
------------------------------
• [4.763 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:47:35.179
    Jan 17 15:47:35.179: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename controllerrevisions 01/17/23 15:47:35.18
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:47:35.203
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:47:35.205
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-45ds8-daemon-set" 01/17/23 15:47:35.251
    STEP: Check that daemon pods launch on every node of the cluster. 01/17/23 15:47:35.257
    Jan 17 15:47:35.261: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 15:47:35.261: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 15:47:35.261: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 15:47:35.266: INFO: Number of nodes with available pods controlled by daemonset e2e-45ds8-daemon-set: 0
    Jan 17 15:47:35.266: INFO: Node ip-10-0-139-213.ec2.internal is running 0 daemon pod, expected 1
    Jan 17 15:47:36.271: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 15:47:36.271: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 15:47:36.271: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 15:47:36.275: INFO: Number of nodes with available pods controlled by daemonset e2e-45ds8-daemon-set: 1
    Jan 17 15:47:36.275: INFO: Node ip-10-0-139-213.ec2.internal is running 0 daemon pod, expected 1
    Jan 17 15:47:37.271: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 15:47:37.271: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 15:47:37.271: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 15:47:37.274: INFO: Number of nodes with available pods controlled by daemonset e2e-45ds8-daemon-set: 3
    Jan 17 15:47:37.274: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-45ds8-daemon-set
    STEP: Confirm DaemonSet "e2e-45ds8-daemon-set" successfully created with "daemonset-name=e2e-45ds8-daemon-set" label 01/17/23 15:47:37.277
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-45ds8-daemon-set" 01/17/23 15:47:37.284
    Jan 17 15:47:37.288: INFO: Located ControllerRevision: "e2e-45ds8-daemon-set-67877468c7"
    STEP: Patching ControllerRevision "e2e-45ds8-daemon-set-67877468c7" 01/17/23 15:47:37.291
    Jan 17 15:47:37.297: INFO: e2e-45ds8-daemon-set-67877468c7 has been patched
    STEP: Create a new ControllerRevision 01/17/23 15:47:37.297
    Jan 17 15:47:37.301: INFO: Created ControllerRevision: e2e-45ds8-daemon-set-77c96bcb85
    STEP: Confirm that there are two ControllerRevisions 01/17/23 15:47:37.302
    Jan 17 15:47:37.302: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 17 15:47:37.304: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-45ds8-daemon-set-67877468c7" 01/17/23 15:47:37.304
    STEP: Confirm that there is only one ControllerRevision 01/17/23 15:47:37.309
    Jan 17 15:47:37.310: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 17 15:47:37.312: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-45ds8-daemon-set-77c96bcb85" 01/17/23 15:47:37.314
    Jan 17 15:47:37.322: INFO: e2e-45ds8-daemon-set-77c96bcb85 has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 01/17/23 15:47:37.322
    W0117 15:47:37.327383      22 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 01/17/23 15:47:37.327
    Jan 17 15:47:37.327: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 17 15:47:38.330: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 17 15:47:38.334: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-45ds8-daemon-set-77c96bcb85=updated" 01/17/23 15:47:38.334
    STEP: Confirm that there is only one ControllerRevision 01/17/23 15:47:38.341
    Jan 17 15:47:38.341: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 17 15:47:38.344: INFO: Found 1 ControllerRevisions
    Jan 17 15:47:38.347: INFO: ControllerRevision "e2e-45ds8-daemon-set-75b564b76d" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-45ds8-daemon-set" 01/17/23 15:47:38.35
    STEP: deleting DaemonSet.extensions e2e-45ds8-daemon-set in namespace controllerrevisions-6661, will wait for the garbage collector to delete the pods 01/17/23 15:47:38.35
    Jan 17 15:47:38.410: INFO: Deleting DaemonSet.extensions e2e-45ds8-daemon-set took: 6.857022ms
    Jan 17 15:47:38.511: INFO: Terminating DaemonSet.extensions e2e-45ds8-daemon-set pods took: 101.035399ms
    Jan 17 15:47:39.914: INFO: Number of nodes with available pods controlled by daemonset e2e-45ds8-daemon-set: 0
    Jan 17 15:47:39.914: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-45ds8-daemon-set
    Jan 17 15:47:39.916: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"116304"},"items":null}

    Jan 17 15:47:39.919: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"116304"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/framework.go:187
    Jan 17 15:47:39.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "controllerrevisions-6661" for this suite. 01/17/23 15:47:39.936
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:47:39.943
Jan 17 15:47:39.943: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename downward-api 01/17/23 15:47:39.943
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:47:39.964
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:47:39.969
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192
STEP: Creating a pod to test downward API volume plugin 01/17/23 15:47:39.971
Jan 17 15:47:40.002: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f4d9ed2b-630d-4191-87dd-5ee43e58672c" in namespace "downward-api-1896" to be "Succeeded or Failed"
Jan 17 15:47:40.007: INFO: Pod "downwardapi-volume-f4d9ed2b-630d-4191-87dd-5ee43e58672c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.365033ms
Jan 17 15:47:42.010: INFO: Pod "downwardapi-volume-f4d9ed2b-630d-4191-87dd-5ee43e58672c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007715146s
Jan 17 15:47:44.011: INFO: Pod "downwardapi-volume-f4d9ed2b-630d-4191-87dd-5ee43e58672c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008701022s
STEP: Saw pod success 01/17/23 15:47:44.011
Jan 17 15:47:44.011: INFO: Pod "downwardapi-volume-f4d9ed2b-630d-4191-87dd-5ee43e58672c" satisfied condition "Succeeded or Failed"
Jan 17 15:47:44.014: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod downwardapi-volume-f4d9ed2b-630d-4191-87dd-5ee43e58672c container client-container: <nil>
STEP: delete the pod 01/17/23 15:47:44.02
Jan 17 15:47:44.032: INFO: Waiting for pod downwardapi-volume-f4d9ed2b-630d-4191-87dd-5ee43e58672c to disappear
Jan 17 15:47:44.034: INFO: Pod downwardapi-volume-f4d9ed2b-630d-4191-87dd-5ee43e58672c no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 17 15:47:44.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1896" for this suite. 01/17/23 15:47:44.039
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","completed":240,"skipped":4429,"failed":0}
------------------------------
• [4.101 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:47:39.943
    Jan 17 15:47:39.943: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename downward-api 01/17/23 15:47:39.943
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:47:39.964
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:47:39.969
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:192
    STEP: Creating a pod to test downward API volume plugin 01/17/23 15:47:39.971
    Jan 17 15:47:40.002: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f4d9ed2b-630d-4191-87dd-5ee43e58672c" in namespace "downward-api-1896" to be "Succeeded or Failed"
    Jan 17 15:47:40.007: INFO: Pod "downwardapi-volume-f4d9ed2b-630d-4191-87dd-5ee43e58672c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.365033ms
    Jan 17 15:47:42.010: INFO: Pod "downwardapi-volume-f4d9ed2b-630d-4191-87dd-5ee43e58672c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007715146s
    Jan 17 15:47:44.011: INFO: Pod "downwardapi-volume-f4d9ed2b-630d-4191-87dd-5ee43e58672c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008701022s
    STEP: Saw pod success 01/17/23 15:47:44.011
    Jan 17 15:47:44.011: INFO: Pod "downwardapi-volume-f4d9ed2b-630d-4191-87dd-5ee43e58672c" satisfied condition "Succeeded or Failed"
    Jan 17 15:47:44.014: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod downwardapi-volume-f4d9ed2b-630d-4191-87dd-5ee43e58672c container client-container: <nil>
    STEP: delete the pod 01/17/23 15:47:44.02
    Jan 17 15:47:44.032: INFO: Waiting for pod downwardapi-volume-f4d9ed2b-630d-4191-87dd-5ee43e58672c to disappear
    Jan 17 15:47:44.034: INFO: Pod downwardapi-volume-f4d9ed2b-630d-4191-87dd-5ee43e58672c no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 17 15:47:44.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-1896" for this suite. 01/17/23 15:47:44.039
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:47:44.045
Jan 17 15:47:44.045: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename webhook 01/17/23 15:47:44.046
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:47:44.078
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:47:44.082
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/17/23 15:47:44.105
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/17/23 15:47:44.359
STEP: Deploying the webhook pod 01/17/23 15:47:44.369
STEP: Wait for the deployment to be ready 01/17/23 15:47:44.38
Jan 17 15:47:44.385: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/17/23 15:47:46.395
STEP: Verifying the service has paired with the endpoint 01/17/23 15:47:46.405
Jan 17 15:47:47.405: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290
Jan 17 15:47:47.408: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6378-crds.webhook.example.com via the AdmissionRegistration API 01/17/23 15:47:47.917
Jan 17 15:47:47.933: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource that should be mutated by the webhook 01/17/23 15:47:48.04
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 17 15:47:50.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5079" for this suite. 01/17/23 15:47:50.598
STEP: Destroying namespace "webhook-5079-markers" for this suite. 01/17/23 15:47:50.604
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","completed":241,"skipped":4468,"failed":0}
------------------------------
• [SLOW TEST] [6.653 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:47:44.045
    Jan 17 15:47:44.045: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename webhook 01/17/23 15:47:44.046
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:47:44.078
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:47:44.082
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/17/23 15:47:44.105
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/17/23 15:47:44.359
    STEP: Deploying the webhook pod 01/17/23 15:47:44.369
    STEP: Wait for the deployment to be ready 01/17/23 15:47:44.38
    Jan 17 15:47:44.385: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/17/23 15:47:46.395
    STEP: Verifying the service has paired with the endpoint 01/17/23 15:47:46.405
    Jan 17 15:47:47.405: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:290
    Jan 17 15:47:47.408: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6378-crds.webhook.example.com via the AdmissionRegistration API 01/17/23 15:47:47.917
    Jan 17 15:47:47.933: INFO: Waiting for webhook configuration to be ready...
    STEP: Creating a custom resource that should be mutated by the webhook 01/17/23 15:47:48.04
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 17 15:47:50.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-5079" for this suite. 01/17/23 15:47:50.598
    STEP: Destroying namespace "webhook-5079-markers" for this suite. 01/17/23 15:47:50.604
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:47:50.699
Jan 17 15:47:50.699: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename endpointslicemirroring 01/17/23 15:47:50.699
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:47:50.772
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:47:50.778
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 01/17/23 15:47:50.835
Jan 17 15:47:50.891: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 01/17/23 15:47:52.895
Jan 17 15:47:52.904: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint 01/17/23 15:47:54.908
Jan 17 15:47:54.916: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:187
Jan 17 15:47:56.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-5841" for this suite. 01/17/23 15:47:56.925
{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","completed":242,"skipped":4500,"failed":0}
------------------------------
• [SLOW TEST] [6.233 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:47:50.699
    Jan 17 15:47:50.699: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename endpointslicemirroring 01/17/23 15:47:50.699
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:47:50.772
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:47:50.778
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 01/17/23 15:47:50.835
    Jan 17 15:47:50.891: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 01/17/23 15:47:52.895
    Jan 17 15:47:52.904: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
    STEP: mirroring deletion of a custom Endpoint 01/17/23 15:47:54.908
    Jan 17 15:47:54.916: INFO: Waiting for 0 EndpointSlices to exist, got 1
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/framework.go:187
    Jan 17 15:47:56.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslicemirroring-5841" for this suite. 01/17/23 15:47:56.925
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:47:56.932
Jan 17 15:47:56.932: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename dns 01/17/23 15:47:56.933
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:47:56.954
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:47:56.957
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 01/17/23 15:47:56.961
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8226.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-8226.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8226.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8226.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8226.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-8226.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8226.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-8226.svc.cluster.local;sleep 1; done
 01/17/23 15:47:56.969
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8226.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-8226.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8226.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-8226.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8226.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-8226.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8226.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-8226.svc.cluster.local;sleep 1; done
 01/17/23 15:47:56.969
STEP: creating a pod to probe DNS 01/17/23 15:47:56.969
STEP: submitting the pod to kubernetes 01/17/23 15:47:56.969
Jan 17 15:47:57.001: INFO: Waiting up to 15m0s for pod "dns-test-6d8b5d29-9719-496a-985f-c78423a1656e" in namespace "dns-8226" to be "running"
Jan 17 15:47:57.007: INFO: Pod "dns-test-6d8b5d29-9719-496a-985f-c78423a1656e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.015627ms
Jan 17 15:47:59.011: INFO: Pod "dns-test-6d8b5d29-9719-496a-985f-c78423a1656e": Phase="Running", Reason="", readiness=true. Elapsed: 2.009576615s
Jan 17 15:47:59.011: INFO: Pod "dns-test-6d8b5d29-9719-496a-985f-c78423a1656e" satisfied condition "running"
STEP: retrieving the pod 01/17/23 15:47:59.011
STEP: looking for the results for each expected name from probers 01/17/23 15:47:59.013
Jan 17 15:47:59.019: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8226.svc.cluster.local from pod dns-8226/dns-test-6d8b5d29-9719-496a-985f-c78423a1656e: the server could not find the requested resource (get pods dns-test-6d8b5d29-9719-496a-985f-c78423a1656e)
Jan 17 15:47:59.023: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8226.svc.cluster.local from pod dns-8226/dns-test-6d8b5d29-9719-496a-985f-c78423a1656e: the server could not find the requested resource (get pods dns-test-6d8b5d29-9719-496a-985f-c78423a1656e)
Jan 17 15:47:59.027: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8226.svc.cluster.local from pod dns-8226/dns-test-6d8b5d29-9719-496a-985f-c78423a1656e: the server could not find the requested resource (get pods dns-test-6d8b5d29-9719-496a-985f-c78423a1656e)
Jan 17 15:47:59.030: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8226.svc.cluster.local from pod dns-8226/dns-test-6d8b5d29-9719-496a-985f-c78423a1656e: the server could not find the requested resource (get pods dns-test-6d8b5d29-9719-496a-985f-c78423a1656e)
Jan 17 15:47:59.033: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8226.svc.cluster.local from pod dns-8226/dns-test-6d8b5d29-9719-496a-985f-c78423a1656e: the server could not find the requested resource (get pods dns-test-6d8b5d29-9719-496a-985f-c78423a1656e)
Jan 17 15:47:59.037: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8226.svc.cluster.local from pod dns-8226/dns-test-6d8b5d29-9719-496a-985f-c78423a1656e: the server could not find the requested resource (get pods dns-test-6d8b5d29-9719-496a-985f-c78423a1656e)
Jan 17 15:47:59.040: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8226.svc.cluster.local from pod dns-8226/dns-test-6d8b5d29-9719-496a-985f-c78423a1656e: the server could not find the requested resource (get pods dns-test-6d8b5d29-9719-496a-985f-c78423a1656e)
Jan 17 15:47:59.043: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8226.svc.cluster.local from pod dns-8226/dns-test-6d8b5d29-9719-496a-985f-c78423a1656e: the server could not find the requested resource (get pods dns-test-6d8b5d29-9719-496a-985f-c78423a1656e)
Jan 17 15:47:59.043: INFO: Lookups using dns-8226/dns-test-6d8b5d29-9719-496a-985f-c78423a1656e failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8226.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8226.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8226.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8226.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8226.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8226.svc.cluster.local jessie_udp@dns-test-service-2.dns-8226.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8226.svc.cluster.local]

Jan 17 15:48:04.072: INFO: DNS probes using dns-8226/dns-test-6d8b5d29-9719-496a-985f-c78423a1656e succeeded

STEP: deleting the pod 01/17/23 15:48:04.072
STEP: deleting the test headless service 01/17/23 15:48:04.087
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan 17 15:48:04.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8226" for this suite. 01/17/23 15:48:04.121
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","completed":243,"skipped":4509,"failed":0}
------------------------------
• [SLOW TEST] [7.201 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:47:56.932
    Jan 17 15:47:56.932: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename dns 01/17/23 15:47:56.933
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:47:56.954
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:47:56.957
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 01/17/23 15:47:56.961
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8226.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-8226.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8226.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8226.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8226.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-8226.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8226.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-8226.svc.cluster.local;sleep 1; done
     01/17/23 15:47:56.969
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8226.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-8226.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8226.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-8226.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8226.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-8226.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8226.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-8226.svc.cluster.local;sleep 1; done
     01/17/23 15:47:56.969
    STEP: creating a pod to probe DNS 01/17/23 15:47:56.969
    STEP: submitting the pod to kubernetes 01/17/23 15:47:56.969
    Jan 17 15:47:57.001: INFO: Waiting up to 15m0s for pod "dns-test-6d8b5d29-9719-496a-985f-c78423a1656e" in namespace "dns-8226" to be "running"
    Jan 17 15:47:57.007: INFO: Pod "dns-test-6d8b5d29-9719-496a-985f-c78423a1656e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.015627ms
    Jan 17 15:47:59.011: INFO: Pod "dns-test-6d8b5d29-9719-496a-985f-c78423a1656e": Phase="Running", Reason="", readiness=true. Elapsed: 2.009576615s
    Jan 17 15:47:59.011: INFO: Pod "dns-test-6d8b5d29-9719-496a-985f-c78423a1656e" satisfied condition "running"
    STEP: retrieving the pod 01/17/23 15:47:59.011
    STEP: looking for the results for each expected name from probers 01/17/23 15:47:59.013
    Jan 17 15:47:59.019: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8226.svc.cluster.local from pod dns-8226/dns-test-6d8b5d29-9719-496a-985f-c78423a1656e: the server could not find the requested resource (get pods dns-test-6d8b5d29-9719-496a-985f-c78423a1656e)
    Jan 17 15:47:59.023: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8226.svc.cluster.local from pod dns-8226/dns-test-6d8b5d29-9719-496a-985f-c78423a1656e: the server could not find the requested resource (get pods dns-test-6d8b5d29-9719-496a-985f-c78423a1656e)
    Jan 17 15:47:59.027: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8226.svc.cluster.local from pod dns-8226/dns-test-6d8b5d29-9719-496a-985f-c78423a1656e: the server could not find the requested resource (get pods dns-test-6d8b5d29-9719-496a-985f-c78423a1656e)
    Jan 17 15:47:59.030: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8226.svc.cluster.local from pod dns-8226/dns-test-6d8b5d29-9719-496a-985f-c78423a1656e: the server could not find the requested resource (get pods dns-test-6d8b5d29-9719-496a-985f-c78423a1656e)
    Jan 17 15:47:59.033: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8226.svc.cluster.local from pod dns-8226/dns-test-6d8b5d29-9719-496a-985f-c78423a1656e: the server could not find the requested resource (get pods dns-test-6d8b5d29-9719-496a-985f-c78423a1656e)
    Jan 17 15:47:59.037: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8226.svc.cluster.local from pod dns-8226/dns-test-6d8b5d29-9719-496a-985f-c78423a1656e: the server could not find the requested resource (get pods dns-test-6d8b5d29-9719-496a-985f-c78423a1656e)
    Jan 17 15:47:59.040: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8226.svc.cluster.local from pod dns-8226/dns-test-6d8b5d29-9719-496a-985f-c78423a1656e: the server could not find the requested resource (get pods dns-test-6d8b5d29-9719-496a-985f-c78423a1656e)
    Jan 17 15:47:59.043: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8226.svc.cluster.local from pod dns-8226/dns-test-6d8b5d29-9719-496a-985f-c78423a1656e: the server could not find the requested resource (get pods dns-test-6d8b5d29-9719-496a-985f-c78423a1656e)
    Jan 17 15:47:59.043: INFO: Lookups using dns-8226/dns-test-6d8b5d29-9719-496a-985f-c78423a1656e failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8226.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8226.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8226.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8226.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8226.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8226.svc.cluster.local jessie_udp@dns-test-service-2.dns-8226.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8226.svc.cluster.local]

    Jan 17 15:48:04.072: INFO: DNS probes using dns-8226/dns-test-6d8b5d29-9719-496a-985f-c78423a1656e succeeded

    STEP: deleting the pod 01/17/23 15:48:04.072
    STEP: deleting the test headless service 01/17/23 15:48:04.087
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan 17 15:48:04.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-8226" for this suite. 01/17/23 15:48:04.121
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:48:04.133
Jan 17 15:48:04.133: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename job 01/17/23 15:48:04.134
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:48:04.164
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:48:04.168
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531
STEP: Creating a suspended job 01/17/23 15:48:04.189
STEP: Patching the Job 01/17/23 15:48:04.196
STEP: Watching for Job to be patched 01/17/23 15:48:04.218
Jan 17 15:48:04.219: INFO: Event ADDED observed for Job e2e-pdvqv in namespace job-476 with labels: map[e2e-job-label:e2e-pdvqv] and annotations: map[batch.kubernetes.io/job-tracking:]
Jan 17 15:48:04.219: INFO: Event MODIFIED found for Job e2e-pdvqv in namespace job-476 with labels: map[e2e-job-label:e2e-pdvqv e2e-pdvqv:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 01/17/23 15:48:04.219
STEP: Watching for Job to be updated 01/17/23 15:48:04.258
Jan 17 15:48:04.259: INFO: Event MODIFIED found for Job e2e-pdvqv in namespace job-476 with labels: map[e2e-job-label:e2e-pdvqv e2e-pdvqv:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 17 15:48:04.259: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 01/17/23 15:48:04.259
Jan 17 15:48:04.262: INFO: Job: e2e-pdvqv as labels: map[e2e-job-label:e2e-pdvqv e2e-pdvqv:patched]
STEP: Waiting for job to complete 01/17/23 15:48:04.262
STEP: Delete a job collection with a labelselector 01/17/23 15:48:14.266
STEP: Watching for Job to be deleted 01/17/23 15:48:14.273
Jan 17 15:48:14.275: INFO: Event MODIFIED observed for Job e2e-pdvqv in namespace job-476 with labels: map[e2e-job-label:e2e-pdvqv e2e-pdvqv:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 17 15:48:14.275: INFO: Event MODIFIED observed for Job e2e-pdvqv in namespace job-476 with labels: map[e2e-job-label:e2e-pdvqv e2e-pdvqv:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 17 15:48:14.275: INFO: Event MODIFIED observed for Job e2e-pdvqv in namespace job-476 with labels: map[e2e-job-label:e2e-pdvqv e2e-pdvqv:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 17 15:48:14.275: INFO: Event MODIFIED observed for Job e2e-pdvqv in namespace job-476 with labels: map[e2e-job-label:e2e-pdvqv e2e-pdvqv:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 17 15:48:14.275: INFO: Event MODIFIED observed for Job e2e-pdvqv in namespace job-476 with labels: map[e2e-job-label:e2e-pdvqv e2e-pdvqv:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 17 15:48:14.275: INFO: Event DELETED found for Job e2e-pdvqv in namespace job-476 with labels: map[e2e-job-label:e2e-pdvqv e2e-pdvqv:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 01/17/23 15:48:14.275
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jan 17 15:48:14.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-476" for this suite. 01/17/23 15:48:14.282
{"msg":"PASSED [sig-apps] Job should manage the lifecycle of a job [Conformance]","completed":244,"skipped":4512,"failed":0}
------------------------------
• [SLOW TEST] [10.162 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:48:04.133
    Jan 17 15:48:04.133: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename job 01/17/23 15:48:04.134
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:48:04.164
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:48:04.168
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:531
    STEP: Creating a suspended job 01/17/23 15:48:04.189
    STEP: Patching the Job 01/17/23 15:48:04.196
    STEP: Watching for Job to be patched 01/17/23 15:48:04.218
    Jan 17 15:48:04.219: INFO: Event ADDED observed for Job e2e-pdvqv in namespace job-476 with labels: map[e2e-job-label:e2e-pdvqv] and annotations: map[batch.kubernetes.io/job-tracking:]
    Jan 17 15:48:04.219: INFO: Event MODIFIED found for Job e2e-pdvqv in namespace job-476 with labels: map[e2e-job-label:e2e-pdvqv e2e-pdvqv:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 01/17/23 15:48:04.219
    STEP: Watching for Job to be updated 01/17/23 15:48:04.258
    Jan 17 15:48:04.259: INFO: Event MODIFIED found for Job e2e-pdvqv in namespace job-476 with labels: map[e2e-job-label:e2e-pdvqv e2e-pdvqv:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 17 15:48:04.259: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 01/17/23 15:48:04.259
    Jan 17 15:48:04.262: INFO: Job: e2e-pdvqv as labels: map[e2e-job-label:e2e-pdvqv e2e-pdvqv:patched]
    STEP: Waiting for job to complete 01/17/23 15:48:04.262
    STEP: Delete a job collection with a labelselector 01/17/23 15:48:14.266
    STEP: Watching for Job to be deleted 01/17/23 15:48:14.273
    Jan 17 15:48:14.275: INFO: Event MODIFIED observed for Job e2e-pdvqv in namespace job-476 with labels: map[e2e-job-label:e2e-pdvqv e2e-pdvqv:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 17 15:48:14.275: INFO: Event MODIFIED observed for Job e2e-pdvqv in namespace job-476 with labels: map[e2e-job-label:e2e-pdvqv e2e-pdvqv:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 17 15:48:14.275: INFO: Event MODIFIED observed for Job e2e-pdvqv in namespace job-476 with labels: map[e2e-job-label:e2e-pdvqv e2e-pdvqv:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 17 15:48:14.275: INFO: Event MODIFIED observed for Job e2e-pdvqv in namespace job-476 with labels: map[e2e-job-label:e2e-pdvqv e2e-pdvqv:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 17 15:48:14.275: INFO: Event MODIFIED observed for Job e2e-pdvqv in namespace job-476 with labels: map[e2e-job-label:e2e-pdvqv e2e-pdvqv:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 17 15:48:14.275: INFO: Event DELETED found for Job e2e-pdvqv in namespace job-476 with labels: map[e2e-job-label:e2e-pdvqv e2e-pdvqv:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 01/17/23 15:48:14.275
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jan 17 15:48:14.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-476" for this suite. 01/17/23 15:48:14.282
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:48:14.296
Jan 17 15:48:14.296: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename container-probe 01/17/23 15:48:14.296
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:48:14.318
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:48:14.322
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148
STEP: Creating pod busybox-dc23bb5c-ae2d-432e-b06f-061761cdebe8 in namespace container-probe-6283 01/17/23 15:48:14.325
Jan 17 15:48:14.366: INFO: Waiting up to 5m0s for pod "busybox-dc23bb5c-ae2d-432e-b06f-061761cdebe8" in namespace "container-probe-6283" to be "not pending"
Jan 17 15:48:14.380: INFO: Pod "busybox-dc23bb5c-ae2d-432e-b06f-061761cdebe8": Phase="Pending", Reason="", readiness=false. Elapsed: 13.913084ms
Jan 17 15:48:16.384: INFO: Pod "busybox-dc23bb5c-ae2d-432e-b06f-061761cdebe8": Phase="Running", Reason="", readiness=true. Elapsed: 2.018658576s
Jan 17 15:48:16.384: INFO: Pod "busybox-dc23bb5c-ae2d-432e-b06f-061761cdebe8" satisfied condition "not pending"
Jan 17 15:48:16.384: INFO: Started pod busybox-dc23bb5c-ae2d-432e-b06f-061761cdebe8 in namespace container-probe-6283
STEP: checking the pod's current state and verifying that restartCount is present 01/17/23 15:48:16.384
Jan 17 15:48:16.387: INFO: Initial restart count of pod busybox-dc23bb5c-ae2d-432e-b06f-061761cdebe8 is 0
STEP: deleting the pod 01/17/23 15:52:16.932
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan 17 15:52:16.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6283" for this suite. 01/17/23 15:52:16.954
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","completed":245,"skipped":4554,"failed":0}
------------------------------
• [SLOW TEST] [242.664 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:48:14.296
    Jan 17 15:48:14.296: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename container-probe 01/17/23 15:48:14.296
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:48:14.318
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:48:14.322
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:148
    STEP: Creating pod busybox-dc23bb5c-ae2d-432e-b06f-061761cdebe8 in namespace container-probe-6283 01/17/23 15:48:14.325
    Jan 17 15:48:14.366: INFO: Waiting up to 5m0s for pod "busybox-dc23bb5c-ae2d-432e-b06f-061761cdebe8" in namespace "container-probe-6283" to be "not pending"
    Jan 17 15:48:14.380: INFO: Pod "busybox-dc23bb5c-ae2d-432e-b06f-061761cdebe8": Phase="Pending", Reason="", readiness=false. Elapsed: 13.913084ms
    Jan 17 15:48:16.384: INFO: Pod "busybox-dc23bb5c-ae2d-432e-b06f-061761cdebe8": Phase="Running", Reason="", readiness=true. Elapsed: 2.018658576s
    Jan 17 15:48:16.384: INFO: Pod "busybox-dc23bb5c-ae2d-432e-b06f-061761cdebe8" satisfied condition "not pending"
    Jan 17 15:48:16.384: INFO: Started pod busybox-dc23bb5c-ae2d-432e-b06f-061761cdebe8 in namespace container-probe-6283
    STEP: checking the pod's current state and verifying that restartCount is present 01/17/23 15:48:16.384
    Jan 17 15:48:16.387: INFO: Initial restart count of pod busybox-dc23bb5c-ae2d-432e-b06f-061761cdebe8 is 0
    STEP: deleting the pod 01/17/23 15:52:16.932
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan 17 15:52:16.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-6283" for this suite. 01/17/23 15:52:16.954
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1745
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:52:16.962
Jan 17 15:52:16.962: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename kubectl 01/17/23 15:52:16.963
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:52:16.991
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:52:16.996
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1732
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1745
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/17/23 15:52:16.999
Jan 17 15:52:16.999: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-8925 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jan 17 15:52:17.081: INFO: stderr: ""
Jan 17 15:52:17.081: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 01/17/23 15:52:17.081
STEP: verifying the pod e2e-test-httpd-pod was created 01/17/23 15:52:22.134
Jan 17 15:52:22.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-8925 get pod e2e-test-httpd-pod -o json'
Jan 17 15:52:22.181: INFO: stderr: ""
Jan 17 15:52:22.181: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"k8s.ovn.org/pod-networks\": \"{\\\"default\\\":{\\\"ip_addresses\\\":[\\\"10.131.1.51/23\\\"],\\\"mac_address\\\":\\\"0a:58:0a:83:01:33\\\",\\\"gateway_ips\\\":[\\\"10.131.0.1\\\"],\\\"ip_address\\\":\\\"10.131.1.51/23\\\",\\\"gateway_ip\\\":\\\"10.131.0.1\\\"}}\",\n            \"k8s.v1.cni.cncf.io/network-status\": \"[{\\n    \\\"name\\\": \\\"ovn-kubernetes\\\",\\n    \\\"interface\\\": \\\"eth0\\\",\\n    \\\"ips\\\": [\\n        \\\"10.131.1.51\\\"\\n    ],\\n    \\\"mac\\\": \\\"0a:58:0a:83:01:33\\\",\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\",\n            \"k8s.v1.cni.cncf.io/networks-status\": \"[{\\n    \\\"name\\\": \\\"ovn-kubernetes\\\",\\n    \\\"interface\\\": \\\"eth0\\\",\\n    \\\"ips\\\": [\\n        \\\"10.131.1.51\\\"\\n    ],\\n    \\\"mac\\\": \\\"0a:58:0a:83:01:33\\\",\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\",\n            \"openshift.io/scc\": \"anyuid\"\n        },\n        \"creationTimestamp\": \"2023-01-17T15:52:17Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-8925\",\n        \"resourceVersion\": \"118748\",\n        \"uid\": \"d36fa128-fe52-4d22-b2bf-b7a9780bf4f4\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"securityContext\": {\n                    \"capabilities\": {\n                        \"drop\": [\n                            \"MKNOD\"\n                        ]\n                    }\n                },\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-zfxcl\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-10-0-151-22.ec2.internal\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {\n            \"seLinuxOptions\": {\n                \"level\": \"s0:c57,c54\"\n            }\n        },\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-zfxcl\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"service-ca.crt\",\n                                        \"path\": \"service-ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"openshift-service-ca.crt\"\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-17T15:52:17Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-17T15:52:17Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-17T15:52:17Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-17T15:52:17Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"cri-o://6ae666d26d8d5563644990f2299036c89b360406235a4be8935cd56c8e2ec614\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-01-17T15:52:17Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.0.151.22\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.131.1.51\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.131.1.51\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-01-17T15:52:17Z\"\n    }\n}\n"
STEP: replace the image in the pod 01/17/23 15:52:22.181
Jan 17 15:52:22.181: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-8925 replace -f -'
Jan 17 15:52:23.957: INFO: stderr: ""
Jan 17 15:52:23.957: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-2 01/17/23 15:52:23.957
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1736
Jan 17 15:52:23.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-8925 delete pods e2e-test-httpd-pod'
Jan 17 15:52:25.858: INFO: stderr: ""
Jan 17 15:52:25.858: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 17 15:52:25.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8925" for this suite. 01/17/23 15:52:25.863
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","completed":246,"skipped":4629,"failed":0}
------------------------------
• [SLOW TEST] [8.907 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1729
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1745

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:52:16.962
    Jan 17 15:52:16.962: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename kubectl 01/17/23 15:52:16.963
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:52:16.991
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:52:16.996
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1732
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1745
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/17/23 15:52:16.999
    Jan 17 15:52:16.999: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-8925 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Jan 17 15:52:17.081: INFO: stderr: ""
    Jan 17 15:52:17.081: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 01/17/23 15:52:17.081
    STEP: verifying the pod e2e-test-httpd-pod was created 01/17/23 15:52:22.134
    Jan 17 15:52:22.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-8925 get pod e2e-test-httpd-pod -o json'
    Jan 17 15:52:22.181: INFO: stderr: ""
    Jan 17 15:52:22.181: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"k8s.ovn.org/pod-networks\": \"{\\\"default\\\":{\\\"ip_addresses\\\":[\\\"10.131.1.51/23\\\"],\\\"mac_address\\\":\\\"0a:58:0a:83:01:33\\\",\\\"gateway_ips\\\":[\\\"10.131.0.1\\\"],\\\"ip_address\\\":\\\"10.131.1.51/23\\\",\\\"gateway_ip\\\":\\\"10.131.0.1\\\"}}\",\n            \"k8s.v1.cni.cncf.io/network-status\": \"[{\\n    \\\"name\\\": \\\"ovn-kubernetes\\\",\\n    \\\"interface\\\": \\\"eth0\\\",\\n    \\\"ips\\\": [\\n        \\\"10.131.1.51\\\"\\n    ],\\n    \\\"mac\\\": \\\"0a:58:0a:83:01:33\\\",\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\",\n            \"k8s.v1.cni.cncf.io/networks-status\": \"[{\\n    \\\"name\\\": \\\"ovn-kubernetes\\\",\\n    \\\"interface\\\": \\\"eth0\\\",\\n    \\\"ips\\\": [\\n        \\\"10.131.1.51\\\"\\n    ],\\n    \\\"mac\\\": \\\"0a:58:0a:83:01:33\\\",\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\",\n            \"openshift.io/scc\": \"anyuid\"\n        },\n        \"creationTimestamp\": \"2023-01-17T15:52:17Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-8925\",\n        \"resourceVersion\": \"118748\",\n        \"uid\": \"d36fa128-fe52-4d22-b2bf-b7a9780bf4f4\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"securityContext\": {\n                    \"capabilities\": {\n                        \"drop\": [\n                            \"MKNOD\"\n                        ]\n                    }\n                },\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-zfxcl\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-10-0-151-22.ec2.internal\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {\n            \"seLinuxOptions\": {\n                \"level\": \"s0:c57,c54\"\n            }\n        },\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-zfxcl\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"service-ca.crt\",\n                                        \"path\": \"service-ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"openshift-service-ca.crt\"\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-17T15:52:17Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-17T15:52:17Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-17T15:52:17Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-17T15:52:17Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"cri-o://6ae666d26d8d5563644990f2299036c89b360406235a4be8935cd56c8e2ec614\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-01-17T15:52:17Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.0.151.22\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.131.1.51\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.131.1.51\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-01-17T15:52:17Z\"\n    }\n}\n"
    STEP: replace the image in the pod 01/17/23 15:52:22.181
    Jan 17 15:52:22.181: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-8925 replace -f -'
    Jan 17 15:52:23.957: INFO: stderr: ""
    Jan 17 15:52:23.957: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-2 01/17/23 15:52:23.957
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1736
    Jan 17 15:52:23.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-8925 delete pods e2e-test-httpd-pod'
    Jan 17 15:52:25.858: INFO: stderr: ""
    Jan 17 15:52:25.858: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 17 15:52:25.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-8925" for this suite. 01/17/23 15:52:25.863
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:52:25.87
Jan 17 15:52:25.870: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename custom-resource-definition 01/17/23 15:52:25.87
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:52:25.921
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:52:25.925
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
Jan 17 15:52:25.927: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 17 15:52:26.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1543" for this suite. 01/17/23 15:52:26.964
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","completed":247,"skipped":4632,"failed":0}
------------------------------
• [1.100 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:52:25.87
    Jan 17 15:52:25.870: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename custom-resource-definition 01/17/23 15:52:25.87
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:52:25.921
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:52:25.925
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    Jan 17 15:52:25.927: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 17 15:52:26.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-1543" for this suite. 01/17/23 15:52:26.964
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1683
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:52:26.97
Jan 17 15:52:26.970: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename kubectl 01/17/23 15:52:26.971
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:52:26.999
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:52:27.002
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1683
Jan 17 15:52:27.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-3906 version'
Jan 17 15:52:27.041: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Jan 17 15:52:27.041: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.4\", GitCommit:\"872a965c6c6526caa949f0c6ac028ef7aff3fb78\", GitTreeState:\"clean\", BuildDate:\"2022-11-09T13:36:36Z\", GoVersion:\"go1.19.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.4+77bec7a\", GitCommit:\"b6d1f054747e9886f61dd85316deac3415e2726f\", GitTreeState:\"clean\", BuildDate:\"2022-12-14T20:18:42Z\", GoVersion:\"go1.19.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 17 15:52:27.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3906" for this suite. 01/17/23 15:52:27.045
{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","completed":248,"skipped":4634,"failed":0}
------------------------------
• [0.090 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1677
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1683

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:52:26.97
    Jan 17 15:52:26.970: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename kubectl 01/17/23 15:52:26.971
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:52:26.999
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:52:27.002
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1683
    Jan 17 15:52:27.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-3906 version'
    Jan 17 15:52:27.041: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    Jan 17 15:52:27.041: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.4\", GitCommit:\"872a965c6c6526caa949f0c6ac028ef7aff3fb78\", GitTreeState:\"clean\", BuildDate:\"2022-11-09T13:36:36Z\", GoVersion:\"go1.19.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.4+77bec7a\", GitCommit:\"b6d1f054747e9886f61dd85316deac3415e2726f\", GitTreeState:\"clean\", BuildDate:\"2022-12-14T20:18:42Z\", GoVersion:\"go1.19.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 17 15:52:27.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-3906" for this suite. 01/17/23 15:52:27.045
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:52:27.061
Jan 17 15:52:27.061: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename sched-pred 01/17/23 15:52:27.061
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:52:27.112
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:52:27.116
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jan 17 15:52:27.119: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 17 15:52:27.136: INFO: Waiting for terminating namespaces to be deleted...
Jan 17 15:52:27.156: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-139-213.ec2.internal before test
Jan 17 15:52:27.202: INFO: aws-ebs-csi-driver-node-5tmvr from openshift-cluster-csi-drivers started at 2023-01-17 12:55:50 +0000 UTC (3 container statuses recorded)
Jan 17 15:52:27.202: INFO: 	Container csi-driver ready: true, restart count 0
Jan 17 15:52:27.202: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Jan 17 15:52:27.202: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
Jan 17 15:52:27.202: INFO: tuned-jzlnj from openshift-cluster-node-tuning-operator started at 2023-01-17 12:55:50 +0000 UTC (1 container statuses recorded)
Jan 17 15:52:27.202: INFO: 	Container tuned ready: true, restart count 0
Jan 17 15:52:27.202: INFO: downloads-8d695cd69-ltk55 from openshift-console started at 2023-01-17 12:57:39 +0000 UTC (1 container statuses recorded)
Jan 17 15:52:27.202: INFO: 	Container download-server ready: true, restart count 0
Jan 17 15:52:27.202: INFO: dns-default-jgzr9 from openshift-dns started at 2023-01-17 12:56:45 +0000 UTC (2 container statuses recorded)
Jan 17 15:52:27.202: INFO: 	Container dns ready: true, restart count 0
Jan 17 15:52:27.202: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 17 15:52:27.202: INFO: node-resolver-gkxnp from openshift-dns started at 2023-01-17 12:55:50 +0000 UTC (1 container statuses recorded)
Jan 17 15:52:27.202: INFO: 	Container dns-node-resolver ready: true, restart count 0
Jan 17 15:52:27.202: INFO: image-registry-6d685bd45d-mk7wb from openshift-image-registry started at 2023-01-17 12:56:45 +0000 UTC (1 container statuses recorded)
Jan 17 15:52:27.202: INFO: 	Container registry ready: true, restart count 0
Jan 17 15:52:27.202: INFO: node-ca-4l49x from openshift-image-registry started at 2023-01-17 12:55:50 +0000 UTC (1 container statuses recorded)
Jan 17 15:52:27.202: INFO: 	Container node-ca ready: true, restart count 0
Jan 17 15:52:27.202: INFO: ingress-canary-xqzwm from openshift-ingress-canary started at 2023-01-17 12:56:45 +0000 UTC (1 container statuses recorded)
Jan 17 15:52:27.202: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Jan 17 15:52:27.202: INFO: router-default-c95cc587f-9clvn from openshift-ingress started at 2023-01-17 12:56:45 +0000 UTC (1 container statuses recorded)
Jan 17 15:52:27.202: INFO: 	Container router ready: true, restart count 0
Jan 17 15:52:27.202: INFO: machine-config-daemon-6hjqr from openshift-machine-config-operator started at 2023-01-17 12:55:50 +0000 UTC (2 container statuses recorded)
Jan 17 15:52:27.202: INFO: 	Container machine-config-daemon ready: true, restart count 0
Jan 17 15:52:27.202: INFO: 	Container oauth-proxy ready: true, restart count 0
Jan 17 15:52:27.202: INFO: alertmanager-main-0 from openshift-monitoring started at 2023-01-17 12:59:05 +0000 UTC (6 container statuses recorded)
Jan 17 15:52:27.202: INFO: 	Container alertmanager ready: true, restart count 1
Jan 17 15:52:27.202: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jan 17 15:52:27.202: INFO: 	Container config-reloader ready: true, restart count 0
Jan 17 15:52:27.202: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 17 15:52:27.202: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Jan 17 15:52:27.202: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jan 17 15:52:27.202: INFO: node-exporter-lwbc8 from openshift-monitoring started at 2023-01-17 12:56:55 +0000 UTC (2 container statuses recorded)
Jan 17 15:52:27.202: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 17 15:52:27.202: INFO: 	Container node-exporter ready: true, restart count 0
Jan 17 15:52:27.202: INFO: prometheus-adapter-cd9bc68fc-dstxm from openshift-monitoring started at 2023-01-17 12:57:45 +0000 UTC (1 container statuses recorded)
Jan 17 15:52:27.202: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jan 17 15:52:27.202: INFO: prometheus-k8s-0 from openshift-monitoring started at 2023-01-17 12:59:05 +0000 UTC (6 container statuses recorded)
Jan 17 15:52:27.202: INFO: 	Container config-reloader ready: true, restart count 0
Jan 17 15:52:27.202: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 17 15:52:27.202: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Jan 17 15:52:27.202: INFO: 	Container prometheus ready: true, restart count 0
Jan 17 15:52:27.202: INFO: 	Container prometheus-proxy ready: true, restart count 0
Jan 17 15:52:27.202: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jan 17 15:52:27.202: INFO: prometheus-operator-admission-webhook-7d4759d465-lgsbt from openshift-monitoring started at 2023-01-17 15:23:22 +0000 UTC (1 container statuses recorded)
Jan 17 15:52:27.202: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Jan 17 15:52:27.202: INFO: thanos-querier-5fccfc4877-prbhx from openshift-monitoring started at 2023-01-17 12:57:02 +0000 UTC (6 container statuses recorded)
Jan 17 15:52:27.202: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 17 15:52:27.202: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Jan 17 15:52:27.202: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Jan 17 15:52:27.202: INFO: 	Container oauth-proxy ready: true, restart count 0
Jan 17 15:52:27.202: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jan 17 15:52:27.202: INFO: 	Container thanos-query ready: true, restart count 0
Jan 17 15:52:27.202: INFO: multus-additional-cni-plugins-bpc6w from openshift-multus started at 2023-01-17 12:55:50 +0000 UTC (1 container statuses recorded)
Jan 17 15:52:27.202: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Jan 17 15:52:27.202: INFO: multus-tt5fs from openshift-multus started at 2023-01-17 12:55:50 +0000 UTC (1 container statuses recorded)
Jan 17 15:52:27.202: INFO: 	Container kube-multus ready: true, restart count 0
Jan 17 15:52:27.202: INFO: network-metrics-daemon-22p8j from openshift-multus started at 2023-01-17 12:55:50 +0000 UTC (2 container statuses recorded)
Jan 17 15:52:27.202: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 17 15:52:27.202: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Jan 17 15:52:27.202: INFO: network-check-target-k685r from openshift-network-diagnostics started at 2023-01-17 12:55:50 +0000 UTC (1 container statuses recorded)
Jan 17 15:52:27.202: INFO: 	Container network-check-target-container ready: true, restart count 0
Jan 17 15:52:27.202: INFO: ovnkube-node-7n8jx from openshift-ovn-kubernetes started at 2023-01-17 12:55:50 +0000 UTC (5 container statuses recorded)
Jan 17 15:52:27.202: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 17 15:52:27.202: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 0
Jan 17 15:52:27.202: INFO: 	Container ovn-acl-logging ready: true, restart count 0
Jan 17 15:52:27.202: INFO: 	Container ovn-controller ready: true, restart count 0
Jan 17 15:52:27.202: INFO: 	Container ovnkube-node ready: true, restart count 0
Jan 17 15:52:27.202: INFO: sonobuoy-systemd-logs-daemon-set-329d7d7181d04e86-mhx5d from sonobuoy started at 2023-01-17 14:47:25 +0000 UTC (2 container statuses recorded)
Jan 17 15:52:27.202: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 17 15:52:27.202: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 17 15:52:27.202: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-151-22.ec2.internal before test
Jan 17 15:52:27.227: INFO: aws-ebs-csi-driver-node-mx922 from openshift-cluster-csi-drivers started at 2023-01-17 12:51:50 +0000 UTC (3 container statuses recorded)
Jan 17 15:52:27.227: INFO: 	Container csi-driver ready: true, restart count 0
Jan 17 15:52:27.227: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Jan 17 15:52:27.227: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Jan 17 15:52:27.227: INFO: tuned-9hhcs from openshift-cluster-node-tuning-operator started at 2023-01-17 12:51:50 +0000 UTC (1 container statuses recorded)
Jan 17 15:52:27.227: INFO: 	Container tuned ready: true, restart count 0
Jan 17 15:52:27.227: INFO: downloads-8d695cd69-gbsf4 from openshift-console started at 2023-01-17 15:23:22 +0000 UTC (1 container statuses recorded)
Jan 17 15:52:27.227: INFO: 	Container download-server ready: true, restart count 0
Jan 17 15:52:27.227: INFO: dns-default-mdfm8 from openshift-dns started at 2023-01-17 15:23:43 +0000 UTC (2 container statuses recorded)
Jan 17 15:52:27.227: INFO: 	Container dns ready: true, restart count 0
Jan 17 15:52:27.227: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 17 15:52:27.227: INFO: node-resolver-fxksv from openshift-dns started at 2023-01-17 12:51:50 +0000 UTC (1 container statuses recorded)
Jan 17 15:52:27.227: INFO: 	Container dns-node-resolver ready: true, restart count 0
Jan 17 15:52:27.227: INFO: node-ca-tpp6r from openshift-image-registry started at 2023-01-17 12:54:40 +0000 UTC (1 container statuses recorded)
Jan 17 15:52:27.227: INFO: 	Container node-ca ready: true, restart count 0
Jan 17 15:52:27.227: INFO: ingress-canary-9tpfk from openshift-ingress-canary started at 2023-01-17 15:23:23 +0000 UTC (1 container statuses recorded)
Jan 17 15:52:27.227: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Jan 17 15:52:27.227: INFO: machine-config-daemon-bgrjb from openshift-machine-config-operator started at 2023-01-17 12:51:50 +0000 UTC (2 container statuses recorded)
Jan 17 15:52:27.227: INFO: 	Container machine-config-daemon ready: true, restart count 0
Jan 17 15:52:27.227: INFO: 	Container oauth-proxy ready: true, restart count 0
Jan 17 15:52:27.227: INFO: alertmanager-main-1 from openshift-monitoring started at 2023-01-17 15:23:24 +0000 UTC (6 container statuses recorded)
Jan 17 15:52:27.227: INFO: 	Container alertmanager ready: true, restart count 1
Jan 17 15:52:27.227: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jan 17 15:52:27.227: INFO: 	Container config-reloader ready: true, restart count 0
Jan 17 15:52:27.227: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 17 15:52:27.227: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Jan 17 15:52:27.227: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jan 17 15:52:27.227: INFO: node-exporter-snggg from openshift-monitoring started at 2023-01-17 12:56:55 +0000 UTC (2 container statuses recorded)
Jan 17 15:52:27.227: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 17 15:52:27.227: INFO: 	Container node-exporter ready: true, restart count 0
Jan 17 15:52:27.227: INFO: multus-additional-cni-plugins-grzml from openshift-multus started at 2023-01-17 12:51:50 +0000 UTC (1 container statuses recorded)
Jan 17 15:52:27.227: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Jan 17 15:52:27.227: INFO: multus-qspfd from openshift-multus started at 2023-01-17 12:51:50 +0000 UTC (1 container statuses recorded)
Jan 17 15:52:27.227: INFO: 	Container kube-multus ready: true, restart count 0
Jan 17 15:52:27.227: INFO: network-metrics-daemon-x4zrh from openshift-multus started at 2023-01-17 12:51:50 +0000 UTC (2 container statuses recorded)
Jan 17 15:52:27.227: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 17 15:52:27.227: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Jan 17 15:52:27.227: INFO: network-check-target-f794b from openshift-network-diagnostics started at 2023-01-17 12:51:50 +0000 UTC (1 container statuses recorded)
Jan 17 15:52:27.227: INFO: 	Container network-check-target-container ready: true, restart count 0
Jan 17 15:52:27.228: INFO: collect-profiles-27899490-z525c from openshift-operator-lifecycle-manager started at 2023-01-17 15:30:00 +0000 UTC (1 container statuses recorded)
Jan 17 15:52:27.228: INFO: 	Container collect-profiles ready: false, restart count 0
Jan 17 15:52:27.228: INFO: collect-profiles-27899505-c4wjm from openshift-operator-lifecycle-manager started at 2023-01-17 15:45:00 +0000 UTC (1 container statuses recorded)
Jan 17 15:52:27.228: INFO: 	Container collect-profiles ready: false, restart count 0
Jan 17 15:52:27.228: INFO: ovnkube-node-bks72 from openshift-ovn-kubernetes started at 2023-01-17 12:51:50 +0000 UTC (5 container statuses recorded)
Jan 17 15:52:27.228: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 17 15:52:27.228: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 0
Jan 17 15:52:27.228: INFO: 	Container ovn-acl-logging ready: true, restart count 0
Jan 17 15:52:27.228: INFO: 	Container ovn-controller ready: true, restart count 0
Jan 17 15:52:27.228: INFO: 	Container ovnkube-node ready: true, restart count 0
Jan 17 15:52:27.228: INFO: sonobuoy from sonobuoy started at 2023-01-17 14:47:22 +0000 UTC (1 container statuses recorded)
Jan 17 15:52:27.228: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 17 15:52:27.228: INFO: sonobuoy-e2e-job-0a9b408aa2f34bad from sonobuoy started at 2023-01-17 14:47:25 +0000 UTC (2 container statuses recorded)
Jan 17 15:52:27.228: INFO: 	Container e2e ready: true, restart count 0
Jan 17 15:52:27.228: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 17 15:52:27.228: INFO: sonobuoy-systemd-logs-daemon-set-329d7d7181d04e86-8knzd from sonobuoy started at 2023-01-17 14:47:25 +0000 UTC (2 container statuses recorded)
Jan 17 15:52:27.228: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 17 15:52:27.228: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 17 15:52:27.228: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-165-14.ec2.internal before test
Jan 17 15:52:27.250: INFO: aws-ebs-csi-driver-node-x9fxp from openshift-cluster-csi-drivers started at 2023-01-17 12:55:53 +0000 UTC (3 container statuses recorded)
Jan 17 15:52:27.250: INFO: 	Container csi-driver ready: true, restart count 0
Jan 17 15:52:27.250: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Jan 17 15:52:27.250: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Jan 17 15:52:27.250: INFO: tuned-cgskr from openshift-cluster-node-tuning-operator started at 2023-01-17 12:55:53 +0000 UTC (1 container statuses recorded)
Jan 17 15:52:27.250: INFO: 	Container tuned ready: true, restart count 0
Jan 17 15:52:27.250: INFO: dns-default-8q4m7 from openshift-dns started at 2023-01-17 12:56:45 +0000 UTC (2 container statuses recorded)
Jan 17 15:52:27.250: INFO: 	Container dns ready: true, restart count 0
Jan 17 15:52:27.250: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 17 15:52:27.250: INFO: node-resolver-dvw44 from openshift-dns started at 2023-01-17 12:55:53 +0000 UTC (1 container statuses recorded)
Jan 17 15:52:27.250: INFO: 	Container dns-node-resolver ready: true, restart count 0
Jan 17 15:52:27.250: INFO: image-registry-6d685bd45d-g2mxt from openshift-image-registry started at 2023-01-17 12:56:45 +0000 UTC (1 container statuses recorded)
Jan 17 15:52:27.250: INFO: 	Container registry ready: true, restart count 0
Jan 17 15:52:27.250: INFO: node-ca-45nfl from openshift-image-registry started at 2023-01-17 12:55:53 +0000 UTC (1 container statuses recorded)
Jan 17 15:52:27.250: INFO: 	Container node-ca ready: true, restart count 0
Jan 17 15:52:27.250: INFO: ingress-canary-q6p6t from openshift-ingress-canary started at 2023-01-17 12:56:45 +0000 UTC (1 container statuses recorded)
Jan 17 15:52:27.250: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Jan 17 15:52:27.250: INFO: router-default-c95cc587f-pj9sc from openshift-ingress started at 2023-01-17 15:23:22 +0000 UTC (1 container statuses recorded)
Jan 17 15:52:27.250: INFO: 	Container router ready: true, restart count 0
Jan 17 15:52:27.250: INFO: machine-config-daemon-v7v4j from openshift-machine-config-operator started at 2023-01-17 12:55:53 +0000 UTC (2 container statuses recorded)
Jan 17 15:52:27.250: INFO: 	Container machine-config-daemon ready: true, restart count 0
Jan 17 15:52:27.250: INFO: 	Container oauth-proxy ready: true, restart count 0
Jan 17 15:52:27.250: INFO: kube-state-metrics-75455b796c-rgbdl from openshift-monitoring started at 2023-01-17 12:56:55 +0000 UTC (3 container statuses recorded)
Jan 17 15:52:27.250: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jan 17 15:52:27.250: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jan 17 15:52:27.250: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jan 17 15:52:27.250: INFO: node-exporter-pdxfc from openshift-monitoring started at 2023-01-17 12:56:55 +0000 UTC (2 container statuses recorded)
Jan 17 15:52:27.250: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 17 15:52:27.250: INFO: 	Container node-exporter ready: true, restart count 0
Jan 17 15:52:27.250: INFO: openshift-state-metrics-5ff95d844f-dl27q from openshift-monitoring started at 2023-01-17 12:56:55 +0000 UTC (3 container statuses recorded)
Jan 17 15:52:27.250: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jan 17 15:52:27.250: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jan 17 15:52:27.250: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Jan 17 15:52:27.250: INFO: prometheus-adapter-cd9bc68fc-ptnx8 from openshift-monitoring started at 2023-01-17 15:23:22 +0000 UTC (1 container statuses recorded)
Jan 17 15:52:27.250: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jan 17 15:52:27.250: INFO: prometheus-k8s-1 from openshift-monitoring started at 2023-01-17 12:58:37 +0000 UTC (6 container statuses recorded)
Jan 17 15:52:27.250: INFO: 	Container config-reloader ready: true, restart count 0
Jan 17 15:52:27.250: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 17 15:52:27.250: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Jan 17 15:52:27.250: INFO: 	Container prometheus ready: true, restart count 0
Jan 17 15:52:27.250: INFO: 	Container prometheus-proxy ready: true, restart count 0
Jan 17 15:52:27.250: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jan 17 15:52:27.250: INFO: prometheus-operator-admission-webhook-7d4759d465-bw8tr from openshift-monitoring started at 2023-01-17 12:56:45 +0000 UTC (1 container statuses recorded)
Jan 17 15:52:27.250: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Jan 17 15:52:27.250: INFO: telemeter-client-585b5cf5bf-zlltk from openshift-monitoring started at 2023-01-17 12:57:00 +0000 UTC (3 container statuses recorded)
Jan 17 15:52:27.250: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 17 15:52:27.250: INFO: 	Container reload ready: true, restart count 0
Jan 17 15:52:27.250: INFO: 	Container telemeter-client ready: true, restart count 0
Jan 17 15:52:27.250: INFO: thanos-querier-5fccfc4877-lc5nk from openshift-monitoring started at 2023-01-17 12:57:02 +0000 UTC (6 container statuses recorded)
Jan 17 15:52:27.250: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 17 15:52:27.250: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Jan 17 15:52:27.250: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Jan 17 15:52:27.250: INFO: 	Container oauth-proxy ready: true, restart count 0
Jan 17 15:52:27.250: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jan 17 15:52:27.250: INFO: 	Container thanos-query ready: true, restart count 0
Jan 17 15:52:27.250: INFO: multus-additional-cni-plugins-zg84j from openshift-multus started at 2023-01-17 12:55:53 +0000 UTC (1 container statuses recorded)
Jan 17 15:52:27.250: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Jan 17 15:52:27.250: INFO: multus-jq6qc from openshift-multus started at 2023-01-17 12:55:53 +0000 UTC (1 container statuses recorded)
Jan 17 15:52:27.250: INFO: 	Container kube-multus ready: true, restart count 0
Jan 17 15:52:27.250: INFO: network-metrics-daemon-gngg5 from openshift-multus started at 2023-01-17 12:55:53 +0000 UTC (2 container statuses recorded)
Jan 17 15:52:27.250: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 17 15:52:27.250: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Jan 17 15:52:27.250: INFO: network-check-source-746dd6c885-gl46q from openshift-network-diagnostics started at 2023-01-17 15:23:22 +0000 UTC (1 container statuses recorded)
Jan 17 15:52:27.250: INFO: 	Container check-endpoints ready: true, restart count 0
Jan 17 15:52:27.250: INFO: network-check-target-v9g9x from openshift-network-diagnostics started at 2023-01-17 12:55:53 +0000 UTC (1 container statuses recorded)
Jan 17 15:52:27.250: INFO: 	Container network-check-target-container ready: true, restart count 0
Jan 17 15:52:27.250: INFO: ovnkube-node-q54mr from openshift-ovn-kubernetes started at 2023-01-17 12:55:53 +0000 UTC (5 container statuses recorded)
Jan 17 15:52:27.250: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 17 15:52:27.250: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 0
Jan 17 15:52:27.250: INFO: 	Container ovn-acl-logging ready: true, restart count 0
Jan 17 15:52:27.250: INFO: 	Container ovn-controller ready: true, restart count 0
Jan 17 15:52:27.250: INFO: 	Container ovnkube-node ready: true, restart count 0
Jan 17 15:52:27.250: INFO: sonobuoy-systemd-logs-daemon-set-329d7d7181d04e86-qcrqh from sonobuoy started at 2023-01-17 14:47:25 +0000 UTC (2 container statuses recorded)
Jan 17 15:52:27.250: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 17 15:52:27.250: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699
STEP: Trying to launch a pod without a label to get a node which can launch it. 01/17/23 15:52:27.25
Jan 17 15:52:27.267: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-6869" to be "running"
Jan 17 15:52:27.270: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 3.277971ms
Jan 17 15:52:29.275: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.007939361s
Jan 17 15:52:29.275: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 01/17/23 15:52:29.281
STEP: Trying to apply a random label on the found node. 01/17/23 15:52:29.297
STEP: verifying the node has the label kubernetes.io/e2e-1dbefbf2-3afa-44a3-9e2f-67e6d00787d4 95 01/17/23 15:52:29.307
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 01/17/23 15:52:29.324
Jan 17 15:52:29.348: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-6869" to be "not pending"
Jan 17 15:52:29.352: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.003687ms
Jan 17 15:52:31.356: INFO: Pod "pod4": Phase="Running", Reason="", readiness=false. Elapsed: 2.008132948s
Jan 17 15:52:31.356: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.0.151.22 on the node which pod4 resides and expect not scheduled 01/17/23 15:52:31.356
Jan 17 15:52:31.368: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-6869" to be "not pending"
Jan 17 15:52:31.371: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.690783ms
Jan 17 15:52:33.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006982108s
Jan 17 15:52:35.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006773105s
Jan 17 15:52:37.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.00739238s
Jan 17 15:52:39.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.007668144s
Jan 17 15:52:41.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.007603254s
Jan 17 15:52:43.380: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.011745669s
Jan 17 15:52:45.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.006571932s
Jan 17 15:52:47.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.008063697s
Jan 17 15:52:49.374: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.006149392s
Jan 17 15:52:51.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.007286401s
Jan 17 15:52:53.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.006980102s
Jan 17 15:52:55.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.007053812s
Jan 17 15:52:57.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.006807387s
Jan 17 15:52:59.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.00785946s
Jan 17 15:53:01.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.007396287s
Jan 17 15:53:03.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.00675871s
Jan 17 15:53:05.377: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.008333514s
Jan 17 15:53:07.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.008172564s
Jan 17 15:53:09.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.007797509s
Jan 17 15:53:11.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.007591095s
Jan 17 15:53:13.374: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.006093533s
Jan 17 15:53:15.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.007031846s
Jan 17 15:53:17.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.007091508s
Jan 17 15:53:19.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.006617397s
Jan 17 15:53:21.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.007485625s
Jan 17 15:53:23.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.008057148s
Jan 17 15:53:25.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.007504295s
Jan 17 15:53:27.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.008076754s
Jan 17 15:53:29.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.006980203s
Jan 17 15:53:31.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.007297173s
Jan 17 15:53:33.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.00808162s
Jan 17 15:53:35.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.006917292s
Jan 17 15:53:37.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.007237663s
Jan 17 15:53:39.374: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.006196228s
Jan 17 15:53:41.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.006730291s
Jan 17 15:53:43.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.006948338s
Jan 17 15:53:45.377: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.008498655s
Jan 17 15:53:47.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.008119386s
Jan 17 15:53:49.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.006484275s
Jan 17 15:53:51.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.00666826s
Jan 17 15:53:53.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.007543915s
Jan 17 15:53:55.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.00628874s
Jan 17 15:53:57.377: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.008251129s
Jan 17 15:53:59.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.006710431s
Jan 17 15:54:01.377: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.008271771s
Jan 17 15:54:03.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.006856462s
Jan 17 15:54:05.378: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.01014217s
Jan 17 15:54:07.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.006230783s
Jan 17 15:54:09.380: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.012053063s
Jan 17 15:54:11.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.007343775s
Jan 17 15:54:13.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.00742744s
Jan 17 15:54:15.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.007469625s
Jan 17 15:54:17.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.007909698s
Jan 17 15:54:19.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.007539464s
Jan 17 15:54:21.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.007795973s
Jan 17 15:54:23.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.00641971s
Jan 17 15:54:25.374: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.00604765s
Jan 17 15:54:27.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.006735692s
Jan 17 15:54:29.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.007153735s
Jan 17 15:54:31.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.006793596s
Jan 17 15:54:33.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.00703578s
Jan 17 15:54:35.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.006490178s
Jan 17 15:54:37.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.007071696s
Jan 17 15:54:39.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.007036129s
Jan 17 15:54:41.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.00756007s
Jan 17 15:54:43.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.007342828s
Jan 17 15:54:45.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.006908623s
Jan 17 15:54:47.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.007660114s
Jan 17 15:54:49.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.007276596s
Jan 17 15:54:51.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.006369017s
Jan 17 15:54:53.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.007143466s
Jan 17 15:54:55.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.007894823s
Jan 17 15:54:57.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.00754424s
Jan 17 15:54:59.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.006506854s
Jan 17 15:55:01.374: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.006071996s
Jan 17 15:55:03.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.006912116s
Jan 17 15:55:05.378: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.009375394s
Jan 17 15:55:07.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.007465571s
Jan 17 15:55:09.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.006979981s
Jan 17 15:55:11.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.006349728s
Jan 17 15:55:13.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.007215929s
Jan 17 15:55:15.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.007205064s
Jan 17 15:55:17.377: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.008368975s
Jan 17 15:55:19.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.007145084s
Jan 17 15:55:21.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.006719566s
Jan 17 15:55:23.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.006619229s
Jan 17 15:55:25.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.007619192s
Jan 17 15:55:27.377: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.008246624s
Jan 17 15:55:29.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.007260067s
Jan 17 15:55:31.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.007790019s
Jan 17 15:55:33.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.006549191s
Jan 17 15:55:35.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.006729956s
Jan 17 15:55:37.377: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.008301171s
Jan 17 15:55:39.382: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.013602449s
Jan 17 15:55:41.377: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.008534615s
Jan 17 15:55:43.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.007169675s
Jan 17 15:55:45.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.007220868s
Jan 17 15:55:47.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.007836691s
Jan 17 15:55:49.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.00750516s
Jan 17 15:55:51.374: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.005913681s
Jan 17 15:55:53.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.007224073s
Jan 17 15:55:55.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.006318523s
Jan 17 15:55:57.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.007534699s
Jan 17 15:55:59.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.007416625s
Jan 17 15:56:01.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.006928897s
Jan 17 15:56:03.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.006952146s
Jan 17 15:56:05.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.007853746s
Jan 17 15:56:07.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.007162975s
Jan 17 15:56:09.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.006740356s
Jan 17 15:56:11.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.007820088s
Jan 17 15:56:13.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.006774206s
Jan 17 15:56:15.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.007639755s
Jan 17 15:56:17.377: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.008950428s
Jan 17 15:56:19.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.007632306s
Jan 17 15:56:21.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.007284757s
Jan 17 15:56:23.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.007332681s
Jan 17 15:56:25.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.007405997s
Jan 17 15:56:27.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.007708619s
Jan 17 15:56:29.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.007418945s
Jan 17 15:56:31.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.007620091s
Jan 17 15:56:33.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.006815441s
Jan 17 15:56:35.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.006399592s
Jan 17 15:56:37.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.007612593s
Jan 17 15:56:39.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.007724181s
Jan 17 15:56:41.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.006901883s
Jan 17 15:56:43.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.006300011s
Jan 17 15:56:45.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.006692392s
Jan 17 15:56:47.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.007901135s
Jan 17 15:56:49.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.007235208s
Jan 17 15:56:51.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.006948688s
Jan 17 15:56:53.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.006281584s
Jan 17 15:56:55.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.007437183s
Jan 17 15:56:57.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.007760097s
Jan 17 15:56:59.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.006786148s
Jan 17 15:57:01.374: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.005863186s
Jan 17 15:57:03.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.006345563s
Jan 17 15:57:05.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.007901411s
Jan 17 15:57:07.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.007077444s
Jan 17 15:57:09.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.00649464s
Jan 17 15:57:11.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.007666466s
Jan 17 15:57:13.374: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.005870966s
Jan 17 15:57:15.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.007678818s
Jan 17 15:57:17.377: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.008771185s
Jan 17 15:57:19.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.007213623s
Jan 17 15:57:21.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.007899384s
Jan 17 15:57:23.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.006634457s
Jan 17 15:57:25.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.007720179s
Jan 17 15:57:27.377: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.008287237s
Jan 17 15:57:29.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.007507263s
Jan 17 15:57:31.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.007829436s
Jan 17 15:57:31.380: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.011352501s
STEP: removing the label kubernetes.io/e2e-1dbefbf2-3afa-44a3-9e2f-67e6d00787d4 off the node ip-10-0-151-22.ec2.internal 01/17/23 15:57:31.38
STEP: verifying the node doesn't have the label kubernetes.io/e2e-1dbefbf2-3afa-44a3-9e2f-67e6d00787d4 01/17/23 15:57:31.391
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Jan 17 15:57:31.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6869" for this suite. 01/17/23 15:57:31.402
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","completed":249,"skipped":4639,"failed":0}
------------------------------
• [SLOW TEST] [304.351 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:52:27.061
    Jan 17 15:52:27.061: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename sched-pred 01/17/23 15:52:27.061
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:52:27.112
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:52:27.116
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Jan 17 15:52:27.119: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan 17 15:52:27.136: INFO: Waiting for terminating namespaces to be deleted...
    Jan 17 15:52:27.156: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-139-213.ec2.internal before test
    Jan 17 15:52:27.202: INFO: aws-ebs-csi-driver-node-5tmvr from openshift-cluster-csi-drivers started at 2023-01-17 12:55:50 +0000 UTC (3 container statuses recorded)
    Jan 17 15:52:27.202: INFO: 	Container csi-driver ready: true, restart count 0
    Jan 17 15:52:27.202: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    Jan 17 15:52:27.202: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
    Jan 17 15:52:27.202: INFO: tuned-jzlnj from openshift-cluster-node-tuning-operator started at 2023-01-17 12:55:50 +0000 UTC (1 container statuses recorded)
    Jan 17 15:52:27.202: INFO: 	Container tuned ready: true, restart count 0
    Jan 17 15:52:27.202: INFO: downloads-8d695cd69-ltk55 from openshift-console started at 2023-01-17 12:57:39 +0000 UTC (1 container statuses recorded)
    Jan 17 15:52:27.202: INFO: 	Container download-server ready: true, restart count 0
    Jan 17 15:52:27.202: INFO: dns-default-jgzr9 from openshift-dns started at 2023-01-17 12:56:45 +0000 UTC (2 container statuses recorded)
    Jan 17 15:52:27.202: INFO: 	Container dns ready: true, restart count 0
    Jan 17 15:52:27.202: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 17 15:52:27.202: INFO: node-resolver-gkxnp from openshift-dns started at 2023-01-17 12:55:50 +0000 UTC (1 container statuses recorded)
    Jan 17 15:52:27.202: INFO: 	Container dns-node-resolver ready: true, restart count 0
    Jan 17 15:52:27.202: INFO: image-registry-6d685bd45d-mk7wb from openshift-image-registry started at 2023-01-17 12:56:45 +0000 UTC (1 container statuses recorded)
    Jan 17 15:52:27.202: INFO: 	Container registry ready: true, restart count 0
    Jan 17 15:52:27.202: INFO: node-ca-4l49x from openshift-image-registry started at 2023-01-17 12:55:50 +0000 UTC (1 container statuses recorded)
    Jan 17 15:52:27.202: INFO: 	Container node-ca ready: true, restart count 0
    Jan 17 15:52:27.202: INFO: ingress-canary-xqzwm from openshift-ingress-canary started at 2023-01-17 12:56:45 +0000 UTC (1 container statuses recorded)
    Jan 17 15:52:27.202: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    Jan 17 15:52:27.202: INFO: router-default-c95cc587f-9clvn from openshift-ingress started at 2023-01-17 12:56:45 +0000 UTC (1 container statuses recorded)
    Jan 17 15:52:27.202: INFO: 	Container router ready: true, restart count 0
    Jan 17 15:52:27.202: INFO: machine-config-daemon-6hjqr from openshift-machine-config-operator started at 2023-01-17 12:55:50 +0000 UTC (2 container statuses recorded)
    Jan 17 15:52:27.202: INFO: 	Container machine-config-daemon ready: true, restart count 0
    Jan 17 15:52:27.202: INFO: 	Container oauth-proxy ready: true, restart count 0
    Jan 17 15:52:27.202: INFO: alertmanager-main-0 from openshift-monitoring started at 2023-01-17 12:59:05 +0000 UTC (6 container statuses recorded)
    Jan 17 15:52:27.202: INFO: 	Container alertmanager ready: true, restart count 1
    Jan 17 15:52:27.202: INFO: 	Container alertmanager-proxy ready: true, restart count 0
    Jan 17 15:52:27.202: INFO: 	Container config-reloader ready: true, restart count 0
    Jan 17 15:52:27.202: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 17 15:52:27.202: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
    Jan 17 15:52:27.202: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Jan 17 15:52:27.202: INFO: node-exporter-lwbc8 from openshift-monitoring started at 2023-01-17 12:56:55 +0000 UTC (2 container statuses recorded)
    Jan 17 15:52:27.202: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 17 15:52:27.202: INFO: 	Container node-exporter ready: true, restart count 0
    Jan 17 15:52:27.202: INFO: prometheus-adapter-cd9bc68fc-dstxm from openshift-monitoring started at 2023-01-17 12:57:45 +0000 UTC (1 container statuses recorded)
    Jan 17 15:52:27.202: INFO: 	Container prometheus-adapter ready: true, restart count 0
    Jan 17 15:52:27.202: INFO: prometheus-k8s-0 from openshift-monitoring started at 2023-01-17 12:59:05 +0000 UTC (6 container statuses recorded)
    Jan 17 15:52:27.202: INFO: 	Container config-reloader ready: true, restart count 0
    Jan 17 15:52:27.202: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 17 15:52:27.202: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
    Jan 17 15:52:27.202: INFO: 	Container prometheus ready: true, restart count 0
    Jan 17 15:52:27.202: INFO: 	Container prometheus-proxy ready: true, restart count 0
    Jan 17 15:52:27.202: INFO: 	Container thanos-sidecar ready: true, restart count 0
    Jan 17 15:52:27.202: INFO: prometheus-operator-admission-webhook-7d4759d465-lgsbt from openshift-monitoring started at 2023-01-17 15:23:22 +0000 UTC (1 container statuses recorded)
    Jan 17 15:52:27.202: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
    Jan 17 15:52:27.202: INFO: thanos-querier-5fccfc4877-prbhx from openshift-monitoring started at 2023-01-17 12:57:02 +0000 UTC (6 container statuses recorded)
    Jan 17 15:52:27.202: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 17 15:52:27.202: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
    Jan 17 15:52:27.202: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
    Jan 17 15:52:27.202: INFO: 	Container oauth-proxy ready: true, restart count 0
    Jan 17 15:52:27.202: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Jan 17 15:52:27.202: INFO: 	Container thanos-query ready: true, restart count 0
    Jan 17 15:52:27.202: INFO: multus-additional-cni-plugins-bpc6w from openshift-multus started at 2023-01-17 12:55:50 +0000 UTC (1 container statuses recorded)
    Jan 17 15:52:27.202: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    Jan 17 15:52:27.202: INFO: multus-tt5fs from openshift-multus started at 2023-01-17 12:55:50 +0000 UTC (1 container statuses recorded)
    Jan 17 15:52:27.202: INFO: 	Container kube-multus ready: true, restart count 0
    Jan 17 15:52:27.202: INFO: network-metrics-daemon-22p8j from openshift-multus started at 2023-01-17 12:55:50 +0000 UTC (2 container statuses recorded)
    Jan 17 15:52:27.202: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 17 15:52:27.202: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    Jan 17 15:52:27.202: INFO: network-check-target-k685r from openshift-network-diagnostics started at 2023-01-17 12:55:50 +0000 UTC (1 container statuses recorded)
    Jan 17 15:52:27.202: INFO: 	Container network-check-target-container ready: true, restart count 0
    Jan 17 15:52:27.202: INFO: ovnkube-node-7n8jx from openshift-ovn-kubernetes started at 2023-01-17 12:55:50 +0000 UTC (5 container statuses recorded)
    Jan 17 15:52:27.202: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 17 15:52:27.202: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 0
    Jan 17 15:52:27.202: INFO: 	Container ovn-acl-logging ready: true, restart count 0
    Jan 17 15:52:27.202: INFO: 	Container ovn-controller ready: true, restart count 0
    Jan 17 15:52:27.202: INFO: 	Container ovnkube-node ready: true, restart count 0
    Jan 17 15:52:27.202: INFO: sonobuoy-systemd-logs-daemon-set-329d7d7181d04e86-mhx5d from sonobuoy started at 2023-01-17 14:47:25 +0000 UTC (2 container statuses recorded)
    Jan 17 15:52:27.202: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 17 15:52:27.202: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 17 15:52:27.202: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-151-22.ec2.internal before test
    Jan 17 15:52:27.227: INFO: aws-ebs-csi-driver-node-mx922 from openshift-cluster-csi-drivers started at 2023-01-17 12:51:50 +0000 UTC (3 container statuses recorded)
    Jan 17 15:52:27.227: INFO: 	Container csi-driver ready: true, restart count 0
    Jan 17 15:52:27.227: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    Jan 17 15:52:27.227: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Jan 17 15:52:27.227: INFO: tuned-9hhcs from openshift-cluster-node-tuning-operator started at 2023-01-17 12:51:50 +0000 UTC (1 container statuses recorded)
    Jan 17 15:52:27.227: INFO: 	Container tuned ready: true, restart count 0
    Jan 17 15:52:27.227: INFO: downloads-8d695cd69-gbsf4 from openshift-console started at 2023-01-17 15:23:22 +0000 UTC (1 container statuses recorded)
    Jan 17 15:52:27.227: INFO: 	Container download-server ready: true, restart count 0
    Jan 17 15:52:27.227: INFO: dns-default-mdfm8 from openshift-dns started at 2023-01-17 15:23:43 +0000 UTC (2 container statuses recorded)
    Jan 17 15:52:27.227: INFO: 	Container dns ready: true, restart count 0
    Jan 17 15:52:27.227: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 17 15:52:27.227: INFO: node-resolver-fxksv from openshift-dns started at 2023-01-17 12:51:50 +0000 UTC (1 container statuses recorded)
    Jan 17 15:52:27.227: INFO: 	Container dns-node-resolver ready: true, restart count 0
    Jan 17 15:52:27.227: INFO: node-ca-tpp6r from openshift-image-registry started at 2023-01-17 12:54:40 +0000 UTC (1 container statuses recorded)
    Jan 17 15:52:27.227: INFO: 	Container node-ca ready: true, restart count 0
    Jan 17 15:52:27.227: INFO: ingress-canary-9tpfk from openshift-ingress-canary started at 2023-01-17 15:23:23 +0000 UTC (1 container statuses recorded)
    Jan 17 15:52:27.227: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    Jan 17 15:52:27.227: INFO: machine-config-daemon-bgrjb from openshift-machine-config-operator started at 2023-01-17 12:51:50 +0000 UTC (2 container statuses recorded)
    Jan 17 15:52:27.227: INFO: 	Container machine-config-daemon ready: true, restart count 0
    Jan 17 15:52:27.227: INFO: 	Container oauth-proxy ready: true, restart count 0
    Jan 17 15:52:27.227: INFO: alertmanager-main-1 from openshift-monitoring started at 2023-01-17 15:23:24 +0000 UTC (6 container statuses recorded)
    Jan 17 15:52:27.227: INFO: 	Container alertmanager ready: true, restart count 1
    Jan 17 15:52:27.227: INFO: 	Container alertmanager-proxy ready: true, restart count 0
    Jan 17 15:52:27.227: INFO: 	Container config-reloader ready: true, restart count 0
    Jan 17 15:52:27.227: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 17 15:52:27.227: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
    Jan 17 15:52:27.227: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Jan 17 15:52:27.227: INFO: node-exporter-snggg from openshift-monitoring started at 2023-01-17 12:56:55 +0000 UTC (2 container statuses recorded)
    Jan 17 15:52:27.227: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 17 15:52:27.227: INFO: 	Container node-exporter ready: true, restart count 0
    Jan 17 15:52:27.227: INFO: multus-additional-cni-plugins-grzml from openshift-multus started at 2023-01-17 12:51:50 +0000 UTC (1 container statuses recorded)
    Jan 17 15:52:27.227: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    Jan 17 15:52:27.227: INFO: multus-qspfd from openshift-multus started at 2023-01-17 12:51:50 +0000 UTC (1 container statuses recorded)
    Jan 17 15:52:27.227: INFO: 	Container kube-multus ready: true, restart count 0
    Jan 17 15:52:27.227: INFO: network-metrics-daemon-x4zrh from openshift-multus started at 2023-01-17 12:51:50 +0000 UTC (2 container statuses recorded)
    Jan 17 15:52:27.227: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 17 15:52:27.227: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    Jan 17 15:52:27.227: INFO: network-check-target-f794b from openshift-network-diagnostics started at 2023-01-17 12:51:50 +0000 UTC (1 container statuses recorded)
    Jan 17 15:52:27.227: INFO: 	Container network-check-target-container ready: true, restart count 0
    Jan 17 15:52:27.228: INFO: collect-profiles-27899490-z525c from openshift-operator-lifecycle-manager started at 2023-01-17 15:30:00 +0000 UTC (1 container statuses recorded)
    Jan 17 15:52:27.228: INFO: 	Container collect-profiles ready: false, restart count 0
    Jan 17 15:52:27.228: INFO: collect-profiles-27899505-c4wjm from openshift-operator-lifecycle-manager started at 2023-01-17 15:45:00 +0000 UTC (1 container statuses recorded)
    Jan 17 15:52:27.228: INFO: 	Container collect-profiles ready: false, restart count 0
    Jan 17 15:52:27.228: INFO: ovnkube-node-bks72 from openshift-ovn-kubernetes started at 2023-01-17 12:51:50 +0000 UTC (5 container statuses recorded)
    Jan 17 15:52:27.228: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 17 15:52:27.228: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 0
    Jan 17 15:52:27.228: INFO: 	Container ovn-acl-logging ready: true, restart count 0
    Jan 17 15:52:27.228: INFO: 	Container ovn-controller ready: true, restart count 0
    Jan 17 15:52:27.228: INFO: 	Container ovnkube-node ready: true, restart count 0
    Jan 17 15:52:27.228: INFO: sonobuoy from sonobuoy started at 2023-01-17 14:47:22 +0000 UTC (1 container statuses recorded)
    Jan 17 15:52:27.228: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan 17 15:52:27.228: INFO: sonobuoy-e2e-job-0a9b408aa2f34bad from sonobuoy started at 2023-01-17 14:47:25 +0000 UTC (2 container statuses recorded)
    Jan 17 15:52:27.228: INFO: 	Container e2e ready: true, restart count 0
    Jan 17 15:52:27.228: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 17 15:52:27.228: INFO: sonobuoy-systemd-logs-daemon-set-329d7d7181d04e86-8knzd from sonobuoy started at 2023-01-17 14:47:25 +0000 UTC (2 container statuses recorded)
    Jan 17 15:52:27.228: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 17 15:52:27.228: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 17 15:52:27.228: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-165-14.ec2.internal before test
    Jan 17 15:52:27.250: INFO: aws-ebs-csi-driver-node-x9fxp from openshift-cluster-csi-drivers started at 2023-01-17 12:55:53 +0000 UTC (3 container statuses recorded)
    Jan 17 15:52:27.250: INFO: 	Container csi-driver ready: true, restart count 0
    Jan 17 15:52:27.250: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    Jan 17 15:52:27.250: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Jan 17 15:52:27.250: INFO: tuned-cgskr from openshift-cluster-node-tuning-operator started at 2023-01-17 12:55:53 +0000 UTC (1 container statuses recorded)
    Jan 17 15:52:27.250: INFO: 	Container tuned ready: true, restart count 0
    Jan 17 15:52:27.250: INFO: dns-default-8q4m7 from openshift-dns started at 2023-01-17 12:56:45 +0000 UTC (2 container statuses recorded)
    Jan 17 15:52:27.250: INFO: 	Container dns ready: true, restart count 0
    Jan 17 15:52:27.250: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 17 15:52:27.250: INFO: node-resolver-dvw44 from openshift-dns started at 2023-01-17 12:55:53 +0000 UTC (1 container statuses recorded)
    Jan 17 15:52:27.250: INFO: 	Container dns-node-resolver ready: true, restart count 0
    Jan 17 15:52:27.250: INFO: image-registry-6d685bd45d-g2mxt from openshift-image-registry started at 2023-01-17 12:56:45 +0000 UTC (1 container statuses recorded)
    Jan 17 15:52:27.250: INFO: 	Container registry ready: true, restart count 0
    Jan 17 15:52:27.250: INFO: node-ca-45nfl from openshift-image-registry started at 2023-01-17 12:55:53 +0000 UTC (1 container statuses recorded)
    Jan 17 15:52:27.250: INFO: 	Container node-ca ready: true, restart count 0
    Jan 17 15:52:27.250: INFO: ingress-canary-q6p6t from openshift-ingress-canary started at 2023-01-17 12:56:45 +0000 UTC (1 container statuses recorded)
    Jan 17 15:52:27.250: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    Jan 17 15:52:27.250: INFO: router-default-c95cc587f-pj9sc from openshift-ingress started at 2023-01-17 15:23:22 +0000 UTC (1 container statuses recorded)
    Jan 17 15:52:27.250: INFO: 	Container router ready: true, restart count 0
    Jan 17 15:52:27.250: INFO: machine-config-daemon-v7v4j from openshift-machine-config-operator started at 2023-01-17 12:55:53 +0000 UTC (2 container statuses recorded)
    Jan 17 15:52:27.250: INFO: 	Container machine-config-daemon ready: true, restart count 0
    Jan 17 15:52:27.250: INFO: 	Container oauth-proxy ready: true, restart count 0
    Jan 17 15:52:27.250: INFO: kube-state-metrics-75455b796c-rgbdl from openshift-monitoring started at 2023-01-17 12:56:55 +0000 UTC (3 container statuses recorded)
    Jan 17 15:52:27.250: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
    Jan 17 15:52:27.250: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
    Jan 17 15:52:27.250: INFO: 	Container kube-state-metrics ready: true, restart count 0
    Jan 17 15:52:27.250: INFO: node-exporter-pdxfc from openshift-monitoring started at 2023-01-17 12:56:55 +0000 UTC (2 container statuses recorded)
    Jan 17 15:52:27.250: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 17 15:52:27.250: INFO: 	Container node-exporter ready: true, restart count 0
    Jan 17 15:52:27.250: INFO: openshift-state-metrics-5ff95d844f-dl27q from openshift-monitoring started at 2023-01-17 12:56:55 +0000 UTC (3 container statuses recorded)
    Jan 17 15:52:27.250: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
    Jan 17 15:52:27.250: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
    Jan 17 15:52:27.250: INFO: 	Container openshift-state-metrics ready: true, restart count 0
    Jan 17 15:52:27.250: INFO: prometheus-adapter-cd9bc68fc-ptnx8 from openshift-monitoring started at 2023-01-17 15:23:22 +0000 UTC (1 container statuses recorded)
    Jan 17 15:52:27.250: INFO: 	Container prometheus-adapter ready: true, restart count 0
    Jan 17 15:52:27.250: INFO: prometheus-k8s-1 from openshift-monitoring started at 2023-01-17 12:58:37 +0000 UTC (6 container statuses recorded)
    Jan 17 15:52:27.250: INFO: 	Container config-reloader ready: true, restart count 0
    Jan 17 15:52:27.250: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 17 15:52:27.250: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
    Jan 17 15:52:27.250: INFO: 	Container prometheus ready: true, restart count 0
    Jan 17 15:52:27.250: INFO: 	Container prometheus-proxy ready: true, restart count 0
    Jan 17 15:52:27.250: INFO: 	Container thanos-sidecar ready: true, restart count 0
    Jan 17 15:52:27.250: INFO: prometheus-operator-admission-webhook-7d4759d465-bw8tr from openshift-monitoring started at 2023-01-17 12:56:45 +0000 UTC (1 container statuses recorded)
    Jan 17 15:52:27.250: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
    Jan 17 15:52:27.250: INFO: telemeter-client-585b5cf5bf-zlltk from openshift-monitoring started at 2023-01-17 12:57:00 +0000 UTC (3 container statuses recorded)
    Jan 17 15:52:27.250: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 17 15:52:27.250: INFO: 	Container reload ready: true, restart count 0
    Jan 17 15:52:27.250: INFO: 	Container telemeter-client ready: true, restart count 0
    Jan 17 15:52:27.250: INFO: thanos-querier-5fccfc4877-lc5nk from openshift-monitoring started at 2023-01-17 12:57:02 +0000 UTC (6 container statuses recorded)
    Jan 17 15:52:27.250: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 17 15:52:27.250: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
    Jan 17 15:52:27.250: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
    Jan 17 15:52:27.250: INFO: 	Container oauth-proxy ready: true, restart count 0
    Jan 17 15:52:27.250: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Jan 17 15:52:27.250: INFO: 	Container thanos-query ready: true, restart count 0
    Jan 17 15:52:27.250: INFO: multus-additional-cni-plugins-zg84j from openshift-multus started at 2023-01-17 12:55:53 +0000 UTC (1 container statuses recorded)
    Jan 17 15:52:27.250: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    Jan 17 15:52:27.250: INFO: multus-jq6qc from openshift-multus started at 2023-01-17 12:55:53 +0000 UTC (1 container statuses recorded)
    Jan 17 15:52:27.250: INFO: 	Container kube-multus ready: true, restart count 0
    Jan 17 15:52:27.250: INFO: network-metrics-daemon-gngg5 from openshift-multus started at 2023-01-17 12:55:53 +0000 UTC (2 container statuses recorded)
    Jan 17 15:52:27.250: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 17 15:52:27.250: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    Jan 17 15:52:27.250: INFO: network-check-source-746dd6c885-gl46q from openshift-network-diagnostics started at 2023-01-17 15:23:22 +0000 UTC (1 container statuses recorded)
    Jan 17 15:52:27.250: INFO: 	Container check-endpoints ready: true, restart count 0
    Jan 17 15:52:27.250: INFO: network-check-target-v9g9x from openshift-network-diagnostics started at 2023-01-17 12:55:53 +0000 UTC (1 container statuses recorded)
    Jan 17 15:52:27.250: INFO: 	Container network-check-target-container ready: true, restart count 0
    Jan 17 15:52:27.250: INFO: ovnkube-node-q54mr from openshift-ovn-kubernetes started at 2023-01-17 12:55:53 +0000 UTC (5 container statuses recorded)
    Jan 17 15:52:27.250: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 17 15:52:27.250: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 0
    Jan 17 15:52:27.250: INFO: 	Container ovn-acl-logging ready: true, restart count 0
    Jan 17 15:52:27.250: INFO: 	Container ovn-controller ready: true, restart count 0
    Jan 17 15:52:27.250: INFO: 	Container ovnkube-node ready: true, restart count 0
    Jan 17 15:52:27.250: INFO: sonobuoy-systemd-logs-daemon-set-329d7d7181d04e86-qcrqh from sonobuoy started at 2023-01-17 14:47:25 +0000 UTC (2 container statuses recorded)
    Jan 17 15:52:27.250: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 17 15:52:27.250: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:699
    STEP: Trying to launch a pod without a label to get a node which can launch it. 01/17/23 15:52:27.25
    Jan 17 15:52:27.267: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-6869" to be "running"
    Jan 17 15:52:27.270: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 3.277971ms
    Jan 17 15:52:29.275: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.007939361s
    Jan 17 15:52:29.275: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 01/17/23 15:52:29.281
    STEP: Trying to apply a random label on the found node. 01/17/23 15:52:29.297
    STEP: verifying the node has the label kubernetes.io/e2e-1dbefbf2-3afa-44a3-9e2f-67e6d00787d4 95 01/17/23 15:52:29.307
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 01/17/23 15:52:29.324
    Jan 17 15:52:29.348: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-6869" to be "not pending"
    Jan 17 15:52:29.352: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.003687ms
    Jan 17 15:52:31.356: INFO: Pod "pod4": Phase="Running", Reason="", readiness=false. Elapsed: 2.008132948s
    Jan 17 15:52:31.356: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.0.151.22 on the node which pod4 resides and expect not scheduled 01/17/23 15:52:31.356
    Jan 17 15:52:31.368: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-6869" to be "not pending"
    Jan 17 15:52:31.371: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.690783ms
    Jan 17 15:52:33.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006982108s
    Jan 17 15:52:35.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006773105s
    Jan 17 15:52:37.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.00739238s
    Jan 17 15:52:39.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.007668144s
    Jan 17 15:52:41.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.007603254s
    Jan 17 15:52:43.380: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.011745669s
    Jan 17 15:52:45.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.006571932s
    Jan 17 15:52:47.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.008063697s
    Jan 17 15:52:49.374: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.006149392s
    Jan 17 15:52:51.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.007286401s
    Jan 17 15:52:53.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.006980102s
    Jan 17 15:52:55.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.007053812s
    Jan 17 15:52:57.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.006807387s
    Jan 17 15:52:59.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.00785946s
    Jan 17 15:53:01.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.007396287s
    Jan 17 15:53:03.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.00675871s
    Jan 17 15:53:05.377: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.008333514s
    Jan 17 15:53:07.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.008172564s
    Jan 17 15:53:09.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.007797509s
    Jan 17 15:53:11.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.007591095s
    Jan 17 15:53:13.374: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.006093533s
    Jan 17 15:53:15.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.007031846s
    Jan 17 15:53:17.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.007091508s
    Jan 17 15:53:19.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.006617397s
    Jan 17 15:53:21.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.007485625s
    Jan 17 15:53:23.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.008057148s
    Jan 17 15:53:25.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.007504295s
    Jan 17 15:53:27.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.008076754s
    Jan 17 15:53:29.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.006980203s
    Jan 17 15:53:31.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.007297173s
    Jan 17 15:53:33.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.00808162s
    Jan 17 15:53:35.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.006917292s
    Jan 17 15:53:37.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.007237663s
    Jan 17 15:53:39.374: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.006196228s
    Jan 17 15:53:41.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.006730291s
    Jan 17 15:53:43.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.006948338s
    Jan 17 15:53:45.377: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.008498655s
    Jan 17 15:53:47.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.008119386s
    Jan 17 15:53:49.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.006484275s
    Jan 17 15:53:51.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.00666826s
    Jan 17 15:53:53.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.007543915s
    Jan 17 15:53:55.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.00628874s
    Jan 17 15:53:57.377: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.008251129s
    Jan 17 15:53:59.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.006710431s
    Jan 17 15:54:01.377: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.008271771s
    Jan 17 15:54:03.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.006856462s
    Jan 17 15:54:05.378: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.01014217s
    Jan 17 15:54:07.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.006230783s
    Jan 17 15:54:09.380: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.012053063s
    Jan 17 15:54:11.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.007343775s
    Jan 17 15:54:13.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.00742744s
    Jan 17 15:54:15.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.007469625s
    Jan 17 15:54:17.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.007909698s
    Jan 17 15:54:19.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.007539464s
    Jan 17 15:54:21.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.007795973s
    Jan 17 15:54:23.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.00641971s
    Jan 17 15:54:25.374: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.00604765s
    Jan 17 15:54:27.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.006735692s
    Jan 17 15:54:29.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.007153735s
    Jan 17 15:54:31.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.006793596s
    Jan 17 15:54:33.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.00703578s
    Jan 17 15:54:35.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.006490178s
    Jan 17 15:54:37.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.007071696s
    Jan 17 15:54:39.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.007036129s
    Jan 17 15:54:41.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.00756007s
    Jan 17 15:54:43.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.007342828s
    Jan 17 15:54:45.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.006908623s
    Jan 17 15:54:47.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.007660114s
    Jan 17 15:54:49.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.007276596s
    Jan 17 15:54:51.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.006369017s
    Jan 17 15:54:53.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.007143466s
    Jan 17 15:54:55.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.007894823s
    Jan 17 15:54:57.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.00754424s
    Jan 17 15:54:59.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.006506854s
    Jan 17 15:55:01.374: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.006071996s
    Jan 17 15:55:03.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.006912116s
    Jan 17 15:55:05.378: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.009375394s
    Jan 17 15:55:07.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.007465571s
    Jan 17 15:55:09.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.006979981s
    Jan 17 15:55:11.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.006349728s
    Jan 17 15:55:13.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.007215929s
    Jan 17 15:55:15.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.007205064s
    Jan 17 15:55:17.377: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.008368975s
    Jan 17 15:55:19.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.007145084s
    Jan 17 15:55:21.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.006719566s
    Jan 17 15:55:23.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.006619229s
    Jan 17 15:55:25.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.007619192s
    Jan 17 15:55:27.377: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.008246624s
    Jan 17 15:55:29.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.007260067s
    Jan 17 15:55:31.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.007790019s
    Jan 17 15:55:33.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.006549191s
    Jan 17 15:55:35.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.006729956s
    Jan 17 15:55:37.377: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.008301171s
    Jan 17 15:55:39.382: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.013602449s
    Jan 17 15:55:41.377: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.008534615s
    Jan 17 15:55:43.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.007169675s
    Jan 17 15:55:45.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.007220868s
    Jan 17 15:55:47.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.007836691s
    Jan 17 15:55:49.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.00750516s
    Jan 17 15:55:51.374: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.005913681s
    Jan 17 15:55:53.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.007224073s
    Jan 17 15:55:55.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.006318523s
    Jan 17 15:55:57.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.007534699s
    Jan 17 15:55:59.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.007416625s
    Jan 17 15:56:01.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.006928897s
    Jan 17 15:56:03.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.006952146s
    Jan 17 15:56:05.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.007853746s
    Jan 17 15:56:07.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.007162975s
    Jan 17 15:56:09.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.006740356s
    Jan 17 15:56:11.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.007820088s
    Jan 17 15:56:13.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.006774206s
    Jan 17 15:56:15.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.007639755s
    Jan 17 15:56:17.377: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.008950428s
    Jan 17 15:56:19.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.007632306s
    Jan 17 15:56:21.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.007284757s
    Jan 17 15:56:23.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.007332681s
    Jan 17 15:56:25.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.007405997s
    Jan 17 15:56:27.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.007708619s
    Jan 17 15:56:29.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.007418945s
    Jan 17 15:56:31.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.007620091s
    Jan 17 15:56:33.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.006815441s
    Jan 17 15:56:35.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.006399592s
    Jan 17 15:56:37.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.007612593s
    Jan 17 15:56:39.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.007724181s
    Jan 17 15:56:41.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.006901883s
    Jan 17 15:56:43.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.006300011s
    Jan 17 15:56:45.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.006692392s
    Jan 17 15:56:47.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.007901135s
    Jan 17 15:56:49.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.007235208s
    Jan 17 15:56:51.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.006948688s
    Jan 17 15:56:53.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.006281584s
    Jan 17 15:56:55.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.007437183s
    Jan 17 15:56:57.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.007760097s
    Jan 17 15:56:59.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.006786148s
    Jan 17 15:57:01.374: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.005863186s
    Jan 17 15:57:03.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.006345563s
    Jan 17 15:57:05.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.007901411s
    Jan 17 15:57:07.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.007077444s
    Jan 17 15:57:09.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.00649464s
    Jan 17 15:57:11.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.007666466s
    Jan 17 15:57:13.374: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.005870966s
    Jan 17 15:57:15.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.007678818s
    Jan 17 15:57:17.377: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.008771185s
    Jan 17 15:57:19.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.007213623s
    Jan 17 15:57:21.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.007899384s
    Jan 17 15:57:23.375: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.006634457s
    Jan 17 15:57:25.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.007720179s
    Jan 17 15:57:27.377: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.008287237s
    Jan 17 15:57:29.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.007507263s
    Jan 17 15:57:31.376: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.007829436s
    Jan 17 15:57:31.380: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.011352501s
    STEP: removing the label kubernetes.io/e2e-1dbefbf2-3afa-44a3-9e2f-67e6d00787d4 off the node ip-10-0-151-22.ec2.internal 01/17/23 15:57:31.38
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-1dbefbf2-3afa-44a3-9e2f-67e6d00787d4 01/17/23 15:57:31.391
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Jan 17 15:57:31.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-6869" for this suite. 01/17/23 15:57:31.402
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:57:31.412
Jan 17 15:57:31.412: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename projected 01/17/23 15:57:31.413
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:57:31.454
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:57:31.456
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67
STEP: Creating a pod to test downward API volume plugin 01/17/23 15:57:31.458
Jan 17 15:57:31.505: INFO: Waiting up to 5m0s for pod "downwardapi-volume-83c2a8f1-d012-4059-9050-561917d47740" in namespace "projected-839" to be "Succeeded or Failed"
Jan 17 15:57:31.525: INFO: Pod "downwardapi-volume-83c2a8f1-d012-4059-9050-561917d47740": Phase="Pending", Reason="", readiness=false. Elapsed: 20.351492ms
Jan 17 15:57:33.529: INFO: Pod "downwardapi-volume-83c2a8f1-d012-4059-9050-561917d47740": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024299344s
Jan 17 15:57:35.530: INFO: Pod "downwardapi-volume-83c2a8f1-d012-4059-9050-561917d47740": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025007751s
STEP: Saw pod success 01/17/23 15:57:35.53
Jan 17 15:57:35.530: INFO: Pod "downwardapi-volume-83c2a8f1-d012-4059-9050-561917d47740" satisfied condition "Succeeded or Failed"
Jan 17 15:57:35.533: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod downwardapi-volume-83c2a8f1-d012-4059-9050-561917d47740 container client-container: <nil>
STEP: delete the pod 01/17/23 15:57:35.544
Jan 17 15:57:35.556: INFO: Waiting for pod downwardapi-volume-83c2a8f1-d012-4059-9050-561917d47740 to disappear
Jan 17 15:57:35.558: INFO: Pod downwardapi-volume-83c2a8f1-d012-4059-9050-561917d47740 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 17 15:57:35.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-839" for this suite. 01/17/23 15:57:35.563
{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","completed":250,"skipped":4642,"failed":0}
------------------------------
• [4.157 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:57:31.412
    Jan 17 15:57:31.412: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename projected 01/17/23 15:57:31.413
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:57:31.454
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:57:31.456
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:67
    STEP: Creating a pod to test downward API volume plugin 01/17/23 15:57:31.458
    Jan 17 15:57:31.505: INFO: Waiting up to 5m0s for pod "downwardapi-volume-83c2a8f1-d012-4059-9050-561917d47740" in namespace "projected-839" to be "Succeeded or Failed"
    Jan 17 15:57:31.525: INFO: Pod "downwardapi-volume-83c2a8f1-d012-4059-9050-561917d47740": Phase="Pending", Reason="", readiness=false. Elapsed: 20.351492ms
    Jan 17 15:57:33.529: INFO: Pod "downwardapi-volume-83c2a8f1-d012-4059-9050-561917d47740": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024299344s
    Jan 17 15:57:35.530: INFO: Pod "downwardapi-volume-83c2a8f1-d012-4059-9050-561917d47740": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025007751s
    STEP: Saw pod success 01/17/23 15:57:35.53
    Jan 17 15:57:35.530: INFO: Pod "downwardapi-volume-83c2a8f1-d012-4059-9050-561917d47740" satisfied condition "Succeeded or Failed"
    Jan 17 15:57:35.533: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod downwardapi-volume-83c2a8f1-d012-4059-9050-561917d47740 container client-container: <nil>
    STEP: delete the pod 01/17/23 15:57:35.544
    Jan 17 15:57:35.556: INFO: Waiting for pod downwardapi-volume-83c2a8f1-d012-4059-9050-561917d47740 to disappear
    Jan 17 15:57:35.558: INFO: Pod downwardapi-volume-83c2a8f1-d012-4059-9050-561917d47740 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 17 15:57:35.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-839" for this suite. 01/17/23 15:57:35.563
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:57:35.569
Jan 17 15:57:35.570: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename podtemplate 01/17/23 15:57:35.57
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:57:35.597
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:57:35.601
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 01/17/23 15:57:35.603
W0117 15:57:35.613293      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "e2e-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "e2e-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "e2e-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "e2e-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Replace a pod template 01/17/23 15:57:35.613
Jan 17 15:57:35.634: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Jan 17 15:57:35.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-117" for this suite. 01/17/23 15:57:35.638
{"msg":"PASSED [sig-node] PodTemplates should replace a pod template [Conformance]","completed":251,"skipped":4655,"failed":0}
------------------------------
• [0.077 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:57:35.569
    Jan 17 15:57:35.570: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename podtemplate 01/17/23 15:57:35.57
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:57:35.597
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:57:35.601
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 01/17/23 15:57:35.603
    W0117 15:57:35.613293      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "e2e-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "e2e-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "e2e-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "e2e-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Replace a pod template 01/17/23 15:57:35.613
    Jan 17 15:57:35.634: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Jan 17 15:57:35.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-117" for this suite. 01/17/23 15:57:35.638
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:57:35.648
Jan 17 15:57:35.648: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename gc 01/17/23 15:57:35.649
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:57:35.675
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:57:35.679
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 01/17/23 15:57:35.681
W0117 15:57:35.690096      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Wait for the Deployment to create new ReplicaSet 01/17/23 15:57:35.69
STEP: delete the deployment 01/17/23 15:57:36.198
STEP: wait for all rs to be garbage collected 01/17/23 15:57:36.205
STEP: expected 0 rs, got 1 rs 01/17/23 15:57:36.212
STEP: expected 0 pods, got 2 pods 01/17/23 15:57:36.216
STEP: Gathering metrics 01/17/23 15:57:36.727
W0117 15:57:36.731581      22 metrics_grabber.go:110] Can't find any pods in namespace kube-system to grab metrics from
W0117 15:57:36.731599      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan 17 15:57:36.731: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan 17 15:57:36.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1470" for this suite. 01/17/23 15:57:36.735
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","completed":252,"skipped":4684,"failed":0}
------------------------------
• [1.094 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:57:35.648
    Jan 17 15:57:35.648: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename gc 01/17/23 15:57:35.649
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:57:35.675
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:57:35.679
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 01/17/23 15:57:35.681
    W0117 15:57:35.690096      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Wait for the Deployment to create new ReplicaSet 01/17/23 15:57:35.69
    STEP: delete the deployment 01/17/23 15:57:36.198
    STEP: wait for all rs to be garbage collected 01/17/23 15:57:36.205
    STEP: expected 0 rs, got 1 rs 01/17/23 15:57:36.212
    STEP: expected 0 pods, got 2 pods 01/17/23 15:57:36.216
    STEP: Gathering metrics 01/17/23 15:57:36.727
    W0117 15:57:36.731581      22 metrics_grabber.go:110] Can't find any pods in namespace kube-system to grab metrics from
    W0117 15:57:36.731599      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jan 17 15:57:36.731: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan 17 15:57:36.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-1470" for this suite. 01/17/23 15:57:36.735
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:57:36.742
Jan 17 15:57:36.743: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename var-expansion 01/17/23 15:57:36.743
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:57:36.77
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:57:36.775
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185
Jan 17 15:57:36.796: INFO: Waiting up to 2m0s for pod "var-expansion-c6b8b194-d14b-442e-87df-1686091c0586" in namespace "var-expansion-2869" to be "container 0 failed with reason CreateContainerConfigError"
Jan 17 15:57:36.800: INFO: Pod "var-expansion-c6b8b194-d14b-442e-87df-1686091c0586": Phase="Pending", Reason="", readiness=false. Elapsed: 3.838376ms
Jan 17 15:57:38.804: INFO: Pod "var-expansion-c6b8b194-d14b-442e-87df-1686091c0586": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007463757s
Jan 17 15:57:38.804: INFO: Pod "var-expansion-c6b8b194-d14b-442e-87df-1686091c0586" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Jan 17 15:57:38.804: INFO: Deleting pod "var-expansion-c6b8b194-d14b-442e-87df-1686091c0586" in namespace "var-expansion-2869"
Jan 17 15:57:38.811: INFO: Wait up to 5m0s for pod "var-expansion-c6b8b194-d14b-442e-87df-1686091c0586" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan 17 15:57:40.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2869" for this suite. 01/17/23 15:57:40.829
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","completed":253,"skipped":4693,"failed":0}
------------------------------
• [4.092 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:57:36.742
    Jan 17 15:57:36.743: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename var-expansion 01/17/23 15:57:36.743
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:57:36.77
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:57:36.775
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:185
    Jan 17 15:57:36.796: INFO: Waiting up to 2m0s for pod "var-expansion-c6b8b194-d14b-442e-87df-1686091c0586" in namespace "var-expansion-2869" to be "container 0 failed with reason CreateContainerConfigError"
    Jan 17 15:57:36.800: INFO: Pod "var-expansion-c6b8b194-d14b-442e-87df-1686091c0586": Phase="Pending", Reason="", readiness=false. Elapsed: 3.838376ms
    Jan 17 15:57:38.804: INFO: Pod "var-expansion-c6b8b194-d14b-442e-87df-1686091c0586": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007463757s
    Jan 17 15:57:38.804: INFO: Pod "var-expansion-c6b8b194-d14b-442e-87df-1686091c0586" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Jan 17 15:57:38.804: INFO: Deleting pod "var-expansion-c6b8b194-d14b-442e-87df-1686091c0586" in namespace "var-expansion-2869"
    Jan 17 15:57:38.811: INFO: Wait up to 5m0s for pod "var-expansion-c6b8b194-d14b-442e-87df-1686091c0586" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan 17 15:57:40.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-2869" for this suite. 01/17/23 15:57:40.829
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:57:40.835
Jan 17 15:57:40.835: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename daemonsets 01/17/23 15:57:40.836
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:57:40.863
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:57:40.865
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822
STEP: Creating simple DaemonSet "daemon-set" 01/17/23 15:57:40.9
STEP: Check that daemon pods launch on every node of the cluster. 01/17/23 15:57:40.909
Jan 17 15:57:40.925: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 15:57:40.925: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 15:57:40.925: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 15:57:40.934: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 17 15:57:40.934: INFO: Node ip-10-0-139-213.ec2.internal is running 0 daemon pod, expected 1
Jan 17 15:57:41.939: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 15:57:41.939: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 15:57:41.939: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 15:57:41.942: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 17 15:57:41.942: INFO: Node ip-10-0-139-213.ec2.internal is running 0 daemon pod, expected 1
Jan 17 15:57:42.941: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 15:57:42.941: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 15:57:42.941: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 15:57:42.947: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan 17 15:57:42.947: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: listing all DeamonSets 01/17/23 15:57:42.949
STEP: DeleteCollection of the DaemonSets 01/17/23 15:57:42.954
STEP: Verify that ReplicaSets have been deleted 01/17/23 15:57:42.964
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
Jan 17 15:57:42.974: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"121191"},"items":null}

Jan 17 15:57:42.980: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"121191"},"items":[{"metadata":{"name":"daemon-set-69mm5","generateName":"daemon-set-","namespace":"daemonsets-5344","uid":"20abe601-8cbf-49c7-9f33-9caa264214d0","resourceVersion":"121158","creationTimestamp":"2023-01-17T15:57:40Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"k8s.ovn.org/pod-networks":"{\"default\":{\"ip_addresses\":[\"10.128.2.183/23\"],\"mac_address\":\"0a:58:0a:80:02:b7\",\"gateway_ips\":[\"10.128.2.1\"],\"ip_address\":\"10.128.2.183/23\",\"gateway_ip\":\"10.128.2.1\"}}","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"ovn-kubernetes\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.128.2.183\"\n    ],\n    \"mac\": \"0a:58:0a:80:02:b7\",\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"ovn-kubernetes\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.128.2.183\"\n    ],\n    \"mac\": \"0a:58:0a:80:02:b7\",\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"b6aa2f3a-128a-4191-bc2d-88c5a28f2f93","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-17T15:57:40Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b6aa2f3a-128a-4191-bc2d-88c5a28f2f93\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"ip-10-0-135-246","operation":"Update","apiVersion":"v1","time":"2023-01-17T15:57:41Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-17T15:57:41Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.2.183\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-01-17T15:57:41Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-j6zd5","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-j6zd5","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-10-0-165-14.ec2.internal","securityContext":{"seLinuxOptions":{"level":"s0:c58,c37"}},"imagePullSecrets":[{"name":"default-dockercfg-bd6pk"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-10-0-165-14.ec2.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-17T15:57:40Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-17T15:57:41Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-17T15:57:41Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-17T15:57:40Z"}],"hostIP":"10.0.165.14","podIP":"10.128.2.183","podIPs":[{"ip":"10.128.2.183"}],"startTime":"2023-01-17T15:57:40Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-17T15:57:41Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://5804518d301732d246de349cd3f51c2b81777f4ab3ec889ce0120300ee567812","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-fqr9m","generateName":"daemon-set-","namespace":"daemonsets-5344","uid":"c4d5cc45-a579-4a2a-a709-db07645f55f0","resourceVersion":"121171","creationTimestamp":"2023-01-17T15:57:40Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"k8s.ovn.org/pod-networks":"{\"default\":{\"ip_addresses\":[\"10.129.2.147/23\"],\"mac_address\":\"0a:58:0a:81:02:93\",\"gateway_ips\":[\"10.129.2.1\"],\"ip_address\":\"10.129.2.147/23\",\"gateway_ip\":\"10.129.2.1\"}}","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"ovn-kubernetes\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.129.2.147\"\n    ],\n    \"mac\": \"0a:58:0a:81:02:93\",\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"ovn-kubernetes\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.129.2.147\"\n    ],\n    \"mac\": \"0a:58:0a:81:02:93\",\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"b6aa2f3a-128a-4191-bc2d-88c5a28f2f93","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"ip-10-0-135-246","operation":"Update","apiVersion":"v1","time":"2023-01-17T15:57:40Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}}},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-17T15:57:40Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b6aa2f3a-128a-4191-bc2d-88c5a28f2f93\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-01-17T15:57:41Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-17T15:57:42Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.129.2.147\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-7rsmg","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-7rsmg","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-10-0-139-213.ec2.internal","securityContext":{"seLinuxOptions":{"level":"s0:c58,c37"}},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-10-0-139-213.ec2.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-17T15:57:40Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-17T15:57:42Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-17T15:57:42Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-17T15:57:40Z"}],"hostIP":"10.0.139.213","podIP":"10.129.2.147","podIPs":[{"ip":"10.129.2.147"}],"startTime":"2023-01-17T15:57:40Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-17T15:57:41Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://ada3cd7af49fd29c1329c37539f20637c948ae344613b43fd59e8106c1898a8a","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-h65ch","generateName":"daemon-set-","namespace":"daemonsets-5344","uid":"ec1ca65f-918b-4343-843a-18d40cea29da","resourceVersion":"121181","creationTimestamp":"2023-01-17T15:57:40Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"k8s.ovn.org/pod-networks":"{\"default\":{\"ip_addresses\":[\"10.131.1.57/23\"],\"mac_address\":\"0a:58:0a:83:01:39\",\"gateway_ips\":[\"10.131.0.1\"],\"ip_address\":\"10.131.1.57/23\",\"gateway_ip\":\"10.131.0.1\"}}","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"ovn-kubernetes\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.131.1.57\"\n    ],\n    \"mac\": \"0a:58:0a:83:01:39\",\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"ovn-kubernetes\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.131.1.57\"\n    ],\n    \"mac\": \"0a:58:0a:83:01:39\",\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"b6aa2f3a-128a-4191-bc2d-88c5a28f2f93","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"ip-10-0-135-246","operation":"Update","apiVersion":"v1","time":"2023-01-17T15:57:40Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}}},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-17T15:57:40Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b6aa2f3a-128a-4191-bc2d-88c5a28f2f93\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-01-17T15:57:41Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-17T15:57:42Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.1.57\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-4gj2t","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-4gj2t","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-10-0-151-22.ec2.internal","securityContext":{"seLinuxOptions":{"level":"s0:c58,c37"}},"imagePullSecrets":[{"name":"default-dockercfg-bd6pk"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-10-0-151-22.ec2.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-17T15:57:40Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-17T15:57:42Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-17T15:57:42Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-17T15:57:40Z"}],"hostIP":"10.0.151.22","podIP":"10.131.1.57","podIPs":[{"ip":"10.131.1.57"}],"startTime":"2023-01-17T15:57:40Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-17T15:57:41Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://a212b9c27a35b09a25286b467522315d79e4b376d2727c49c2e6e346b3f42a5a","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan 17 15:57:42.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5344" for this suite. 01/17/23 15:57:43.002
{"msg":"PASSED [sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]","completed":254,"skipped":4707,"failed":0}
------------------------------
• [2.214 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:57:40.835
    Jan 17 15:57:40.835: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename daemonsets 01/17/23 15:57:40.836
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:57:40.863
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:57:40.865
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:822
    STEP: Creating simple DaemonSet "daemon-set" 01/17/23 15:57:40.9
    STEP: Check that daemon pods launch on every node of the cluster. 01/17/23 15:57:40.909
    Jan 17 15:57:40.925: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 15:57:40.925: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 15:57:40.925: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 15:57:40.934: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 17 15:57:40.934: INFO: Node ip-10-0-139-213.ec2.internal is running 0 daemon pod, expected 1
    Jan 17 15:57:41.939: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 15:57:41.939: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 15:57:41.939: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 15:57:41.942: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 17 15:57:41.942: INFO: Node ip-10-0-139-213.ec2.internal is running 0 daemon pod, expected 1
    Jan 17 15:57:42.941: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 15:57:42.941: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 15:57:42.941: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 15:57:42.947: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jan 17 15:57:42.947: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: listing all DeamonSets 01/17/23 15:57:42.949
    STEP: DeleteCollection of the DaemonSets 01/17/23 15:57:42.954
    STEP: Verify that ReplicaSets have been deleted 01/17/23 15:57:42.964
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    Jan 17 15:57:42.974: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"121191"},"items":null}

    Jan 17 15:57:42.980: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"121191"},"items":[{"metadata":{"name":"daemon-set-69mm5","generateName":"daemon-set-","namespace":"daemonsets-5344","uid":"20abe601-8cbf-49c7-9f33-9caa264214d0","resourceVersion":"121158","creationTimestamp":"2023-01-17T15:57:40Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"k8s.ovn.org/pod-networks":"{\"default\":{\"ip_addresses\":[\"10.128.2.183/23\"],\"mac_address\":\"0a:58:0a:80:02:b7\",\"gateway_ips\":[\"10.128.2.1\"],\"ip_address\":\"10.128.2.183/23\",\"gateway_ip\":\"10.128.2.1\"}}","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"ovn-kubernetes\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.128.2.183\"\n    ],\n    \"mac\": \"0a:58:0a:80:02:b7\",\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"ovn-kubernetes\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.128.2.183\"\n    ],\n    \"mac\": \"0a:58:0a:80:02:b7\",\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"b6aa2f3a-128a-4191-bc2d-88c5a28f2f93","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-17T15:57:40Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b6aa2f3a-128a-4191-bc2d-88c5a28f2f93\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"ip-10-0-135-246","operation":"Update","apiVersion":"v1","time":"2023-01-17T15:57:41Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-17T15:57:41Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.2.183\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-01-17T15:57:41Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-j6zd5","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-j6zd5","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-10-0-165-14.ec2.internal","securityContext":{"seLinuxOptions":{"level":"s0:c58,c37"}},"imagePullSecrets":[{"name":"default-dockercfg-bd6pk"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-10-0-165-14.ec2.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-17T15:57:40Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-17T15:57:41Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-17T15:57:41Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-17T15:57:40Z"}],"hostIP":"10.0.165.14","podIP":"10.128.2.183","podIPs":[{"ip":"10.128.2.183"}],"startTime":"2023-01-17T15:57:40Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-17T15:57:41Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://5804518d301732d246de349cd3f51c2b81777f4ab3ec889ce0120300ee567812","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-fqr9m","generateName":"daemon-set-","namespace":"daemonsets-5344","uid":"c4d5cc45-a579-4a2a-a709-db07645f55f0","resourceVersion":"121171","creationTimestamp":"2023-01-17T15:57:40Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"k8s.ovn.org/pod-networks":"{\"default\":{\"ip_addresses\":[\"10.129.2.147/23\"],\"mac_address\":\"0a:58:0a:81:02:93\",\"gateway_ips\":[\"10.129.2.1\"],\"ip_address\":\"10.129.2.147/23\",\"gateway_ip\":\"10.129.2.1\"}}","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"ovn-kubernetes\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.129.2.147\"\n    ],\n    \"mac\": \"0a:58:0a:81:02:93\",\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"ovn-kubernetes\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.129.2.147\"\n    ],\n    \"mac\": \"0a:58:0a:81:02:93\",\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"b6aa2f3a-128a-4191-bc2d-88c5a28f2f93","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"ip-10-0-135-246","operation":"Update","apiVersion":"v1","time":"2023-01-17T15:57:40Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}}},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-17T15:57:40Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b6aa2f3a-128a-4191-bc2d-88c5a28f2f93\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-01-17T15:57:41Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-17T15:57:42Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.129.2.147\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-7rsmg","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-7rsmg","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-10-0-139-213.ec2.internal","securityContext":{"seLinuxOptions":{"level":"s0:c58,c37"}},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-10-0-139-213.ec2.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-17T15:57:40Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-17T15:57:42Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-17T15:57:42Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-17T15:57:40Z"}],"hostIP":"10.0.139.213","podIP":"10.129.2.147","podIPs":[{"ip":"10.129.2.147"}],"startTime":"2023-01-17T15:57:40Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-17T15:57:41Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://ada3cd7af49fd29c1329c37539f20637c948ae344613b43fd59e8106c1898a8a","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-h65ch","generateName":"daemon-set-","namespace":"daemonsets-5344","uid":"ec1ca65f-918b-4343-843a-18d40cea29da","resourceVersion":"121181","creationTimestamp":"2023-01-17T15:57:40Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"k8s.ovn.org/pod-networks":"{\"default\":{\"ip_addresses\":[\"10.131.1.57/23\"],\"mac_address\":\"0a:58:0a:83:01:39\",\"gateway_ips\":[\"10.131.0.1\"],\"ip_address\":\"10.131.1.57/23\",\"gateway_ip\":\"10.131.0.1\"}}","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"ovn-kubernetes\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.131.1.57\"\n    ],\n    \"mac\": \"0a:58:0a:83:01:39\",\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"ovn-kubernetes\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.131.1.57\"\n    ],\n    \"mac\": \"0a:58:0a:83:01:39\",\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"b6aa2f3a-128a-4191-bc2d-88c5a28f2f93","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"ip-10-0-135-246","operation":"Update","apiVersion":"v1","time":"2023-01-17T15:57:40Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}}},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-17T15:57:40Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b6aa2f3a-128a-4191-bc2d-88c5a28f2f93\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-01-17T15:57:41Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-17T15:57:42Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.1.57\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-4gj2t","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-4gj2t","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-10-0-151-22.ec2.internal","securityContext":{"seLinuxOptions":{"level":"s0:c58,c37"}},"imagePullSecrets":[{"name":"default-dockercfg-bd6pk"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-10-0-151-22.ec2.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-17T15:57:40Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-17T15:57:42Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-17T15:57:42Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-17T15:57:40Z"}],"hostIP":"10.0.151.22","podIP":"10.131.1.57","podIPs":[{"ip":"10.131.1.57"}],"startTime":"2023-01-17T15:57:40Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-17T15:57:41Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://a212b9c27a35b09a25286b467522315d79e4b376d2727c49c2e6e346b3f42a5a","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan 17 15:57:42.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-5344" for this suite. 01/17/23 15:57:43.002
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:57:43.049
Jan 17 15:57:43.049: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename downward-api 01/17/23 15:57:43.05
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:57:43.089
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:57:43.091
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165
STEP: Creating a pod to test downward api env vars 01/17/23 15:57:43.093
Jan 17 15:57:43.110: INFO: Waiting up to 5m0s for pod "downward-api-ef567e6f-4efa-4132-82cc-950655e2153f" in namespace "downward-api-9594" to be "Succeeded or Failed"
Jan 17 15:57:43.115: INFO: Pod "downward-api-ef567e6f-4efa-4132-82cc-950655e2153f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.327479ms
Jan 17 15:57:45.121: INFO: Pod "downward-api-ef567e6f-4efa-4132-82cc-950655e2153f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010672616s
Jan 17 15:57:47.118: INFO: Pod "downward-api-ef567e6f-4efa-4132-82cc-950655e2153f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007934611s
STEP: Saw pod success 01/17/23 15:57:47.118
Jan 17 15:57:47.118: INFO: Pod "downward-api-ef567e6f-4efa-4132-82cc-950655e2153f" satisfied condition "Succeeded or Failed"
Jan 17 15:57:47.122: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod downward-api-ef567e6f-4efa-4132-82cc-950655e2153f container dapi-container: <nil>
STEP: delete the pod 01/17/23 15:57:47.13
Jan 17 15:57:47.142: INFO: Waiting for pod downward-api-ef567e6f-4efa-4132-82cc-950655e2153f to disappear
Jan 17 15:57:47.146: INFO: Pod downward-api-ef567e6f-4efa-4132-82cc-950655e2153f no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Jan 17 15:57:47.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9594" for this suite. 01/17/23 15:57:47.151
{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","completed":255,"skipped":4711,"failed":0}
------------------------------
• [4.108 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:57:43.049
    Jan 17 15:57:43.049: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename downward-api 01/17/23 15:57:43.05
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:57:43.089
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:57:43.091
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:165
    STEP: Creating a pod to test downward api env vars 01/17/23 15:57:43.093
    Jan 17 15:57:43.110: INFO: Waiting up to 5m0s for pod "downward-api-ef567e6f-4efa-4132-82cc-950655e2153f" in namespace "downward-api-9594" to be "Succeeded or Failed"
    Jan 17 15:57:43.115: INFO: Pod "downward-api-ef567e6f-4efa-4132-82cc-950655e2153f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.327479ms
    Jan 17 15:57:45.121: INFO: Pod "downward-api-ef567e6f-4efa-4132-82cc-950655e2153f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010672616s
    Jan 17 15:57:47.118: INFO: Pod "downward-api-ef567e6f-4efa-4132-82cc-950655e2153f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007934611s
    STEP: Saw pod success 01/17/23 15:57:47.118
    Jan 17 15:57:47.118: INFO: Pod "downward-api-ef567e6f-4efa-4132-82cc-950655e2153f" satisfied condition "Succeeded or Failed"
    Jan 17 15:57:47.122: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod downward-api-ef567e6f-4efa-4132-82cc-950655e2153f container dapi-container: <nil>
    STEP: delete the pod 01/17/23 15:57:47.13
    Jan 17 15:57:47.142: INFO: Waiting for pod downward-api-ef567e6f-4efa-4132-82cc-950655e2153f to disappear
    Jan 17 15:57:47.146: INFO: Pod downward-api-ef567e6f-4efa-4132-82cc-950655e2153f no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Jan 17 15:57:47.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-9594" for this suite. 01/17/23 15:57:47.151
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:57:47.157
Jan 17 15:57:47.157: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename custom-resource-definition 01/17/23 15:57:47.158
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:57:47.204
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:57:47.207
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
Jan 17 15:57:47.209: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 17 15:57:50.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6295" for this suite. 01/17/23 15:57:50.32
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","completed":256,"skipped":4715,"failed":0}
------------------------------
• [3.170 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:57:47.157
    Jan 17 15:57:47.157: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename custom-resource-definition 01/17/23 15:57:47.158
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:57:47.204
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:57:47.207
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    Jan 17 15:57:47.209: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 17 15:57:50.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-6295" for this suite. 01/17/23 15:57:50.32
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:57:50.327
Jan 17 15:57:50.327: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename kubelet-test 01/17/23 15:57:50.328
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:57:50.358
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:57:50.361
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
Jan 17 15:57:50.389: INFO: Waiting up to 5m0s for pod "busybox-scheduling-def1f64a-9228-42b3-b4f9-686f49c487ca" in namespace "kubelet-test-8105" to be "running and ready"
Jan 17 15:57:50.393: INFO: Pod "busybox-scheduling-def1f64a-9228-42b3-b4f9-686f49c487ca": Phase="Pending", Reason="", readiness=false. Elapsed: 4.332075ms
Jan 17 15:57:50.393: INFO: The phase of Pod busybox-scheduling-def1f64a-9228-42b3-b4f9-686f49c487ca is Pending, waiting for it to be Running (with Ready = true)
Jan 17 15:57:52.398: INFO: Pod "busybox-scheduling-def1f64a-9228-42b3-b4f9-686f49c487ca": Phase="Running", Reason="", readiness=true. Elapsed: 2.00881805s
Jan 17 15:57:52.398: INFO: The phase of Pod busybox-scheduling-def1f64a-9228-42b3-b4f9-686f49c487ca is Running (Ready = true)
Jan 17 15:57:52.398: INFO: Pod "busybox-scheduling-def1f64a-9228-42b3-b4f9-686f49c487ca" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Jan 17 15:57:52.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8105" for this suite. 01/17/23 15:57:52.421
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","completed":257,"skipped":4717,"failed":0}
------------------------------
• [2.102 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:57:50.327
    Jan 17 15:57:50.327: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename kubelet-test 01/17/23 15:57:50.328
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:57:50.358
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:57:50.361
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    Jan 17 15:57:50.389: INFO: Waiting up to 5m0s for pod "busybox-scheduling-def1f64a-9228-42b3-b4f9-686f49c487ca" in namespace "kubelet-test-8105" to be "running and ready"
    Jan 17 15:57:50.393: INFO: Pod "busybox-scheduling-def1f64a-9228-42b3-b4f9-686f49c487ca": Phase="Pending", Reason="", readiness=false. Elapsed: 4.332075ms
    Jan 17 15:57:50.393: INFO: The phase of Pod busybox-scheduling-def1f64a-9228-42b3-b4f9-686f49c487ca is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 15:57:52.398: INFO: Pod "busybox-scheduling-def1f64a-9228-42b3-b4f9-686f49c487ca": Phase="Running", Reason="", readiness=true. Elapsed: 2.00881805s
    Jan 17 15:57:52.398: INFO: The phase of Pod busybox-scheduling-def1f64a-9228-42b3-b4f9-686f49c487ca is Running (Ready = true)
    Jan 17 15:57:52.398: INFO: Pod "busybox-scheduling-def1f64a-9228-42b3-b4f9-686f49c487ca" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Jan 17 15:57:52.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-8105" for this suite. 01/17/23 15:57:52.421
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:130
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:57:52.429
Jan 17 15:57:52.429: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename container-lifecycle-hook 01/17/23 15:57:52.43
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:57:52.453
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:57:52.456
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 01/17/23 15:57:52.465
Jan 17 15:57:52.484: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-5579" to be "running and ready"
Jan 17 15:57:52.487: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.645112ms
Jan 17 15:57:52.487: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 17 15:57:54.491: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.006310681s
Jan 17 15:57:54.491: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan 17 15:57:54.491: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:130
STEP: create the pod with lifecycle hook 01/17/23 15:57:54.494
Jan 17 15:57:54.506: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-5579" to be "running and ready"
Jan 17 15:57:54.509: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.91883ms
Jan 17 15:57:54.509: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 17 15:57:56.513: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.00706177s
Jan 17 15:57:56.513: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
Jan 17 15:57:56.513: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 01/17/23 15:57:56.516
STEP: delete the pod with lifecycle hook 01/17/23 15:57:56.522
Jan 17 15:57:56.528: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 17 15:57:56.531: INFO: Pod pod-with-poststart-http-hook still exists
Jan 17 15:57:58.532: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 17 15:57:58.536: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Jan 17 15:57:58.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5579" for this suite. 01/17/23 15:57:58.54
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","completed":258,"skipped":4717,"failed":0}
------------------------------
• [SLOW TEST] [6.117 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:57:52.429
    Jan 17 15:57:52.429: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/17/23 15:57:52.43
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:57:52.453
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:57:52.456
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 01/17/23 15:57:52.465
    Jan 17 15:57:52.484: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-5579" to be "running and ready"
    Jan 17 15:57:52.487: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.645112ms
    Jan 17 15:57:52.487: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 15:57:54.491: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.006310681s
    Jan 17 15:57:54.491: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan 17 15:57:54.491: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:130
    STEP: create the pod with lifecycle hook 01/17/23 15:57:54.494
    Jan 17 15:57:54.506: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-5579" to be "running and ready"
    Jan 17 15:57:54.509: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.91883ms
    Jan 17 15:57:54.509: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 15:57:56.513: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.00706177s
    Jan 17 15:57:56.513: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    Jan 17 15:57:56.513: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 01/17/23 15:57:56.516
    STEP: delete the pod with lifecycle hook 01/17/23 15:57:56.522
    Jan 17 15:57:56.528: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jan 17 15:57:56.531: INFO: Pod pod-with-poststart-http-hook still exists
    Jan 17 15:57:58.532: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jan 17 15:57:58.536: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Jan 17 15:57:58.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-5579" for this suite. 01/17/23 15:57:58.54
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:57:58.547
Jan 17 15:57:58.547: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename emptydir 01/17/23 15:57:58.547
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:57:58.57
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:57:58.573
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186
STEP: Creating a pod to test emptydir 0777 on node default medium 01/17/23 15:57:58.575
Jan 17 15:57:58.604: INFO: Waiting up to 5m0s for pod "pod-362ea1c6-ba20-4511-b33a-432da30d88a1" in namespace "emptydir-3001" to be "Succeeded or Failed"
Jan 17 15:57:58.608: INFO: Pod "pod-362ea1c6-ba20-4511-b33a-432da30d88a1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.246975ms
Jan 17 15:58:00.612: INFO: Pod "pod-362ea1c6-ba20-4511-b33a-432da30d88a1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008708972s
Jan 17 15:58:02.613: INFO: Pod "pod-362ea1c6-ba20-4511-b33a-432da30d88a1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009176209s
STEP: Saw pod success 01/17/23 15:58:02.613
Jan 17 15:58:02.613: INFO: Pod "pod-362ea1c6-ba20-4511-b33a-432da30d88a1" satisfied condition "Succeeded or Failed"
Jan 17 15:58:02.616: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-362ea1c6-ba20-4511-b33a-432da30d88a1 container test-container: <nil>
STEP: delete the pod 01/17/23 15:58:02.622
Jan 17 15:58:02.635: INFO: Waiting for pod pod-362ea1c6-ba20-4511-b33a-432da30d88a1 to disappear
Jan 17 15:58:02.639: INFO: Pod pod-362ea1c6-ba20-4511-b33a-432da30d88a1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 17 15:58:02.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3001" for this suite. 01/17/23 15:58:02.643
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":259,"skipped":4718,"failed":0}
------------------------------
• [4.106 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:57:58.547
    Jan 17 15:57:58.547: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename emptydir 01/17/23 15:57:58.547
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:57:58.57
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:57:58.573
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:186
    STEP: Creating a pod to test emptydir 0777 on node default medium 01/17/23 15:57:58.575
    Jan 17 15:57:58.604: INFO: Waiting up to 5m0s for pod "pod-362ea1c6-ba20-4511-b33a-432da30d88a1" in namespace "emptydir-3001" to be "Succeeded or Failed"
    Jan 17 15:57:58.608: INFO: Pod "pod-362ea1c6-ba20-4511-b33a-432da30d88a1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.246975ms
    Jan 17 15:58:00.612: INFO: Pod "pod-362ea1c6-ba20-4511-b33a-432da30d88a1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008708972s
    Jan 17 15:58:02.613: INFO: Pod "pod-362ea1c6-ba20-4511-b33a-432da30d88a1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009176209s
    STEP: Saw pod success 01/17/23 15:58:02.613
    Jan 17 15:58:02.613: INFO: Pod "pod-362ea1c6-ba20-4511-b33a-432da30d88a1" satisfied condition "Succeeded or Failed"
    Jan 17 15:58:02.616: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-362ea1c6-ba20-4511-b33a-432da30d88a1 container test-container: <nil>
    STEP: delete the pod 01/17/23 15:58:02.622
    Jan 17 15:58:02.635: INFO: Waiting for pod pod-362ea1c6-ba20-4511-b33a-432da30d88a1 to disappear
    Jan 17 15:58:02.639: INFO: Pod pod-362ea1c6-ba20-4511-b33a-432da30d88a1 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 17 15:58:02.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-3001" for this suite. 01/17/23 15:58:02.643
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:58:02.653
Jan 17 15:58:02.653: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename downward-api 01/17/23 15:58:02.653
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:58:02.683
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:58:02.689
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129
STEP: Creating the pod 01/17/23 15:58:02.694
Jan 17 15:58:02.715: INFO: Waiting up to 5m0s for pod "labelsupdate38136029-2038-4a5f-8695-756c23490fd9" in namespace "downward-api-2615" to be "running and ready"
Jan 17 15:58:02.720: INFO: Pod "labelsupdate38136029-2038-4a5f-8695-756c23490fd9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.960753ms
Jan 17 15:58:02.720: INFO: The phase of Pod labelsupdate38136029-2038-4a5f-8695-756c23490fd9 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 15:58:04.724: INFO: Pod "labelsupdate38136029-2038-4a5f-8695-756c23490fd9": Phase="Running", Reason="", readiness=true. Elapsed: 2.008892895s
Jan 17 15:58:04.724: INFO: The phase of Pod labelsupdate38136029-2038-4a5f-8695-756c23490fd9 is Running (Ready = true)
Jan 17 15:58:04.724: INFO: Pod "labelsupdate38136029-2038-4a5f-8695-756c23490fd9" satisfied condition "running and ready"
Jan 17 15:58:05.251: INFO: Successfully updated pod "labelsupdate38136029-2038-4a5f-8695-756c23490fd9"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 17 15:58:09.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2615" for this suite. 01/17/23 15:58:09.281
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","completed":260,"skipped":4729,"failed":0}
------------------------------
• [SLOW TEST] [6.635 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:58:02.653
    Jan 17 15:58:02.653: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename downward-api 01/17/23 15:58:02.653
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:58:02.683
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:58:02.689
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:129
    STEP: Creating the pod 01/17/23 15:58:02.694
    Jan 17 15:58:02.715: INFO: Waiting up to 5m0s for pod "labelsupdate38136029-2038-4a5f-8695-756c23490fd9" in namespace "downward-api-2615" to be "running and ready"
    Jan 17 15:58:02.720: INFO: Pod "labelsupdate38136029-2038-4a5f-8695-756c23490fd9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.960753ms
    Jan 17 15:58:02.720: INFO: The phase of Pod labelsupdate38136029-2038-4a5f-8695-756c23490fd9 is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 15:58:04.724: INFO: Pod "labelsupdate38136029-2038-4a5f-8695-756c23490fd9": Phase="Running", Reason="", readiness=true. Elapsed: 2.008892895s
    Jan 17 15:58:04.724: INFO: The phase of Pod labelsupdate38136029-2038-4a5f-8695-756c23490fd9 is Running (Ready = true)
    Jan 17 15:58:04.724: INFO: Pod "labelsupdate38136029-2038-4a5f-8695-756c23490fd9" satisfied condition "running and ready"
    Jan 17 15:58:05.251: INFO: Successfully updated pod "labelsupdate38136029-2038-4a5f-8695-756c23490fd9"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 17 15:58:09.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-2615" for this suite. 01/17/23 15:58:09.281
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:58:09.288
Jan 17 15:58:09.288: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename emptydir 01/17/23 15:58:09.289
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:58:09.327
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:58:09.329
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196
STEP: Creating a pod to test emptydir 0644 on node default medium 01/17/23 15:58:09.332
Jan 17 15:58:09.378: INFO: Waiting up to 5m0s for pod "pod-651e2b85-cb7e-406b-af21-ba6fff5da284" in namespace "emptydir-6790" to be "Succeeded or Failed"
Jan 17 15:58:09.388: INFO: Pod "pod-651e2b85-cb7e-406b-af21-ba6fff5da284": Phase="Pending", Reason="", readiness=false. Elapsed: 9.556668ms
Jan 17 15:58:11.391: INFO: Pod "pod-651e2b85-cb7e-406b-af21-ba6fff5da284": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013266137s
Jan 17 15:58:13.392: INFO: Pod "pod-651e2b85-cb7e-406b-af21-ba6fff5da284": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014056632s
STEP: Saw pod success 01/17/23 15:58:13.392
Jan 17 15:58:13.392: INFO: Pod "pod-651e2b85-cb7e-406b-af21-ba6fff5da284" satisfied condition "Succeeded or Failed"
Jan 17 15:58:13.395: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-651e2b85-cb7e-406b-af21-ba6fff5da284 container test-container: <nil>
STEP: delete the pod 01/17/23 15:58:13.401
Jan 17 15:58:13.413: INFO: Waiting for pod pod-651e2b85-cb7e-406b-af21-ba6fff5da284 to disappear
Jan 17 15:58:13.416: INFO: Pod pod-651e2b85-cb7e-406b-af21-ba6fff5da284 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 17 15:58:13.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6790" for this suite. 01/17/23 15:58:13.42
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":261,"skipped":4747,"failed":0}
------------------------------
• [4.137 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:58:09.288
    Jan 17 15:58:09.288: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename emptydir 01/17/23 15:58:09.289
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:58:09.327
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:58:09.329
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:196
    STEP: Creating a pod to test emptydir 0644 on node default medium 01/17/23 15:58:09.332
    Jan 17 15:58:09.378: INFO: Waiting up to 5m0s for pod "pod-651e2b85-cb7e-406b-af21-ba6fff5da284" in namespace "emptydir-6790" to be "Succeeded or Failed"
    Jan 17 15:58:09.388: INFO: Pod "pod-651e2b85-cb7e-406b-af21-ba6fff5da284": Phase="Pending", Reason="", readiness=false. Elapsed: 9.556668ms
    Jan 17 15:58:11.391: INFO: Pod "pod-651e2b85-cb7e-406b-af21-ba6fff5da284": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013266137s
    Jan 17 15:58:13.392: INFO: Pod "pod-651e2b85-cb7e-406b-af21-ba6fff5da284": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014056632s
    STEP: Saw pod success 01/17/23 15:58:13.392
    Jan 17 15:58:13.392: INFO: Pod "pod-651e2b85-cb7e-406b-af21-ba6fff5da284" satisfied condition "Succeeded or Failed"
    Jan 17 15:58:13.395: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-651e2b85-cb7e-406b-af21-ba6fff5da284 container test-container: <nil>
    STEP: delete the pod 01/17/23 15:58:13.401
    Jan 17 15:58:13.413: INFO: Waiting for pod pod-651e2b85-cb7e-406b-af21-ba6fff5da284 to disappear
    Jan 17 15:58:13.416: INFO: Pod pod-651e2b85-cb7e-406b-af21-ba6fff5da284 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 17 15:58:13.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-6790" for this suite. 01/17/23 15:58:13.42
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:58:13.427
Jan 17 15:58:13.427: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename init-container 01/17/23 15:58:13.427
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:58:13.456
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:58:13.459
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254
STEP: creating the pod 01/17/23 15:58:13.461
Jan 17 15:58:13.461: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Jan 17 15:58:16.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6113" for this suite. 01/17/23 15:58:16.555
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","completed":262,"skipped":4803,"failed":0}
------------------------------
• [3.134 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:58:13.427
    Jan 17 15:58:13.427: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename init-container 01/17/23 15:58:13.427
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:58:13.456
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:58:13.459
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:254
    STEP: creating the pod 01/17/23 15:58:13.461
    Jan 17 15:58:13.461: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan 17 15:58:16.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-6113" for this suite. 01/17/23 15:58:16.555
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:58:16.562
Jan 17 15:58:16.562: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename secrets 01/17/23 15:58:16.563
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:58:16.59
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:58:16.595
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56
STEP: Creating secret with name secret-test-66991774-ca74-4bff-8576-1fb2d8833c15 01/17/23 15:58:16.6
STEP: Creating a pod to test consume secrets 01/17/23 15:58:16.608
Jan 17 15:58:16.633: INFO: Waiting up to 5m0s for pod "pod-secrets-7f0d0b08-9989-4605-b8c6-da7fb1cd0692" in namespace "secrets-4075" to be "Succeeded or Failed"
Jan 17 15:58:16.642: INFO: Pod "pod-secrets-7f0d0b08-9989-4605-b8c6-da7fb1cd0692": Phase="Pending", Reason="", readiness=false. Elapsed: 8.670236ms
Jan 17 15:58:18.646: INFO: Pod "pod-secrets-7f0d0b08-9989-4605-b8c6-da7fb1cd0692": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012893367s
Jan 17 15:58:20.645: INFO: Pod "pod-secrets-7f0d0b08-9989-4605-b8c6-da7fb1cd0692": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012394433s
STEP: Saw pod success 01/17/23 15:58:20.645
Jan 17 15:58:20.646: INFO: Pod "pod-secrets-7f0d0b08-9989-4605-b8c6-da7fb1cd0692" satisfied condition "Succeeded or Failed"
Jan 17 15:58:20.649: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-secrets-7f0d0b08-9989-4605-b8c6-da7fb1cd0692 container secret-volume-test: <nil>
STEP: delete the pod 01/17/23 15:58:20.667
Jan 17 15:58:20.681: INFO: Waiting for pod pod-secrets-7f0d0b08-9989-4605-b8c6-da7fb1cd0692 to disappear
Jan 17 15:58:20.684: INFO: Pod pod-secrets-7f0d0b08-9989-4605-b8c6-da7fb1cd0692 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan 17 15:58:20.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4075" for this suite. 01/17/23 15:58:20.688
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":263,"skipped":4830,"failed":0}
------------------------------
• [4.131 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:58:16.562
    Jan 17 15:58:16.562: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename secrets 01/17/23 15:58:16.563
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:58:16.59
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:58:16.595
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:56
    STEP: Creating secret with name secret-test-66991774-ca74-4bff-8576-1fb2d8833c15 01/17/23 15:58:16.6
    STEP: Creating a pod to test consume secrets 01/17/23 15:58:16.608
    Jan 17 15:58:16.633: INFO: Waiting up to 5m0s for pod "pod-secrets-7f0d0b08-9989-4605-b8c6-da7fb1cd0692" in namespace "secrets-4075" to be "Succeeded or Failed"
    Jan 17 15:58:16.642: INFO: Pod "pod-secrets-7f0d0b08-9989-4605-b8c6-da7fb1cd0692": Phase="Pending", Reason="", readiness=false. Elapsed: 8.670236ms
    Jan 17 15:58:18.646: INFO: Pod "pod-secrets-7f0d0b08-9989-4605-b8c6-da7fb1cd0692": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012893367s
    Jan 17 15:58:20.645: INFO: Pod "pod-secrets-7f0d0b08-9989-4605-b8c6-da7fb1cd0692": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012394433s
    STEP: Saw pod success 01/17/23 15:58:20.645
    Jan 17 15:58:20.646: INFO: Pod "pod-secrets-7f0d0b08-9989-4605-b8c6-da7fb1cd0692" satisfied condition "Succeeded or Failed"
    Jan 17 15:58:20.649: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-secrets-7f0d0b08-9989-4605-b8c6-da7fb1cd0692 container secret-volume-test: <nil>
    STEP: delete the pod 01/17/23 15:58:20.667
    Jan 17 15:58:20.681: INFO: Waiting for pod pod-secrets-7f0d0b08-9989-4605-b8c6-da7fb1cd0692 to disappear
    Jan 17 15:58:20.684: INFO: Pod pod-secrets-7f0d0b08-9989-4605-b8c6-da7fb1cd0692 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan 17 15:58:20.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-4075" for this suite. 01/17/23 15:58:20.688
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:58:20.694
Jan 17 15:58:20.694: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename pod-network-test 01/17/23 15:58:20.695
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:58:20.719
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:58:20.723
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-5329 01/17/23 15:58:20.725
STEP: creating a selector 01/17/23 15:58:20.726
STEP: Creating the service pods in kubernetes 01/17/23 15:58:20.726
Jan 17 15:58:20.726: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 17 15:58:20.829: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-5329" to be "running and ready"
Jan 17 15:58:20.833: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.919456ms
Jan 17 15:58:20.833: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 15:58:22.837: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.007404205s
Jan 17 15:58:22.837: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 17 15:58:24.838: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.008420301s
Jan 17 15:58:24.838: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 17 15:58:26.838: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.008364242s
Jan 17 15:58:26.838: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 17 15:58:28.837: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.007626822s
Jan 17 15:58:28.837: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 17 15:58:30.837: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.008059729s
Jan 17 15:58:30.837: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 17 15:58:32.839: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.009536182s
Jan 17 15:58:32.839: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan 17 15:58:32.839: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan 17 15:58:32.842: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-5329" to be "running and ready"
Jan 17 15:58:32.845: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.626791ms
Jan 17 15:58:32.845: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan 17 15:58:32.845: INFO: Pod "netserver-1" satisfied condition "running and ready"
Jan 17 15:58:32.848: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-5329" to be "running and ready"
Jan 17 15:58:32.851: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.902615ms
Jan 17 15:58:32.851: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Jan 17 15:58:32.851: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 01/17/23 15:58:32.853
Jan 17 15:58:32.879: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-5329" to be "running"
Jan 17 15:58:32.882: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.636769ms
Jan 17 15:58:34.887: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008081307s
Jan 17 15:58:34.887: INFO: Pod "test-container-pod" satisfied condition "running"
Jan 17 15:58:34.890: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-5329" to be "running"
Jan 17 15:58:34.893: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.946189ms
Jan 17 15:58:34.893: INFO: Pod "host-test-container-pod" satisfied condition "running"
Jan 17 15:58:34.896: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Jan 17 15:58:34.896: INFO: Going to poll 10.129.2.148 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Jan 17 15:58:34.898: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.129.2.148 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5329 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 15:58:34.898: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
Jan 17 15:58:34.899: INFO: ExecWithOptions: Clientset creation
Jan 17 15:58:34.899: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-5329/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.129.2.148+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 17 15:58:36.023: INFO: Found all 1 expected endpoints: [netserver-0]
Jan 17 15:58:36.023: INFO: Going to poll 10.131.1.67 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Jan 17 15:58:36.027: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.131.1.67 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5329 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 15:58:36.027: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
Jan 17 15:58:36.027: INFO: ExecWithOptions: Clientset creation
Jan 17 15:58:36.027: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-5329/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.131.1.67+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 17 15:58:37.130: INFO: Found all 1 expected endpoints: [netserver-1]
Jan 17 15:58:37.130: INFO: Going to poll 10.128.2.184 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Jan 17 15:58:37.134: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.128.2.184 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5329 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 15:58:37.134: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
Jan 17 15:58:37.134: INFO: ExecWithOptions: Clientset creation
Jan 17 15:58:37.134: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-5329/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.128.2.184+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 17 15:58:38.245: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Jan 17 15:58:38.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5329" for this suite. 01/17/23 15:58:38.25
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","completed":264,"skipped":4852,"failed":0}
------------------------------
• [SLOW TEST] [17.561 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:58:20.694
    Jan 17 15:58:20.694: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename pod-network-test 01/17/23 15:58:20.695
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:58:20.719
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:58:20.723
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-5329 01/17/23 15:58:20.725
    STEP: creating a selector 01/17/23 15:58:20.726
    STEP: Creating the service pods in kubernetes 01/17/23 15:58:20.726
    Jan 17 15:58:20.726: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan 17 15:58:20.829: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-5329" to be "running and ready"
    Jan 17 15:58:20.833: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.919456ms
    Jan 17 15:58:20.833: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 15:58:22.837: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.007404205s
    Jan 17 15:58:22.837: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 17 15:58:24.838: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.008420301s
    Jan 17 15:58:24.838: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 17 15:58:26.838: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.008364242s
    Jan 17 15:58:26.838: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 17 15:58:28.837: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.007626822s
    Jan 17 15:58:28.837: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 17 15:58:30.837: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.008059729s
    Jan 17 15:58:30.837: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 17 15:58:32.839: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.009536182s
    Jan 17 15:58:32.839: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan 17 15:58:32.839: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan 17 15:58:32.842: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-5329" to be "running and ready"
    Jan 17 15:58:32.845: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.626791ms
    Jan 17 15:58:32.845: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan 17 15:58:32.845: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Jan 17 15:58:32.848: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-5329" to be "running and ready"
    Jan 17 15:58:32.851: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.902615ms
    Jan 17 15:58:32.851: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Jan 17 15:58:32.851: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 01/17/23 15:58:32.853
    Jan 17 15:58:32.879: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-5329" to be "running"
    Jan 17 15:58:32.882: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.636769ms
    Jan 17 15:58:34.887: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008081307s
    Jan 17 15:58:34.887: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan 17 15:58:34.890: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-5329" to be "running"
    Jan 17 15:58:34.893: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.946189ms
    Jan 17 15:58:34.893: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Jan 17 15:58:34.896: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Jan 17 15:58:34.896: INFO: Going to poll 10.129.2.148 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Jan 17 15:58:34.898: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.129.2.148 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5329 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 17 15:58:34.898: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    Jan 17 15:58:34.899: INFO: ExecWithOptions: Clientset creation
    Jan 17 15:58:34.899: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-5329/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.129.2.148+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 17 15:58:36.023: INFO: Found all 1 expected endpoints: [netserver-0]
    Jan 17 15:58:36.023: INFO: Going to poll 10.131.1.67 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Jan 17 15:58:36.027: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.131.1.67 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5329 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 17 15:58:36.027: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    Jan 17 15:58:36.027: INFO: ExecWithOptions: Clientset creation
    Jan 17 15:58:36.027: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-5329/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.131.1.67+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 17 15:58:37.130: INFO: Found all 1 expected endpoints: [netserver-1]
    Jan 17 15:58:37.130: INFO: Going to poll 10.128.2.184 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Jan 17 15:58:37.134: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.128.2.184 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5329 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 17 15:58:37.134: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    Jan 17 15:58:37.134: INFO: ExecWithOptions: Clientset creation
    Jan 17 15:58:37.134: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-5329/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.128.2.184+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 17 15:58:38.245: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Jan 17 15:58:38.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-5329" for this suite. 01/17/23 15:58:38.25
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:58:38.256
Jan 17 15:58:38.256: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename configmap 01/17/23 15:58:38.257
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:58:38.295
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:58:38.297
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92
STEP: Creating configMap configmap-4246/configmap-test-d25cf5a6-ce8c-4e01-a572-49446a7d403b 01/17/23 15:58:38.299
STEP: Creating a pod to test consume configMaps 01/17/23 15:58:38.306
Jan 17 15:58:38.330: INFO: Waiting up to 5m0s for pod "pod-configmaps-e7917eb8-e588-4fd8-a5aa-1c5ed35905b3" in namespace "configmap-4246" to be "Succeeded or Failed"
Jan 17 15:58:38.335: INFO: Pod "pod-configmaps-e7917eb8-e588-4fd8-a5aa-1c5ed35905b3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.475215ms
Jan 17 15:58:40.340: INFO: Pod "pod-configmaps-e7917eb8-e588-4fd8-a5aa-1c5ed35905b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009775644s
Jan 17 15:58:42.340: INFO: Pod "pod-configmaps-e7917eb8-e588-4fd8-a5aa-1c5ed35905b3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009366149s
STEP: Saw pod success 01/17/23 15:58:42.34
Jan 17 15:58:42.340: INFO: Pod "pod-configmaps-e7917eb8-e588-4fd8-a5aa-1c5ed35905b3" satisfied condition "Succeeded or Failed"
Jan 17 15:58:42.343: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-configmaps-e7917eb8-e588-4fd8-a5aa-1c5ed35905b3 container env-test: <nil>
STEP: delete the pod 01/17/23 15:58:42.35
Jan 17 15:58:42.365: INFO: Waiting for pod pod-configmaps-e7917eb8-e588-4fd8-a5aa-1c5ed35905b3 to disappear
Jan 17 15:58:42.375: INFO: Pod pod-configmaps-e7917eb8-e588-4fd8-a5aa-1c5ed35905b3 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Jan 17 15:58:42.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4246" for this suite. 01/17/23 15:58:42.382
{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","completed":265,"skipped":4882,"failed":0}
------------------------------
• [4.132 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:58:38.256
    Jan 17 15:58:38.256: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename configmap 01/17/23 15:58:38.257
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:58:38.295
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:58:38.297
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:92
    STEP: Creating configMap configmap-4246/configmap-test-d25cf5a6-ce8c-4e01-a572-49446a7d403b 01/17/23 15:58:38.299
    STEP: Creating a pod to test consume configMaps 01/17/23 15:58:38.306
    Jan 17 15:58:38.330: INFO: Waiting up to 5m0s for pod "pod-configmaps-e7917eb8-e588-4fd8-a5aa-1c5ed35905b3" in namespace "configmap-4246" to be "Succeeded or Failed"
    Jan 17 15:58:38.335: INFO: Pod "pod-configmaps-e7917eb8-e588-4fd8-a5aa-1c5ed35905b3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.475215ms
    Jan 17 15:58:40.340: INFO: Pod "pod-configmaps-e7917eb8-e588-4fd8-a5aa-1c5ed35905b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009775644s
    Jan 17 15:58:42.340: INFO: Pod "pod-configmaps-e7917eb8-e588-4fd8-a5aa-1c5ed35905b3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009366149s
    STEP: Saw pod success 01/17/23 15:58:42.34
    Jan 17 15:58:42.340: INFO: Pod "pod-configmaps-e7917eb8-e588-4fd8-a5aa-1c5ed35905b3" satisfied condition "Succeeded or Failed"
    Jan 17 15:58:42.343: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-configmaps-e7917eb8-e588-4fd8-a5aa-1c5ed35905b3 container env-test: <nil>
    STEP: delete the pod 01/17/23 15:58:42.35
    Jan 17 15:58:42.365: INFO: Waiting for pod pod-configmaps-e7917eb8-e588-4fd8-a5aa-1c5ed35905b3 to disappear
    Jan 17 15:58:42.375: INFO: Pod pod-configmaps-e7917eb8-e588-4fd8-a5aa-1c5ed35905b3 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 17 15:58:42.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-4246" for this suite. 01/17/23 15:58:42.382
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:58:42.389
Jan 17 15:58:42.389: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename configmap 01/17/23 15:58:42.39
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:58:42.413
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:58:42.415
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239
Jan 17 15:58:42.429: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-601acab5-613d-417a-8915-d8edf070125c 01/17/23 15:58:42.429
STEP: Creating configMap with name cm-test-opt-upd-3f8c8c74-fa10-43a0-9117-cc4151616558 01/17/23 15:58:42.442
STEP: Creating the pod 01/17/23 15:58:42.447
Jan 17 15:58:42.482: INFO: Waiting up to 5m0s for pod "pod-configmaps-9bfe6fe4-b39d-4800-a2c3-b723c4eef352" in namespace "configmap-323" to be "running and ready"
Jan 17 15:58:42.496: INFO: Pod "pod-configmaps-9bfe6fe4-b39d-4800-a2c3-b723c4eef352": Phase="Pending", Reason="", readiness=false. Elapsed: 13.876605ms
Jan 17 15:58:42.496: INFO: The phase of Pod pod-configmaps-9bfe6fe4-b39d-4800-a2c3-b723c4eef352 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 15:58:44.501: INFO: Pod "pod-configmaps-9bfe6fe4-b39d-4800-a2c3-b723c4eef352": Phase="Running", Reason="", readiness=true. Elapsed: 2.018341799s
Jan 17 15:58:44.501: INFO: The phase of Pod pod-configmaps-9bfe6fe4-b39d-4800-a2c3-b723c4eef352 is Running (Ready = true)
Jan 17 15:58:44.501: INFO: Pod "pod-configmaps-9bfe6fe4-b39d-4800-a2c3-b723c4eef352" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-601acab5-613d-417a-8915-d8edf070125c 01/17/23 15:58:44.522
STEP: Updating configmap cm-test-opt-upd-3f8c8c74-fa10-43a0-9117-cc4151616558 01/17/23 15:58:44.527
STEP: Creating configMap with name cm-test-opt-create-c59f5923-a345-4562-ab01-24b12bb1161c 01/17/23 15:58:44.532
STEP: waiting to observe update in volume 01/17/23 15:58:44.537
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 17 15:58:46.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-323" for this suite. 01/17/23 15:58:46.57
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":266,"skipped":4902,"failed":0}
------------------------------
• [4.187 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:58:42.389
    Jan 17 15:58:42.389: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename configmap 01/17/23 15:58:42.39
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:58:42.413
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:58:42.415
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:239
    Jan 17 15:58:42.429: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
    STEP: Creating configMap with name cm-test-opt-del-601acab5-613d-417a-8915-d8edf070125c 01/17/23 15:58:42.429
    STEP: Creating configMap with name cm-test-opt-upd-3f8c8c74-fa10-43a0-9117-cc4151616558 01/17/23 15:58:42.442
    STEP: Creating the pod 01/17/23 15:58:42.447
    Jan 17 15:58:42.482: INFO: Waiting up to 5m0s for pod "pod-configmaps-9bfe6fe4-b39d-4800-a2c3-b723c4eef352" in namespace "configmap-323" to be "running and ready"
    Jan 17 15:58:42.496: INFO: Pod "pod-configmaps-9bfe6fe4-b39d-4800-a2c3-b723c4eef352": Phase="Pending", Reason="", readiness=false. Elapsed: 13.876605ms
    Jan 17 15:58:42.496: INFO: The phase of Pod pod-configmaps-9bfe6fe4-b39d-4800-a2c3-b723c4eef352 is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 15:58:44.501: INFO: Pod "pod-configmaps-9bfe6fe4-b39d-4800-a2c3-b723c4eef352": Phase="Running", Reason="", readiness=true. Elapsed: 2.018341799s
    Jan 17 15:58:44.501: INFO: The phase of Pod pod-configmaps-9bfe6fe4-b39d-4800-a2c3-b723c4eef352 is Running (Ready = true)
    Jan 17 15:58:44.501: INFO: Pod "pod-configmaps-9bfe6fe4-b39d-4800-a2c3-b723c4eef352" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-601acab5-613d-417a-8915-d8edf070125c 01/17/23 15:58:44.522
    STEP: Updating configmap cm-test-opt-upd-3f8c8c74-fa10-43a0-9117-cc4151616558 01/17/23 15:58:44.527
    STEP: Creating configMap with name cm-test-opt-create-c59f5923-a345-4562-ab01-24b12bb1161c 01/17/23 15:58:44.532
    STEP: waiting to observe update in volume 01/17/23 15:58:44.537
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 17 15:58:46.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-323" for this suite. 01/17/23 15:58:46.57
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:58:46.577
Jan 17 15:58:46.577: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename services 01/17/23 15:58:46.578
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:58:46.6
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:58:46.603
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221
STEP: creating service in namespace services-4137 01/17/23 15:58:46.605
Jan 17 15:58:46.646: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-4137" to be "running and ready"
Jan 17 15:58:46.656: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 9.064353ms
Jan 17 15:58:46.656: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Jan 17 15:58:48.660: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.013268197s
Jan 17 15:58:48.660: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
Jan 17 15:58:48.660: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
Jan 17 15:58:48.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-4137 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Jan 17 15:58:48.804: INFO: rc: 7
Jan 17 15:58:48.816: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jan 17 15:58:48.819: INFO: Pod kube-proxy-mode-detector no longer exists
Jan 17 15:58:48.819: INFO: Couldn't detect KubeProxy mode - test failure may be expected: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-4137 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode:
Command stdout:

stderr:
+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode
command terminated with exit code 7

error:
exit status 7
STEP: creating service affinity-nodeport-timeout in namespace services-4137 01/17/23 15:58:48.819
STEP: creating replication controller affinity-nodeport-timeout in namespace services-4137 01/17/23 15:58:48.833
I0117 15:58:48.839454      22 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-4137, replica count: 3
I0117 15:58:51.891191      22 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 17 15:58:51.905: INFO: Creating new exec pod
Jan 17 15:58:51.918: INFO: Waiting up to 5m0s for pod "execpod-affinitylcpwm" in namespace "services-4137" to be "running"
Jan 17 15:58:51.921: INFO: Pod "execpod-affinitylcpwm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.655508ms
Jan 17 15:58:53.924: INFO: Pod "execpod-affinitylcpwm": Phase="Running", Reason="", readiness=true. Elapsed: 2.006156894s
Jan 17 15:58:53.924: INFO: Pod "execpod-affinitylcpwm" satisfied condition "running"
Jan 17 15:58:54.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-4137 exec execpod-affinitylcpwm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
Jan 17 15:58:56.081: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Jan 17 15:58:56.081: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 17 15:58:56.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-4137 exec execpod-affinitylcpwm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.95.227 80'
Jan 17 15:58:56.184: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.95.227 80\nConnection to 172.30.95.227 80 port [tcp/http] succeeded!\n"
Jan 17 15:58:56.184: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 17 15:58:56.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-4137 exec execpod-affinitylcpwm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.165.14 31861'
Jan 17 15:58:57.301: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.165.14 31861\nConnection to 10.0.165.14 31861 port [tcp/*] succeeded!\n"
Jan 17 15:58:57.301: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 17 15:58:57.301: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-4137 exec execpod-affinitylcpwm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.151.22 31861'
Jan 17 15:58:58.453: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.151.22 31861\nConnection to 10.0.151.22 31861 port [tcp/*] succeeded!\n"
Jan 17 15:58:58.453: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 17 15:58:58.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-4137 exec execpod-affinitylcpwm -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.139.213:31861/ ; done'
Jan 17 15:58:59.652: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:31861/\n"
Jan 17 15:58:59.652: INFO: stdout: "\naffinity-nodeport-timeout-qf546\naffinity-nodeport-timeout-qf546\naffinity-nodeport-timeout-qf546\naffinity-nodeport-timeout-qf546\naffinity-nodeport-timeout-qf546\naffinity-nodeport-timeout-qf546\naffinity-nodeport-timeout-qf546\naffinity-nodeport-timeout-qf546\naffinity-nodeport-timeout-qf546\naffinity-nodeport-timeout-qf546\naffinity-nodeport-timeout-qf546\naffinity-nodeport-timeout-qf546\naffinity-nodeport-timeout-qf546\naffinity-nodeport-timeout-qf546\naffinity-nodeport-timeout-qf546\naffinity-nodeport-timeout-qf546"
Jan 17 15:58:59.652: INFO: Received response from host: affinity-nodeport-timeout-qf546
Jan 17 15:58:59.652: INFO: Received response from host: affinity-nodeport-timeout-qf546
Jan 17 15:58:59.652: INFO: Received response from host: affinity-nodeport-timeout-qf546
Jan 17 15:58:59.652: INFO: Received response from host: affinity-nodeport-timeout-qf546
Jan 17 15:58:59.652: INFO: Received response from host: affinity-nodeport-timeout-qf546
Jan 17 15:58:59.652: INFO: Received response from host: affinity-nodeport-timeout-qf546
Jan 17 15:58:59.652: INFO: Received response from host: affinity-nodeport-timeout-qf546
Jan 17 15:58:59.652: INFO: Received response from host: affinity-nodeport-timeout-qf546
Jan 17 15:58:59.652: INFO: Received response from host: affinity-nodeport-timeout-qf546
Jan 17 15:58:59.652: INFO: Received response from host: affinity-nodeport-timeout-qf546
Jan 17 15:58:59.652: INFO: Received response from host: affinity-nodeport-timeout-qf546
Jan 17 15:58:59.652: INFO: Received response from host: affinity-nodeport-timeout-qf546
Jan 17 15:58:59.652: INFO: Received response from host: affinity-nodeport-timeout-qf546
Jan 17 15:58:59.652: INFO: Received response from host: affinity-nodeport-timeout-qf546
Jan 17 15:58:59.652: INFO: Received response from host: affinity-nodeport-timeout-qf546
Jan 17 15:58:59.652: INFO: Received response from host: affinity-nodeport-timeout-qf546
Jan 17 15:58:59.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-4137 exec execpod-affinitylcpwm -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.139.213:31861/'
Jan 17 15:58:59.769: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.139.213:31861/\n"
Jan 17 15:58:59.769: INFO: stdout: "affinity-nodeport-timeout-qf546"
Jan 17 15:59:19.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-4137 exec execpod-affinitylcpwm -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.139.213:31861/'
Jan 17 15:59:20.915: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.139.213:31861/\n"
Jan 17 15:59:20.915: INFO: stdout: "affinity-nodeport-timeout-wft84"
Jan 17 15:59:20.915: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-4137, will wait for the garbage collector to delete the pods 01/17/23 15:59:20.93
Jan 17 15:59:20.989: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 6.203191ms
Jan 17 15:59:21.090: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 100.992302ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 17 15:59:23.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4137" for this suite. 01/17/23 15:59:23.444
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","completed":267,"skipped":4914,"failed":0}
------------------------------
• [SLOW TEST] [36.876 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:58:46.577
    Jan 17 15:58:46.577: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename services 01/17/23 15:58:46.578
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:58:46.6
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:58:46.603
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2221
    STEP: creating service in namespace services-4137 01/17/23 15:58:46.605
    Jan 17 15:58:46.646: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-4137" to be "running and ready"
    Jan 17 15:58:46.656: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 9.064353ms
    Jan 17 15:58:46.656: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 15:58:48.660: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.013268197s
    Jan 17 15:58:48.660: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
    Jan 17 15:58:48.660: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
    Jan 17 15:58:48.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-4137 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
    Jan 17 15:58:48.804: INFO: rc: 7
    Jan 17 15:58:48.816: INFO: Waiting for pod kube-proxy-mode-detector to disappear
    Jan 17 15:58:48.819: INFO: Pod kube-proxy-mode-detector no longer exists
    Jan 17 15:58:48.819: INFO: Couldn't detect KubeProxy mode - test failure may be expected: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-4137 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode:
    Command stdout:

    stderr:
    + curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode
    command terminated with exit code 7

    error:
    exit status 7
    STEP: creating service affinity-nodeport-timeout in namespace services-4137 01/17/23 15:58:48.819
    STEP: creating replication controller affinity-nodeport-timeout in namespace services-4137 01/17/23 15:58:48.833
    I0117 15:58:48.839454      22 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-4137, replica count: 3
    I0117 15:58:51.891191      22 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 17 15:58:51.905: INFO: Creating new exec pod
    Jan 17 15:58:51.918: INFO: Waiting up to 5m0s for pod "execpod-affinitylcpwm" in namespace "services-4137" to be "running"
    Jan 17 15:58:51.921: INFO: Pod "execpod-affinitylcpwm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.655508ms
    Jan 17 15:58:53.924: INFO: Pod "execpod-affinitylcpwm": Phase="Running", Reason="", readiness=true. Elapsed: 2.006156894s
    Jan 17 15:58:53.924: INFO: Pod "execpod-affinitylcpwm" satisfied condition "running"
    Jan 17 15:58:54.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-4137 exec execpod-affinitylcpwm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
    Jan 17 15:58:56.081: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
    Jan 17 15:58:56.081: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 17 15:58:56.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-4137 exec execpod-affinitylcpwm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.95.227 80'
    Jan 17 15:58:56.184: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.95.227 80\nConnection to 172.30.95.227 80 port [tcp/http] succeeded!\n"
    Jan 17 15:58:56.184: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 17 15:58:56.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-4137 exec execpod-affinitylcpwm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.165.14 31861'
    Jan 17 15:58:57.301: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.165.14 31861\nConnection to 10.0.165.14 31861 port [tcp/*] succeeded!\n"
    Jan 17 15:58:57.301: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 17 15:58:57.301: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-4137 exec execpod-affinitylcpwm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.151.22 31861'
    Jan 17 15:58:58.453: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.151.22 31861\nConnection to 10.0.151.22 31861 port [tcp/*] succeeded!\n"
    Jan 17 15:58:58.453: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 17 15:58:58.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-4137 exec execpod-affinitylcpwm -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.139.213:31861/ ; done'
    Jan 17 15:58:59.652: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.139.213:31861/\n"
    Jan 17 15:58:59.652: INFO: stdout: "\naffinity-nodeport-timeout-qf546\naffinity-nodeport-timeout-qf546\naffinity-nodeport-timeout-qf546\naffinity-nodeport-timeout-qf546\naffinity-nodeport-timeout-qf546\naffinity-nodeport-timeout-qf546\naffinity-nodeport-timeout-qf546\naffinity-nodeport-timeout-qf546\naffinity-nodeport-timeout-qf546\naffinity-nodeport-timeout-qf546\naffinity-nodeport-timeout-qf546\naffinity-nodeport-timeout-qf546\naffinity-nodeport-timeout-qf546\naffinity-nodeport-timeout-qf546\naffinity-nodeport-timeout-qf546\naffinity-nodeport-timeout-qf546"
    Jan 17 15:58:59.652: INFO: Received response from host: affinity-nodeport-timeout-qf546
    Jan 17 15:58:59.652: INFO: Received response from host: affinity-nodeport-timeout-qf546
    Jan 17 15:58:59.652: INFO: Received response from host: affinity-nodeport-timeout-qf546
    Jan 17 15:58:59.652: INFO: Received response from host: affinity-nodeport-timeout-qf546
    Jan 17 15:58:59.652: INFO: Received response from host: affinity-nodeport-timeout-qf546
    Jan 17 15:58:59.652: INFO: Received response from host: affinity-nodeport-timeout-qf546
    Jan 17 15:58:59.652: INFO: Received response from host: affinity-nodeport-timeout-qf546
    Jan 17 15:58:59.652: INFO: Received response from host: affinity-nodeport-timeout-qf546
    Jan 17 15:58:59.652: INFO: Received response from host: affinity-nodeport-timeout-qf546
    Jan 17 15:58:59.652: INFO: Received response from host: affinity-nodeport-timeout-qf546
    Jan 17 15:58:59.652: INFO: Received response from host: affinity-nodeport-timeout-qf546
    Jan 17 15:58:59.652: INFO: Received response from host: affinity-nodeport-timeout-qf546
    Jan 17 15:58:59.652: INFO: Received response from host: affinity-nodeport-timeout-qf546
    Jan 17 15:58:59.652: INFO: Received response from host: affinity-nodeport-timeout-qf546
    Jan 17 15:58:59.652: INFO: Received response from host: affinity-nodeport-timeout-qf546
    Jan 17 15:58:59.652: INFO: Received response from host: affinity-nodeport-timeout-qf546
    Jan 17 15:58:59.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-4137 exec execpod-affinitylcpwm -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.139.213:31861/'
    Jan 17 15:58:59.769: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.139.213:31861/\n"
    Jan 17 15:58:59.769: INFO: stdout: "affinity-nodeport-timeout-qf546"
    Jan 17 15:59:19.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-4137 exec execpod-affinitylcpwm -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.139.213:31861/'
    Jan 17 15:59:20.915: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.139.213:31861/\n"
    Jan 17 15:59:20.915: INFO: stdout: "affinity-nodeport-timeout-wft84"
    Jan 17 15:59:20.915: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-4137, will wait for the garbage collector to delete the pods 01/17/23 15:59:20.93
    Jan 17 15:59:20.989: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 6.203191ms
    Jan 17 15:59:21.090: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 100.992302ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 17 15:59:23.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-4137" for this suite. 01/17/23 15:59:23.444
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:59:23.453
Jan 17 15:59:23.453: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename job 01/17/23 15:59:23.454
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:59:23.479
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:59:23.486
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254
STEP: Creating a job 01/17/23 15:59:23.489
W0117 15:59:23.510639      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring job reaches completions 01/17/23 15:59:23.51
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jan 17 15:59:35.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7316" for this suite. 01/17/23 15:59:35.519
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","completed":268,"skipped":4914,"failed":0}
------------------------------
• [SLOW TEST] [12.072 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:59:23.453
    Jan 17 15:59:23.453: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename job 01/17/23 15:59:23.454
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:59:23.479
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:59:23.486
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:254
    STEP: Creating a job 01/17/23 15:59:23.489
    W0117 15:59:23.510639      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Ensuring job reaches completions 01/17/23 15:59:23.51
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jan 17 15:59:35.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-7316" for this suite. 01/17/23 15:59:35.519
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:527
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:59:35.525
Jan 17 15:59:35.526: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename security-context-test 01/17/23 15:59:35.526
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:59:35.547
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:59:35.549
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:527
Jan 17 15:59:35.586: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-14a22602-a7bb-4f84-bba2-0d1e0d3fb7a4" in namespace "security-context-test-1864" to be "Succeeded or Failed"
Jan 17 15:59:35.593: INFO: Pod "busybox-privileged-false-14a22602-a7bb-4f84-bba2-0d1e0d3fb7a4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.846464ms
Jan 17 15:59:37.597: INFO: Pod "busybox-privileged-false-14a22602-a7bb-4f84-bba2-0d1e0d3fb7a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010743796s
Jan 17 15:59:39.597: INFO: Pod "busybox-privileged-false-14a22602-a7bb-4f84-bba2-0d1e0d3fb7a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010777083s
Jan 17 15:59:39.597: INFO: Pod "busybox-privileged-false-14a22602-a7bb-4f84-bba2-0d1e0d3fb7a4" satisfied condition "Succeeded or Failed"
Jan 17 15:59:39.604: INFO: Got logs for pod "busybox-privileged-false-14a22602-a7bb-4f84-bba2-0d1e0d3fb7a4": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jan 17 15:59:39.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-1864" for this suite. 01/17/23 15:59:39.608
{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","completed":269,"skipped":4914,"failed":0}
------------------------------
• [4.089 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:490
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:527

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:59:35.525
    Jan 17 15:59:35.526: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename security-context-test 01/17/23 15:59:35.526
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:59:35.547
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:59:35.549
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:527
    Jan 17 15:59:35.586: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-14a22602-a7bb-4f84-bba2-0d1e0d3fb7a4" in namespace "security-context-test-1864" to be "Succeeded or Failed"
    Jan 17 15:59:35.593: INFO: Pod "busybox-privileged-false-14a22602-a7bb-4f84-bba2-0d1e0d3fb7a4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.846464ms
    Jan 17 15:59:37.597: INFO: Pod "busybox-privileged-false-14a22602-a7bb-4f84-bba2-0d1e0d3fb7a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010743796s
    Jan 17 15:59:39.597: INFO: Pod "busybox-privileged-false-14a22602-a7bb-4f84-bba2-0d1e0d3fb7a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010777083s
    Jan 17 15:59:39.597: INFO: Pod "busybox-privileged-false-14a22602-a7bb-4f84-bba2-0d1e0d3fb7a4" satisfied condition "Succeeded or Failed"
    Jan 17 15:59:39.604: INFO: Got logs for pod "busybox-privileged-false-14a22602-a7bb-4f84-bba2-0d1e0d3fb7a4": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jan 17 15:59:39.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-1864" for this suite. 01/17/23 15:59:39.608
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:59:39.615
Jan 17 15:59:39.615: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename projected 01/17/23 15:59:39.616
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:59:39.633
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:59:39.635
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43
STEP: Creating configMap with name configmap-projected-all-test-volume-bfa99ae2-20f3-41f8-ba26-bdfb33dda9fe 01/17/23 15:59:39.638
STEP: Creating secret with name secret-projected-all-test-volume-c7a1edc1-f54c-44da-86bd-649b33627722 01/17/23 15:59:39.643
STEP: Creating a pod to test Check all projections for projected volume plugin 01/17/23 15:59:39.652
Jan 17 15:59:39.679: INFO: Waiting up to 5m0s for pod "projected-volume-c33c3853-171b-4caa-b9a5-a6e8f130e820" in namespace "projected-1105" to be "Succeeded or Failed"
Jan 17 15:59:39.683: INFO: Pod "projected-volume-c33c3853-171b-4caa-b9a5-a6e8f130e820": Phase="Pending", Reason="", readiness=false. Elapsed: 3.948996ms
Jan 17 15:59:41.687: INFO: Pod "projected-volume-c33c3853-171b-4caa-b9a5-a6e8f130e820": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008128907s
Jan 17 15:59:43.688: INFO: Pod "projected-volume-c33c3853-171b-4caa-b9a5-a6e8f130e820": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008995258s
STEP: Saw pod success 01/17/23 15:59:43.688
Jan 17 15:59:43.688: INFO: Pod "projected-volume-c33c3853-171b-4caa-b9a5-a6e8f130e820" satisfied condition "Succeeded or Failed"
Jan 17 15:59:43.691: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod projected-volume-c33c3853-171b-4caa-b9a5-a6e8f130e820 container projected-all-volume-test: <nil>
STEP: delete the pod 01/17/23 15:59:43.697
Jan 17 15:59:43.711: INFO: Waiting for pod projected-volume-c33c3853-171b-4caa-b9a5-a6e8f130e820 to disappear
Jan 17 15:59:43.714: INFO: Pod projected-volume-c33c3853-171b-4caa-b9a5-a6e8f130e820 no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:187
Jan 17 15:59:43.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1105" for this suite. 01/17/23 15:59:43.721
{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","completed":270,"skipped":4921,"failed":0}
------------------------------
• [4.115 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:59:39.615
    Jan 17 15:59:39.615: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename projected 01/17/23 15:59:39.616
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:59:39.633
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:59:39.635
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:43
    STEP: Creating configMap with name configmap-projected-all-test-volume-bfa99ae2-20f3-41f8-ba26-bdfb33dda9fe 01/17/23 15:59:39.638
    STEP: Creating secret with name secret-projected-all-test-volume-c7a1edc1-f54c-44da-86bd-649b33627722 01/17/23 15:59:39.643
    STEP: Creating a pod to test Check all projections for projected volume plugin 01/17/23 15:59:39.652
    Jan 17 15:59:39.679: INFO: Waiting up to 5m0s for pod "projected-volume-c33c3853-171b-4caa-b9a5-a6e8f130e820" in namespace "projected-1105" to be "Succeeded or Failed"
    Jan 17 15:59:39.683: INFO: Pod "projected-volume-c33c3853-171b-4caa-b9a5-a6e8f130e820": Phase="Pending", Reason="", readiness=false. Elapsed: 3.948996ms
    Jan 17 15:59:41.687: INFO: Pod "projected-volume-c33c3853-171b-4caa-b9a5-a6e8f130e820": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008128907s
    Jan 17 15:59:43.688: INFO: Pod "projected-volume-c33c3853-171b-4caa-b9a5-a6e8f130e820": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008995258s
    STEP: Saw pod success 01/17/23 15:59:43.688
    Jan 17 15:59:43.688: INFO: Pod "projected-volume-c33c3853-171b-4caa-b9a5-a6e8f130e820" satisfied condition "Succeeded or Failed"
    Jan 17 15:59:43.691: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod projected-volume-c33c3853-171b-4caa-b9a5-a6e8f130e820 container projected-all-volume-test: <nil>
    STEP: delete the pod 01/17/23 15:59:43.697
    Jan 17 15:59:43.711: INFO: Waiting for pod projected-volume-c33c3853-171b-4caa-b9a5-a6e8f130e820 to disappear
    Jan 17 15:59:43.714: INFO: Pod projected-volume-c33c3853-171b-4caa-b9a5-a6e8f130e820 no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/framework.go:187
    Jan 17 15:59:43.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1105" for this suite. 01/17/23 15:59:43.721
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:59:43.73
Jan 17 15:59:43.730: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename pods 01/17/23 15:59:43.731
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:59:43.768
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:59:43.771
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397
STEP: creating the pod 01/17/23 15:59:43.773
STEP: submitting the pod to kubernetes 01/17/23 15:59:43.773
Jan 17 15:59:43.803: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-1f561053-f165-4520-aa85-ef851f177883" in namespace "pods-8513" to be "running and ready"
Jan 17 15:59:43.811: INFO: Pod "pod-update-activedeadlineseconds-1f561053-f165-4520-aa85-ef851f177883": Phase="Pending", Reason="", readiness=false. Elapsed: 7.445912ms
Jan 17 15:59:43.811: INFO: The phase of Pod pod-update-activedeadlineseconds-1f561053-f165-4520-aa85-ef851f177883 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 15:59:45.815: INFO: Pod "pod-update-activedeadlineseconds-1f561053-f165-4520-aa85-ef851f177883": Phase="Running", Reason="", readiness=true. Elapsed: 2.011399985s
Jan 17 15:59:45.815: INFO: The phase of Pod pod-update-activedeadlineseconds-1f561053-f165-4520-aa85-ef851f177883 is Running (Ready = true)
Jan 17 15:59:45.815: INFO: Pod "pod-update-activedeadlineseconds-1f561053-f165-4520-aa85-ef851f177883" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 01/17/23 15:59:45.818
STEP: updating the pod 01/17/23 15:59:45.821
Jan 17 15:59:46.340: INFO: Successfully updated pod "pod-update-activedeadlineseconds-1f561053-f165-4520-aa85-ef851f177883"
Jan 17 15:59:46.340: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-1f561053-f165-4520-aa85-ef851f177883" in namespace "pods-8513" to be "terminated with reason DeadlineExceeded"
Jan 17 15:59:46.345: INFO: Pod "pod-update-activedeadlineseconds-1f561053-f165-4520-aa85-ef851f177883": Phase="Running", Reason="", readiness=true. Elapsed: 5.317406ms
Jan 17 15:59:48.350: INFO: Pod "pod-update-activedeadlineseconds-1f561053-f165-4520-aa85-ef851f177883": Phase="Running", Reason="", readiness=true. Elapsed: 2.009688594s
Jan 17 15:59:50.349: INFO: Pod "pod-update-activedeadlineseconds-1f561053-f165-4520-aa85-ef851f177883": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.009311281s
Jan 17 15:59:50.349: INFO: Pod "pod-update-activedeadlineseconds-1f561053-f165-4520-aa85-ef851f177883" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 17 15:59:50.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8513" for this suite. 01/17/23 15:59:50.354
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","completed":271,"skipped":4936,"failed":0}
------------------------------
• [SLOW TEST] [6.630 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:59:43.73
    Jan 17 15:59:43.730: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename pods 01/17/23 15:59:43.731
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:59:43.768
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:59:43.771
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:397
    STEP: creating the pod 01/17/23 15:59:43.773
    STEP: submitting the pod to kubernetes 01/17/23 15:59:43.773
    Jan 17 15:59:43.803: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-1f561053-f165-4520-aa85-ef851f177883" in namespace "pods-8513" to be "running and ready"
    Jan 17 15:59:43.811: INFO: Pod "pod-update-activedeadlineseconds-1f561053-f165-4520-aa85-ef851f177883": Phase="Pending", Reason="", readiness=false. Elapsed: 7.445912ms
    Jan 17 15:59:43.811: INFO: The phase of Pod pod-update-activedeadlineseconds-1f561053-f165-4520-aa85-ef851f177883 is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 15:59:45.815: INFO: Pod "pod-update-activedeadlineseconds-1f561053-f165-4520-aa85-ef851f177883": Phase="Running", Reason="", readiness=true. Elapsed: 2.011399985s
    Jan 17 15:59:45.815: INFO: The phase of Pod pod-update-activedeadlineseconds-1f561053-f165-4520-aa85-ef851f177883 is Running (Ready = true)
    Jan 17 15:59:45.815: INFO: Pod "pod-update-activedeadlineseconds-1f561053-f165-4520-aa85-ef851f177883" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 01/17/23 15:59:45.818
    STEP: updating the pod 01/17/23 15:59:45.821
    Jan 17 15:59:46.340: INFO: Successfully updated pod "pod-update-activedeadlineseconds-1f561053-f165-4520-aa85-ef851f177883"
    Jan 17 15:59:46.340: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-1f561053-f165-4520-aa85-ef851f177883" in namespace "pods-8513" to be "terminated with reason DeadlineExceeded"
    Jan 17 15:59:46.345: INFO: Pod "pod-update-activedeadlineseconds-1f561053-f165-4520-aa85-ef851f177883": Phase="Running", Reason="", readiness=true. Elapsed: 5.317406ms
    Jan 17 15:59:48.350: INFO: Pod "pod-update-activedeadlineseconds-1f561053-f165-4520-aa85-ef851f177883": Phase="Running", Reason="", readiness=true. Elapsed: 2.009688594s
    Jan 17 15:59:50.349: INFO: Pod "pod-update-activedeadlineseconds-1f561053-f165-4520-aa85-ef851f177883": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.009311281s
    Jan 17 15:59:50.349: INFO: Pod "pod-update-activedeadlineseconds-1f561053-f165-4520-aa85-ef851f177883" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 17 15:59:50.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-8513" for this suite. 01/17/23 15:59:50.354
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:59:50.36
Jan 17 15:59:50.360: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename configmap 01/17/23 15:59:50.361
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:59:50.382
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:59:50.385
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46
STEP: Creating configMap with name configmap-test-volume-cbd0f69a-699f-4fee-9d39-ba17012f8112 01/17/23 15:59:50.388
STEP: Creating a pod to test consume configMaps 01/17/23 15:59:50.397
Jan 17 15:59:50.412: INFO: Waiting up to 5m0s for pod "pod-configmaps-2bff4d87-e3cc-4dca-b5e8-e4883045af6b" in namespace "configmap-5447" to be "Succeeded or Failed"
Jan 17 15:59:50.415: INFO: Pod "pod-configmaps-2bff4d87-e3cc-4dca-b5e8-e4883045af6b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.117726ms
Jan 17 15:59:52.421: INFO: Pod "pod-configmaps-2bff4d87-e3cc-4dca-b5e8-e4883045af6b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008672481s
Jan 17 15:59:54.420: INFO: Pod "pod-configmaps-2bff4d87-e3cc-4dca-b5e8-e4883045af6b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007392517s
STEP: Saw pod success 01/17/23 15:59:54.42
Jan 17 15:59:54.420: INFO: Pod "pod-configmaps-2bff4d87-e3cc-4dca-b5e8-e4883045af6b" satisfied condition "Succeeded or Failed"
Jan 17 15:59:54.423: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-configmaps-2bff4d87-e3cc-4dca-b5e8-e4883045af6b container agnhost-container: <nil>
STEP: delete the pod 01/17/23 15:59:54.429
Jan 17 15:59:54.439: INFO: Waiting for pod pod-configmaps-2bff4d87-e3cc-4dca-b5e8-e4883045af6b to disappear
Jan 17 15:59:54.442: INFO: Pod pod-configmaps-2bff4d87-e3cc-4dca-b5e8-e4883045af6b no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 17 15:59:54.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5447" for this suite. 01/17/23 15:59:54.447
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","completed":272,"skipped":4936,"failed":0}
------------------------------
• [4.092 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:59:50.36
    Jan 17 15:59:50.360: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename configmap 01/17/23 15:59:50.361
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:59:50.382
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:59:50.385
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:46
    STEP: Creating configMap with name configmap-test-volume-cbd0f69a-699f-4fee-9d39-ba17012f8112 01/17/23 15:59:50.388
    STEP: Creating a pod to test consume configMaps 01/17/23 15:59:50.397
    Jan 17 15:59:50.412: INFO: Waiting up to 5m0s for pod "pod-configmaps-2bff4d87-e3cc-4dca-b5e8-e4883045af6b" in namespace "configmap-5447" to be "Succeeded or Failed"
    Jan 17 15:59:50.415: INFO: Pod "pod-configmaps-2bff4d87-e3cc-4dca-b5e8-e4883045af6b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.117726ms
    Jan 17 15:59:52.421: INFO: Pod "pod-configmaps-2bff4d87-e3cc-4dca-b5e8-e4883045af6b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008672481s
    Jan 17 15:59:54.420: INFO: Pod "pod-configmaps-2bff4d87-e3cc-4dca-b5e8-e4883045af6b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007392517s
    STEP: Saw pod success 01/17/23 15:59:54.42
    Jan 17 15:59:54.420: INFO: Pod "pod-configmaps-2bff4d87-e3cc-4dca-b5e8-e4883045af6b" satisfied condition "Succeeded or Failed"
    Jan 17 15:59:54.423: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-configmaps-2bff4d87-e3cc-4dca-b5e8-e4883045af6b container agnhost-container: <nil>
    STEP: delete the pod 01/17/23 15:59:54.429
    Jan 17 15:59:54.439: INFO: Waiting for pod pod-configmaps-2bff4d87-e3cc-4dca-b5e8-e4883045af6b to disappear
    Jan 17 15:59:54.442: INFO: Pod pod-configmaps-2bff4d87-e3cc-4dca-b5e8-e4883045af6b no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 17 15:59:54.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-5447" for this suite. 01/17/23 15:59:54.447
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:59:54.452
Jan 17 15:59:54.452: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename daemonsets 01/17/23 15:59:54.453
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:59:54.472
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:59:54.475
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861
STEP: Creating simple DaemonSet "daemon-set" 01/17/23 15:59:54.527
STEP: Check that daemon pods launch on every node of the cluster. 01/17/23 15:59:54.533
Jan 17 15:59:54.539: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 15:59:54.539: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 15:59:54.539: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 15:59:54.542: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 17 15:59:54.542: INFO: Node ip-10-0-139-213.ec2.internal is running 0 daemon pod, expected 1
Jan 17 15:59:55.546: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 15:59:55.546: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 15:59:55.546: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 15:59:55.550: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 17 15:59:55.550: INFO: Node ip-10-0-151-22.ec2.internal is running 0 daemon pod, expected 1
Jan 17 15:59:56.547: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 15:59:56.547: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 15:59:56.547: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 15:59:56.551: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan 17 15:59:56.551: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Getting /status 01/17/23 15:59:56.554
Jan 17 15:59:56.557: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 01/17/23 15:59:56.557
Jan 17 15:59:56.566: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 01/17/23 15:59:56.566
Jan 17 15:59:56.567: INFO: Observed &DaemonSet event: ADDED
Jan 17 15:59:56.567: INFO: Observed &DaemonSet event: MODIFIED
Jan 17 15:59:56.568: INFO: Observed &DaemonSet event: MODIFIED
Jan 17 15:59:56.568: INFO: Observed &DaemonSet event: MODIFIED
Jan 17 15:59:56.568: INFO: Observed &DaemonSet event: MODIFIED
Jan 17 15:59:56.568: INFO: Found daemon set daemon-set in namespace daemonsets-951 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 17 15:59:56.568: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 01/17/23 15:59:56.568
STEP: watching for the daemon set status to be patched 01/17/23 15:59:56.574
Jan 17 15:59:56.575: INFO: Observed &DaemonSet event: ADDED
Jan 17 15:59:56.575: INFO: Observed &DaemonSet event: MODIFIED
Jan 17 15:59:56.575: INFO: Observed &DaemonSet event: MODIFIED
Jan 17 15:59:56.575: INFO: Observed &DaemonSet event: MODIFIED
Jan 17 15:59:56.575: INFO: Observed &DaemonSet event: MODIFIED
Jan 17 15:59:56.575: INFO: Observed daemon set daemon-set in namespace daemonsets-951 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 17 15:59:56.575: INFO: Observed &DaemonSet event: MODIFIED
Jan 17 15:59:56.575: INFO: Found daemon set daemon-set in namespace daemonsets-951 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Jan 17 15:59:56.575: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 01/17/23 15:59:56.578
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-951, will wait for the garbage collector to delete the pods 01/17/23 15:59:56.578
Jan 17 15:59:56.638: INFO: Deleting DaemonSet.extensions daemon-set took: 6.333103ms
Jan 17 15:59:56.739: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.975316ms
Jan 17 15:59:59.343: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 17 15:59:59.343: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 17 15:59:59.346: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"123790"},"items":null}

Jan 17 15:59:59.349: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"123790"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan 17 15:59:59.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-951" for this suite. 01/17/23 15:59:59.366
{"msg":"PASSED [sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]","completed":273,"skipped":4943,"failed":0}
------------------------------
• [4.920 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:59:54.452
    Jan 17 15:59:54.452: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename daemonsets 01/17/23 15:59:54.453
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:59:54.472
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:59:54.475
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:861
    STEP: Creating simple DaemonSet "daemon-set" 01/17/23 15:59:54.527
    STEP: Check that daemon pods launch on every node of the cluster. 01/17/23 15:59:54.533
    Jan 17 15:59:54.539: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 15:59:54.539: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 15:59:54.539: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 15:59:54.542: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 17 15:59:54.542: INFO: Node ip-10-0-139-213.ec2.internal is running 0 daemon pod, expected 1
    Jan 17 15:59:55.546: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 15:59:55.546: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 15:59:55.546: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 15:59:55.550: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 17 15:59:55.550: INFO: Node ip-10-0-151-22.ec2.internal is running 0 daemon pod, expected 1
    Jan 17 15:59:56.547: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 15:59:56.547: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 15:59:56.547: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 15:59:56.551: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jan 17 15:59:56.551: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Getting /status 01/17/23 15:59:56.554
    Jan 17 15:59:56.557: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 01/17/23 15:59:56.557
    Jan 17 15:59:56.566: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 01/17/23 15:59:56.566
    Jan 17 15:59:56.567: INFO: Observed &DaemonSet event: ADDED
    Jan 17 15:59:56.567: INFO: Observed &DaemonSet event: MODIFIED
    Jan 17 15:59:56.568: INFO: Observed &DaemonSet event: MODIFIED
    Jan 17 15:59:56.568: INFO: Observed &DaemonSet event: MODIFIED
    Jan 17 15:59:56.568: INFO: Observed &DaemonSet event: MODIFIED
    Jan 17 15:59:56.568: INFO: Found daemon set daemon-set in namespace daemonsets-951 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan 17 15:59:56.568: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 01/17/23 15:59:56.568
    STEP: watching for the daemon set status to be patched 01/17/23 15:59:56.574
    Jan 17 15:59:56.575: INFO: Observed &DaemonSet event: ADDED
    Jan 17 15:59:56.575: INFO: Observed &DaemonSet event: MODIFIED
    Jan 17 15:59:56.575: INFO: Observed &DaemonSet event: MODIFIED
    Jan 17 15:59:56.575: INFO: Observed &DaemonSet event: MODIFIED
    Jan 17 15:59:56.575: INFO: Observed &DaemonSet event: MODIFIED
    Jan 17 15:59:56.575: INFO: Observed daemon set daemon-set in namespace daemonsets-951 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan 17 15:59:56.575: INFO: Observed &DaemonSet event: MODIFIED
    Jan 17 15:59:56.575: INFO: Found daemon set daemon-set in namespace daemonsets-951 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Jan 17 15:59:56.575: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 01/17/23 15:59:56.578
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-951, will wait for the garbage collector to delete the pods 01/17/23 15:59:56.578
    Jan 17 15:59:56.638: INFO: Deleting DaemonSet.extensions daemon-set took: 6.333103ms
    Jan 17 15:59:56.739: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.975316ms
    Jan 17 15:59:59.343: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 17 15:59:59.343: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 17 15:59:59.346: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"123790"},"items":null}

    Jan 17 15:59:59.349: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"123790"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan 17 15:59:59.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-951" for this suite. 01/17/23 15:59:59.366
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 15:59:59.373
Jan 17 15:59:59.373: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename container-probe 01/17/23 15:59:59.373
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:59:59.395
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:59:59.399
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165
STEP: Creating pod liveness-d777480c-b845-4df7-88a2-851ac8fff1ee in namespace container-probe-1199 01/17/23 15:59:59.401
Jan 17 15:59:59.427: INFO: Waiting up to 5m0s for pod "liveness-d777480c-b845-4df7-88a2-851ac8fff1ee" in namespace "container-probe-1199" to be "not pending"
Jan 17 15:59:59.434: INFO: Pod "liveness-d777480c-b845-4df7-88a2-851ac8fff1ee": Phase="Pending", Reason="", readiness=false. Elapsed: 7.41172ms
Jan 17 16:00:01.438: INFO: Pod "liveness-d777480c-b845-4df7-88a2-851ac8fff1ee": Phase="Running", Reason="", readiness=true. Elapsed: 2.010892075s
Jan 17 16:00:01.438: INFO: Pod "liveness-d777480c-b845-4df7-88a2-851ac8fff1ee" satisfied condition "not pending"
Jan 17 16:00:01.438: INFO: Started pod liveness-d777480c-b845-4df7-88a2-851ac8fff1ee in namespace container-probe-1199
STEP: checking the pod's current state and verifying that restartCount is present 01/17/23 16:00:01.438
Jan 17 16:00:01.441: INFO: Initial restart count of pod liveness-d777480c-b845-4df7-88a2-851ac8fff1ee is 0
Jan 17 16:00:21.503: INFO: Restart count of pod container-probe-1199/liveness-d777480c-b845-4df7-88a2-851ac8fff1ee is now 1 (20.062152688s elapsed)
STEP: deleting the pod 01/17/23 16:00:21.503
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan 17 16:00:21.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1199" for this suite. 01/17/23 16:00:21.52
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","completed":274,"skipped":4950,"failed":0}
------------------------------
• [SLOW TEST] [22.155 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 15:59:59.373
    Jan 17 15:59:59.373: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename container-probe 01/17/23 15:59:59.373
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 15:59:59.395
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 15:59:59.399
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:165
    STEP: Creating pod liveness-d777480c-b845-4df7-88a2-851ac8fff1ee in namespace container-probe-1199 01/17/23 15:59:59.401
    Jan 17 15:59:59.427: INFO: Waiting up to 5m0s for pod "liveness-d777480c-b845-4df7-88a2-851ac8fff1ee" in namespace "container-probe-1199" to be "not pending"
    Jan 17 15:59:59.434: INFO: Pod "liveness-d777480c-b845-4df7-88a2-851ac8fff1ee": Phase="Pending", Reason="", readiness=false. Elapsed: 7.41172ms
    Jan 17 16:00:01.438: INFO: Pod "liveness-d777480c-b845-4df7-88a2-851ac8fff1ee": Phase="Running", Reason="", readiness=true. Elapsed: 2.010892075s
    Jan 17 16:00:01.438: INFO: Pod "liveness-d777480c-b845-4df7-88a2-851ac8fff1ee" satisfied condition "not pending"
    Jan 17 16:00:01.438: INFO: Started pod liveness-d777480c-b845-4df7-88a2-851ac8fff1ee in namespace container-probe-1199
    STEP: checking the pod's current state and verifying that restartCount is present 01/17/23 16:00:01.438
    Jan 17 16:00:01.441: INFO: Initial restart count of pod liveness-d777480c-b845-4df7-88a2-851ac8fff1ee is 0
    Jan 17 16:00:21.503: INFO: Restart count of pod container-probe-1199/liveness-d777480c-b845-4df7-88a2-851ac8fff1ee is now 1 (20.062152688s elapsed)
    STEP: deleting the pod 01/17/23 16:00:21.503
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan 17 16:00:21.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-1199" for this suite. 01/17/23 16:00:21.52
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:00:21.528
Jan 17 16:00:21.528: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename var-expansion 01/17/23 16:00:21.529
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:00:21.558
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:00:21.56
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151
Jan 17 16:00:21.585: INFO: Waiting up to 2m0s for pod "var-expansion-2d31878e-c1b1-4305-aab2-2f3f7d0b3d96" in namespace "var-expansion-6666" to be "container 0 failed with reason CreateContainerConfigError"
Jan 17 16:00:21.589: INFO: Pod "var-expansion-2d31878e-c1b1-4305-aab2-2f3f7d0b3d96": Phase="Pending", Reason="", readiness=false. Elapsed: 4.363091ms
Jan 17 16:00:23.594: INFO: Pod "var-expansion-2d31878e-c1b1-4305-aab2-2f3f7d0b3d96": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009070278s
Jan 17 16:00:23.594: INFO: Pod "var-expansion-2d31878e-c1b1-4305-aab2-2f3f7d0b3d96" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Jan 17 16:00:23.594: INFO: Deleting pod "var-expansion-2d31878e-c1b1-4305-aab2-2f3f7d0b3d96" in namespace "var-expansion-6666"
Jan 17 16:00:23.600: INFO: Wait up to 5m0s for pod "var-expansion-2d31878e-c1b1-4305-aab2-2f3f7d0b3d96" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan 17 16:00:27.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6666" for this suite. 01/17/23 16:00:27.619
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","completed":275,"skipped":4963,"failed":0}
------------------------------
• [SLOW TEST] [6.097 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:00:21.528
    Jan 17 16:00:21.528: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename var-expansion 01/17/23 16:00:21.529
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:00:21.558
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:00:21.56
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:151
    Jan 17 16:00:21.585: INFO: Waiting up to 2m0s for pod "var-expansion-2d31878e-c1b1-4305-aab2-2f3f7d0b3d96" in namespace "var-expansion-6666" to be "container 0 failed with reason CreateContainerConfigError"
    Jan 17 16:00:21.589: INFO: Pod "var-expansion-2d31878e-c1b1-4305-aab2-2f3f7d0b3d96": Phase="Pending", Reason="", readiness=false. Elapsed: 4.363091ms
    Jan 17 16:00:23.594: INFO: Pod "var-expansion-2d31878e-c1b1-4305-aab2-2f3f7d0b3d96": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009070278s
    Jan 17 16:00:23.594: INFO: Pod "var-expansion-2d31878e-c1b1-4305-aab2-2f3f7d0b3d96" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Jan 17 16:00:23.594: INFO: Deleting pod "var-expansion-2d31878e-c1b1-4305-aab2-2f3f7d0b3d96" in namespace "var-expansion-6666"
    Jan 17 16:00:23.600: INFO: Wait up to 5m0s for pod "var-expansion-2d31878e-c1b1-4305-aab2-2f3f7d0b3d96" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan 17 16:00:27.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-6666" for this suite. 01/17/23 16:00:27.619
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:00:27.626
Jan 17 16:00:27.626: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename containers 01/17/23 16:00:27.627
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:00:27.659
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:00:27.662
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58
STEP: Creating a pod to test override arguments 01/17/23 16:00:27.664
Jan 17 16:00:27.705: INFO: Waiting up to 5m0s for pod "client-containers-e818a058-976b-4fb8-9945-1c0193578dfa" in namespace "containers-9558" to be "Succeeded or Failed"
Jan 17 16:00:27.708: INFO: Pod "client-containers-e818a058-976b-4fb8-9945-1c0193578dfa": Phase="Pending", Reason="", readiness=false. Elapsed: 3.356606ms
Jan 17 16:00:29.712: INFO: Pod "client-containers-e818a058-976b-4fb8-9945-1c0193578dfa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006860966s
Jan 17 16:00:31.712: INFO: Pod "client-containers-e818a058-976b-4fb8-9945-1c0193578dfa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007539178s
STEP: Saw pod success 01/17/23 16:00:31.713
Jan 17 16:00:31.713: INFO: Pod "client-containers-e818a058-976b-4fb8-9945-1c0193578dfa" satisfied condition "Succeeded or Failed"
Jan 17 16:00:31.716: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod client-containers-e818a058-976b-4fb8-9945-1c0193578dfa container agnhost-container: <nil>
STEP: delete the pod 01/17/23 16:00:31.724
Jan 17 16:00:31.736: INFO: Waiting for pod client-containers-e818a058-976b-4fb8-9945-1c0193578dfa to disappear
Jan 17 16:00:31.739: INFO: Pod client-containers-e818a058-976b-4fb8-9945-1c0193578dfa no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Jan 17 16:00:31.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9558" for this suite. 01/17/23 16:00:31.744
{"msg":"PASSED [sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]","completed":276,"skipped":4967,"failed":0}
------------------------------
• [4.125 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:00:27.626
    Jan 17 16:00:27.626: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename containers 01/17/23 16:00:27.627
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:00:27.659
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:00:27.662
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:58
    STEP: Creating a pod to test override arguments 01/17/23 16:00:27.664
    Jan 17 16:00:27.705: INFO: Waiting up to 5m0s for pod "client-containers-e818a058-976b-4fb8-9945-1c0193578dfa" in namespace "containers-9558" to be "Succeeded or Failed"
    Jan 17 16:00:27.708: INFO: Pod "client-containers-e818a058-976b-4fb8-9945-1c0193578dfa": Phase="Pending", Reason="", readiness=false. Elapsed: 3.356606ms
    Jan 17 16:00:29.712: INFO: Pod "client-containers-e818a058-976b-4fb8-9945-1c0193578dfa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006860966s
    Jan 17 16:00:31.712: INFO: Pod "client-containers-e818a058-976b-4fb8-9945-1c0193578dfa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007539178s
    STEP: Saw pod success 01/17/23 16:00:31.713
    Jan 17 16:00:31.713: INFO: Pod "client-containers-e818a058-976b-4fb8-9945-1c0193578dfa" satisfied condition "Succeeded or Failed"
    Jan 17 16:00:31.716: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod client-containers-e818a058-976b-4fb8-9945-1c0193578dfa container agnhost-container: <nil>
    STEP: delete the pod 01/17/23 16:00:31.724
    Jan 17 16:00:31.736: INFO: Waiting for pod client-containers-e818a058-976b-4fb8-9945-1c0193578dfa to disappear
    Jan 17 16:00:31.739: INFO: Pod client-containers-e818a058-976b-4fb8-9945-1c0193578dfa no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Jan 17 16:00:31.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-9558" for this suite. 01/17/23 16:00:31.744
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:00:31.751
Jan 17 16:00:31.751: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename webhook 01/17/23 16:00:31.752
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:00:31.776
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:00:31.778
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/17/23 16:00:31.808
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/17/23 16:00:32.119
STEP: Deploying the webhook pod 01/17/23 16:00:32.129
STEP: Wait for the deployment to be ready 01/17/23 16:00:32.14
Jan 17 16:00:32.147: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/17/23 16:00:34.16
STEP: Verifying the service has paired with the endpoint 01/17/23 16:00:34.172
Jan 17 16:00:35.173: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581
STEP: Listing all of the created validation webhooks 01/17/23 16:00:35.238
STEP: Creating a configMap that does not comply to the validation webhook rules 01/17/23 16:00:35.266
STEP: Deleting the collection of validation webhooks 01/17/23 16:00:35.291
STEP: Creating a configMap that does not comply to the validation webhook rules 01/17/23 16:00:35.353
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 17 16:00:35.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5302" for this suite. 01/17/23 16:00:35.368
STEP: Destroying namespace "webhook-5302-markers" for this suite. 01/17/23 16:00:35.375
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","completed":277,"skipped":4974,"failed":0}
------------------------------
• [3.712 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:00:31.751
    Jan 17 16:00:31.751: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename webhook 01/17/23 16:00:31.752
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:00:31.776
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:00:31.778
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/17/23 16:00:31.808
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/17/23 16:00:32.119
    STEP: Deploying the webhook pod 01/17/23 16:00:32.129
    STEP: Wait for the deployment to be ready 01/17/23 16:00:32.14
    Jan 17 16:00:32.147: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/17/23 16:00:34.16
    STEP: Verifying the service has paired with the endpoint 01/17/23 16:00:34.172
    Jan 17 16:00:35.173: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:581
    STEP: Listing all of the created validation webhooks 01/17/23 16:00:35.238
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/17/23 16:00:35.266
    STEP: Deleting the collection of validation webhooks 01/17/23 16:00:35.291
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/17/23 16:00:35.353
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 17 16:00:35.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-5302" for this suite. 01/17/23 16:00:35.368
    STEP: Destroying namespace "webhook-5302-markers" for this suite. 01/17/23 16:00:35.375
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:00:35.465
Jan 17 16:00:35.465: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename namespaces 01/17/23 16:00:35.466
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:00:35.506
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:00:35.509
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267
STEP: creating a Namespace 01/17/23 16:00:35.511
STEP: patching the Namespace 01/17/23 16:00:35.547
STEP: get the Namespace and ensuring it has the label 01/17/23 16:00:35.568
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Jan 17 16:00:35.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-5320" for this suite. 01/17/23 16:00:35.577
STEP: Destroying namespace "nspatchtest-5034a007-648a-4b56-bc70-9f243785da9a-9739" for this suite. 01/17/23 16:00:35.587
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","completed":278,"skipped":5058,"failed":0}
------------------------------
• [0.134 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:00:35.465
    Jan 17 16:00:35.465: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename namespaces 01/17/23 16:00:35.466
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:00:35.506
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:00:35.509
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:267
    STEP: creating a Namespace 01/17/23 16:00:35.511
    STEP: patching the Namespace 01/17/23 16:00:35.547
    STEP: get the Namespace and ensuring it has the label 01/17/23 16:00:35.568
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Jan 17 16:00:35.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-5320" for this suite. 01/17/23 16:00:35.577
    STEP: Destroying namespace "nspatchtest-5034a007-648a-4b56-bc70-9f243785da9a-9739" for this suite. 01/17/23 16:00:35.587
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:00:35.599
Jan 17 16:00:35.599: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename crd-publish-openapi 01/17/23 16:00:35.6
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:00:35.653
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:00:35.655
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 01/17/23 16:00:35.659
Jan 17 16:00:35.659: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 01/17/23 16:00:57.938
Jan 17 16:00:57.939: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
Jan 17 16:01:04.457: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 17 16:01:28.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2002" for this suite. 01/17/23 16:01:28.111
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","completed":279,"skipped":5060,"failed":0}
------------------------------
• [SLOW TEST] [52.518 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:00:35.599
    Jan 17 16:00:35.599: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename crd-publish-openapi 01/17/23 16:00:35.6
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:00:35.653
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:00:35.655
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:308
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 01/17/23 16:00:35.659
    Jan 17 16:00:35.659: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 01/17/23 16:00:57.938
    Jan 17 16:00:57.939: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    Jan 17 16:01:04.457: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 17 16:01:28.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-2002" for this suite. 01/17/23 16:01:28.111
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:01:28.118
Jan 17 16:01:28.118: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 01/17/23 16:01:28.118
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:01:28.15
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:01:28.153
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 01/17/23 16:01:28.157
STEP: Creating hostNetwork=false pod 01/17/23 16:01:28.157
Jan 17 16:01:28.176: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-9701" to be "running and ready"
Jan 17 16:01:28.190: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 14.181839ms
Jan 17 16:01:28.190: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 17 16:01:30.195: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.01904393s
Jan 17 16:01:30.195: INFO: The phase of Pod test-pod is Running (Ready = true)
Jan 17 16:01:30.195: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 01/17/23 16:01:30.198
Jan 17 16:01:30.211: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-9701" to be "running and ready"
Jan 17 16:01:30.214: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.872117ms
Jan 17 16:01:30.214: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 17 16:01:32.238: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.026803802s
Jan 17 16:01:32.238: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
Jan 17 16:01:32.238: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 01/17/23 16:01:32.274
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 01/17/23 16:01:32.274
Jan 17 16:01:32.275: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9701 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 16:01:32.275: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
Jan 17 16:01:32.275: INFO: ExecWithOptions: Clientset creation
Jan 17 16:01:32.275: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9701/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 17 16:01:32.425: INFO: Exec stderr: ""
Jan 17 16:01:32.425: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9701 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 16:01:32.425: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
Jan 17 16:01:32.425: INFO: ExecWithOptions: Clientset creation
Jan 17 16:01:32.425: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9701/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 17 16:01:32.474: INFO: Exec stderr: ""
Jan 17 16:01:32.474: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9701 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 16:01:32.474: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
Jan 17 16:01:32.474: INFO: ExecWithOptions: Clientset creation
Jan 17 16:01:32.474: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9701/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 17 16:01:32.525: INFO: Exec stderr: ""
Jan 17 16:01:32.525: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9701 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 16:01:32.525: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
Jan 17 16:01:32.525: INFO: ExecWithOptions: Clientset creation
Jan 17 16:01:32.525: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9701/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 17 16:01:32.643: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 01/17/23 16:01:32.643
Jan 17 16:01:32.643: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9701 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 16:01:32.643: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
Jan 17 16:01:32.643: INFO: ExecWithOptions: Clientset creation
Jan 17 16:01:32.643: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9701/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Jan 17 16:01:32.691: INFO: Exec stderr: ""
Jan 17 16:01:32.691: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9701 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 16:01:32.691: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
Jan 17 16:01:32.691: INFO: ExecWithOptions: Clientset creation
Jan 17 16:01:32.691: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9701/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Jan 17 16:01:32.737: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 01/17/23 16:01:32.737
Jan 17 16:01:32.737: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9701 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 16:01:32.737: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
Jan 17 16:01:32.738: INFO: ExecWithOptions: Clientset creation
Jan 17 16:01:32.738: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9701/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 17 16:01:32.865: INFO: Exec stderr: ""
Jan 17 16:01:32.865: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9701 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 16:01:32.865: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
Jan 17 16:01:32.865: INFO: ExecWithOptions: Clientset creation
Jan 17 16:01:32.865: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9701/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 17 16:01:32.919: INFO: Exec stderr: ""
Jan 17 16:01:32.919: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9701 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 16:01:32.919: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
Jan 17 16:01:32.920: INFO: ExecWithOptions: Clientset creation
Jan 17 16:01:32.920: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9701/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 17 16:01:32.968: INFO: Exec stderr: ""
Jan 17 16:01:32.968: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9701 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 16:01:32.968: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
Jan 17 16:01:32.969: INFO: ExecWithOptions: Clientset creation
Jan 17 16:01:32.969: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9701/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 17 16:01:33.016: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:187
Jan 17 16:01:33.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-9701" for this suite. 01/17/23 16:01:33.024
{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","completed":280,"skipped":5073,"failed":0}
------------------------------
• [4.913 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:01:28.118
    Jan 17 16:01:28.118: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 01/17/23 16:01:28.118
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:01:28.15
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:01:28.153
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 01/17/23 16:01:28.157
    STEP: Creating hostNetwork=false pod 01/17/23 16:01:28.157
    Jan 17 16:01:28.176: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-9701" to be "running and ready"
    Jan 17 16:01:28.190: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 14.181839ms
    Jan 17 16:01:28.190: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 16:01:30.195: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.01904393s
    Jan 17 16:01:30.195: INFO: The phase of Pod test-pod is Running (Ready = true)
    Jan 17 16:01:30.195: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 01/17/23 16:01:30.198
    Jan 17 16:01:30.211: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-9701" to be "running and ready"
    Jan 17 16:01:30.214: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.872117ms
    Jan 17 16:01:30.214: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 16:01:32.238: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.026803802s
    Jan 17 16:01:32.238: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    Jan 17 16:01:32.238: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 01/17/23 16:01:32.274
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 01/17/23 16:01:32.274
    Jan 17 16:01:32.275: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9701 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 17 16:01:32.275: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    Jan 17 16:01:32.275: INFO: ExecWithOptions: Clientset creation
    Jan 17 16:01:32.275: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9701/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan 17 16:01:32.425: INFO: Exec stderr: ""
    Jan 17 16:01:32.425: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9701 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 17 16:01:32.425: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    Jan 17 16:01:32.425: INFO: ExecWithOptions: Clientset creation
    Jan 17 16:01:32.425: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9701/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan 17 16:01:32.474: INFO: Exec stderr: ""
    Jan 17 16:01:32.474: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9701 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 17 16:01:32.474: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    Jan 17 16:01:32.474: INFO: ExecWithOptions: Clientset creation
    Jan 17 16:01:32.474: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9701/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan 17 16:01:32.525: INFO: Exec stderr: ""
    Jan 17 16:01:32.525: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9701 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 17 16:01:32.525: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    Jan 17 16:01:32.525: INFO: ExecWithOptions: Clientset creation
    Jan 17 16:01:32.525: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9701/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan 17 16:01:32.643: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 01/17/23 16:01:32.643
    Jan 17 16:01:32.643: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9701 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 17 16:01:32.643: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    Jan 17 16:01:32.643: INFO: ExecWithOptions: Clientset creation
    Jan 17 16:01:32.643: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9701/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Jan 17 16:01:32.691: INFO: Exec stderr: ""
    Jan 17 16:01:32.691: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9701 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 17 16:01:32.691: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    Jan 17 16:01:32.691: INFO: ExecWithOptions: Clientset creation
    Jan 17 16:01:32.691: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9701/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Jan 17 16:01:32.737: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 01/17/23 16:01:32.737
    Jan 17 16:01:32.737: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9701 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 17 16:01:32.737: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    Jan 17 16:01:32.738: INFO: ExecWithOptions: Clientset creation
    Jan 17 16:01:32.738: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9701/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan 17 16:01:32.865: INFO: Exec stderr: ""
    Jan 17 16:01:32.865: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9701 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 17 16:01:32.865: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    Jan 17 16:01:32.865: INFO: ExecWithOptions: Clientset creation
    Jan 17 16:01:32.865: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9701/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan 17 16:01:32.919: INFO: Exec stderr: ""
    Jan 17 16:01:32.919: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9701 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 17 16:01:32.919: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    Jan 17 16:01:32.920: INFO: ExecWithOptions: Clientset creation
    Jan 17 16:01:32.920: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9701/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan 17 16:01:32.968: INFO: Exec stderr: ""
    Jan 17 16:01:32.968: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9701 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 17 16:01:32.968: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    Jan 17 16:01:32.969: INFO: ExecWithOptions: Clientset creation
    Jan 17 16:01:32.969: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9701/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan 17 16:01:33.016: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/framework.go:187
    Jan 17 16:01:33.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-9701" for this suite. 01/17/23 16:01:33.024
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:01:33.031
Jan 17 16:01:33.031: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename downward-api 01/17/23 16:01:33.032
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:01:33.056
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:01:33.059
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216
STEP: Creating a pod to test downward api env vars 01/17/23 16:01:33.062
Jan 17 16:01:33.091: INFO: Waiting up to 5m0s for pod "downward-api-fb488d9e-548d-4bbd-8084-fc7a8caa4918" in namespace "downward-api-4630" to be "Succeeded or Failed"
Jan 17 16:01:33.097: INFO: Pod "downward-api-fb488d9e-548d-4bbd-8084-fc7a8caa4918": Phase="Pending", Reason="", readiness=false. Elapsed: 6.004599ms
Jan 17 16:01:35.103: INFO: Pod "downward-api-fb488d9e-548d-4bbd-8084-fc7a8caa4918": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011640437s
Jan 17 16:01:37.102: INFO: Pod "downward-api-fb488d9e-548d-4bbd-8084-fc7a8caa4918": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010592388s
STEP: Saw pod success 01/17/23 16:01:37.102
Jan 17 16:01:37.102: INFO: Pod "downward-api-fb488d9e-548d-4bbd-8084-fc7a8caa4918" satisfied condition "Succeeded or Failed"
Jan 17 16:01:37.105: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod downward-api-fb488d9e-548d-4bbd-8084-fc7a8caa4918 container dapi-container: <nil>
STEP: delete the pod 01/17/23 16:01:37.111
Jan 17 16:01:37.124: INFO: Waiting for pod downward-api-fb488d9e-548d-4bbd-8084-fc7a8caa4918 to disappear
Jan 17 16:01:37.127: INFO: Pod downward-api-fb488d9e-548d-4bbd-8084-fc7a8caa4918 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Jan 17 16:01:37.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4630" for this suite. 01/17/23 16:01:37.132
{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","completed":281,"skipped":5079,"failed":0}
------------------------------
• [4.106 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:01:33.031
    Jan 17 16:01:33.031: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename downward-api 01/17/23 16:01:33.032
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:01:33.056
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:01:33.059
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:216
    STEP: Creating a pod to test downward api env vars 01/17/23 16:01:33.062
    Jan 17 16:01:33.091: INFO: Waiting up to 5m0s for pod "downward-api-fb488d9e-548d-4bbd-8084-fc7a8caa4918" in namespace "downward-api-4630" to be "Succeeded or Failed"
    Jan 17 16:01:33.097: INFO: Pod "downward-api-fb488d9e-548d-4bbd-8084-fc7a8caa4918": Phase="Pending", Reason="", readiness=false. Elapsed: 6.004599ms
    Jan 17 16:01:35.103: INFO: Pod "downward-api-fb488d9e-548d-4bbd-8084-fc7a8caa4918": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011640437s
    Jan 17 16:01:37.102: INFO: Pod "downward-api-fb488d9e-548d-4bbd-8084-fc7a8caa4918": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010592388s
    STEP: Saw pod success 01/17/23 16:01:37.102
    Jan 17 16:01:37.102: INFO: Pod "downward-api-fb488d9e-548d-4bbd-8084-fc7a8caa4918" satisfied condition "Succeeded or Failed"
    Jan 17 16:01:37.105: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod downward-api-fb488d9e-548d-4bbd-8084-fc7a8caa4918 container dapi-container: <nil>
    STEP: delete the pod 01/17/23 16:01:37.111
    Jan 17 16:01:37.124: INFO: Waiting for pod downward-api-fb488d9e-548d-4bbd-8084-fc7a8caa4918 to disappear
    Jan 17 16:01:37.127: INFO: Pod downward-api-fb488d9e-548d-4bbd-8084-fc7a8caa4918 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Jan 17 16:01:37.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-4630" for this suite. 01/17/23 16:01:37.132
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:01:37.138
Jan 17 16:01:37.138: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename webhook 01/17/23 16:01:37.139
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:01:37.163
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:01:37.167
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/17/23 16:01:37.199
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/17/23 16:01:37.542
STEP: Deploying the webhook pod 01/17/23 16:01:37.552
STEP: Wait for the deployment to be ready 01/17/23 16:01:37.562
Jan 17 16:01:37.570: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/17/23 16:01:39.602
STEP: Verifying the service has paired with the endpoint 01/17/23 16:01:39.644
Jan 17 16:01:40.644: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340
Jan 17 16:01:40.647: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9218-crds.webhook.example.com via the AdmissionRegistration API 01/17/23 16:01:41.158
STEP: Creating a custom resource that should be mutated by the webhook 01/17/23 16:01:41.171
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 17 16:01:43.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8078" for this suite. 01/17/23 16:01:43.748
STEP: Destroying namespace "webhook-8078-markers" for this suite. 01/17/23 16:01:43.76
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","completed":282,"skipped":5101,"failed":0}
------------------------------
• [SLOW TEST] [6.738 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:01:37.138
    Jan 17 16:01:37.138: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename webhook 01/17/23 16:01:37.139
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:01:37.163
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:01:37.167
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/17/23 16:01:37.199
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/17/23 16:01:37.542
    STEP: Deploying the webhook pod 01/17/23 16:01:37.552
    STEP: Wait for the deployment to be ready 01/17/23 16:01:37.562
    Jan 17 16:01:37.570: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/17/23 16:01:39.602
    STEP: Verifying the service has paired with the endpoint 01/17/23 16:01:39.644
    Jan 17 16:01:40.644: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:340
    Jan 17 16:01:40.647: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9218-crds.webhook.example.com via the AdmissionRegistration API 01/17/23 16:01:41.158
    STEP: Creating a custom resource that should be mutated by the webhook 01/17/23 16:01:41.171
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 17 16:01:43.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-8078" for this suite. 01/17/23 16:01:43.748
    STEP: Destroying namespace "webhook-8078-markers" for this suite. 01/17/23 16:01:43.76
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:01:43.876
Jan 17 16:01:43.876: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename kubelet-test 01/17/23 16:01:43.877
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:01:43.979
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:01:43.983
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
Jan 17 16:01:44.038: INFO: Waiting up to 5m0s for pod "busybox-readonly-fsb2800c47-72c7-47f4-a908-2ceb920f6c11" in namespace "kubelet-test-6682" to be "running and ready"
Jan 17 16:01:44.054: INFO: Pod "busybox-readonly-fsb2800c47-72c7-47f4-a908-2ceb920f6c11": Phase="Pending", Reason="", readiness=false. Elapsed: 16.08222ms
Jan 17 16:01:44.054: INFO: The phase of Pod busybox-readonly-fsb2800c47-72c7-47f4-a908-2ceb920f6c11 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 16:01:46.060: INFO: Pod "busybox-readonly-fsb2800c47-72c7-47f4-a908-2ceb920f6c11": Phase="Running", Reason="", readiness=true. Elapsed: 2.021657691s
Jan 17 16:01:46.060: INFO: The phase of Pod busybox-readonly-fsb2800c47-72c7-47f4-a908-2ceb920f6c11 is Running (Ready = true)
Jan 17 16:01:46.060: INFO: Pod "busybox-readonly-fsb2800c47-72c7-47f4-a908-2ceb920f6c11" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Jan 17 16:01:46.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6682" for this suite. 01/17/23 16:01:46.073
{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","completed":283,"skipped":5103,"failed":0}
------------------------------
• [2.203 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:01:43.876
    Jan 17 16:01:43.876: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename kubelet-test 01/17/23 16:01:43.877
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:01:43.979
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:01:43.983
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    Jan 17 16:01:44.038: INFO: Waiting up to 5m0s for pod "busybox-readonly-fsb2800c47-72c7-47f4-a908-2ceb920f6c11" in namespace "kubelet-test-6682" to be "running and ready"
    Jan 17 16:01:44.054: INFO: Pod "busybox-readonly-fsb2800c47-72c7-47f4-a908-2ceb920f6c11": Phase="Pending", Reason="", readiness=false. Elapsed: 16.08222ms
    Jan 17 16:01:44.054: INFO: The phase of Pod busybox-readonly-fsb2800c47-72c7-47f4-a908-2ceb920f6c11 is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 16:01:46.060: INFO: Pod "busybox-readonly-fsb2800c47-72c7-47f4-a908-2ceb920f6c11": Phase="Running", Reason="", readiness=true. Elapsed: 2.021657691s
    Jan 17 16:01:46.060: INFO: The phase of Pod busybox-readonly-fsb2800c47-72c7-47f4-a908-2ceb920f6c11 is Running (Ready = true)
    Jan 17 16:01:46.060: INFO: Pod "busybox-readonly-fsb2800c47-72c7-47f4-a908-2ceb920f6c11" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Jan 17 16:01:46.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-6682" for this suite. 01/17/23 16:01:46.073
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:01:46.08
Jan 17 16:01:46.080: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename resourcequota 01/17/23 16:01:46.08
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:01:46.104
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:01:46.11
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438
STEP: Counting existing ResourceQuota 01/17/23 16:01:46.116
STEP: Creating a ResourceQuota 01/17/23 16:01:51.12
STEP: Ensuring resource quota status is calculated 01/17/23 16:01:51.125
STEP: Creating a ReplicaSet 01/17/23 16:01:53.129
STEP: Ensuring resource quota status captures replicaset creation 01/17/23 16:01:53.154
STEP: Deleting a ReplicaSet 01/17/23 16:01:55.159
STEP: Ensuring resource quota status released usage 01/17/23 16:01:55.164
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 17 16:01:57.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7532" for this suite. 01/17/23 16:01:57.174
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","completed":284,"skipped":5114,"failed":0}
------------------------------
• [SLOW TEST] [11.102 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:01:46.08
    Jan 17 16:01:46.080: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename resourcequota 01/17/23 16:01:46.08
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:01:46.104
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:01:46.11
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:438
    STEP: Counting existing ResourceQuota 01/17/23 16:01:46.116
    STEP: Creating a ResourceQuota 01/17/23 16:01:51.12
    STEP: Ensuring resource quota status is calculated 01/17/23 16:01:51.125
    STEP: Creating a ReplicaSet 01/17/23 16:01:53.129
    STEP: Ensuring resource quota status captures replicaset creation 01/17/23 16:01:53.154
    STEP: Deleting a ReplicaSet 01/17/23 16:01:55.159
    STEP: Ensuring resource quota status released usage 01/17/23 16:01:55.164
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 17 16:01:57.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-7532" for this suite. 01/17/23 16:01:57.174
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:01:57.182
Jan 17 16:01:57.182: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename disruption 01/17/23 16:01:57.183
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:01:57.213
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:01:57.216
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346
STEP: Creating a pdb that targets all three pods in a test replica set 01/17/23 16:01:57.218
STEP: Waiting for the pdb to be processed 01/17/23 16:01:57.225
STEP: First trying to evict a pod which shouldn't be evictable 01/17/23 16:01:59.238
STEP: Waiting for all pods to be running 01/17/23 16:01:59.238
Jan 17 16:01:59.241: INFO: pods: 0 < 3
STEP: locating a running pod 01/17/23 16:02:01.247
STEP: Updating the pdb to allow a pod to be evicted 01/17/23 16:02:01.259
STEP: Waiting for the pdb to be processed 01/17/23 16:02:01.267
STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/17/23 16:02:03.273
STEP: Waiting for all pods to be running 01/17/23 16:02:03.273
STEP: Waiting for the pdb to observed all healthy pods 01/17/23 16:02:03.277
STEP: Patching the pdb to disallow a pod to be evicted 01/17/23 16:02:03.295
STEP: Waiting for the pdb to be processed 01/17/23 16:02:03.311
STEP: Waiting for all pods to be running 01/17/23 16:02:05.323
STEP: locating a running pod 01/17/23 16:02:05.327
STEP: Deleting the pdb to allow a pod to be evicted 01/17/23 16:02:05.336
STEP: Waiting for the pdb to be deleted 01/17/23 16:02:05.342
STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/17/23 16:02:05.344
STEP: Waiting for all pods to be running 01/17/23 16:02:05.344
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Jan 17 16:02:05.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-7952" for this suite. 01/17/23 16:02:05.363
{"msg":"PASSED [sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]","completed":285,"skipped":5128,"failed":0}
------------------------------
• [SLOW TEST] [8.189 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:01:57.182
    Jan 17 16:01:57.182: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename disruption 01/17/23 16:01:57.183
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:01:57.213
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:01:57.216
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:346
    STEP: Creating a pdb that targets all three pods in a test replica set 01/17/23 16:01:57.218
    STEP: Waiting for the pdb to be processed 01/17/23 16:01:57.225
    STEP: First trying to evict a pod which shouldn't be evictable 01/17/23 16:01:59.238
    STEP: Waiting for all pods to be running 01/17/23 16:01:59.238
    Jan 17 16:01:59.241: INFO: pods: 0 < 3
    STEP: locating a running pod 01/17/23 16:02:01.247
    STEP: Updating the pdb to allow a pod to be evicted 01/17/23 16:02:01.259
    STEP: Waiting for the pdb to be processed 01/17/23 16:02:01.267
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/17/23 16:02:03.273
    STEP: Waiting for all pods to be running 01/17/23 16:02:03.273
    STEP: Waiting for the pdb to observed all healthy pods 01/17/23 16:02:03.277
    STEP: Patching the pdb to disallow a pod to be evicted 01/17/23 16:02:03.295
    STEP: Waiting for the pdb to be processed 01/17/23 16:02:03.311
    STEP: Waiting for all pods to be running 01/17/23 16:02:05.323
    STEP: locating a running pod 01/17/23 16:02:05.327
    STEP: Deleting the pdb to allow a pod to be evicted 01/17/23 16:02:05.336
    STEP: Waiting for the pdb to be deleted 01/17/23 16:02:05.342
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/17/23 16:02:05.344
    STEP: Waiting for all pods to be running 01/17/23 16:02:05.344
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Jan 17 16:02:05.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-7952" for this suite. 01/17/23 16:02:05.363
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:02:05.371
Jan 17 16:02:05.371: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename prestop 01/17/23 16:02:05.372
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:02:05.401
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:02:05.406
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-7414 01/17/23 16:02:05.409
STEP: Waiting for pods to come up. 01/17/23 16:02:05.433
Jan 17 16:02:05.433: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-7414" to be "running"
Jan 17 16:02:05.438: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 5.032891ms
Jan 17 16:02:07.442: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.00908819s
Jan 17 16:02:07.442: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-7414 01/17/23 16:02:07.445
Jan 17 16:02:07.458: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-7414" to be "running"
Jan 17 16:02:07.461: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 3.509126ms
Jan 17 16:02:09.466: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.007902721s
Jan 17 16:02:09.466: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 01/17/23 16:02:09.466
Jan 17 16:02:14.479: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 01/17/23 16:02:14.479
[AfterEach] [sig-node] PreStop
  test/e2e/framework/framework.go:187
Jan 17 16:02:14.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-7414" for this suite. 01/17/23 16:02:14.498
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","completed":286,"skipped":5132,"failed":0}
------------------------------
• [SLOW TEST] [9.133 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:02:05.371
    Jan 17 16:02:05.371: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename prestop 01/17/23 16:02:05.372
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:02:05.401
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:02:05.406
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-7414 01/17/23 16:02:05.409
    STEP: Waiting for pods to come up. 01/17/23 16:02:05.433
    Jan 17 16:02:05.433: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-7414" to be "running"
    Jan 17 16:02:05.438: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 5.032891ms
    Jan 17 16:02:07.442: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.00908819s
    Jan 17 16:02:07.442: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-7414 01/17/23 16:02:07.445
    Jan 17 16:02:07.458: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-7414" to be "running"
    Jan 17 16:02:07.461: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 3.509126ms
    Jan 17 16:02:09.466: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.007902721s
    Jan 17 16:02:09.466: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 01/17/23 16:02:09.466
    Jan 17 16:02:14.479: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 01/17/23 16:02:14.479
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/framework.go:187
    Jan 17 16:02:14.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "prestop-7414" for this suite. 01/17/23 16:02:14.498
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:02:14.505
Jan 17 16:02:14.505: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename projected 01/17/23 16:02:14.506
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:02:14.539
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:02:14.542
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206
STEP: Creating a pod to test downward API volume plugin 01/17/23 16:02:14.546
Jan 17 16:02:14.567: INFO: Waiting up to 5m0s for pod "downwardapi-volume-491d9dba-6c75-412c-bec6-500df06b9b76" in namespace "projected-4581" to be "Succeeded or Failed"
Jan 17 16:02:14.582: INFO: Pod "downwardapi-volume-491d9dba-6c75-412c-bec6-500df06b9b76": Phase="Pending", Reason="", readiness=false. Elapsed: 15.235871ms
Jan 17 16:02:16.587: INFO: Pod "downwardapi-volume-491d9dba-6c75-412c-bec6-500df06b9b76": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019990702s
Jan 17 16:02:18.585: INFO: Pod "downwardapi-volume-491d9dba-6c75-412c-bec6-500df06b9b76": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018615786s
STEP: Saw pod success 01/17/23 16:02:18.585
Jan 17 16:02:18.585: INFO: Pod "downwardapi-volume-491d9dba-6c75-412c-bec6-500df06b9b76" satisfied condition "Succeeded or Failed"
Jan 17 16:02:18.589: INFO: Trying to get logs from node ip-10-0-139-213.ec2.internal pod downwardapi-volume-491d9dba-6c75-412c-bec6-500df06b9b76 container client-container: <nil>
STEP: delete the pod 01/17/23 16:02:18.601
Jan 17 16:02:18.621: INFO: Waiting for pod downwardapi-volume-491d9dba-6c75-412c-bec6-500df06b9b76 to disappear
Jan 17 16:02:18.629: INFO: Pod downwardapi-volume-491d9dba-6c75-412c-bec6-500df06b9b76 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 17 16:02:18.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4581" for this suite. 01/17/23 16:02:18.639
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","completed":287,"skipped":5142,"failed":0}
------------------------------
• [4.140 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:02:14.505
    Jan 17 16:02:14.505: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename projected 01/17/23 16:02:14.506
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:02:14.539
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:02:14.542
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:206
    STEP: Creating a pod to test downward API volume plugin 01/17/23 16:02:14.546
    Jan 17 16:02:14.567: INFO: Waiting up to 5m0s for pod "downwardapi-volume-491d9dba-6c75-412c-bec6-500df06b9b76" in namespace "projected-4581" to be "Succeeded or Failed"
    Jan 17 16:02:14.582: INFO: Pod "downwardapi-volume-491d9dba-6c75-412c-bec6-500df06b9b76": Phase="Pending", Reason="", readiness=false. Elapsed: 15.235871ms
    Jan 17 16:02:16.587: INFO: Pod "downwardapi-volume-491d9dba-6c75-412c-bec6-500df06b9b76": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019990702s
    Jan 17 16:02:18.585: INFO: Pod "downwardapi-volume-491d9dba-6c75-412c-bec6-500df06b9b76": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018615786s
    STEP: Saw pod success 01/17/23 16:02:18.585
    Jan 17 16:02:18.585: INFO: Pod "downwardapi-volume-491d9dba-6c75-412c-bec6-500df06b9b76" satisfied condition "Succeeded or Failed"
    Jan 17 16:02:18.589: INFO: Trying to get logs from node ip-10-0-139-213.ec2.internal pod downwardapi-volume-491d9dba-6c75-412c-bec6-500df06b9b76 container client-container: <nil>
    STEP: delete the pod 01/17/23 16:02:18.601
    Jan 17 16:02:18.621: INFO: Waiting for pod downwardapi-volume-491d9dba-6c75-412c-bec6-500df06b9b76 to disappear
    Jan 17 16:02:18.629: INFO: Pod downwardapi-volume-491d9dba-6c75-412c-bec6-500df06b9b76 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 17 16:02:18.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4581" for this suite. 01/17/23 16:02:18.639
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:906
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:02:18.646
Jan 17 16:02:18.646: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename statefulset 01/17/23 16:02:18.647
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:02:18.669
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:02:18.673
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-1004 01/17/23 16:02:18.675
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:906
Jan 17 16:02:18.725: INFO: Found 0 stateful pods, waiting for 1
Jan 17 16:02:28.730: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 01/17/23 16:02:28.736
W0117 16:02:28.747287      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jan 17 16:02:28.753: INFO: Found 1 stateful pods, waiting for 2
Jan 17 16:02:38.758: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 17 16:02:38.758: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 01/17/23 16:02:38.764
STEP: Delete all of the StatefulSets 01/17/23 16:02:38.768
STEP: Verify that StatefulSets have been deleted 01/17/23 16:02:38.776
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 17 16:02:38.779: INFO: Deleting all statefulset in ns statefulset-1004
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan 17 16:02:38.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1004" for this suite. 01/17/23 16:02:38.807
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]","completed":288,"skipped":5167,"failed":0}
------------------------------
• [SLOW TEST] [20.167 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:906

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:02:18.646
    Jan 17 16:02:18.646: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename statefulset 01/17/23 16:02:18.647
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:02:18.669
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:02:18.673
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-1004 01/17/23 16:02:18.675
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:906
    Jan 17 16:02:18.725: INFO: Found 0 stateful pods, waiting for 1
    Jan 17 16:02:28.730: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 01/17/23 16:02:28.736
    W0117 16:02:28.747287      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jan 17 16:02:28.753: INFO: Found 1 stateful pods, waiting for 2
    Jan 17 16:02:38.758: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 17 16:02:38.758: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 01/17/23 16:02:38.764
    STEP: Delete all of the StatefulSets 01/17/23 16:02:38.768
    STEP: Verify that StatefulSets have been deleted 01/17/23 16:02:38.776
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan 17 16:02:38.779: INFO: Deleting all statefulset in ns statefulset-1004
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan 17 16:02:38.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-1004" for this suite. 01/17/23 16:02:38.807
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1248
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:02:38.814
Jan 17 16:02:38.814: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename kubectl 01/17/23 16:02:38.815
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:02:38.844
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:02:38.85
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1248
STEP: validating cluster-info 01/17/23 16:02:38.854
Jan 17 16:02:38.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-9359 cluster-info'
Jan 17 16:02:38.906: INFO: stderr: ""
Jan 17 16:02:38.906: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://172.30.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 17 16:02:38.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9359" for this suite. 01/17/23 16:02:38.911
{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","completed":289,"skipped":5170,"failed":0}
------------------------------
• [0.108 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1242
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:02:38.814
    Jan 17 16:02:38.814: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename kubectl 01/17/23 16:02:38.815
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:02:38.844
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:02:38.85
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1248
    STEP: validating cluster-info 01/17/23 16:02:38.854
    Jan 17 16:02:38.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-9359 cluster-info'
    Jan 17 16:02:38.906: INFO: stderr: ""
    Jan 17 16:02:38.906: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://172.30.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 17 16:02:38.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-9359" for this suite. 01/17/23 16:02:38.911
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:02:38.923
Jan 17 16:02:38.923: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename subpath 01/17/23 16:02:38.923
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:02:38.956
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:02:38.96
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/17/23 16:02:38.962
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-pqfn 01/17/23 16:02:38.977
STEP: Creating a pod to test atomic-volume-subpath 01/17/23 16:02:38.977
Jan 17 16:02:39.010: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-pqfn" in namespace "subpath-2119" to be "Succeeded or Failed"
Jan 17 16:02:39.027: INFO: Pod "pod-subpath-test-secret-pqfn": Phase="Pending", Reason="", readiness=false. Elapsed: 16.697883ms
Jan 17 16:02:41.035: INFO: Pod "pod-subpath-test-secret-pqfn": Phase="Running", Reason="", readiness=true. Elapsed: 2.024117574s
Jan 17 16:02:43.031: INFO: Pod "pod-subpath-test-secret-pqfn": Phase="Running", Reason="", readiness=true. Elapsed: 4.020755473s
Jan 17 16:02:45.032: INFO: Pod "pod-subpath-test-secret-pqfn": Phase="Running", Reason="", readiness=true. Elapsed: 6.021271363s
Jan 17 16:02:47.031: INFO: Pod "pod-subpath-test-secret-pqfn": Phase="Running", Reason="", readiness=true. Elapsed: 8.020915669s
Jan 17 16:02:49.032: INFO: Pod "pod-subpath-test-secret-pqfn": Phase="Running", Reason="", readiness=true. Elapsed: 10.021344463s
Jan 17 16:02:51.032: INFO: Pod "pod-subpath-test-secret-pqfn": Phase="Running", Reason="", readiness=true. Elapsed: 12.021109972s
Jan 17 16:02:53.032: INFO: Pod "pod-subpath-test-secret-pqfn": Phase="Running", Reason="", readiness=true. Elapsed: 14.021530334s
Jan 17 16:02:55.033: INFO: Pod "pod-subpath-test-secret-pqfn": Phase="Running", Reason="", readiness=true. Elapsed: 16.02210367s
Jan 17 16:02:57.032: INFO: Pod "pod-subpath-test-secret-pqfn": Phase="Running", Reason="", readiness=true. Elapsed: 18.021737144s
Jan 17 16:02:59.031: INFO: Pod "pod-subpath-test-secret-pqfn": Phase="Running", Reason="", readiness=true. Elapsed: 20.020769911s
Jan 17 16:03:01.033: INFO: Pod "pod-subpath-test-secret-pqfn": Phase="Running", Reason="", readiness=false. Elapsed: 22.022263066s
Jan 17 16:03:03.035: INFO: Pod "pod-subpath-test-secret-pqfn": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.024690125s
STEP: Saw pod success 01/17/23 16:03:03.035
Jan 17 16:03:03.035: INFO: Pod "pod-subpath-test-secret-pqfn" satisfied condition "Succeeded or Failed"
Jan 17 16:03:03.039: INFO: Trying to get logs from node ip-10-0-165-14.ec2.internal pod pod-subpath-test-secret-pqfn container test-container-subpath-secret-pqfn: <nil>
STEP: delete the pod 01/17/23 16:03:03.052
Jan 17 16:03:03.096: INFO: Waiting for pod pod-subpath-test-secret-pqfn to disappear
Jan 17 16:03:03.099: INFO: Pod pod-subpath-test-secret-pqfn no longer exists
STEP: Deleting pod pod-subpath-test-secret-pqfn 01/17/23 16:03:03.099
Jan 17 16:03:03.099: INFO: Deleting pod "pod-subpath-test-secret-pqfn" in namespace "subpath-2119"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Jan 17 16:03:03.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2119" for this suite. 01/17/23 16:03:03.107
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]","completed":290,"skipped":5210,"failed":0}
------------------------------
• [SLOW TEST] [24.191 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:02:38.923
    Jan 17 16:02:38.923: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename subpath 01/17/23 16:02:38.923
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:02:38.956
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:02:38.96
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/17/23 16:02:38.962
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-pqfn 01/17/23 16:02:38.977
    STEP: Creating a pod to test atomic-volume-subpath 01/17/23 16:02:38.977
    Jan 17 16:02:39.010: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-pqfn" in namespace "subpath-2119" to be "Succeeded or Failed"
    Jan 17 16:02:39.027: INFO: Pod "pod-subpath-test-secret-pqfn": Phase="Pending", Reason="", readiness=false. Elapsed: 16.697883ms
    Jan 17 16:02:41.035: INFO: Pod "pod-subpath-test-secret-pqfn": Phase="Running", Reason="", readiness=true. Elapsed: 2.024117574s
    Jan 17 16:02:43.031: INFO: Pod "pod-subpath-test-secret-pqfn": Phase="Running", Reason="", readiness=true. Elapsed: 4.020755473s
    Jan 17 16:02:45.032: INFO: Pod "pod-subpath-test-secret-pqfn": Phase="Running", Reason="", readiness=true. Elapsed: 6.021271363s
    Jan 17 16:02:47.031: INFO: Pod "pod-subpath-test-secret-pqfn": Phase="Running", Reason="", readiness=true. Elapsed: 8.020915669s
    Jan 17 16:02:49.032: INFO: Pod "pod-subpath-test-secret-pqfn": Phase="Running", Reason="", readiness=true. Elapsed: 10.021344463s
    Jan 17 16:02:51.032: INFO: Pod "pod-subpath-test-secret-pqfn": Phase="Running", Reason="", readiness=true. Elapsed: 12.021109972s
    Jan 17 16:02:53.032: INFO: Pod "pod-subpath-test-secret-pqfn": Phase="Running", Reason="", readiness=true. Elapsed: 14.021530334s
    Jan 17 16:02:55.033: INFO: Pod "pod-subpath-test-secret-pqfn": Phase="Running", Reason="", readiness=true. Elapsed: 16.02210367s
    Jan 17 16:02:57.032: INFO: Pod "pod-subpath-test-secret-pqfn": Phase="Running", Reason="", readiness=true. Elapsed: 18.021737144s
    Jan 17 16:02:59.031: INFO: Pod "pod-subpath-test-secret-pqfn": Phase="Running", Reason="", readiness=true. Elapsed: 20.020769911s
    Jan 17 16:03:01.033: INFO: Pod "pod-subpath-test-secret-pqfn": Phase="Running", Reason="", readiness=false. Elapsed: 22.022263066s
    Jan 17 16:03:03.035: INFO: Pod "pod-subpath-test-secret-pqfn": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.024690125s
    STEP: Saw pod success 01/17/23 16:03:03.035
    Jan 17 16:03:03.035: INFO: Pod "pod-subpath-test-secret-pqfn" satisfied condition "Succeeded or Failed"
    Jan 17 16:03:03.039: INFO: Trying to get logs from node ip-10-0-165-14.ec2.internal pod pod-subpath-test-secret-pqfn container test-container-subpath-secret-pqfn: <nil>
    STEP: delete the pod 01/17/23 16:03:03.052
    Jan 17 16:03:03.096: INFO: Waiting for pod pod-subpath-test-secret-pqfn to disappear
    Jan 17 16:03:03.099: INFO: Pod pod-subpath-test-secret-pqfn no longer exists
    STEP: Deleting pod pod-subpath-test-secret-pqfn 01/17/23 16:03:03.099
    Jan 17 16:03:03.099: INFO: Deleting pod "pod-subpath-test-secret-pqfn" in namespace "subpath-2119"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Jan 17 16:03:03.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-2119" for this suite. 01/17/23 16:03:03.107
  << End Captured GinkgoWriter Output
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1650
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:03:03.114
Jan 17 16:03:03.114: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename kubectl 01/17/23 16:03:03.114
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:03:03.137
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:03:03.141
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1650
STEP: creating Agnhost RC 01/17/23 16:03:03.145
Jan 17 16:03:03.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-6727 create -f -'
Jan 17 16:03:04.765: INFO: stderr: ""
Jan 17 16:03:04.765: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 01/17/23 16:03:04.765
Jan 17 16:03:05.769: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 17 16:03:05.769: INFO: Found 0 / 1
Jan 17 16:03:06.769: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 17 16:03:06.769: INFO: Found 1 / 1
Jan 17 16:03:06.769: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 01/17/23 16:03:06.769
Jan 17 16:03:06.772: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 17 16:03:06.772: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 17 16:03:06.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-6727 patch pod agnhost-primary-lkk5q -p {"metadata":{"annotations":{"x":"y"}}}'
Jan 17 16:03:06.831: INFO: stderr: ""
Jan 17 16:03:06.831: INFO: stdout: "pod/agnhost-primary-lkk5q patched\n"
STEP: checking annotations 01/17/23 16:03:06.831
Jan 17 16:03:06.835: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 17 16:03:06.835: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 17 16:03:06.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6727" for this suite. 01/17/23 16:03:06.839
{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","completed":291,"skipped":5210,"failed":0}
------------------------------
• [3.732 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1644
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:03:03.114
    Jan 17 16:03:03.114: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename kubectl 01/17/23 16:03:03.114
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:03:03.137
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:03:03.141
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1650
    STEP: creating Agnhost RC 01/17/23 16:03:03.145
    Jan 17 16:03:03.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-6727 create -f -'
    Jan 17 16:03:04.765: INFO: stderr: ""
    Jan 17 16:03:04.765: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 01/17/23 16:03:04.765
    Jan 17 16:03:05.769: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 17 16:03:05.769: INFO: Found 0 / 1
    Jan 17 16:03:06.769: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 17 16:03:06.769: INFO: Found 1 / 1
    Jan 17 16:03:06.769: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 01/17/23 16:03:06.769
    Jan 17 16:03:06.772: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 17 16:03:06.772: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jan 17 16:03:06.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-6727 patch pod agnhost-primary-lkk5q -p {"metadata":{"annotations":{"x":"y"}}}'
    Jan 17 16:03:06.831: INFO: stderr: ""
    Jan 17 16:03:06.831: INFO: stdout: "pod/agnhost-primary-lkk5q patched\n"
    STEP: checking annotations 01/17/23 16:03:06.831
    Jan 17 16:03:06.835: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 17 16:03:06.835: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 17 16:03:06.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-6727" for this suite. 01/17/23 16:03:06.839
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:03:06.846
Jan 17 16:03:06.846: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename runtimeclass 01/17/23 16:03:06.847
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:03:06.868
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:03:06.871
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
Jan 17 16:03:06.907: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-3071 to be scheduled
Jan 17 16:03:06.913: INFO: 1 pods are not scheduled: [runtimeclass-3071/test-runtimeclass-runtimeclass-3071-preconfigured-handler-btjjj(cf80dc08-484c-45c2-a850-ca48eec6a37c)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Jan 17 16:03:08.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-3071" for this suite. 01/17/23 16:03:08.935
{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]","completed":292,"skipped":5219,"failed":0}
------------------------------
• [2.095 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:03:06.846
    Jan 17 16:03:06.846: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename runtimeclass 01/17/23 16:03:06.847
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:03:06.868
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:03:06.871
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    Jan 17 16:03:06.907: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-3071 to be scheduled
    Jan 17 16:03:06.913: INFO: 1 pods are not scheduled: [runtimeclass-3071/test-runtimeclass-runtimeclass-3071-preconfigured-handler-btjjj(cf80dc08-484c-45c2-a850-ca48eec6a37c)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Jan 17 16:03:08.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-3071" for this suite. 01/17/23 16:03:08.935
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:03:08.942
Jan 17 16:03:08.942: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename disruption 01/17/23 16:03:08.943
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:03:08.978
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:03:08.983
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163
STEP: Waiting for the pdb to be processed 01/17/23 16:03:08.999
STEP: Updating PodDisruptionBudget status 01/17/23 16:03:11.012
STEP: Waiting for all pods to be running 01/17/23 16:03:11.028
Jan 17 16:03:11.031: INFO: running pods: 0 < 1
STEP: locating a running pod 01/17/23 16:03:13.036
STEP: Waiting for the pdb to be processed 01/17/23 16:03:13.047
STEP: Patching PodDisruptionBudget status 01/17/23 16:03:13.055
STEP: Waiting for the pdb to be processed 01/17/23 16:03:13.063
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Jan 17 16:03:13.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-1937" for this suite. 01/17/23 16:03:13.071
{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","completed":293,"skipped":5248,"failed":0}
------------------------------
• [4.135 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:03:08.942
    Jan 17 16:03:08.942: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename disruption 01/17/23 16:03:08.943
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:03:08.978
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:03:08.983
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:163
    STEP: Waiting for the pdb to be processed 01/17/23 16:03:08.999
    STEP: Updating PodDisruptionBudget status 01/17/23 16:03:11.012
    STEP: Waiting for all pods to be running 01/17/23 16:03:11.028
    Jan 17 16:03:11.031: INFO: running pods: 0 < 1
    STEP: locating a running pod 01/17/23 16:03:13.036
    STEP: Waiting for the pdb to be processed 01/17/23 16:03:13.047
    STEP: Patching PodDisruptionBudget status 01/17/23 16:03:13.055
    STEP: Waiting for the pdb to be processed 01/17/23 16:03:13.063
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Jan 17 16:03:13.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-1937" for this suite. 01/17/23 16:03:13.071
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:03:13.077
Jan 17 16:03:13.077: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename crd-publish-openapi 01/17/23 16:03:13.078
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:03:13.103
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:03:13.108
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68
Jan 17 16:03:13.112: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 01/17/23 16:03:20.416
Jan 17 16:03:20.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=crd-publish-openapi-3803 --namespace=crd-publish-openapi-3803 create -f -'
Jan 17 16:03:21.632: INFO: stderr: ""
Jan 17 16:03:21.632: INFO: stdout: "e2e-test-crd-publish-openapi-5800-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jan 17 16:03:21.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=crd-publish-openapi-3803 --namespace=crd-publish-openapi-3803 delete e2e-test-crd-publish-openapi-5800-crds test-foo'
Jan 17 16:03:21.703: INFO: stderr: ""
Jan 17 16:03:21.703: INFO: stdout: "e2e-test-crd-publish-openapi-5800-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Jan 17 16:03:21.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=crd-publish-openapi-3803 --namespace=crd-publish-openapi-3803 apply -f -'
Jan 17 16:03:22.612: INFO: stderr: ""
Jan 17 16:03:22.612: INFO: stdout: "e2e-test-crd-publish-openapi-5800-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jan 17 16:03:22.613: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=crd-publish-openapi-3803 --namespace=crd-publish-openapi-3803 delete e2e-test-crd-publish-openapi-5800-crds test-foo'
Jan 17 16:03:22.663: INFO: stderr: ""
Jan 17 16:03:22.663: INFO: stdout: "e2e-test-crd-publish-openapi-5800-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 01/17/23 16:03:22.663
Jan 17 16:03:22.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=crd-publish-openapi-3803 --namespace=crd-publish-openapi-3803 create -f -'
Jan 17 16:03:22.910: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 01/17/23 16:03:22.91
Jan 17 16:03:22.910: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=crd-publish-openapi-3803 --namespace=crd-publish-openapi-3803 create -f -'
Jan 17 16:03:23.158: INFO: rc: 1
Jan 17 16:03:23.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=crd-publish-openapi-3803 --namespace=crd-publish-openapi-3803 apply -f -'
Jan 17 16:03:23.411: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 01/17/23 16:03:23.412
Jan 17 16:03:23.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=crd-publish-openapi-3803 --namespace=crd-publish-openapi-3803 create -f -'
Jan 17 16:03:23.675: INFO: rc: 1
Jan 17 16:03:23.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=crd-publish-openapi-3803 --namespace=crd-publish-openapi-3803 apply -f -'
Jan 17 16:03:23.929: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 01/17/23 16:03:23.929
Jan 17 16:03:23.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=crd-publish-openapi-3803 explain e2e-test-crd-publish-openapi-5800-crds'
Jan 17 16:03:24.179: INFO: stderr: ""
Jan 17 16:03:24.180: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5800-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 01/17/23 16:03:24.18
Jan 17 16:03:24.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=crd-publish-openapi-3803 explain e2e-test-crd-publish-openapi-5800-crds.metadata'
Jan 17 16:03:24.430: INFO: stderr: ""
Jan 17 16:03:24.430: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5800-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Jan 17 16:03:24.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=crd-publish-openapi-3803 explain e2e-test-crd-publish-openapi-5800-crds.spec'
Jan 17 16:03:25.579: INFO: stderr: ""
Jan 17 16:03:25.579: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5800-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Jan 17 16:03:25.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=crd-publish-openapi-3803 explain e2e-test-crd-publish-openapi-5800-crds.spec.bars'
Jan 17 16:03:25.829: INFO: stderr: ""
Jan 17 16:03:25.829: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5800-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 01/17/23 16:03:25.829
Jan 17 16:03:25.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=crd-publish-openapi-3803 explain e2e-test-crd-publish-openapi-5800-crds.spec.bars2'
Jan 17 16:03:26.077: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 17 16:03:32.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3803" for this suite. 01/17/23 16:03:32.553
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","completed":294,"skipped":5254,"failed":0}
------------------------------
• [SLOW TEST] [19.481 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:03:13.077
    Jan 17 16:03:13.077: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename crd-publish-openapi 01/17/23 16:03:13.078
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:03:13.103
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:03:13.108
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:68
    Jan 17 16:03:13.112: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 01/17/23 16:03:20.416
    Jan 17 16:03:20.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=crd-publish-openapi-3803 --namespace=crd-publish-openapi-3803 create -f -'
    Jan 17 16:03:21.632: INFO: stderr: ""
    Jan 17 16:03:21.632: INFO: stdout: "e2e-test-crd-publish-openapi-5800-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Jan 17 16:03:21.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=crd-publish-openapi-3803 --namespace=crd-publish-openapi-3803 delete e2e-test-crd-publish-openapi-5800-crds test-foo'
    Jan 17 16:03:21.703: INFO: stderr: ""
    Jan 17 16:03:21.703: INFO: stdout: "e2e-test-crd-publish-openapi-5800-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    Jan 17 16:03:21.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=crd-publish-openapi-3803 --namespace=crd-publish-openapi-3803 apply -f -'
    Jan 17 16:03:22.612: INFO: stderr: ""
    Jan 17 16:03:22.612: INFO: stdout: "e2e-test-crd-publish-openapi-5800-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Jan 17 16:03:22.613: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=crd-publish-openapi-3803 --namespace=crd-publish-openapi-3803 delete e2e-test-crd-publish-openapi-5800-crds test-foo'
    Jan 17 16:03:22.663: INFO: stderr: ""
    Jan 17 16:03:22.663: INFO: stdout: "e2e-test-crd-publish-openapi-5800-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 01/17/23 16:03:22.663
    Jan 17 16:03:22.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=crd-publish-openapi-3803 --namespace=crd-publish-openapi-3803 create -f -'
    Jan 17 16:03:22.910: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 01/17/23 16:03:22.91
    Jan 17 16:03:22.910: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=crd-publish-openapi-3803 --namespace=crd-publish-openapi-3803 create -f -'
    Jan 17 16:03:23.158: INFO: rc: 1
    Jan 17 16:03:23.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=crd-publish-openapi-3803 --namespace=crd-publish-openapi-3803 apply -f -'
    Jan 17 16:03:23.411: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 01/17/23 16:03:23.412
    Jan 17 16:03:23.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=crd-publish-openapi-3803 --namespace=crd-publish-openapi-3803 create -f -'
    Jan 17 16:03:23.675: INFO: rc: 1
    Jan 17 16:03:23.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=crd-publish-openapi-3803 --namespace=crd-publish-openapi-3803 apply -f -'
    Jan 17 16:03:23.929: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 01/17/23 16:03:23.929
    Jan 17 16:03:23.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=crd-publish-openapi-3803 explain e2e-test-crd-publish-openapi-5800-crds'
    Jan 17 16:03:24.179: INFO: stderr: ""
    Jan 17 16:03:24.180: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5800-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 01/17/23 16:03:24.18
    Jan 17 16:03:24.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=crd-publish-openapi-3803 explain e2e-test-crd-publish-openapi-5800-crds.metadata'
    Jan 17 16:03:24.430: INFO: stderr: ""
    Jan 17 16:03:24.430: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5800-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    Jan 17 16:03:24.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=crd-publish-openapi-3803 explain e2e-test-crd-publish-openapi-5800-crds.spec'
    Jan 17 16:03:25.579: INFO: stderr: ""
    Jan 17 16:03:25.579: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5800-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    Jan 17 16:03:25.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=crd-publish-openapi-3803 explain e2e-test-crd-publish-openapi-5800-crds.spec.bars'
    Jan 17 16:03:25.829: INFO: stderr: ""
    Jan 17 16:03:25.829: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5800-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 01/17/23 16:03:25.829
    Jan 17 16:03:25.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=crd-publish-openapi-3803 explain e2e-test-crd-publish-openapi-5800-crds.spec.bars2'
    Jan 17 16:03:26.077: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 17 16:03:32.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-3803" for this suite. 01/17/23 16:03:32.553
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:86
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:03:32.561
Jan 17 16:03:32.561: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename disruption 01/17/23 16:03:32.562
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:03:32.578
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:03:32.58
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:03:32.583
Jan 17 16:03:32.583: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename disruption-2 01/17/23 16:03:32.584
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:03:32.601
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:03:32.606
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:86
STEP: Waiting for the pdb to be processed 01/17/23 16:03:32.618
STEP: Waiting for the pdb to be processed 01/17/23 16:03:32.649
STEP: Waiting for the pdb to be processed 01/17/23 16:03:32.675
STEP: listing a collection of PDBs across all namespaces 01/17/23 16:03:32.703
STEP: listing a collection of PDBs in namespace disruption-3278 01/17/23 16:03:32.707
STEP: deleting a collection of PDBs 01/17/23 16:03:32.713
STEP: Waiting for the PDB collection to be deleted 01/17/23 16:03:32.728
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:187
Jan 17 16:03:32.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-5230" for this suite. 01/17/23 16:03:32.734
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Jan 17 16:03:32.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-3278" for this suite. 01/17/23 16:03:32.751
{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","completed":295,"skipped":5323,"failed":0}
------------------------------
• [0.196 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:77
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:03:32.561
    Jan 17 16:03:32.561: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename disruption 01/17/23 16:03:32.562
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:03:32.578
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:03:32.58
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:03:32.583
    Jan 17 16:03:32.583: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename disruption-2 01/17/23 16:03:32.584
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:03:32.601
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:03:32.606
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:86
    STEP: Waiting for the pdb to be processed 01/17/23 16:03:32.618
    STEP: Waiting for the pdb to be processed 01/17/23 16:03:32.649
    STEP: Waiting for the pdb to be processed 01/17/23 16:03:32.675
    STEP: listing a collection of PDBs across all namespaces 01/17/23 16:03:32.703
    STEP: listing a collection of PDBs in namespace disruption-3278 01/17/23 16:03:32.707
    STEP: deleting a collection of PDBs 01/17/23 16:03:32.713
    STEP: Waiting for the PDB collection to be deleted 01/17/23 16:03:32.728
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/framework.go:187
    Jan 17 16:03:32.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-2-5230" for this suite. 01/17/23 16:03:32.734
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Jan 17 16:03:32.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-3278" for this suite. 01/17/23 16:03:32.751
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:03:32.757
Jan 17 16:03:32.758: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename kubelet-test 01/17/23 16:03:32.758
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:03:32.777
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:03:32.781
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Jan 17 16:03:36.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9478" for this suite. 01/17/23 16:03:36.836
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","completed":296,"skipped":5333,"failed":0}
------------------------------
• [4.083 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:03:32.757
    Jan 17 16:03:32.758: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename kubelet-test 01/17/23 16:03:32.758
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:03:32.777
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:03:32.781
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Jan 17 16:03:36.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-9478" for this suite. 01/17/23 16:03:36.836
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:03:36.842
Jan 17 16:03:36.842: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename downward-api 01/17/23 16:03:36.843
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:03:36.862
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:03:36.865
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248
STEP: Creating a pod to test downward API volume plugin 01/17/23 16:03:36.867
Jan 17 16:03:36.891: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f724570f-ed0a-4127-9b55-4bdfa783e166" in namespace "downward-api-1533" to be "Succeeded or Failed"
Jan 17 16:03:36.894: INFO: Pod "downwardapi-volume-f724570f-ed0a-4127-9b55-4bdfa783e166": Phase="Pending", Reason="", readiness=false. Elapsed: 3.334553ms
Jan 17 16:03:38.900: INFO: Pod "downwardapi-volume-f724570f-ed0a-4127-9b55-4bdfa783e166": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009535484s
Jan 17 16:03:40.898: INFO: Pod "downwardapi-volume-f724570f-ed0a-4127-9b55-4bdfa783e166": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007499502s
STEP: Saw pod success 01/17/23 16:03:40.898
Jan 17 16:03:40.899: INFO: Pod "downwardapi-volume-f724570f-ed0a-4127-9b55-4bdfa783e166" satisfied condition "Succeeded or Failed"
Jan 17 16:03:40.902: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod downwardapi-volume-f724570f-ed0a-4127-9b55-4bdfa783e166 container client-container: <nil>
STEP: delete the pod 01/17/23 16:03:40.913
Jan 17 16:03:40.927: INFO: Waiting for pod downwardapi-volume-f724570f-ed0a-4127-9b55-4bdfa783e166 to disappear
Jan 17 16:03:40.929: INFO: Pod downwardapi-volume-f724570f-ed0a-4127-9b55-4bdfa783e166 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 17 16:03:40.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1533" for this suite. 01/17/23 16:03:40.932
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","completed":297,"skipped":5366,"failed":0}
------------------------------
• [4.097 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:03:36.842
    Jan 17 16:03:36.842: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename downward-api 01/17/23 16:03:36.843
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:03:36.862
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:03:36.865
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:248
    STEP: Creating a pod to test downward API volume plugin 01/17/23 16:03:36.867
    Jan 17 16:03:36.891: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f724570f-ed0a-4127-9b55-4bdfa783e166" in namespace "downward-api-1533" to be "Succeeded or Failed"
    Jan 17 16:03:36.894: INFO: Pod "downwardapi-volume-f724570f-ed0a-4127-9b55-4bdfa783e166": Phase="Pending", Reason="", readiness=false. Elapsed: 3.334553ms
    Jan 17 16:03:38.900: INFO: Pod "downwardapi-volume-f724570f-ed0a-4127-9b55-4bdfa783e166": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009535484s
    Jan 17 16:03:40.898: INFO: Pod "downwardapi-volume-f724570f-ed0a-4127-9b55-4bdfa783e166": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007499502s
    STEP: Saw pod success 01/17/23 16:03:40.898
    Jan 17 16:03:40.899: INFO: Pod "downwardapi-volume-f724570f-ed0a-4127-9b55-4bdfa783e166" satisfied condition "Succeeded or Failed"
    Jan 17 16:03:40.902: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod downwardapi-volume-f724570f-ed0a-4127-9b55-4bdfa783e166 container client-container: <nil>
    STEP: delete the pod 01/17/23 16:03:40.913
    Jan 17 16:03:40.927: INFO: Waiting for pod downwardapi-volume-f724570f-ed0a-4127-9b55-4bdfa783e166 to disappear
    Jan 17 16:03:40.929: INFO: Pod downwardapi-volume-f724570f-ed0a-4127-9b55-4bdfa783e166 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 17 16:03:40.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-1533" for this suite. 01/17/23 16:03:40.932
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:03:40.939
Jan 17 16:03:40.939: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename disruption 01/17/23 16:03:40.94
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:03:40.956
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:03:40.96
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140
STEP: Waiting for the pdb to be processed 01/17/23 16:03:40.97
STEP: Waiting for all pods to be running 01/17/23 16:03:41.051
Jan 17 16:03:41.055: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Jan 17 16:03:43.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-5532" for this suite. 01/17/23 16:03:43.072
{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","completed":298,"skipped":5379,"failed":0}
------------------------------
• [2.138 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:03:40.939
    Jan 17 16:03:40.939: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename disruption 01/17/23 16:03:40.94
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:03:40.956
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:03:40.96
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:140
    STEP: Waiting for the pdb to be processed 01/17/23 16:03:40.97
    STEP: Waiting for all pods to be running 01/17/23 16:03:41.051
    Jan 17 16:03:41.055: INFO: running pods: 0 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Jan 17 16:03:43.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-5532" for this suite. 01/17/23 16:03:43.072
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:03:43.078
Jan 17 16:03:43.078: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename daemonsets 01/17/23 16:03:43.079
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:03:43.098
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:03:43.102
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373
Jan 17 16:03:43.162: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 01/17/23 16:03:43.168
Jan 17 16:03:43.175: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 16:03:43.175: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 16:03:43.175: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 16:03:43.179: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 17 16:03:43.179: INFO: Node ip-10-0-139-213.ec2.internal is running 0 daemon pod, expected 1
Jan 17 16:03:44.183: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 16:03:44.183: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 16:03:44.183: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 16:03:44.186: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 17 16:03:44.186: INFO: Node ip-10-0-151-22.ec2.internal is running 0 daemon pod, expected 1
Jan 17 16:03:45.183: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 16:03:45.184: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 16:03:45.184: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 16:03:45.187: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan 17 16:03:45.187: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Update daemon pods image. 01/17/23 16:03:45.197
STEP: Check that daemon pods images are updated. 01/17/23 16:03:45.209
Jan 17 16:03:45.211: INFO: Wrong image for pod: daemon-set-7zq7j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 17 16:03:45.211: INFO: Wrong image for pod: daemon-set-sts22. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 17 16:03:45.211: INFO: Wrong image for pod: daemon-set-w22rd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 17 16:03:45.216: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 16:03:45.216: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 16:03:45.216: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 16:03:46.222: INFO: Wrong image for pod: daemon-set-7zq7j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 17 16:03:46.222: INFO: Wrong image for pod: daemon-set-sts22. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 17 16:03:46.226: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 16:03:46.226: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 16:03:46.226: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 16:03:47.220: INFO: Wrong image for pod: daemon-set-7zq7j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 17 16:03:47.220: INFO: Pod daemon-set-d4gnz is not available
Jan 17 16:03:47.220: INFO: Wrong image for pod: daemon-set-sts22. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 17 16:03:47.224: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 16:03:47.224: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 16:03:47.224: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 16:03:48.220: INFO: Wrong image for pod: daemon-set-7zq7j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 17 16:03:48.225: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 16:03:48.225: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 16:03:48.225: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 16:03:49.220: INFO: Wrong image for pod: daemon-set-7zq7j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 17 16:03:49.220: INFO: Pod daemon-set-m2qt5 is not available
Jan 17 16:03:49.225: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 16:03:49.225: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 16:03:49.225: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 16:03:50.226: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 16:03:50.226: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 16:03:50.226: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 16:03:51.221: INFO: Pod daemon-set-rt587 is not available
Jan 17 16:03:51.224: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 16:03:51.224: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 16:03:51.224: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster. 01/17/23 16:03:51.225
Jan 17 16:03:51.229: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 16:03:51.229: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 16:03:51.229: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 16:03:51.232: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 17 16:03:51.232: INFO: Node ip-10-0-151-22.ec2.internal is running 0 daemon pod, expected 1
Jan 17 16:03:52.237: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 16:03:52.237: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 16:03:52.237: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 16:03:52.240: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 17 16:03:52.240: INFO: Node ip-10-0-151-22.ec2.internal is running 0 daemon pod, expected 1
Jan 17 16:03:53.237: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 16:03:53.237: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 16:03:53.237: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 16:03:53.239: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan 17 16:03:53.239: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 01/17/23 16:03:53.254
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5520, will wait for the garbage collector to delete the pods 01/17/23 16:03:53.254
Jan 17 16:03:53.312: INFO: Deleting DaemonSet.extensions daemon-set took: 5.750431ms
Jan 17 16:03:53.413: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.910403ms
Jan 17 16:03:55.517: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 17 16:03:55.517: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 17 16:03:55.521: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"127800"},"items":null}

Jan 17 16:03:55.523: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"127800"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan 17 16:03:55.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5520" for this suite. 01/17/23 16:03:55.546
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","completed":299,"skipped":5381,"failed":0}
------------------------------
• [SLOW TEST] [12.474 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:03:43.078
    Jan 17 16:03:43.078: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename daemonsets 01/17/23 16:03:43.079
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:03:43.098
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:03:43.102
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:373
    Jan 17 16:03:43.162: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 01/17/23 16:03:43.168
    Jan 17 16:03:43.175: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 16:03:43.175: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 16:03:43.175: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 16:03:43.179: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 17 16:03:43.179: INFO: Node ip-10-0-139-213.ec2.internal is running 0 daemon pod, expected 1
    Jan 17 16:03:44.183: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 16:03:44.183: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 16:03:44.183: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 16:03:44.186: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 17 16:03:44.186: INFO: Node ip-10-0-151-22.ec2.internal is running 0 daemon pod, expected 1
    Jan 17 16:03:45.183: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 16:03:45.184: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 16:03:45.184: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 16:03:45.187: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jan 17 16:03:45.187: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Update daemon pods image. 01/17/23 16:03:45.197
    STEP: Check that daemon pods images are updated. 01/17/23 16:03:45.209
    Jan 17 16:03:45.211: INFO: Wrong image for pod: daemon-set-7zq7j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 17 16:03:45.211: INFO: Wrong image for pod: daemon-set-sts22. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 17 16:03:45.211: INFO: Wrong image for pod: daemon-set-w22rd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 17 16:03:45.216: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 16:03:45.216: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 16:03:45.216: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 16:03:46.222: INFO: Wrong image for pod: daemon-set-7zq7j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 17 16:03:46.222: INFO: Wrong image for pod: daemon-set-sts22. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 17 16:03:46.226: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 16:03:46.226: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 16:03:46.226: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 16:03:47.220: INFO: Wrong image for pod: daemon-set-7zq7j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 17 16:03:47.220: INFO: Pod daemon-set-d4gnz is not available
    Jan 17 16:03:47.220: INFO: Wrong image for pod: daemon-set-sts22. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 17 16:03:47.224: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 16:03:47.224: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 16:03:47.224: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 16:03:48.220: INFO: Wrong image for pod: daemon-set-7zq7j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 17 16:03:48.225: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 16:03:48.225: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 16:03:48.225: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 16:03:49.220: INFO: Wrong image for pod: daemon-set-7zq7j. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 17 16:03:49.220: INFO: Pod daemon-set-m2qt5 is not available
    Jan 17 16:03:49.225: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 16:03:49.225: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 16:03:49.225: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 16:03:50.226: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 16:03:50.226: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 16:03:50.226: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 16:03:51.221: INFO: Pod daemon-set-rt587 is not available
    Jan 17 16:03:51.224: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 16:03:51.224: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 16:03:51.224: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    STEP: Check that daemon pods are still running on every node of the cluster. 01/17/23 16:03:51.225
    Jan 17 16:03:51.229: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 16:03:51.229: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 16:03:51.229: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 16:03:51.232: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 17 16:03:51.232: INFO: Node ip-10-0-151-22.ec2.internal is running 0 daemon pod, expected 1
    Jan 17 16:03:52.237: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 16:03:52.237: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 16:03:52.237: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 16:03:52.240: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 17 16:03:52.240: INFO: Node ip-10-0-151-22.ec2.internal is running 0 daemon pod, expected 1
    Jan 17 16:03:53.237: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 16:03:53.237: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 16:03:53.237: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 16:03:53.239: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jan 17 16:03:53.239: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 01/17/23 16:03:53.254
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5520, will wait for the garbage collector to delete the pods 01/17/23 16:03:53.254
    Jan 17 16:03:53.312: INFO: Deleting DaemonSet.extensions daemon-set took: 5.750431ms
    Jan 17 16:03:53.413: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.910403ms
    Jan 17 16:03:55.517: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 17 16:03:55.517: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 17 16:03:55.521: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"127800"},"items":null}

    Jan 17 16:03:55.523: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"127800"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan 17 16:03:55.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-5520" for this suite. 01/17/23 16:03:55.546
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:03:55.554
Jan 17 16:03:55.554: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename security-context 01/17/23 16:03:55.554
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:03:55.628
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:03:55.633
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/17/23 16:03:55.647
Jan 17 16:03:55.668: INFO: Waiting up to 5m0s for pod "security-context-2645f5d4-cb54-498b-a1d5-9ed7433de628" in namespace "security-context-2383" to be "Succeeded or Failed"
Jan 17 16:03:55.672: INFO: Pod "security-context-2645f5d4-cb54-498b-a1d5-9ed7433de628": Phase="Pending", Reason="", readiness=false. Elapsed: 3.713752ms
Jan 17 16:03:57.677: INFO: Pod "security-context-2645f5d4-cb54-498b-a1d5-9ed7433de628": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008792827s
Jan 17 16:03:59.675: INFO: Pod "security-context-2645f5d4-cb54-498b-a1d5-9ed7433de628": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007372127s
STEP: Saw pod success 01/17/23 16:03:59.675
Jan 17 16:03:59.675: INFO: Pod "security-context-2645f5d4-cb54-498b-a1d5-9ed7433de628" satisfied condition "Succeeded or Failed"
Jan 17 16:03:59.678: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod security-context-2645f5d4-cb54-498b-a1d5-9ed7433de628 container test-container: <nil>
STEP: delete the pod 01/17/23 16:03:59.684
Jan 17 16:03:59.697: INFO: Waiting for pod security-context-2645f5d4-cb54-498b-a1d5-9ed7433de628 to disappear
Jan 17 16:03:59.700: INFO: Pod security-context-2645f5d4-cb54-498b-a1d5-9ed7433de628 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jan 17 16:03:59.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-2383" for this suite. 01/17/23 16:03:59.705
{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","completed":300,"skipped":5455,"failed":0}
------------------------------
• [4.158 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:03:55.554
    Jan 17 16:03:55.554: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename security-context 01/17/23 16:03:55.554
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:03:55.628
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:03:55.633
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:97
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/17/23 16:03:55.647
    Jan 17 16:03:55.668: INFO: Waiting up to 5m0s for pod "security-context-2645f5d4-cb54-498b-a1d5-9ed7433de628" in namespace "security-context-2383" to be "Succeeded or Failed"
    Jan 17 16:03:55.672: INFO: Pod "security-context-2645f5d4-cb54-498b-a1d5-9ed7433de628": Phase="Pending", Reason="", readiness=false. Elapsed: 3.713752ms
    Jan 17 16:03:57.677: INFO: Pod "security-context-2645f5d4-cb54-498b-a1d5-9ed7433de628": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008792827s
    Jan 17 16:03:59.675: INFO: Pod "security-context-2645f5d4-cb54-498b-a1d5-9ed7433de628": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007372127s
    STEP: Saw pod success 01/17/23 16:03:59.675
    Jan 17 16:03:59.675: INFO: Pod "security-context-2645f5d4-cb54-498b-a1d5-9ed7433de628" satisfied condition "Succeeded or Failed"
    Jan 17 16:03:59.678: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod security-context-2645f5d4-cb54-498b-a1d5-9ed7433de628 container test-container: <nil>
    STEP: delete the pod 01/17/23 16:03:59.684
    Jan 17 16:03:59.697: INFO: Waiting for pod security-context-2645f5d4-cb54-498b-a1d5-9ed7433de628 to disappear
    Jan 17 16:03:59.700: INFO: Pod security-context-2645f5d4-cb54-498b-a1d5-9ed7433de628 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jan 17 16:03:59.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-2383" for this suite. 01/17/23 16:03:59.705
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:03:59.712
Jan 17 16:03:59.712: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename configmap 01/17/23 16:03:59.712
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:03:59.729
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:03:59.739
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108
STEP: Creating configMap with name configmap-test-volume-map-57a93b85-31ee-4b90-a978-b73ef6928bde 01/17/23 16:03:59.745
STEP: Creating a pod to test consume configMaps 01/17/23 16:03:59.755
Jan 17 16:03:59.788: INFO: Waiting up to 5m0s for pod "pod-configmaps-21e0f7e5-d374-431c-80d8-ad68dec5be0a" in namespace "configmap-4614" to be "Succeeded or Failed"
Jan 17 16:03:59.791: INFO: Pod "pod-configmaps-21e0f7e5-d374-431c-80d8-ad68dec5be0a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.980159ms
Jan 17 16:04:01.794: INFO: Pod "pod-configmaps-21e0f7e5-d374-431c-80d8-ad68dec5be0a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006218477s
Jan 17 16:04:03.795: INFO: Pod "pod-configmaps-21e0f7e5-d374-431c-80d8-ad68dec5be0a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007920408s
STEP: Saw pod success 01/17/23 16:04:03.795
Jan 17 16:04:03.796: INFO: Pod "pod-configmaps-21e0f7e5-d374-431c-80d8-ad68dec5be0a" satisfied condition "Succeeded or Failed"
Jan 17 16:04:03.798: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-configmaps-21e0f7e5-d374-431c-80d8-ad68dec5be0a container agnhost-container: <nil>
STEP: delete the pod 01/17/23 16:04:03.803
Jan 17 16:04:03.817: INFO: Waiting for pod pod-configmaps-21e0f7e5-d374-431c-80d8-ad68dec5be0a to disappear
Jan 17 16:04:03.820: INFO: Pod pod-configmaps-21e0f7e5-d374-431c-80d8-ad68dec5be0a no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 17 16:04:03.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4614" for this suite. 01/17/23 16:04:03.823
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","completed":301,"skipped":5461,"failed":0}
------------------------------
• [4.118 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:03:59.712
    Jan 17 16:03:59.712: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename configmap 01/17/23 16:03:59.712
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:03:59.729
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:03:59.739
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:108
    STEP: Creating configMap with name configmap-test-volume-map-57a93b85-31ee-4b90-a978-b73ef6928bde 01/17/23 16:03:59.745
    STEP: Creating a pod to test consume configMaps 01/17/23 16:03:59.755
    Jan 17 16:03:59.788: INFO: Waiting up to 5m0s for pod "pod-configmaps-21e0f7e5-d374-431c-80d8-ad68dec5be0a" in namespace "configmap-4614" to be "Succeeded or Failed"
    Jan 17 16:03:59.791: INFO: Pod "pod-configmaps-21e0f7e5-d374-431c-80d8-ad68dec5be0a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.980159ms
    Jan 17 16:04:01.794: INFO: Pod "pod-configmaps-21e0f7e5-d374-431c-80d8-ad68dec5be0a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006218477s
    Jan 17 16:04:03.795: INFO: Pod "pod-configmaps-21e0f7e5-d374-431c-80d8-ad68dec5be0a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007920408s
    STEP: Saw pod success 01/17/23 16:04:03.795
    Jan 17 16:04:03.796: INFO: Pod "pod-configmaps-21e0f7e5-d374-431c-80d8-ad68dec5be0a" satisfied condition "Succeeded or Failed"
    Jan 17 16:04:03.798: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-configmaps-21e0f7e5-d374-431c-80d8-ad68dec5be0a container agnhost-container: <nil>
    STEP: delete the pod 01/17/23 16:04:03.803
    Jan 17 16:04:03.817: INFO: Waiting for pod pod-configmaps-21e0f7e5-d374-431c-80d8-ad68dec5be0a to disappear
    Jan 17 16:04:03.820: INFO: Pod pod-configmaps-21e0f7e5-d374-431c-80d8-ad68dec5be0a no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 17 16:04:03.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-4614" for this suite. 01/17/23 16:04:03.823
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:04:03.832
Jan 17 16:04:03.832: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename gc 01/17/23 16:04:03.833
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:04:03.848
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:04:03.853
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 01/17/23 16:04:03.855
W0117 16:04:03.862937      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Wait for the Deployment to create new ReplicaSet 01/17/23 16:04:03.862
STEP: delete the deployment 01/17/23 16:04:04.387
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 01/17/23 16:04:04.394
STEP: Gathering metrics 01/17/23 16:04:04.917
W0117 16:04:04.920540      22 metrics_grabber.go:110] Can't find any pods in namespace kube-system to grab metrics from
W0117 16:04:04.920554      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan 17 16:04:04.920: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan 17 16:04:04.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9042" for this suite. 01/17/23 16:04:04.925
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","completed":302,"skipped":5576,"failed":0}
------------------------------
• [1.100 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:04:03.832
    Jan 17 16:04:03.832: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename gc 01/17/23 16:04:03.833
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:04:03.848
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:04:03.853
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 01/17/23 16:04:03.855
    W0117 16:04:03.862937      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Wait for the Deployment to create new ReplicaSet 01/17/23 16:04:03.862
    STEP: delete the deployment 01/17/23 16:04:04.387
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 01/17/23 16:04:04.394
    STEP: Gathering metrics 01/17/23 16:04:04.917
    W0117 16:04:04.920540      22 metrics_grabber.go:110] Can't find any pods in namespace kube-system to grab metrics from
    W0117 16:04:04.920554      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jan 17 16:04:04.920: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan 17 16:04:04.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-9042" for this suite. 01/17/23 16:04:04.925
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1274
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:04:04.933
Jan 17 16:04:04.933: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename kubectl 01/17/23 16:04:04.934
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:04:04.953
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:04:04.956
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1274
Jan 17 16:04:04.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-4873 create -f -'
Jan 17 16:04:05.972: INFO: stderr: ""
Jan 17 16:04:05.972: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Jan 17 16:04:05.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-4873 create -f -'
Jan 17 16:04:06.882: INFO: stderr: ""
Jan 17 16:04:06.882: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 01/17/23 16:04:06.882
Jan 17 16:04:07.887: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 17 16:04:07.887: INFO: Found 1 / 1
Jan 17 16:04:07.887: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan 17 16:04:07.889: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 17 16:04:07.889: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 17 16:04:07.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-4873 describe pod agnhost-primary-plhw9'
Jan 17 16:04:07.942: INFO: stderr: ""
Jan 17 16:04:07.942: INFO: stdout: "Name:             agnhost-primary-plhw9\nNamespace:        kubectl-4873\nPriority:         0\nService Account:  default\nNode:             ip-10-0-151-22.ec2.internal/10.0.151.22\nStart Time:       Tue, 17 Jan 2023 16:04:05 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      k8s.ovn.org/pod-networks:\n                    {\"default\":{\"ip_addresses\":[\"10.131.1.109/23\"],\"mac_address\":\"0a:58:0a:83:01:6d\",\"gateway_ips\":[\"10.131.0.1\"],\"ip_address\":\"10.131.1.109/2...\n                  k8s.v1.cni.cncf.io/network-status:\n                    [{\n                        \"name\": \"ovn-kubernetes\",\n                        \"interface\": \"eth0\",\n                        \"ips\": [\n                            \"10.131.1.109\"\n                        ],\n                        \"mac\": \"0a:58:0a:83:01:6d\",\n                        \"default\": true,\n                        \"dns\": {}\n                    }]\n                  k8s.v1.cni.cncf.io/networks-status:\n                    [{\n                        \"name\": \"ovn-kubernetes\",\n                        \"interface\": \"eth0\",\n                        \"ips\": [\n                            \"10.131.1.109\"\n                        ],\n                        \"mac\": \"0a:58:0a:83:01:6d\",\n                        \"default\": true,\n                        \"dns\": {}\n                    }]\n                  openshift.io/scc: anyuid\nStatus:           Running\nIP:               10.131.1.109\nIPs:\n  IP:           10.131.1.109\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   cri-o://9964539bc95354b3b4b6c30703f097bac80a177410c082e91509c0a570ab9413\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.40\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:37dd9ad84b34913b4bc57e00b0a28e8b00f18ca6a5ecec69461aa57402e5c787\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Tue, 17 Jan 2023 16:04:06 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-kkxtg (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-kkxtg:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\n    ConfigMapName:           openshift-service-ca.crt\n    ConfigMapOptional:       <nil>\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason          Age   From               Message\n  ----    ------          ----  ----               -------\n  Normal  Scheduled       1s    default-scheduler  Successfully assigned kubectl-4873/agnhost-primary-plhw9 to ip-10-0-151-22.ec2.internal\n  Normal  AddedInterface  1s    multus             Add eth0 [10.131.1.109/23] from ovn-kubernetes\n  Normal  Pulled          1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.40\" already present on machine\n  Normal  Created         1s    kubelet            Created container agnhost-primary\n  Normal  Started         1s    kubelet            Started container agnhost-primary\n"
Jan 17 16:04:07.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-4873 describe rc agnhost-primary'
Jan 17 16:04:07.998: INFO: stderr: ""
Jan 17 16:04:07.998: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-4873\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.40\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-plhw9\n"
Jan 17 16:04:07.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-4873 describe service agnhost-primary'
Jan 17 16:04:08.051: INFO: stderr: ""
Jan 17 16:04:08.051: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-4873\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                172.30.100.244\nIPs:               172.30.100.244\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.131.1.109:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jan 17 16:04:08.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-4873 describe node ip-10-0-135-246.ec2.internal'
Jan 17 16:04:08.365: INFO: stderr: ""
Jan 17 16:04:08.365: INFO: stdout: "Name:               ip-10-0-135-246.ec2.internal\nRoles:              control-plane,master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=m6i.xlarge\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=us-east-1\n                    failure-domain.beta.kubernetes.io/zone=us-east-1a\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-10-0-135-246.ec2.internal\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node-role.kubernetes.io/master=\n                    node.kubernetes.io/instance-type=m6i.xlarge\n                    node.openshift.io/os_id=rhcos\n                    topology.ebs.csi.aws.com/zone=us-east-1a\n                    topology.kubernetes.io/region=us-east-1\n                    topology.kubernetes.io/zone=us-east-1a\nAnnotations:        cloud.network.openshift.io/egress-ipconfig:\n                      [{\"interface\":\"eni-02083fb75eaaa84cc\",\"ifaddr\":{\"ipv4\":\"10.0.128.0/20\"},\"capacity\":{\"ipv4\":14,\"ipv6\":15}}]\n                    csi.volume.kubernetes.io/nodeid: {\"ebs.csi.aws.com\":\"i-0d084ed22cb4570b2\"}\n                    k8s.ovn.org/host-addresses: [\"10.0.135.246\"]\n                    k8s.ovn.org/l3-gateway-config:\n                      {\"default\":{\"mode\":\"shared\",\"interface-id\":\"br-ex_ip-10-0-135-246.ec2.internal\",\"mac-address\":\"02:48:8f:80:a8:05\",\"ip-addresses\":[\"10.0.13...\n                    k8s.ovn.org/node-chassis-id: 03e72bd2-40f4-4bc2-90a7-792643f23494\n                    k8s.ovn.org/node-gateway-router-lrp-ifaddr: {\"ipv4\":\"100.64.0.2/16\"}\n                    k8s.ovn.org/node-mgmt-port-mac-address: a6:00:3a:7a:7b:18\n                    k8s.ovn.org/node-primary-ifaddr: {\"ipv4\":\"10.0.135.246/20\"}\n                    k8s.ovn.org/node-subnets: {\"default\":\"10.128.0.0/23\"}\n                    machine.openshift.io/machine: openshift-machine-api/maszulik-rlbbb-master-0\n                    machineconfiguration.openshift.io/controlPlaneTopology: HighlyAvailable\n                    machineconfiguration.openshift.io/currentConfig: rendered-master-3451104d0601207912e04b05a5d2dcb6\n                    machineconfiguration.openshift.io/desiredConfig: rendered-master-3451104d0601207912e04b05a5d2dcb6\n                    machineconfiguration.openshift.io/desiredDrain: uncordon-rendered-master-3451104d0601207912e04b05a5d2dcb6\n                    machineconfiguration.openshift.io/lastAppliedDrain: uncordon-rendered-master-3451104d0601207912e04b05a5d2dcb6\n                    machineconfiguration.openshift.io/reason: \n                    machineconfiguration.openshift.io/state: Done\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Tue, 17 Jan 2023 12:45:30 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  ip-10-0-135-246.ec2.internal\n  AcquireTime:     <unset>\n  RenewTime:       Tue, 17 Jan 2023 16:04:02 +0000\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Tue, 17 Jan 2023 16:01:51 +0000   Tue, 17 Jan 2023 12:45:30 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Tue, 17 Jan 2023 16:01:51 +0000   Tue, 17 Jan 2023 12:45:30 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Tue, 17 Jan 2023 16:01:51 +0000   Tue, 17 Jan 2023 12:45:30 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Tue, 17 Jan 2023 16:01:51 +0000   Tue, 17 Jan 2023 12:47:43 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:   10.0.135.246\n  Hostname:     ip-10-0-135-246.ec2.internal\n  InternalDNS:  ip-10-0-135-246.ec2.internal\nCapacity:\n  attachable-volumes-aws-ebs:  39\n  cpu:                         4\n  ephemeral-storage:           125293548Ki\n  hugepages-1Gi:               0\n  hugepages-2Mi:               0\n  memory:                      16129408Ki\n  pods:                        250\nAllocatable:\n  attachable-volumes-aws-ebs:  39\n  cpu:                         3500m\n  ephemeral-storage:           114396791822\n  hugepages-1Gi:               0\n  hugepages-2Mi:               0\n  memory:                      14978432Ki\n  pods:                        250\nSystem Info:\n  Machine ID:                                       ec2abc352828bff4b5235884c7b9c3a6\n  System UUID:                                      ec2abc35-2828-bff4-b523-5884c7b9c3a6\n  Boot ID:                                          5a46b4c7-25c5-41c7-9690-4c9d03faef4b\n  Kernel Version:                                   4.18.0-372.40.1.el8_6.x86_64\n  OS Image:                                         Red Hat Enterprise Linux CoreOS 412.86.202301061548-0 (Ootpa)\n  Operating System:                                 linux\n  Architecture:                                     amd64\n  Container Runtime Version:                        cri-o://1.25.1-5.rhaos4.12.git6005903.el8\n  Kubelet Version:                                  v1.25.4+77bec7a\n  Kube-Proxy Version:                               v1.25.4+77bec7a\nProviderID:                                         aws:///us-east-1a/i-0d084ed22cb4570b2\nNon-terminated Pods:                                (63 in total)\n  Namespace                                         Name                                                           CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                                         ----                                                           ------------  ----------  ---------------  -------------  ---\n  openshift-apiserver-operator                      openshift-apiserver-operator-7ddc8958fb-zqcl9                  10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         3h18m\n  openshift-apiserver                               apiserver-5fb6ffcbd6-xzvls                                     110m (3%)     0 (0%)      250Mi (1%)       0 (0%)         3h7m\n  openshift-authentication-operator                 authentication-operator-65f78f5bc6-jklnl                       20m (0%)      0 (0%)      200Mi (1%)       0 (0%)         3h18m\n  openshift-authentication                          oauth-openshift-bf45995b4-s2mfq                                10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         3h5m\n  openshift-cloud-controller-manager-operator       cluster-cloud-controller-manager-operator-6b6d666976-64zz4     20m (0%)      0 (0%)      75Mi (0%)        0 (0%)         3h17m\n  openshift-cloud-credential-operator               cloud-credential-operator-6ffc47fc7f-6h8b9                     20m (0%)      0 (0%)      170Mi (1%)       0 (0%)         3h18m\n  openshift-cloud-network-config-controller         cloud-network-config-controller-d9f74866d-x99lf                10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         3h18m\n  openshift-cluster-csi-drivers                     aws-ebs-csi-driver-controller-7464b474df-qfqzj                 110m (3%)     0 (0%)      400Mi (2%)       0 (0%)         3h15m\n  openshift-cluster-csi-drivers                     aws-ebs-csi-driver-node-jkjc8                                  30m (0%)      0 (0%)      150Mi (1%)       0 (0%)         3h15m\n  openshift-cluster-machine-approver                machine-approver-745689cc9c-v2wjr                              20m (0%)      0 (0%)      70Mi (0%)        0 (0%)         3h17m\n  openshift-cluster-node-tuning-operator            cluster-node-tuning-operator-8648f4dc6c-xzgpc                  10m (0%)      0 (0%)      20Mi (0%)        0 (0%)         3h18m\n  openshift-cluster-node-tuning-operator            tuned-zglbw                                                    10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         3h14m\n  openshift-cluster-storage-operator                cluster-storage-operator-66c94c594-6hh8n                       10m (0%)      0 (0%)      20Mi (0%)        0 (0%)         3h18m\n  openshift-cluster-storage-operator                csi-snapshot-controller-operator-7d599d59ff-s7fnt              10m (0%)      0 (0%)      65Mi (0%)        0 (0%)         3h18m\n  openshift-config-operator                         openshift-config-operator-77b6ddc656-2pvfk                     10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         3h18m\n  openshift-console                                 console-7dfd69cc54-6xljq                                       10m (0%)      0 (0%)      100Mi (0%)       0 (0%)         3h5m\n  openshift-controller-manager-operator             openshift-controller-manager-operator-67f74c4b9b-74lcr         10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         3h18m\n  openshift-controller-manager                      controller-manager-757b94b9d-gjn7l                             100m (2%)     0 (0%)      100Mi (0%)       0 (0%)         3h8m\n  openshift-dns-operator                            dns-operator-64688bfdd4-lrr9x                                  20m (0%)      0 (0%)      69Mi (0%)        0 (0%)         3h18m\n  openshift-dns                                     dns-default-8fghg                                              60m (1%)      0 (0%)      110Mi (0%)       0 (0%)         3h15m\n  openshift-dns                                     node-resolver-lkc88                                            5m (0%)       0 (0%)      21Mi (0%)        0 (0%)         3h15m\n  openshift-etcd-operator                           etcd-operator-79996f5bfb-2hxqs                                 10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         3h18m\n  openshift-etcd                                    etcd-guard-ip-10-0-135-246.ec2.internal                        10m (0%)      0 (0%)      5Mi (0%)         0 (0%)         3h14m\n  openshift-etcd                                    etcd-ip-10-0-135-246.ec2.internal                              360m (10%)    0 (0%)      910Mi (6%)       0 (0%)         3h3m\n  openshift-image-registry                          cluster-image-registry-operator-77bbb4466-v52fx                10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         3h18m\n  openshift-image-registry                          node-ca-k4hpq                                                  10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         3h9m\n  openshift-ingress-operator                        ingress-operator-597cb6fb8f-rs5db                              20m (0%)      0 (0%)      96Mi (0%)        0 (0%)         3h18m\n  openshift-insights                                insights-operator-86b7db95f-g8n2k                              10m (0%)      0 (0%)      30Mi (0%)        0 (0%)         3h18m\n  openshift-kube-apiserver-operator                 kube-apiserver-operator-78f848f7c5-m8gnq                       10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         3h18m\n  openshift-kube-apiserver                          kube-apiserver-guard-ip-10-0-135-246.ec2.internal              10m (0%)      0 (0%)      5Mi (0%)         0 (0%)         3h5m\n  openshift-kube-apiserver                          kube-apiserver-ip-10-0-135-246.ec2.internal                    290m (8%)     0 (0%)      1224Mi (8%)      0 (0%)         3h5m\n  openshift-kube-controller-manager-operator        kube-controller-manager-operator-697bdbf7-ltgjk                10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         3h18m\n  openshift-kube-controller-manager                 kube-controller-manager-guard-ip-10-0-135-246.ec2.internal     10m (0%)      0 (0%)      5Mi (0%)         0 (0%)         3h13m\n  openshift-kube-controller-manager                 kube-controller-manager-ip-10-0-135-246.ec2.internal           80m (2%)      0 (0%)      500Mi (3%)       0 (0%)         3h3m\n  openshift-kube-scheduler-operator                 openshift-kube-scheduler-operator-699ddd695-pbf2v              10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         3h18m\n  openshift-kube-scheduler                          openshift-kube-scheduler-guard-ip-10-0-135-246.ec2.internal    10m (0%)      0 (0%)      5Mi (0%)         0 (0%)         3h9m\n  openshift-kube-scheduler                          openshift-kube-scheduler-ip-10-0-135-246.ec2.internal          25m (0%)      0 (0%)      150Mi (1%)       0 (0%)         3h9m\n  openshift-kube-storage-version-migrator-operator  kube-storage-version-migrator-operator-579cb6b6b8-rw9rf        10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         3h18m\n  openshift-machine-api                             cluster-autoscaler-operator-647cbf4d9d-fs5p6                   30m (0%)      0 (0%)      70Mi (0%)        0 (0%)         3h17m\n  openshift-machine-api                             cluster-baremetal-operator-856f996786-xmtvg                    20m (0%)      0 (0%)      70Mi (0%)        0 (0%)         3h17m\n  openshift-machine-api                             control-plane-machine-set-operator-ff9599746-fjtfk             10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         3h17m\n  openshift-machine-api                             machine-api-operator-5576f85d6c-gb9jw                          20m (0%)      0 (0%)      70Mi (0%)        0 (0%)         3h18m\n  openshift-machine-config-operator                 machine-config-daemon-l2dpx                                    40m (1%)      0 (0%)      100Mi (0%)       0 (0%)         3h16m\n  openshift-machine-config-operator                 machine-config-operator-649f7f8847-d8kf8                       20m (0%)      0 (0%)      50Mi (0%)        0 (0%)         3h18m\n  openshift-machine-config-operator                 machine-config-server-9r5ft                                    20m (0%)      0 (0%)      50Mi (0%)        0 (0%)         3h14m\n  openshift-marketplace                             marketplace-operator-5c4786fc48-xkln4                          10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         3h18m\n  openshift-monitoring                              cluster-monitoring-operator-56b769b58f-nzmrf                   11m (0%)      0 (0%)      95Mi (0%)        0 (0%)         3h18m\n  openshift-monitoring                              node-exporter-8742b                                            9m (0%)       0 (0%)      47Mi (0%)        0 (0%)         3h7m\n  openshift-multus                                  multus-additional-cni-plugins-2kgnv                            10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         3h17m\n  openshift-multus                                  multus-xbgzd                                                   10m (0%)      0 (0%)      65Mi (0%)        0 (0%)         3h17m\n  openshift-multus                                  network-metrics-daemon-w5n4b                                   20m (0%)      0 (0%)      120Mi (0%)       0 (0%)         3h17m\n  openshift-network-diagnostics                     network-check-target-hg7qf                                     10m (0%)      0 (0%)      15Mi (0%)        0 (0%)         3h17m\n  openshift-network-operator                        network-operator-6668bb7647-rmxdr                              10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         3h18m\n  openshift-oauth-apiserver                         apiserver-789447d489-j5mht                                     150m (4%)     0 (0%)      200Mi (1%)       0 (0%)         3h11m\n  openshift-operator-lifecycle-manager              catalog-operator-756ccdbd48-jf2ln                              10m (0%)      0 (0%)      80Mi (0%)        0 (0%)         3h18m\n  openshift-operator-lifecycle-manager              olm-operator-6ff789c848-jshzb                                  10m (0%)      0 (0%)      160Mi (1%)       0 (0%)         3h18m\n  openshift-operator-lifecycle-manager              package-server-manager-58c47669df-fvbpd                        10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         3h18m\n  openshift-operator-lifecycle-manager              packageserver-7c75f6cd8f-hh7k4                                 10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         3h15m\n  openshift-ovn-kubernetes                          ovnkube-master-6ck75                                           60m (1%)      0 (0%)      1520Mi (10%)     0 (0%)         3h17m\n  openshift-ovn-kubernetes                          ovnkube-node-mxt6z                                             50m (1%)      0 (0%)      660Mi (4%)       0 (0%)         3h17m\n  openshift-route-controller-manager                route-controller-manager-68688dcf8f-zflf6                      100m (2%)     0 (0%)      100Mi (0%)       0 (0%)         3h8m\n  openshift-service-ca-operator                     service-ca-operator-78ddbb4597-gz56x                           10m (0%)      0 (0%)      80Mi (0%)        0 (0%)         3h18m\n  sonobuoy                                          sonobuoy-systemd-logs-daemon-set-329d7d7181d04e86-hbc9k        0 (0%)        0 (0%)      0 (0%)           0 (0%)         76m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                    Requests      Limits\n  --------                    --------      ------\n  cpu                         2170m (62%)   0 (0%)\n  memory                      9172Mi (62%)  0 (0%)\n  ephemeral-storage           0 (0%)        0 (0%)\n  hugepages-1Gi               0 (0%)        0 (0%)\n  hugepages-2Mi               0 (0%)        0 (0%)\n  attachable-volumes-aws-ebs  0             0\nEvents:\n  Type     Reason                     Age                    From                 Message\n  ----     ------                     ----                   ----                 -------\n  Normal   RegisteredNode             3h18m                  node-controller      Node ip-10-0-135-246.ec2.internal event: Registered Node ip-10-0-135-246.ec2.internal in Controller\n  Warning  ErrorReconcilingNode       3h16m (x2 over 3h16m)  controlplane         [k8s.ovn.org/node-chassis-id annotation not found for node ip-10-0-135-246.ec2.internal, macAddress annotation not found for node \"ip-10-0-135-246.ec2.internal\" , k8s.ovn.org/l3-gateway-config annotation not found for node \"ip-10-0-135-246.ec2.internal\"]\n  Normal   Uncordon                   3h14m                  machineconfigdaemon  Update completed for config rendered-master-3451104d0601207912e04b05a5d2dcb6 and node has been uncordoned\n  Normal   NodeDone                   3h14m                  machineconfigdaemon  Setting node ip-10-0-135-246.ec2.internal, currentConfig rendered-master-3451104d0601207912e04b05a5d2dcb6 to Done\n  Normal   ConfigDriftMonitorStarted  3h14m                  machineconfigdaemon  Config Drift Monitor started, watching against rendered-master-3451104d0601207912e04b05a5d2dcb6\n  Normal   RegisteredNode             3h9m                   node-controller      Node ip-10-0-135-246.ec2.internal event: Registered Node ip-10-0-135-246.ec2.internal in Controller\n  Normal   RegisteredNode             3h6m                   node-controller      Node ip-10-0-135-246.ec2.internal event: Registered Node ip-10-0-135-246.ec2.internal in Controller\n  Normal   RegisteredNode             3h3m                   node-controller      Node ip-10-0-135-246.ec2.internal event: Registered Node ip-10-0-135-246.ec2.internal in Controller\n  Normal   RegisteredNode             3h2m                   node-controller      Node ip-10-0-135-246.ec2.internal event: Registered Node ip-10-0-135-246.ec2.internal in Controller\n"
Jan 17 16:04:08.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-4873 describe namespace kubectl-4873'
Jan 17 16:04:08.421: INFO: stderr: ""
Jan 17 16:04:08.421: INFO: stdout: "Name:         kubectl-4873\nLabels:       e2e-framework=kubectl\n              e2e-run=277599aa-dbb9-45ba-ba19-6ee6b0fa425c\n              kubernetes.io/metadata.name=kubectl-4873\n              pod-security.kubernetes.io/audit=privileged\n              pod-security.kubernetes.io/audit-version=v1.24\n              pod-security.kubernetes.io/enforce=baseline\n              pod-security.kubernetes.io/warn=privileged\n              pod-security.kubernetes.io/warn-version=v1.24\nAnnotations:  openshift.io/sa.scc.mcs: s0:c63,c2\n              openshift.io/sa.scc.supplemental-groups: 1003910000/10000\n              openshift.io/sa.scc.uid-range: 1003910000/10000\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 17 16:04:08.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4873" for this suite. 01/17/23 16:04:08.425
{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","completed":303,"skipped":5597,"failed":0}
------------------------------
• [3.499 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1268
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1274

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:04:04.933
    Jan 17 16:04:04.933: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename kubectl 01/17/23 16:04:04.934
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:04:04.953
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:04:04.956
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1274
    Jan 17 16:04:04.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-4873 create -f -'
    Jan 17 16:04:05.972: INFO: stderr: ""
    Jan 17 16:04:05.972: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    Jan 17 16:04:05.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-4873 create -f -'
    Jan 17 16:04:06.882: INFO: stderr: ""
    Jan 17 16:04:06.882: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 01/17/23 16:04:06.882
    Jan 17 16:04:07.887: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 17 16:04:07.887: INFO: Found 1 / 1
    Jan 17 16:04:07.887: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Jan 17 16:04:07.889: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 17 16:04:07.889: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jan 17 16:04:07.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-4873 describe pod agnhost-primary-plhw9'
    Jan 17 16:04:07.942: INFO: stderr: ""
    Jan 17 16:04:07.942: INFO: stdout: "Name:             agnhost-primary-plhw9\nNamespace:        kubectl-4873\nPriority:         0\nService Account:  default\nNode:             ip-10-0-151-22.ec2.internal/10.0.151.22\nStart Time:       Tue, 17 Jan 2023 16:04:05 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      k8s.ovn.org/pod-networks:\n                    {\"default\":{\"ip_addresses\":[\"10.131.1.109/23\"],\"mac_address\":\"0a:58:0a:83:01:6d\",\"gateway_ips\":[\"10.131.0.1\"],\"ip_address\":\"10.131.1.109/2...\n                  k8s.v1.cni.cncf.io/network-status:\n                    [{\n                        \"name\": \"ovn-kubernetes\",\n                        \"interface\": \"eth0\",\n                        \"ips\": [\n                            \"10.131.1.109\"\n                        ],\n                        \"mac\": \"0a:58:0a:83:01:6d\",\n                        \"default\": true,\n                        \"dns\": {}\n                    }]\n                  k8s.v1.cni.cncf.io/networks-status:\n                    [{\n                        \"name\": \"ovn-kubernetes\",\n                        \"interface\": \"eth0\",\n                        \"ips\": [\n                            \"10.131.1.109\"\n                        ],\n                        \"mac\": \"0a:58:0a:83:01:6d\",\n                        \"default\": true,\n                        \"dns\": {}\n                    }]\n                  openshift.io/scc: anyuid\nStatus:           Running\nIP:               10.131.1.109\nIPs:\n  IP:           10.131.1.109\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   cri-o://9964539bc95354b3b4b6c30703f097bac80a177410c082e91509c0a570ab9413\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.40\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:37dd9ad84b34913b4bc57e00b0a28e8b00f18ca6a5ecec69461aa57402e5c787\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Tue, 17 Jan 2023 16:04:06 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-kkxtg (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-kkxtg:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\n    ConfigMapName:           openshift-service-ca.crt\n    ConfigMapOptional:       <nil>\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason          Age   From               Message\n  ----    ------          ----  ----               -------\n  Normal  Scheduled       1s    default-scheduler  Successfully assigned kubectl-4873/agnhost-primary-plhw9 to ip-10-0-151-22.ec2.internal\n  Normal  AddedInterface  1s    multus             Add eth0 [10.131.1.109/23] from ovn-kubernetes\n  Normal  Pulled          1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.40\" already present on machine\n  Normal  Created         1s    kubelet            Created container agnhost-primary\n  Normal  Started         1s    kubelet            Started container agnhost-primary\n"
    Jan 17 16:04:07.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-4873 describe rc agnhost-primary'
    Jan 17 16:04:07.998: INFO: stderr: ""
    Jan 17 16:04:07.998: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-4873\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.40\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-plhw9\n"
    Jan 17 16:04:07.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-4873 describe service agnhost-primary'
    Jan 17 16:04:08.051: INFO: stderr: ""
    Jan 17 16:04:08.051: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-4873\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                172.30.100.244\nIPs:               172.30.100.244\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.131.1.109:6379\nSession Affinity:  None\nEvents:            <none>\n"
    Jan 17 16:04:08.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-4873 describe node ip-10-0-135-246.ec2.internal'
    Jan 17 16:04:08.365: INFO: stderr: ""
    Jan 17 16:04:08.365: INFO: stdout: "Name:               ip-10-0-135-246.ec2.internal\nRoles:              control-plane,master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=m6i.xlarge\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=us-east-1\n                    failure-domain.beta.kubernetes.io/zone=us-east-1a\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-10-0-135-246.ec2.internal\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node-role.kubernetes.io/master=\n                    node.kubernetes.io/instance-type=m6i.xlarge\n                    node.openshift.io/os_id=rhcos\n                    topology.ebs.csi.aws.com/zone=us-east-1a\n                    topology.kubernetes.io/region=us-east-1\n                    topology.kubernetes.io/zone=us-east-1a\nAnnotations:        cloud.network.openshift.io/egress-ipconfig:\n                      [{\"interface\":\"eni-02083fb75eaaa84cc\",\"ifaddr\":{\"ipv4\":\"10.0.128.0/20\"},\"capacity\":{\"ipv4\":14,\"ipv6\":15}}]\n                    csi.volume.kubernetes.io/nodeid: {\"ebs.csi.aws.com\":\"i-0d084ed22cb4570b2\"}\n                    k8s.ovn.org/host-addresses: [\"10.0.135.246\"]\n                    k8s.ovn.org/l3-gateway-config:\n                      {\"default\":{\"mode\":\"shared\",\"interface-id\":\"br-ex_ip-10-0-135-246.ec2.internal\",\"mac-address\":\"02:48:8f:80:a8:05\",\"ip-addresses\":[\"10.0.13...\n                    k8s.ovn.org/node-chassis-id: 03e72bd2-40f4-4bc2-90a7-792643f23494\n                    k8s.ovn.org/node-gateway-router-lrp-ifaddr: {\"ipv4\":\"100.64.0.2/16\"}\n                    k8s.ovn.org/node-mgmt-port-mac-address: a6:00:3a:7a:7b:18\n                    k8s.ovn.org/node-primary-ifaddr: {\"ipv4\":\"10.0.135.246/20\"}\n                    k8s.ovn.org/node-subnets: {\"default\":\"10.128.0.0/23\"}\n                    machine.openshift.io/machine: openshift-machine-api/maszulik-rlbbb-master-0\n                    machineconfiguration.openshift.io/controlPlaneTopology: HighlyAvailable\n                    machineconfiguration.openshift.io/currentConfig: rendered-master-3451104d0601207912e04b05a5d2dcb6\n                    machineconfiguration.openshift.io/desiredConfig: rendered-master-3451104d0601207912e04b05a5d2dcb6\n                    machineconfiguration.openshift.io/desiredDrain: uncordon-rendered-master-3451104d0601207912e04b05a5d2dcb6\n                    machineconfiguration.openshift.io/lastAppliedDrain: uncordon-rendered-master-3451104d0601207912e04b05a5d2dcb6\n                    machineconfiguration.openshift.io/reason: \n                    machineconfiguration.openshift.io/state: Done\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Tue, 17 Jan 2023 12:45:30 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  ip-10-0-135-246.ec2.internal\n  AcquireTime:     <unset>\n  RenewTime:       Tue, 17 Jan 2023 16:04:02 +0000\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Tue, 17 Jan 2023 16:01:51 +0000   Tue, 17 Jan 2023 12:45:30 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Tue, 17 Jan 2023 16:01:51 +0000   Tue, 17 Jan 2023 12:45:30 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Tue, 17 Jan 2023 16:01:51 +0000   Tue, 17 Jan 2023 12:45:30 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Tue, 17 Jan 2023 16:01:51 +0000   Tue, 17 Jan 2023 12:47:43 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:   10.0.135.246\n  Hostname:     ip-10-0-135-246.ec2.internal\n  InternalDNS:  ip-10-0-135-246.ec2.internal\nCapacity:\n  attachable-volumes-aws-ebs:  39\n  cpu:                         4\n  ephemeral-storage:           125293548Ki\n  hugepages-1Gi:               0\n  hugepages-2Mi:               0\n  memory:                      16129408Ki\n  pods:                        250\nAllocatable:\n  attachable-volumes-aws-ebs:  39\n  cpu:                         3500m\n  ephemeral-storage:           114396791822\n  hugepages-1Gi:               0\n  hugepages-2Mi:               0\n  memory:                      14978432Ki\n  pods:                        250\nSystem Info:\n  Machine ID:                                       ec2abc352828bff4b5235884c7b9c3a6\n  System UUID:                                      ec2abc35-2828-bff4-b523-5884c7b9c3a6\n  Boot ID:                                          5a46b4c7-25c5-41c7-9690-4c9d03faef4b\n  Kernel Version:                                   4.18.0-372.40.1.el8_6.x86_64\n  OS Image:                                         Red Hat Enterprise Linux CoreOS 412.86.202301061548-0 (Ootpa)\n  Operating System:                                 linux\n  Architecture:                                     amd64\n  Container Runtime Version:                        cri-o://1.25.1-5.rhaos4.12.git6005903.el8\n  Kubelet Version:                                  v1.25.4+77bec7a\n  Kube-Proxy Version:                               v1.25.4+77bec7a\nProviderID:                                         aws:///us-east-1a/i-0d084ed22cb4570b2\nNon-terminated Pods:                                (63 in total)\n  Namespace                                         Name                                                           CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                                         ----                                                           ------------  ----------  ---------------  -------------  ---\n  openshift-apiserver-operator                      openshift-apiserver-operator-7ddc8958fb-zqcl9                  10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         3h18m\n  openshift-apiserver                               apiserver-5fb6ffcbd6-xzvls                                     110m (3%)     0 (0%)      250Mi (1%)       0 (0%)         3h7m\n  openshift-authentication-operator                 authentication-operator-65f78f5bc6-jklnl                       20m (0%)      0 (0%)      200Mi (1%)       0 (0%)         3h18m\n  openshift-authentication                          oauth-openshift-bf45995b4-s2mfq                                10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         3h5m\n  openshift-cloud-controller-manager-operator       cluster-cloud-controller-manager-operator-6b6d666976-64zz4     20m (0%)      0 (0%)      75Mi (0%)        0 (0%)         3h17m\n  openshift-cloud-credential-operator               cloud-credential-operator-6ffc47fc7f-6h8b9                     20m (0%)      0 (0%)      170Mi (1%)       0 (0%)         3h18m\n  openshift-cloud-network-config-controller         cloud-network-config-controller-d9f74866d-x99lf                10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         3h18m\n  openshift-cluster-csi-drivers                     aws-ebs-csi-driver-controller-7464b474df-qfqzj                 110m (3%)     0 (0%)      400Mi (2%)       0 (0%)         3h15m\n  openshift-cluster-csi-drivers                     aws-ebs-csi-driver-node-jkjc8                                  30m (0%)      0 (0%)      150Mi (1%)       0 (0%)         3h15m\n  openshift-cluster-machine-approver                machine-approver-745689cc9c-v2wjr                              20m (0%)      0 (0%)      70Mi (0%)        0 (0%)         3h17m\n  openshift-cluster-node-tuning-operator            cluster-node-tuning-operator-8648f4dc6c-xzgpc                  10m (0%)      0 (0%)      20Mi (0%)        0 (0%)         3h18m\n  openshift-cluster-node-tuning-operator            tuned-zglbw                                                    10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         3h14m\n  openshift-cluster-storage-operator                cluster-storage-operator-66c94c594-6hh8n                       10m (0%)      0 (0%)      20Mi (0%)        0 (0%)         3h18m\n  openshift-cluster-storage-operator                csi-snapshot-controller-operator-7d599d59ff-s7fnt              10m (0%)      0 (0%)      65Mi (0%)        0 (0%)         3h18m\n  openshift-config-operator                         openshift-config-operator-77b6ddc656-2pvfk                     10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         3h18m\n  openshift-console                                 console-7dfd69cc54-6xljq                                       10m (0%)      0 (0%)      100Mi (0%)       0 (0%)         3h5m\n  openshift-controller-manager-operator             openshift-controller-manager-operator-67f74c4b9b-74lcr         10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         3h18m\n  openshift-controller-manager                      controller-manager-757b94b9d-gjn7l                             100m (2%)     0 (0%)      100Mi (0%)       0 (0%)         3h8m\n  openshift-dns-operator                            dns-operator-64688bfdd4-lrr9x                                  20m (0%)      0 (0%)      69Mi (0%)        0 (0%)         3h18m\n  openshift-dns                                     dns-default-8fghg                                              60m (1%)      0 (0%)      110Mi (0%)       0 (0%)         3h15m\n  openshift-dns                                     node-resolver-lkc88                                            5m (0%)       0 (0%)      21Mi (0%)        0 (0%)         3h15m\n  openshift-etcd-operator                           etcd-operator-79996f5bfb-2hxqs                                 10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         3h18m\n  openshift-etcd                                    etcd-guard-ip-10-0-135-246.ec2.internal                        10m (0%)      0 (0%)      5Mi (0%)         0 (0%)         3h14m\n  openshift-etcd                                    etcd-ip-10-0-135-246.ec2.internal                              360m (10%)    0 (0%)      910Mi (6%)       0 (0%)         3h3m\n  openshift-image-registry                          cluster-image-registry-operator-77bbb4466-v52fx                10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         3h18m\n  openshift-image-registry                          node-ca-k4hpq                                                  10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         3h9m\n  openshift-ingress-operator                        ingress-operator-597cb6fb8f-rs5db                              20m (0%)      0 (0%)      96Mi (0%)        0 (0%)         3h18m\n  openshift-insights                                insights-operator-86b7db95f-g8n2k                              10m (0%)      0 (0%)      30Mi (0%)        0 (0%)         3h18m\n  openshift-kube-apiserver-operator                 kube-apiserver-operator-78f848f7c5-m8gnq                       10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         3h18m\n  openshift-kube-apiserver                          kube-apiserver-guard-ip-10-0-135-246.ec2.internal              10m (0%)      0 (0%)      5Mi (0%)         0 (0%)         3h5m\n  openshift-kube-apiserver                          kube-apiserver-ip-10-0-135-246.ec2.internal                    290m (8%)     0 (0%)      1224Mi (8%)      0 (0%)         3h5m\n  openshift-kube-controller-manager-operator        kube-controller-manager-operator-697bdbf7-ltgjk                10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         3h18m\n  openshift-kube-controller-manager                 kube-controller-manager-guard-ip-10-0-135-246.ec2.internal     10m (0%)      0 (0%)      5Mi (0%)         0 (0%)         3h13m\n  openshift-kube-controller-manager                 kube-controller-manager-ip-10-0-135-246.ec2.internal           80m (2%)      0 (0%)      500Mi (3%)       0 (0%)         3h3m\n  openshift-kube-scheduler-operator                 openshift-kube-scheduler-operator-699ddd695-pbf2v              10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         3h18m\n  openshift-kube-scheduler                          openshift-kube-scheduler-guard-ip-10-0-135-246.ec2.internal    10m (0%)      0 (0%)      5Mi (0%)         0 (0%)         3h9m\n  openshift-kube-scheduler                          openshift-kube-scheduler-ip-10-0-135-246.ec2.internal          25m (0%)      0 (0%)      150Mi (1%)       0 (0%)         3h9m\n  openshift-kube-storage-version-migrator-operator  kube-storage-version-migrator-operator-579cb6b6b8-rw9rf        10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         3h18m\n  openshift-machine-api                             cluster-autoscaler-operator-647cbf4d9d-fs5p6                   30m (0%)      0 (0%)      70Mi (0%)        0 (0%)         3h17m\n  openshift-machine-api                             cluster-baremetal-operator-856f996786-xmtvg                    20m (0%)      0 (0%)      70Mi (0%)        0 (0%)         3h17m\n  openshift-machine-api                             control-plane-machine-set-operator-ff9599746-fjtfk             10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         3h17m\n  openshift-machine-api                             machine-api-operator-5576f85d6c-gb9jw                          20m (0%)      0 (0%)      70Mi (0%)        0 (0%)         3h18m\n  openshift-machine-config-operator                 machine-config-daemon-l2dpx                                    40m (1%)      0 (0%)      100Mi (0%)       0 (0%)         3h16m\n  openshift-machine-config-operator                 machine-config-operator-649f7f8847-d8kf8                       20m (0%)      0 (0%)      50Mi (0%)        0 (0%)         3h18m\n  openshift-machine-config-operator                 machine-config-server-9r5ft                                    20m (0%)      0 (0%)      50Mi (0%)        0 (0%)         3h14m\n  openshift-marketplace                             marketplace-operator-5c4786fc48-xkln4                          10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         3h18m\n  openshift-monitoring                              cluster-monitoring-operator-56b769b58f-nzmrf                   11m (0%)      0 (0%)      95Mi (0%)        0 (0%)         3h18m\n  openshift-monitoring                              node-exporter-8742b                                            9m (0%)       0 (0%)      47Mi (0%)        0 (0%)         3h7m\n  openshift-multus                                  multus-additional-cni-plugins-2kgnv                            10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         3h17m\n  openshift-multus                                  multus-xbgzd                                                   10m (0%)      0 (0%)      65Mi (0%)        0 (0%)         3h17m\n  openshift-multus                                  network-metrics-daemon-w5n4b                                   20m (0%)      0 (0%)      120Mi (0%)       0 (0%)         3h17m\n  openshift-network-diagnostics                     network-check-target-hg7qf                                     10m (0%)      0 (0%)      15Mi (0%)        0 (0%)         3h17m\n  openshift-network-operator                        network-operator-6668bb7647-rmxdr                              10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         3h18m\n  openshift-oauth-apiserver                         apiserver-789447d489-j5mht                                     150m (4%)     0 (0%)      200Mi (1%)       0 (0%)         3h11m\n  openshift-operator-lifecycle-manager              catalog-operator-756ccdbd48-jf2ln                              10m (0%)      0 (0%)      80Mi (0%)        0 (0%)         3h18m\n  openshift-operator-lifecycle-manager              olm-operator-6ff789c848-jshzb                                  10m (0%)      0 (0%)      160Mi (1%)       0 (0%)         3h18m\n  openshift-operator-lifecycle-manager              package-server-manager-58c47669df-fvbpd                        10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         3h18m\n  openshift-operator-lifecycle-manager              packageserver-7c75f6cd8f-hh7k4                                 10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         3h15m\n  openshift-ovn-kubernetes                          ovnkube-master-6ck75                                           60m (1%)      0 (0%)      1520Mi (10%)     0 (0%)         3h17m\n  openshift-ovn-kubernetes                          ovnkube-node-mxt6z                                             50m (1%)      0 (0%)      660Mi (4%)       0 (0%)         3h17m\n  openshift-route-controller-manager                route-controller-manager-68688dcf8f-zflf6                      100m (2%)     0 (0%)      100Mi (0%)       0 (0%)         3h8m\n  openshift-service-ca-operator                     service-ca-operator-78ddbb4597-gz56x                           10m (0%)      0 (0%)      80Mi (0%)        0 (0%)         3h18m\n  sonobuoy                                          sonobuoy-systemd-logs-daemon-set-329d7d7181d04e86-hbc9k        0 (0%)        0 (0%)      0 (0%)           0 (0%)         76m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                    Requests      Limits\n  --------                    --------      ------\n  cpu                         2170m (62%)   0 (0%)\n  memory                      9172Mi (62%)  0 (0%)\n  ephemeral-storage           0 (0%)        0 (0%)\n  hugepages-1Gi               0 (0%)        0 (0%)\n  hugepages-2Mi               0 (0%)        0 (0%)\n  attachable-volumes-aws-ebs  0             0\nEvents:\n  Type     Reason                     Age                    From                 Message\n  ----     ------                     ----                   ----                 -------\n  Normal   RegisteredNode             3h18m                  node-controller      Node ip-10-0-135-246.ec2.internal event: Registered Node ip-10-0-135-246.ec2.internal in Controller\n  Warning  ErrorReconcilingNode       3h16m (x2 over 3h16m)  controlplane         [k8s.ovn.org/node-chassis-id annotation not found for node ip-10-0-135-246.ec2.internal, macAddress annotation not found for node \"ip-10-0-135-246.ec2.internal\" , k8s.ovn.org/l3-gateway-config annotation not found for node \"ip-10-0-135-246.ec2.internal\"]\n  Normal   Uncordon                   3h14m                  machineconfigdaemon  Update completed for config rendered-master-3451104d0601207912e04b05a5d2dcb6 and node has been uncordoned\n  Normal   NodeDone                   3h14m                  machineconfigdaemon  Setting node ip-10-0-135-246.ec2.internal, currentConfig rendered-master-3451104d0601207912e04b05a5d2dcb6 to Done\n  Normal   ConfigDriftMonitorStarted  3h14m                  machineconfigdaemon  Config Drift Monitor started, watching against rendered-master-3451104d0601207912e04b05a5d2dcb6\n  Normal   RegisteredNode             3h9m                   node-controller      Node ip-10-0-135-246.ec2.internal event: Registered Node ip-10-0-135-246.ec2.internal in Controller\n  Normal   RegisteredNode             3h6m                   node-controller      Node ip-10-0-135-246.ec2.internal event: Registered Node ip-10-0-135-246.ec2.internal in Controller\n  Normal   RegisteredNode             3h3m                   node-controller      Node ip-10-0-135-246.ec2.internal event: Registered Node ip-10-0-135-246.ec2.internal in Controller\n  Normal   RegisteredNode             3h2m                   node-controller      Node ip-10-0-135-246.ec2.internal event: Registered Node ip-10-0-135-246.ec2.internal in Controller\n"
    Jan 17 16:04:08.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-4873 describe namespace kubectl-4873'
    Jan 17 16:04:08.421: INFO: stderr: ""
    Jan 17 16:04:08.421: INFO: stdout: "Name:         kubectl-4873\nLabels:       e2e-framework=kubectl\n              e2e-run=277599aa-dbb9-45ba-ba19-6ee6b0fa425c\n              kubernetes.io/metadata.name=kubectl-4873\n              pod-security.kubernetes.io/audit=privileged\n              pod-security.kubernetes.io/audit-version=v1.24\n              pod-security.kubernetes.io/enforce=baseline\n              pod-security.kubernetes.io/warn=privileged\n              pod-security.kubernetes.io/warn-version=v1.24\nAnnotations:  openshift.io/sa.scc.mcs: s0:c63,c2\n              openshift.io/sa.scc.supplemental-groups: 1003910000/10000\n              openshift.io/sa.scc.uid-range: 1003910000/10000\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 17 16:04:08.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-4873" for this suite. 01/17/23 16:04:08.425
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:04:08.433
Jan 17 16:04:08.433: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename emptydir 01/17/23 16:04:08.433
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:04:08.453
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:04:08.457
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116
STEP: Creating a pod to test emptydir 0777 on tmpfs 01/17/23 16:04:08.46
W0117 16:04:08.484446      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 17 16:04:08.484: INFO: Waiting up to 5m0s for pod "pod-fc94109f-8d9b-40ad-ae74-d734f816b9fd" in namespace "emptydir-6989" to be "Succeeded or Failed"
Jan 17 16:04:08.488: INFO: Pod "pod-fc94109f-8d9b-40ad-ae74-d734f816b9fd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.778833ms
Jan 17 16:04:10.491: INFO: Pod "pod-fc94109f-8d9b-40ad-ae74-d734f816b9fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006492852s
Jan 17 16:04:12.492: INFO: Pod "pod-fc94109f-8d9b-40ad-ae74-d734f816b9fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008229209s
STEP: Saw pod success 01/17/23 16:04:12.492
Jan 17 16:04:12.492: INFO: Pod "pod-fc94109f-8d9b-40ad-ae74-d734f816b9fd" satisfied condition "Succeeded or Failed"
Jan 17 16:04:12.501: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-fc94109f-8d9b-40ad-ae74-d734f816b9fd container test-container: <nil>
STEP: delete the pod 01/17/23 16:04:12.507
Jan 17 16:04:12.526: INFO: Waiting for pod pod-fc94109f-8d9b-40ad-ae74-d734f816b9fd to disappear
Jan 17 16:04:12.528: INFO: Pod pod-fc94109f-8d9b-40ad-ae74-d734f816b9fd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 17 16:04:12.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6989" for this suite. 01/17/23 16:04:12.533
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":304,"skipped":5624,"failed":0}
------------------------------
• [4.109 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:04:08.433
    Jan 17 16:04:08.433: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename emptydir 01/17/23 16:04:08.433
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:04:08.453
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:04:08.457
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:116
    STEP: Creating a pod to test emptydir 0777 on tmpfs 01/17/23 16:04:08.46
    W0117 16:04:08.484446      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jan 17 16:04:08.484: INFO: Waiting up to 5m0s for pod "pod-fc94109f-8d9b-40ad-ae74-d734f816b9fd" in namespace "emptydir-6989" to be "Succeeded or Failed"
    Jan 17 16:04:08.488: INFO: Pod "pod-fc94109f-8d9b-40ad-ae74-d734f816b9fd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.778833ms
    Jan 17 16:04:10.491: INFO: Pod "pod-fc94109f-8d9b-40ad-ae74-d734f816b9fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006492852s
    Jan 17 16:04:12.492: INFO: Pod "pod-fc94109f-8d9b-40ad-ae74-d734f816b9fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008229209s
    STEP: Saw pod success 01/17/23 16:04:12.492
    Jan 17 16:04:12.492: INFO: Pod "pod-fc94109f-8d9b-40ad-ae74-d734f816b9fd" satisfied condition "Succeeded or Failed"
    Jan 17 16:04:12.501: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-fc94109f-8d9b-40ad-ae74-d734f816b9fd container test-container: <nil>
    STEP: delete the pod 01/17/23 16:04:12.507
    Jan 17 16:04:12.526: INFO: Waiting for pod pod-fc94109f-8d9b-40ad-ae74-d734f816b9fd to disappear
    Jan 17 16:04:12.528: INFO: Pod pod-fc94109f-8d9b-40ad-ae74-d734f816b9fd no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 17 16:04:12.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-6989" for this suite. 01/17/23 16:04:12.533
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:04:12.542
Jan 17 16:04:12.542: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename services 01/17/23 16:04:12.542
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:04:12.561
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:04:12.563
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852
STEP: creating service multi-endpoint-test in namespace services-9070 01/17/23 16:04:12.568
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9070 to expose endpoints map[] 01/17/23 16:04:12.592
Jan 17 16:04:12.619: INFO: successfully validated that service multi-endpoint-test in namespace services-9070 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-9070 01/17/23 16:04:12.619
Jan 17 16:04:12.659: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-9070" to be "running and ready"
Jan 17 16:04:12.674: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 14.546868ms
Jan 17 16:04:12.674: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 16:04:14.679: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.019483989s
Jan 17 16:04:14.679: INFO: The phase of Pod pod1 is Running (Ready = true)
Jan 17 16:04:14.679: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9070 to expose endpoints map[pod1:[100]] 01/17/23 16:04:14.682
Jan 17 16:04:14.691: INFO: successfully validated that service multi-endpoint-test in namespace services-9070 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-9070 01/17/23 16:04:14.691
Jan 17 16:04:14.701: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-9070" to be "running and ready"
Jan 17 16:04:14.705: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.260914ms
Jan 17 16:04:14.705: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 16:04:16.710: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.008384947s
Jan 17 16:04:16.710: INFO: The phase of Pod pod2 is Running (Ready = true)
Jan 17 16:04:16.710: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9070 to expose endpoints map[pod1:[100] pod2:[101]] 01/17/23 16:04:16.712
Jan 17 16:04:16.724: INFO: successfully validated that service multi-endpoint-test in namespace services-9070 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 01/17/23 16:04:16.724
Jan 17 16:04:16.724: INFO: Creating new exec pod
Jan 17 16:04:16.737: INFO: Waiting up to 5m0s for pod "execpod2fl4n" in namespace "services-9070" to be "running"
Jan 17 16:04:16.739: INFO: Pod "execpod2fl4n": Phase="Pending", Reason="", readiness=false. Elapsed: 2.295434ms
Jan 17 16:04:18.742: INFO: Pod "execpod2fl4n": Phase="Running", Reason="", readiness=true. Elapsed: 2.005376199s
Jan 17 16:04:18.742: INFO: Pod "execpod2fl4n" satisfied condition "running"
Jan 17 16:04:19.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-9070 exec execpod2fl4n -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
Jan 17 16:04:19.877: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Jan 17 16:04:19.877: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 17 16:04:19.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-9070 exec execpod2fl4n -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.58.124 80'
Jan 17 16:04:19.986: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.58.124 80\nConnection to 172.30.58.124 80 port [tcp/http] succeeded!\n"
Jan 17 16:04:19.986: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 17 16:04:19.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-9070 exec execpod2fl4n -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
Jan 17 16:04:20.084: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Jan 17 16:04:20.084: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 17 16:04:20.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-9070 exec execpod2fl4n -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.58.124 81'
Jan 17 16:04:20.178: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.58.124 81\nConnection to 172.30.58.124 81 port [tcp/*] succeeded!\n"
Jan 17 16:04:20.178: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-9070 01/17/23 16:04:20.178
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9070 to expose endpoints map[pod2:[101]] 01/17/23 16:04:20.193
Jan 17 16:04:21.217: INFO: successfully validated that service multi-endpoint-test in namespace services-9070 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-9070 01/17/23 16:04:21.217
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9070 to expose endpoints map[] 01/17/23 16:04:21.239
Jan 17 16:04:22.262: INFO: successfully validated that service multi-endpoint-test in namespace services-9070 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 17 16:04:22.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9070" for this suite. 01/17/23 16:04:22.301
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","completed":305,"skipped":5627,"failed":0}
------------------------------
• [SLOW TEST] [9.778 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:04:12.542
    Jan 17 16:04:12.542: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename services 01/17/23 16:04:12.542
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:04:12.561
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:04:12.563
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:852
    STEP: creating service multi-endpoint-test in namespace services-9070 01/17/23 16:04:12.568
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9070 to expose endpoints map[] 01/17/23 16:04:12.592
    Jan 17 16:04:12.619: INFO: successfully validated that service multi-endpoint-test in namespace services-9070 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-9070 01/17/23 16:04:12.619
    Jan 17 16:04:12.659: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-9070" to be "running and ready"
    Jan 17 16:04:12.674: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 14.546868ms
    Jan 17 16:04:12.674: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 16:04:14.679: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.019483989s
    Jan 17 16:04:14.679: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jan 17 16:04:14.679: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9070 to expose endpoints map[pod1:[100]] 01/17/23 16:04:14.682
    Jan 17 16:04:14.691: INFO: successfully validated that service multi-endpoint-test in namespace services-9070 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-9070 01/17/23 16:04:14.691
    Jan 17 16:04:14.701: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-9070" to be "running and ready"
    Jan 17 16:04:14.705: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.260914ms
    Jan 17 16:04:14.705: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 16:04:16.710: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.008384947s
    Jan 17 16:04:16.710: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jan 17 16:04:16.710: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9070 to expose endpoints map[pod1:[100] pod2:[101]] 01/17/23 16:04:16.712
    Jan 17 16:04:16.724: INFO: successfully validated that service multi-endpoint-test in namespace services-9070 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 01/17/23 16:04:16.724
    Jan 17 16:04:16.724: INFO: Creating new exec pod
    Jan 17 16:04:16.737: INFO: Waiting up to 5m0s for pod "execpod2fl4n" in namespace "services-9070" to be "running"
    Jan 17 16:04:16.739: INFO: Pod "execpod2fl4n": Phase="Pending", Reason="", readiness=false. Elapsed: 2.295434ms
    Jan 17 16:04:18.742: INFO: Pod "execpod2fl4n": Phase="Running", Reason="", readiness=true. Elapsed: 2.005376199s
    Jan 17 16:04:18.742: INFO: Pod "execpod2fl4n" satisfied condition "running"
    Jan 17 16:04:19.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-9070 exec execpod2fl4n -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
    Jan 17 16:04:19.877: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    Jan 17 16:04:19.877: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 17 16:04:19.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-9070 exec execpod2fl4n -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.58.124 80'
    Jan 17 16:04:19.986: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.58.124 80\nConnection to 172.30.58.124 80 port [tcp/http] succeeded!\n"
    Jan 17 16:04:19.986: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 17 16:04:19.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-9070 exec execpod2fl4n -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
    Jan 17 16:04:20.084: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    Jan 17 16:04:20.084: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 17 16:04:20.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-9070 exec execpod2fl4n -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.58.124 81'
    Jan 17 16:04:20.178: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.58.124 81\nConnection to 172.30.58.124 81 port [tcp/*] succeeded!\n"
    Jan 17 16:04:20.178: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod1 in namespace services-9070 01/17/23 16:04:20.178
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9070 to expose endpoints map[pod2:[101]] 01/17/23 16:04:20.193
    Jan 17 16:04:21.217: INFO: successfully validated that service multi-endpoint-test in namespace services-9070 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-9070 01/17/23 16:04:21.217
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9070 to expose endpoints map[] 01/17/23 16:04:21.239
    Jan 17 16:04:22.262: INFO: successfully validated that service multi-endpoint-test in namespace services-9070 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 17 16:04:22.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-9070" for this suite. 01/17/23 16:04:22.301
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:04:22.32
Jan 17 16:04:22.320: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename proxy 01/17/23 16:04:22.321
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:04:22.342
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:04:22.346
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 01/17/23 16:04:22.387
STEP: creating replication controller proxy-service-mx2z5 in namespace proxy-4776 01/17/23 16:04:22.387
I0117 16:04:22.398511      22 runners.go:193] Created replication controller with name: proxy-service-mx2z5, namespace: proxy-4776, replica count: 1
I0117 16:04:23.449900      22 runners.go:193] proxy-service-mx2z5 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0117 16:04:24.450721      22 runners.go:193] proxy-service-mx2z5 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0117 16:04:25.451686      22 runners.go:193] proxy-service-mx2z5 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 17 16:04:25.455: INFO: setup took 3.100930353s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 01/17/23 16:04:25.455
Jan 17 16:04:25.468: INFO: (0) /api/v1/namespaces/proxy-4776/services/http:proxy-service-mx2z5:portname2/proxy/: bar (200; 12.511229ms)
Jan 17 16:04:25.468: INFO: (0) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:1080/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:1080/proxy/rewriteme">... (200; 12.445961ms)
Jan 17 16:04:25.468: INFO: (0) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76/proxy/rewriteme">test</a> (200; 12.43874ms)
Jan 17 16:04:25.468: INFO: (0) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:162/proxy/: bar (200; 12.620406ms)
Jan 17 16:04:25.468: INFO: (0) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:162/proxy/: bar (200; 12.856094ms)
Jan 17 16:04:25.469: INFO: (0) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:160/proxy/: foo (200; 12.879902ms)
Jan 17 16:04:25.469: INFO: (0) /api/v1/namespaces/proxy-4776/services/proxy-service-mx2z5:portname2/proxy/: bar (200; 12.864058ms)
Jan 17 16:04:25.469: INFO: (0) /api/v1/namespaces/proxy-4776/services/http:proxy-service-mx2z5:portname1/proxy/: foo (200; 12.922657ms)
Jan 17 16:04:25.469: INFO: (0) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:160/proxy/: foo (200; 13.079104ms)
Jan 17 16:04:25.469: INFO: (0) /api/v1/namespaces/proxy-4776/services/https:proxy-service-mx2z5:tlsportname2/proxy/: tls qux (200; 12.957886ms)
Jan 17 16:04:25.469: INFO: (0) /api/v1/namespaces/proxy-4776/services/proxy-service-mx2z5:portname1/proxy/: foo (200; 13.039249ms)
Jan 17 16:04:25.469: INFO: (0) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:462/proxy/: tls qux (200; 13.04207ms)
Jan 17 16:04:25.469: INFO: (0) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:1080/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:1080/proxy/rewriteme">test<... (200; 12.931451ms)
Jan 17 16:04:25.469: INFO: (0) /api/v1/namespaces/proxy-4776/services/https:proxy-service-mx2z5:tlsportname1/proxy/: tls baz (200; 13.021732ms)
Jan 17 16:04:25.469: INFO: (0) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:460/proxy/: tls baz (200; 13.126246ms)
Jan 17 16:04:25.469: INFO: (0) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:443/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:443/proxy/tlsrewritem... (200; 13.028357ms)
Jan 17 16:04:25.473: INFO: (1) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:443/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:443/proxy/tlsrewritem... (200; 3.786653ms)
Jan 17 16:04:25.473: INFO: (1) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76/proxy/rewriteme">test</a> (200; 4.141494ms)
Jan 17 16:04:25.473: INFO: (1) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:462/proxy/: tls qux (200; 4.099388ms)
Jan 17 16:04:25.473: INFO: (1) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:160/proxy/: foo (200; 4.285675ms)
Jan 17 16:04:25.473: INFO: (1) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:160/proxy/: foo (200; 4.447446ms)
Jan 17 16:04:25.473: INFO: (1) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:162/proxy/: bar (200; 4.449439ms)
Jan 17 16:04:25.474: INFO: (1) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:162/proxy/: bar (200; 5.254927ms)
Jan 17 16:04:25.474: INFO: (1) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:1080/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:1080/proxy/rewriteme">... (200; 5.340407ms)
Jan 17 16:04:25.474: INFO: (1) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:460/proxy/: tls baz (200; 5.38878ms)
Jan 17 16:04:25.474: INFO: (1) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:1080/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:1080/proxy/rewriteme">test<... (200; 5.373455ms)
Jan 17 16:04:25.476: INFO: (1) /api/v1/namespaces/proxy-4776/services/https:proxy-service-mx2z5:tlsportname1/proxy/: tls baz (200; 6.690432ms)
Jan 17 16:04:25.476: INFO: (1) /api/v1/namespaces/proxy-4776/services/http:proxy-service-mx2z5:portname1/proxy/: foo (200; 7.408388ms)
Jan 17 16:04:25.476: INFO: (1) /api/v1/namespaces/proxy-4776/services/proxy-service-mx2z5:portname1/proxy/: foo (200; 7.433345ms)
Jan 17 16:04:25.477: INFO: (1) /api/v1/namespaces/proxy-4776/services/https:proxy-service-mx2z5:tlsportname2/proxy/: tls qux (200; 7.557818ms)
Jan 17 16:04:25.478: INFO: (1) /api/v1/namespaces/proxy-4776/services/proxy-service-mx2z5:portname2/proxy/: bar (200; 8.936711ms)
Jan 17 16:04:25.478: INFO: (1) /api/v1/namespaces/proxy-4776/services/http:proxy-service-mx2z5:portname2/proxy/: bar (200; 9.01177ms)
Jan 17 16:04:25.482: INFO: (2) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:162/proxy/: bar (200; 4.090366ms)
Jan 17 16:04:25.482: INFO: (2) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:443/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:443/proxy/tlsrewritem... (200; 4.261168ms)
Jan 17 16:04:25.483: INFO: (2) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:460/proxy/: tls baz (200; 5.12019ms)
Jan 17 16:04:25.483: INFO: (2) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76/proxy/rewriteme">test</a> (200; 5.153379ms)
Jan 17 16:04:25.484: INFO: (2) /api/v1/namespaces/proxy-4776/services/proxy-service-mx2z5:portname2/proxy/: bar (200; 5.962417ms)
Jan 17 16:04:25.484: INFO: (2) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:1080/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:1080/proxy/rewriteme">test<... (200; 6.100292ms)
Jan 17 16:04:25.485: INFO: (2) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:160/proxy/: foo (200; 6.452171ms)
Jan 17 16:04:25.485: INFO: (2) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:1080/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:1080/proxy/rewriteme">... (200; 6.425503ms)
Jan 17 16:04:25.485: INFO: (2) /api/v1/namespaces/proxy-4776/services/proxy-service-mx2z5:portname1/proxy/: foo (200; 6.646007ms)
Jan 17 16:04:25.485: INFO: (2) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:162/proxy/: bar (200; 6.600042ms)
Jan 17 16:04:25.485: INFO: (2) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:462/proxy/: tls qux (200; 6.819506ms)
Jan 17 16:04:25.485: INFO: (2) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:160/proxy/: foo (200; 7.215658ms)
Jan 17 16:04:25.485: INFO: (2) /api/v1/namespaces/proxy-4776/services/https:proxy-service-mx2z5:tlsportname1/proxy/: tls baz (200; 7.493065ms)
Jan 17 16:04:25.486: INFO: (2) /api/v1/namespaces/proxy-4776/services/https:proxy-service-mx2z5:tlsportname2/proxy/: tls qux (200; 7.607833ms)
Jan 17 16:04:25.486: INFO: (2) /api/v1/namespaces/proxy-4776/services/http:proxy-service-mx2z5:portname1/proxy/: foo (200; 7.682208ms)
Jan 17 16:04:25.486: INFO: (2) /api/v1/namespaces/proxy-4776/services/http:proxy-service-mx2z5:portname2/proxy/: bar (200; 7.664835ms)
Jan 17 16:04:25.489: INFO: (3) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76/proxy/rewriteme">test</a> (200; 3.480718ms)
Jan 17 16:04:25.490: INFO: (3) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:162/proxy/: bar (200; 4.128127ms)
Jan 17 16:04:25.490: INFO: (3) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:443/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:443/proxy/tlsrewritem... (200; 4.132404ms)
Jan 17 16:04:25.490: INFO: (3) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:162/proxy/: bar (200; 4.504385ms)
Jan 17 16:04:25.491: INFO: (3) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:460/proxy/: tls baz (200; 4.697997ms)
Jan 17 16:04:25.491: INFO: (3) /api/v1/namespaces/proxy-4776/services/http:proxy-service-mx2z5:portname2/proxy/: bar (200; 5.359255ms)
Jan 17 16:04:25.492: INFO: (3) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:160/proxy/: foo (200; 5.734029ms)
Jan 17 16:04:25.492: INFO: (3) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:160/proxy/: foo (200; 5.86506ms)
Jan 17 16:04:25.492: INFO: (3) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:462/proxy/: tls qux (200; 5.840286ms)
Jan 17 16:04:25.492: INFO: (3) /api/v1/namespaces/proxy-4776/services/https:proxy-service-mx2z5:tlsportname2/proxy/: tls qux (200; 6.100561ms)
Jan 17 16:04:25.493: INFO: (3) /api/v1/namespaces/proxy-4776/services/proxy-service-mx2z5:portname2/proxy/: bar (200; 7.258084ms)
Jan 17 16:04:25.493: INFO: (3) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:1080/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:1080/proxy/rewriteme">test<... (200; 7.254329ms)
Jan 17 16:04:25.493: INFO: (3) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:1080/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:1080/proxy/rewriteme">... (200; 7.294055ms)
Jan 17 16:04:25.494: INFO: (3) /api/v1/namespaces/proxy-4776/services/http:proxy-service-mx2z5:portname1/proxy/: foo (200; 8.460801ms)
Jan 17 16:04:25.495: INFO: (3) /api/v1/namespaces/proxy-4776/services/https:proxy-service-mx2z5:tlsportname1/proxy/: tls baz (200; 8.488175ms)
Jan 17 16:04:25.495: INFO: (3) /api/v1/namespaces/proxy-4776/services/proxy-service-mx2z5:portname1/proxy/: foo (200; 8.523368ms)
Jan 17 16:04:25.499: INFO: (4) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:160/proxy/: foo (200; 4.500641ms)
Jan 17 16:04:25.499: INFO: (4) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:462/proxy/: tls qux (200; 4.597808ms)
Jan 17 16:04:25.499: INFO: (4) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:160/proxy/: foo (200; 4.761876ms)
Jan 17 16:04:25.499: INFO: (4) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:443/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:443/proxy/tlsrewritem... (200; 4.81394ms)
Jan 17 16:04:25.499: INFO: (4) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:162/proxy/: bar (200; 4.782829ms)
Jan 17 16:04:25.500: INFO: (4) /api/v1/namespaces/proxy-4776/services/proxy-service-mx2z5:portname1/proxy/: foo (200; 5.178995ms)
Jan 17 16:04:25.500: INFO: (4) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:460/proxy/: tls baz (200; 5.237128ms)
Jan 17 16:04:25.500: INFO: (4) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:162/proxy/: bar (200; 5.60741ms)
Jan 17 16:04:25.501: INFO: (4) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:1080/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:1080/proxy/rewriteme">... (200; 5.808848ms)
Jan 17 16:04:25.501: INFO: (4) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:1080/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:1080/proxy/rewriteme">test<... (200; 5.868347ms)
Jan 17 16:04:25.501: INFO: (4) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76/proxy/rewriteme">test</a> (200; 5.792009ms)
Jan 17 16:04:25.501: INFO: (4) /api/v1/namespaces/proxy-4776/services/https:proxy-service-mx2z5:tlsportname2/proxy/: tls qux (200; 6.395966ms)
Jan 17 16:04:25.502: INFO: (4) /api/v1/namespaces/proxy-4776/services/https:proxy-service-mx2z5:tlsportname1/proxy/: tls baz (200; 7.094ms)
Jan 17 16:04:25.502: INFO: (4) /api/v1/namespaces/proxy-4776/services/http:proxy-service-mx2z5:portname1/proxy/: foo (200; 7.60087ms)
Jan 17 16:04:25.502: INFO: (4) /api/v1/namespaces/proxy-4776/services/http:proxy-service-mx2z5:portname2/proxy/: bar (200; 7.513218ms)
Jan 17 16:04:25.502: INFO: (4) /api/v1/namespaces/proxy-4776/services/proxy-service-mx2z5:portname2/proxy/: bar (200; 7.53757ms)
Jan 17 16:04:25.507: INFO: (5) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:160/proxy/: foo (200; 4.694666ms)
Jan 17 16:04:25.507: INFO: (5) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:443/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:443/proxy/tlsrewritem... (200; 4.49812ms)
Jan 17 16:04:25.507: INFO: (5) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:162/proxy/: bar (200; 4.861936ms)
Jan 17 16:04:25.507: INFO: (5) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:462/proxy/: tls qux (200; 4.797667ms)
Jan 17 16:04:25.507: INFO: (5) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:1080/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:1080/proxy/rewriteme">test<... (200; 5.103154ms)
Jan 17 16:04:25.508: INFO: (5) /api/v1/namespaces/proxy-4776/services/proxy-service-mx2z5:portname2/proxy/: bar (200; 5.299919ms)
Jan 17 16:04:25.508: INFO: (5) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76/proxy/rewriteme">test</a> (200; 5.966741ms)
Jan 17 16:04:25.508: INFO: (5) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:162/proxy/: bar (200; 5.912324ms)
Jan 17 16:04:25.508: INFO: (5) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:1080/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:1080/proxy/rewriteme">... (200; 5.826201ms)
Jan 17 16:04:25.508: INFO: (5) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:160/proxy/: foo (200; 6.00637ms)
Jan 17 16:04:25.508: INFO: (5) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:460/proxy/: tls baz (200; 6.146798ms)
Jan 17 16:04:25.509: INFO: (5) /api/v1/namespaces/proxy-4776/services/https:proxy-service-mx2z5:tlsportname2/proxy/: tls qux (200; 6.594775ms)
Jan 17 16:04:25.509: INFO: (5) /api/v1/namespaces/proxy-4776/services/proxy-service-mx2z5:portname1/proxy/: foo (200; 7.00159ms)
Jan 17 16:04:25.510: INFO: (5) /api/v1/namespaces/proxy-4776/services/http:proxy-service-mx2z5:portname2/proxy/: bar (200; 7.668612ms)
Jan 17 16:04:25.510: INFO: (5) /api/v1/namespaces/proxy-4776/services/https:proxy-service-mx2z5:tlsportname1/proxy/: tls baz (200; 7.929281ms)
Jan 17 16:04:25.510: INFO: (5) /api/v1/namespaces/proxy-4776/services/http:proxy-service-mx2z5:portname1/proxy/: foo (200; 8.102255ms)
Jan 17 16:04:25.515: INFO: (6) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:160/proxy/: foo (200; 4.465608ms)
Jan 17 16:04:25.515: INFO: (6) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:460/proxy/: tls baz (200; 4.358385ms)
Jan 17 16:04:25.515: INFO: (6) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:443/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:443/proxy/tlsrewritem... (200; 4.293756ms)
Jan 17 16:04:25.515: INFO: (6) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:162/proxy/: bar (200; 4.618733ms)
Jan 17 16:04:25.515: INFO: (6) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:1080/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:1080/proxy/rewriteme">test<... (200; 4.444567ms)
Jan 17 16:04:25.516: INFO: (6) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:160/proxy/: foo (200; 4.965604ms)
Jan 17 16:04:25.516: INFO: (6) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:162/proxy/: bar (200; 5.646394ms)
Jan 17 16:04:25.516: INFO: (6) /api/v1/namespaces/proxy-4776/services/proxy-service-mx2z5:portname2/proxy/: bar (200; 5.749169ms)
Jan 17 16:04:25.516: INFO: (6) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76/proxy/rewriteme">test</a> (200; 5.724333ms)
Jan 17 16:04:25.516: INFO: (6) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:1080/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:1080/proxy/rewriteme">... (200; 5.700082ms)
Jan 17 16:04:25.517: INFO: (6) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:462/proxy/: tls qux (200; 5.674962ms)
Jan 17 16:04:25.517: INFO: (6) /api/v1/namespaces/proxy-4776/services/https:proxy-service-mx2z5:tlsportname1/proxy/: tls baz (200; 5.956542ms)
Jan 17 16:04:25.517: INFO: (6) /api/v1/namespaces/proxy-4776/services/http:proxy-service-mx2z5:portname1/proxy/: foo (200; 6.569649ms)
Jan 17 16:04:25.518: INFO: (6) /api/v1/namespaces/proxy-4776/services/https:proxy-service-mx2z5:tlsportname2/proxy/: tls qux (200; 7.19745ms)
Jan 17 16:04:25.518: INFO: (6) /api/v1/namespaces/proxy-4776/services/http:proxy-service-mx2z5:portname2/proxy/: bar (200; 7.483924ms)
Jan 17 16:04:25.518: INFO: (6) /api/v1/namespaces/proxy-4776/services/proxy-service-mx2z5:portname1/proxy/: foo (200; 7.182176ms)
Jan 17 16:04:25.523: INFO: (7) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:162/proxy/: bar (200; 4.90209ms)
Jan 17 16:04:25.523: INFO: (7) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76/proxy/rewriteme">test</a> (200; 4.873692ms)
Jan 17 16:04:25.523: INFO: (7) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:160/proxy/: foo (200; 4.970507ms)
Jan 17 16:04:25.523: INFO: (7) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:162/proxy/: bar (200; 5.259784ms)
Jan 17 16:04:25.524: INFO: (7) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:1080/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:1080/proxy/rewriteme">... (200; 5.2563ms)
Jan 17 16:04:25.524: INFO: (7) /api/v1/namespaces/proxy-4776/services/http:proxy-service-mx2z5:portname2/proxy/: bar (200; 5.454616ms)
Jan 17 16:04:25.524: INFO: (7) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:160/proxy/: foo (200; 5.786275ms)
Jan 17 16:04:25.525: INFO: (7) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:1080/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:1080/proxy/rewriteme">test<... (200; 6.28317ms)
Jan 17 16:04:25.525: INFO: (7) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:462/proxy/: tls qux (200; 6.258678ms)
Jan 17 16:04:25.525: INFO: (7) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:460/proxy/: tls baz (200; 6.509073ms)
Jan 17 16:04:25.525: INFO: (7) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:443/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:443/proxy/tlsrewritem... (200; 6.332782ms)
Jan 17 16:04:25.525: INFO: (7) /api/v1/namespaces/proxy-4776/services/https:proxy-service-mx2z5:tlsportname2/proxy/: tls qux (200; 6.560915ms)
Jan 17 16:04:25.525: INFO: (7) /api/v1/namespaces/proxy-4776/services/https:proxy-service-mx2z5:tlsportname1/proxy/: tls baz (200; 7.270078ms)
Jan 17 16:04:25.526: INFO: (7) /api/v1/namespaces/proxy-4776/services/proxy-service-mx2z5:portname2/proxy/: bar (200; 7.427626ms)
Jan 17 16:04:25.527: INFO: (7) /api/v1/namespaces/proxy-4776/services/http:proxy-service-mx2z5:portname1/proxy/: foo (200; 8.385498ms)
Jan 17 16:04:25.527: INFO: (7) /api/v1/namespaces/proxy-4776/services/proxy-service-mx2z5:portname1/proxy/: foo (200; 8.229756ms)
Jan 17 16:04:25.532: INFO: (8) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76/proxy/rewriteme">test</a> (200; 5.198763ms)
Jan 17 16:04:25.532: INFO: (8) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:160/proxy/: foo (200; 5.135152ms)
Jan 17 16:04:25.532: INFO: (8) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:460/proxy/: tls baz (200; 5.450057ms)
Jan 17 16:04:25.532: INFO: (8) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:160/proxy/: foo (200; 5.419461ms)
Jan 17 16:04:25.533: INFO: (8) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:162/proxy/: bar (200; 6.176706ms)
Jan 17 16:04:25.533: INFO: (8) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:1080/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:1080/proxy/rewriteme">... (200; 6.36774ms)
Jan 17 16:04:25.534: INFO: (8) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:162/proxy/: bar (200; 6.600496ms)
Jan 17 16:04:25.534: INFO: (8) /api/v1/namespaces/proxy-4776/services/https:proxy-service-mx2z5:tlsportname2/proxy/: tls qux (200; 6.886778ms)
Jan 17 16:04:25.534: INFO: (8) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:462/proxy/: tls qux (200; 6.857796ms)
Jan 17 16:04:25.534: INFO: (8) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:1080/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:1080/proxy/rewriteme">test<... (200; 7.477069ms)
Jan 17 16:04:25.534: INFO: (8) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:443/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:443/proxy/tlsrewritem... (200; 7.455404ms)
Jan 17 16:04:25.535: INFO: (8) /api/v1/namespaces/proxy-4776/services/http:proxy-service-mx2z5:portname1/proxy/: foo (200; 7.981327ms)
Jan 17 16:04:25.535: INFO: (8) /api/v1/namespaces/proxy-4776/services/http:proxy-service-mx2z5:portname2/proxy/: bar (200; 8.520415ms)
Jan 17 16:04:25.536: INFO: (8) /api/v1/namespaces/proxy-4776/services/https:proxy-service-mx2z5:tlsportname1/proxy/: tls baz (200; 8.657707ms)
Jan 17 16:04:25.536: INFO: (8) /api/v1/namespaces/proxy-4776/services/proxy-service-mx2z5:portname1/proxy/: foo (200; 8.945527ms)
Jan 17 16:04:25.536: INFO: (8) /api/v1/namespaces/proxy-4776/services/proxy-service-mx2z5:portname2/proxy/: bar (200; 9.003334ms)
Jan 17 16:04:25.541: INFO: (9) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:162/proxy/: bar (200; 4.729551ms)
Jan 17 16:04:25.541: INFO: (9) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:460/proxy/: tls baz (200; 4.893135ms)
Jan 17 16:04:25.541: INFO: (9) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:1080/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:1080/proxy/rewriteme">... (200; 5.222304ms)
Jan 17 16:04:25.541: INFO: (9) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:160/proxy/: foo (200; 5.452377ms)
Jan 17 16:04:25.541: INFO: (9) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:162/proxy/: bar (200; 5.365818ms)
Jan 17 16:04:25.542: INFO: (9) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:160/proxy/: foo (200; 5.436762ms)
Jan 17 16:04:25.542: INFO: (9) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:1080/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:1080/proxy/rewriteme">test<... (200; 6.401528ms)
Jan 17 16:04:25.543: INFO: (9) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:443/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:443/proxy/tlsrewritem... (200; 6.375352ms)
Jan 17 16:04:25.543: INFO: (9) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:462/proxy/: tls qux (200; 6.689612ms)
Jan 17 16:04:25.543: INFO: (9) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76/proxy/rewriteme">test</a> (200; 6.761282ms)
Jan 17 16:04:25.544: INFO: (9) /api/v1/namespaces/proxy-4776/services/https:proxy-service-mx2z5:tlsportname2/proxy/: tls qux (200; 7.93089ms)
Jan 17 16:04:25.544: INFO: (9) /api/v1/namespaces/proxy-4776/services/http:proxy-service-mx2z5:portname1/proxy/: foo (200; 8.452283ms)
Jan 17 16:04:25.544: INFO: (9) /api/v1/namespaces/proxy-4776/services/http:proxy-service-mx2z5:portname2/proxy/: bar (200; 8.496718ms)
Jan 17 16:04:25.545: INFO: (9) /api/v1/namespaces/proxy-4776/services/https:proxy-service-mx2z5:tlsportname1/proxy/: tls baz (200; 8.656642ms)
Jan 17 16:04:25.545: INFO: (9) /api/v1/namespaces/proxy-4776/services/proxy-service-mx2z5:portname1/proxy/: foo (200; 8.88162ms)
Jan 17 16:04:25.545: INFO: (9) /api/v1/namespaces/proxy-4776/services/proxy-service-mx2z5:portname2/proxy/: bar (200; 8.813878ms)
Jan 17 16:04:25.550: INFO: (10) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:1080/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:1080/proxy/rewriteme">test<... (200; 5.22973ms)
Jan 17 16:04:25.550: INFO: (10) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:443/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:443/proxy/tlsrewritem... (200; 5.424456ms)
Jan 17 16:04:25.551: INFO: (10) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:162/proxy/: bar (200; 6.18279ms)
Jan 17 16:04:25.551: INFO: (10) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76/proxy/rewriteme">test</a> (200; 6.335304ms)
Jan 17 16:04:25.551: INFO: (10) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:160/proxy/: foo (200; 6.31237ms)
Jan 17 16:04:25.551: INFO: (10) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:1080/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:1080/proxy/rewriteme">... (200; 6.414292ms)
Jan 17 16:04:25.552: INFO: (10) /api/v1/namespaces/proxy-4776/services/proxy-service-mx2z5:portname2/proxy/: bar (200; 6.835941ms)
Jan 17 16:04:25.552: INFO: (10) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:462/proxy/: tls qux (200; 7.355156ms)
Jan 17 16:04:25.553: INFO: (10) /api/v1/namespaces/proxy-4776/services/proxy-service-mx2z5:portname1/proxy/: foo (200; 7.760637ms)
Jan 17 16:04:25.553: INFO: (10) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:162/proxy/: bar (200; 8.288636ms)
Jan 17 16:04:25.554: INFO: (10) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:460/proxy/: tls baz (200; 8.410678ms)
Jan 17 16:04:25.554: INFO: (10) /api/v1/namespaces/proxy-4776/services/http:proxy-service-mx2z5:portname2/proxy/: bar (200; 8.585487ms)
Jan 17 16:04:25.554: INFO: (10) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:160/proxy/: foo (200; 9.010152ms)
Jan 17 16:04:25.554: INFO: (10) /api/v1/namespaces/proxy-4776/services/https:proxy-service-mx2z5:tlsportname2/proxy/: tls qux (200; 9.032134ms)
Jan 17 16:04:25.555: INFO: (10) /api/v1/namespaces/proxy-4776/services/https:proxy-service-mx2z5:tlsportname1/proxy/: tls baz (200; 9.459253ms)
Jan 17 16:04:25.555: INFO: (10) /api/v1/namespaces/proxy-4776/services/http:proxy-service-mx2z5:portname1/proxy/: foo (200; 9.813481ms)
Jan 17 16:04:25.562: INFO: (11) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:462/proxy/: tls qux (200; 6.243246ms)
Jan 17 16:04:25.562: INFO: (11) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:443/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:443/proxy/tlsrewritem... (200; 6.704975ms)
Jan 17 16:04:25.562: INFO: (11) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:1080/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:1080/proxy/rewriteme">... (200; 6.383268ms)
Jan 17 16:04:25.562: INFO: (11) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:1080/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:1080/proxy/rewriteme">test<... (200; 6.413033ms)
Jan 17 16:04:25.562: INFO: (11) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:460/proxy/: tls baz (200; 6.643871ms)
Jan 17 16:04:25.562: INFO: (11) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:162/proxy/: bar (200; 6.694552ms)
Jan 17 16:04:25.563: INFO: (11) /api/v1/namespaces/proxy-4776/services/proxy-service-mx2z5:portname2/proxy/: bar (200; 7.760278ms)
Jan 17 16:04:25.563: INFO: (11) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:162/proxy/: bar (200; 8.188796ms)
Jan 17 16:04:25.564: INFO: (11) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:160/proxy/: foo (200; 8.093763ms)
Jan 17 16:04:25.564: INFO: (11) /api/v1/namespaces/proxy-4776/services/https:proxy-service-mx2z5:tlsportname2/proxy/: tls qux (200; 8.516573ms)
Jan 17 16:04:25.564: INFO: (11) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76/proxy/rewriteme">test</a> (200; 8.594014ms)
Jan 17 16:04:25.564: INFO: (11) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:160/proxy/: foo (200; 8.326486ms)
Jan 17 16:04:25.565: INFO: (11) /api/v1/namespaces/proxy-4776/services/http:proxy-service-mx2z5:portname1/proxy/: foo (200; 9.793235ms)
Jan 17 16:04:25.568: INFO: (11) /api/v1/namespaces/proxy-4776/services/https:proxy-service-mx2z5:tlsportname1/proxy/: tls baz (200; 12.559139ms)
Jan 17 16:04:25.568: INFO: (11) /api/v1/namespaces/proxy-4776/services/http:proxy-service-mx2z5:portname2/proxy/: bar (200; 12.946999ms)
Jan 17 16:04:25.568: INFO: (11) /api/v1/namespaces/proxy-4776/services/proxy-service-mx2z5:portname1/proxy/: foo (200; 12.697649ms)
Jan 17 16:04:25.573: INFO: (12) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:162/proxy/: bar (200; 5.074147ms)
Jan 17 16:04:25.573: INFO: (12) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:160/proxy/: foo (200; 4.864068ms)
Jan 17 16:04:25.574: INFO: (12) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:1080/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:1080/proxy/rewriteme">... (200; 5.462555ms)
Jan 17 16:04:25.574: INFO: (12) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:460/proxy/: tls baz (200; 5.470087ms)
Jan 17 16:04:25.574: INFO: (12) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:162/proxy/: bar (200; 5.755775ms)
Jan 17 16:04:25.575: INFO: (12) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76/proxy/rewriteme">test</a> (200; 6.702721ms)
Jan 17 16:04:25.575: INFO: (12) /api/v1/namespaces/proxy-4776/services/proxy-service-mx2z5:portname1/proxy/: foo (200; 7.001062ms)
Jan 17 16:04:25.575: INFO: (12) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:160/proxy/: foo (200; 7.20708ms)
Jan 17 16:04:25.577: INFO: (12) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:1080/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:1080/proxy/rewriteme">test<... (200; 8.625807ms)
Jan 17 16:04:25.577: INFO: (12) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:462/proxy/: tls qux (200; 8.60694ms)
Jan 17 16:04:25.577: INFO: (12) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:443/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:443/proxy/tlsrewritem... (200; 8.759091ms)
Jan 17 16:04:25.577: INFO: (12) /api/v1/namespaces/proxy-4776/services/proxy-service-mx2z5:portname2/proxy/: bar (200; 9.157376ms)
Jan 17 16:04:25.577: INFO: (12) /api/v1/namespaces/proxy-4776/services/http:proxy-service-mx2z5:portname2/proxy/: bar (200; 9.286353ms)
Jan 17 16:04:25.578: INFO: (12) /api/v1/namespaces/proxy-4776/services/https:proxy-service-mx2z5:tlsportname1/proxy/: tls baz (200; 9.459181ms)
Jan 17 16:04:25.578: INFO: (12) /api/v1/namespaces/proxy-4776/services/https:proxy-service-mx2z5:tlsportname2/proxy/: tls qux (200; 9.932438ms)
Jan 17 16:04:25.578: INFO: (12) /api/v1/namespaces/proxy-4776/services/http:proxy-service-mx2z5:portname1/proxy/: foo (200; 10.257047ms)
Jan 17 16:04:25.584: INFO: (13) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:160/proxy/: foo (200; 4.865841ms)
Jan 17 16:04:25.585: INFO: (13) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:162/proxy/: bar (200; 6.94589ms)
Jan 17 16:04:25.585: INFO: (13) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:162/proxy/: bar (200; 6.892653ms)
Jan 17 16:04:25.585: INFO: (13) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:443/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:443/proxy/tlsrewritem... (200; 6.748758ms)
Jan 17 16:04:25.586: INFO: (13) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:1080/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:1080/proxy/rewriteme">test<... (200; 6.956202ms)
Jan 17 16:04:25.587: INFO: (13) /api/v1/namespaces/proxy-4776/services/proxy-service-mx2z5:portname2/proxy/: bar (200; 8.037212ms)
Jan 17 16:04:25.593: INFO: (13) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:160/proxy/: foo (200; 14.415453ms)
Jan 17 16:04:25.593: INFO: (13) /api/v1/namespaces/proxy-4776/services/https:proxy-service-mx2z5:tlsportname2/proxy/: tls qux (200; 14.505626ms)
Jan 17 16:04:25.593: INFO: (13) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:462/proxy/: tls qux (200; 14.669139ms)
Jan 17 16:04:25.593: INFO: (13) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:1080/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:1080/proxy/rewriteme">... (200; 14.48913ms)
Jan 17 16:04:25.593: INFO: (13) /api/v1/namespaces/proxy-4776/services/http:proxy-service-mx2z5:portname1/proxy/: foo (200; 14.544512ms)
Jan 17 16:04:25.593: INFO: (13) /api/v1/namespaces/proxy-4776/services/http:proxy-service-mx2z5:portname2/proxy/: bar (200; 14.710188ms)
Jan 17 16:04:25.593: INFO: (13) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76/proxy/rewriteme">test</a> (200; 14.582223ms)
Jan 17 16:04:25.593: INFO: (13) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:460/proxy/: tls baz (200; 14.647469ms)
Jan 17 16:04:25.593: INFO: (13) /api/v1/namespaces/proxy-4776/services/proxy-service-mx2z5:portname1/proxy/: foo (200; 14.500411ms)
Jan 17 16:04:25.594: INFO: (13) /api/v1/namespaces/proxy-4776/services/https:proxy-service-mx2z5:tlsportname1/proxy/: tls baz (200; 15.065982ms)
Jan 17 16:04:25.600: INFO: (14) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:160/proxy/: foo (200; 5.886122ms)
Jan 17 16:04:25.600: INFO: (14) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:1080/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:1080/proxy/rewriteme">... (200; 6.32548ms)
Jan 17 16:04:25.600: INFO: (14) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:1080/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:1080/proxy/rewriteme">test<... (200; 6.42817ms)
Jan 17 16:04:25.601: INFO: (14) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:462/proxy/: tls qux (200; 6.679376ms)
Jan 17 16:04:25.601: INFO: (14) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:162/proxy/: bar (200; 6.640285ms)
Jan 17 16:04:25.601: INFO: (14) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76/proxy/rewriteme">test</a> (200; 6.540974ms)
Jan 17 16:04:25.601: INFO: (14) /api/v1/namespaces/proxy-4776/services/proxy-service-mx2z5:portname1/proxy/: foo (200; 7.265602ms)
Jan 17 16:04:25.602: INFO: (14) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:162/proxy/: bar (200; 7.878192ms)
Jan 17 16:04:25.602: INFO: (14) /api/v1/namespaces/proxy-4776/services/http:proxy-service-mx2z5:portname1/proxy/: foo (200; 7.900734ms)
Jan 17 16:04:25.602: INFO: (14) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:460/proxy/: tls baz (200; 8.159183ms)
Jan 17 16:04:25.602: INFO: (14) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:160/proxy/: foo (200; 7.928391ms)
Jan 17 16:04:25.602: INFO: (14) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:443/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:443/proxy/tlsrewritem... (200; 8.136629ms)
Jan 17 16:04:25.603: INFO: (14) /api/v1/namespaces/proxy-4776/services/https:proxy-service-mx2z5:tlsportname1/proxy/: tls baz (200; 8.834575ms)
Jan 17 16:04:25.603: INFO: (14) /api/v1/namespaces/proxy-4776/services/http:proxy-service-mx2z5:portname2/proxy/: bar (200; 9.102463ms)
Jan 17 16:04:25.604: INFO: (14) /api/v1/namespaces/proxy-4776/services/proxy-service-mx2z5:portname2/proxy/: bar (200; 9.682336ms)
Jan 17 16:04:25.604: INFO: (14) /api/v1/namespaces/proxy-4776/services/https:proxy-service-mx2z5:tlsportname2/proxy/: tls qux (200; 10.081698ms)
Jan 17 16:04:25.610: INFO: (15) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:443/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:443/proxy/tlsrewritem... (200; 5.19391ms)
Jan 17 16:04:25.610: INFO: (15) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:160/proxy/: foo (200; 5.561612ms)
Jan 17 16:04:25.610: INFO: (15) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:160/proxy/: foo (200; 5.72869ms)
Jan 17 16:04:25.610: INFO: (15) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:462/proxy/: tls qux (200; 5.866495ms)
Jan 17 16:04:25.611: INFO: (15) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76/proxy/rewriteme">test</a> (200; 6.256263ms)
Jan 17 16:04:25.611: INFO: (15) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:1080/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:1080/proxy/rewriteme">... (200; 6.79509ms)
Jan 17 16:04:25.612: INFO: (15) /api/v1/namespaces/proxy-4776/services/proxy-service-mx2z5:portname2/proxy/: bar (200; 7.376019ms)
Jan 17 16:04:25.612: INFO: (15) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:162/proxy/: bar (200; 7.909994ms)
Jan 17 16:04:25.612: INFO: (15) /api/v1/namespaces/proxy-4776/services/http:proxy-service-mx2z5:portname1/proxy/: foo (200; 7.859077ms)
Jan 17 16:04:25.613: INFO: (15) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:1080/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:1080/proxy/rewriteme">test<... (200; 8.044033ms)
Jan 17 16:04:25.613: INFO: (15) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:162/proxy/: bar (200; 8.395315ms)
Jan 17 16:04:25.613: INFO: (15) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:460/proxy/: tls baz (200; 8.169909ms)
Jan 17 16:04:25.614: INFO: (15) /api/v1/namespaces/proxy-4776/services/https:proxy-service-mx2z5:tlsportname2/proxy/: tls qux (200; 9.332764ms)
Jan 17 16:04:25.614: INFO: (15) /api/v1/namespaces/proxy-4776/services/http:proxy-service-mx2z5:portname2/proxy/: bar (200; 9.797986ms)
Jan 17 16:04:25.615: INFO: (15) /api/v1/namespaces/proxy-4776/services/proxy-service-mx2z5:portname1/proxy/: foo (200; 10.603151ms)
Jan 17 16:04:25.615: INFO: (15) /api/v1/namespaces/proxy-4776/services/https:proxy-service-mx2z5:tlsportname1/proxy/: tls baz (200; 10.698069ms)
Jan 17 16:04:25.620: INFO: (16) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:1080/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:1080/proxy/rewriteme">test<... (200; 4.527218ms)
Jan 17 16:04:25.620: INFO: (16) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:1080/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:1080/proxy/rewriteme">... (200; 4.658145ms)
Jan 17 16:04:25.620: INFO: (16) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:443/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:443/proxy/tlsrewritem... (200; 5.001014ms)
Jan 17 16:04:25.621: INFO: (16) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:160/proxy/: foo (200; 6.063049ms)
Jan 17 16:04:25.621: INFO: (16) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:162/proxy/: bar (200; 6.129722ms)
Jan 17 16:04:25.621: INFO: (16) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:162/proxy/: bar (200; 5.967639ms)
Jan 17 16:04:25.621: INFO: (16) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76/proxy/rewriteme">test</a> (200; 6.484851ms)
Jan 17 16:04:25.622: INFO: (16) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:160/proxy/: foo (200; 6.334067ms)
Jan 17 16:04:25.622: INFO: (16) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:460/proxy/: tls baz (200; 6.524142ms)
Jan 17 16:04:25.622: INFO: (16) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:462/proxy/: tls qux (200; 7.0251ms)
Jan 17 16:04:25.622: INFO: (16) /api/v1/namespaces/proxy-4776/services/https:proxy-service-mx2z5:tlsportname1/proxy/: tls baz (200; 7.29435ms)
Jan 17 16:04:25.623: INFO: (16) /api/v1/namespaces/proxy-4776/services/http:proxy-service-mx2z5:portname1/proxy/: foo (200; 7.541209ms)
Jan 17 16:04:25.623: INFO: (16) /api/v1/namespaces/proxy-4776/services/proxy-service-mx2z5:portname1/proxy/: foo (200; 7.774541ms)
Jan 17 16:04:25.623: INFO: (16) /api/v1/namespaces/proxy-4776/services/https:proxy-service-mx2z5:tlsportname2/proxy/: tls qux (200; 7.579201ms)
Jan 17 16:04:25.623: INFO: (16) /api/v1/namespaces/proxy-4776/services/proxy-service-mx2z5:portname2/proxy/: bar (200; 7.675282ms)
Jan 17 16:04:25.624: INFO: (16) /api/v1/namespaces/proxy-4776/services/http:proxy-service-mx2z5:portname2/proxy/: bar (200; 8.55429ms)
Jan 17 16:04:25.629: INFO: (17) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:1080/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:1080/proxy/rewriteme">... (200; 4.431862ms)
Jan 17 16:04:25.629: INFO: (17) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76/proxy/rewriteme">test</a> (200; 5.223814ms)
Jan 17 16:04:25.629: INFO: (17) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:162/proxy/: bar (200; 5.385548ms)
Jan 17 16:04:25.629: INFO: (17) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:462/proxy/: tls qux (200; 5.388584ms)
Jan 17 16:04:25.630: INFO: (17) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:162/proxy/: bar (200; 6.085161ms)
Jan 17 16:04:25.630: INFO: (17) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:460/proxy/: tls baz (200; 6.568477ms)
Jan 17 16:04:25.631: INFO: (17) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:443/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:443/proxy/tlsrewritem... (200; 6.549432ms)
Jan 17 16:04:25.631: INFO: (17) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:160/proxy/: foo (200; 6.789755ms)
Jan 17 16:04:25.631: INFO: (17) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:160/proxy/: foo (200; 6.945862ms)
Jan 17 16:04:25.631: INFO: (17) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:1080/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:1080/proxy/rewriteme">test<... (200; 7.009094ms)
Jan 17 16:04:25.632: INFO: (17) /api/v1/namespaces/proxy-4776/services/https:proxy-service-mx2z5:tlsportname1/proxy/: tls baz (200; 8.301495ms)
Jan 17 16:04:25.633: INFO: (17) /api/v1/namespaces/proxy-4776/services/http:proxy-service-mx2z5:portname2/proxy/: bar (200; 9.047776ms)
Jan 17 16:04:25.633: INFO: (17) /api/v1/namespaces/proxy-4776/services/https:proxy-service-mx2z5:tlsportname2/proxy/: tls qux (200; 8.843765ms)
Jan 17 16:04:25.633: INFO: (17) /api/v1/namespaces/proxy-4776/services/http:proxy-service-mx2z5:portname1/proxy/: foo (200; 9.123114ms)
Jan 17 16:04:25.633: INFO: (17) /api/v1/namespaces/proxy-4776/services/proxy-service-mx2z5:portname1/proxy/: foo (200; 9.216137ms)
Jan 17 16:04:25.633: INFO: (17) /api/v1/namespaces/proxy-4776/services/proxy-service-mx2z5:portname2/proxy/: bar (200; 9.404463ms)
Jan 17 16:04:25.638: INFO: (18) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76/proxy/rewriteme">test</a> (200; 4.60415ms)
Jan 17 16:04:25.639: INFO: (18) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:162/proxy/: bar (200; 5.68389ms)
Jan 17 16:04:25.640: INFO: (18) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:1080/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:1080/proxy/rewriteme">test<... (200; 6.450338ms)
Jan 17 16:04:25.641: INFO: (18) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:443/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:443/proxy/tlsrewritem... (200; 6.768162ms)
Jan 17 16:04:25.641: INFO: (18) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:1080/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:1080/proxy/rewriteme">... (200; 7.267508ms)
Jan 17 16:04:25.641: INFO: (18) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:160/proxy/: foo (200; 7.10272ms)
Jan 17 16:04:25.641: INFO: (18) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:460/proxy/: tls baz (200; 7.404038ms)
Jan 17 16:04:25.641: INFO: (18) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:162/proxy/: bar (200; 7.437415ms)
Jan 17 16:04:25.641: INFO: (18) /api/v1/namespaces/proxy-4776/services/proxy-service-mx2z5:portname1/proxy/: foo (200; 7.751838ms)
Jan 17 16:04:25.642: INFO: (18) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:462/proxy/: tls qux (200; 7.983984ms)
Jan 17 16:04:25.642: INFO: (18) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:160/proxy/: foo (200; 8.231086ms)
Jan 17 16:04:25.642: INFO: (18) /api/v1/namespaces/proxy-4776/services/http:proxy-service-mx2z5:portname1/proxy/: foo (200; 8.787343ms)
Jan 17 16:04:25.642: INFO: (18) /api/v1/namespaces/proxy-4776/services/proxy-service-mx2z5:portname2/proxy/: bar (200; 8.567751ms)
Jan 17 16:04:25.643: INFO: (18) /api/v1/namespaces/proxy-4776/services/https:proxy-service-mx2z5:tlsportname2/proxy/: tls qux (200; 9.382383ms)
Jan 17 16:04:25.643: INFO: (18) /api/v1/namespaces/proxy-4776/services/https:proxy-service-mx2z5:tlsportname1/proxy/: tls baz (200; 9.626333ms)
Jan 17 16:04:25.644: INFO: (18) /api/v1/namespaces/proxy-4776/services/http:proxy-service-mx2z5:portname2/proxy/: bar (200; 9.697284ms)
Jan 17 16:04:25.649: INFO: (19) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:460/proxy/: tls baz (200; 5.082042ms)
Jan 17 16:04:25.649: INFO: (19) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76/proxy/rewriteme">test</a> (200; 5.041965ms)
Jan 17 16:04:25.649: INFO: (19) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:160/proxy/: foo (200; 5.16207ms)
Jan 17 16:04:25.649: INFO: (19) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:462/proxy/: tls qux (200; 5.37864ms)
Jan 17 16:04:25.650: INFO: (19) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:1080/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:1080/proxy/rewriteme">... (200; 5.831033ms)
Jan 17 16:04:25.650: INFO: (19) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:443/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:443/proxy/tlsrewritem... (200; 6.498753ms)
Jan 17 16:04:25.650: INFO: (19) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:160/proxy/: foo (200; 6.515282ms)
Jan 17 16:04:25.650: INFO: (19) /api/v1/namespaces/proxy-4776/services/https:proxy-service-mx2z5:tlsportname1/proxy/: tls baz (200; 6.495854ms)
Jan 17 16:04:25.650: INFO: (19) /api/v1/namespaces/proxy-4776/services/proxy-service-mx2z5:portname2/proxy/: bar (200; 6.676422ms)
Jan 17 16:04:25.650: INFO: (19) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:162/proxy/: bar (200; 6.63847ms)
Jan 17 16:04:25.652: INFO: (19) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:162/proxy/: bar (200; 8.524383ms)
Jan 17 16:04:25.652: INFO: (19) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:1080/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:1080/proxy/rewriteme">test<... (200; 8.603171ms)
Jan 17 16:04:25.653: INFO: (19) /api/v1/namespaces/proxy-4776/services/http:proxy-service-mx2z5:portname2/proxy/: bar (200; 9.769402ms)
Jan 17 16:04:25.654: INFO: (19) /api/v1/namespaces/proxy-4776/services/proxy-service-mx2z5:portname1/proxy/: foo (200; 10.175009ms)
Jan 17 16:04:25.654: INFO: (19) /api/v1/namespaces/proxy-4776/services/http:proxy-service-mx2z5:portname1/proxy/: foo (200; 10.333046ms)
Jan 17 16:04:25.654: INFO: (19) /api/v1/namespaces/proxy-4776/services/https:proxy-service-mx2z5:tlsportname2/proxy/: tls qux (200; 10.448732ms)
STEP: deleting ReplicationController proxy-service-mx2z5 in namespace proxy-4776, will wait for the garbage collector to delete the pods 01/17/23 16:04:25.654
Jan 17 16:04:25.715: INFO: Deleting ReplicationController proxy-service-mx2z5 took: 6.109275ms
Jan 17 16:04:25.816: INFO: Terminating ReplicationController proxy-service-mx2z5 pods took: 100.834226ms
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Jan 17 16:04:28.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-4776" for this suite. 01/17/23 16:04:28.627
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","completed":306,"skipped":5636,"failed":0}
------------------------------
• [SLOW TEST] [6.317 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:04:22.32
    Jan 17 16:04:22.320: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename proxy 01/17/23 16:04:22.321
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:04:22.342
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:04:22.346
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 01/17/23 16:04:22.387
    STEP: creating replication controller proxy-service-mx2z5 in namespace proxy-4776 01/17/23 16:04:22.387
    I0117 16:04:22.398511      22 runners.go:193] Created replication controller with name: proxy-service-mx2z5, namespace: proxy-4776, replica count: 1
    I0117 16:04:23.449900      22 runners.go:193] proxy-service-mx2z5 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0117 16:04:24.450721      22 runners.go:193] proxy-service-mx2z5 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
    I0117 16:04:25.451686      22 runners.go:193] proxy-service-mx2z5 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 17 16:04:25.455: INFO: setup took 3.100930353s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 01/17/23 16:04:25.455
    Jan 17 16:04:25.468: INFO: (0) /api/v1/namespaces/proxy-4776/services/http:proxy-service-mx2z5:portname2/proxy/: bar (200; 12.511229ms)
    Jan 17 16:04:25.468: INFO: (0) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:1080/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:1080/proxy/rewriteme">... (200; 12.445961ms)
    Jan 17 16:04:25.468: INFO: (0) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76/proxy/rewriteme">test</a> (200; 12.43874ms)
    Jan 17 16:04:25.468: INFO: (0) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:162/proxy/: bar (200; 12.620406ms)
    Jan 17 16:04:25.468: INFO: (0) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:162/proxy/: bar (200; 12.856094ms)
    Jan 17 16:04:25.469: INFO: (0) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:160/proxy/: foo (200; 12.879902ms)
    Jan 17 16:04:25.469: INFO: (0) /api/v1/namespaces/proxy-4776/services/proxy-service-mx2z5:portname2/proxy/: bar (200; 12.864058ms)
    Jan 17 16:04:25.469: INFO: (0) /api/v1/namespaces/proxy-4776/services/http:proxy-service-mx2z5:portname1/proxy/: foo (200; 12.922657ms)
    Jan 17 16:04:25.469: INFO: (0) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:160/proxy/: foo (200; 13.079104ms)
    Jan 17 16:04:25.469: INFO: (0) /api/v1/namespaces/proxy-4776/services/https:proxy-service-mx2z5:tlsportname2/proxy/: tls qux (200; 12.957886ms)
    Jan 17 16:04:25.469: INFO: (0) /api/v1/namespaces/proxy-4776/services/proxy-service-mx2z5:portname1/proxy/: foo (200; 13.039249ms)
    Jan 17 16:04:25.469: INFO: (0) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:462/proxy/: tls qux (200; 13.04207ms)
    Jan 17 16:04:25.469: INFO: (0) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:1080/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:1080/proxy/rewriteme">test<... (200; 12.931451ms)
    Jan 17 16:04:25.469: INFO: (0) /api/v1/namespaces/proxy-4776/services/https:proxy-service-mx2z5:tlsportname1/proxy/: tls baz (200; 13.021732ms)
    Jan 17 16:04:25.469: INFO: (0) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:460/proxy/: tls baz (200; 13.126246ms)
    Jan 17 16:04:25.469: INFO: (0) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:443/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:443/proxy/tlsrewritem... (200; 13.028357ms)
    Jan 17 16:04:25.473: INFO: (1) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:443/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:443/proxy/tlsrewritem... (200; 3.786653ms)
    Jan 17 16:04:25.473: INFO: (1) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76/proxy/rewriteme">test</a> (200; 4.141494ms)
    Jan 17 16:04:25.473: INFO: (1) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:462/proxy/: tls qux (200; 4.099388ms)
    Jan 17 16:04:25.473: INFO: (1) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:160/proxy/: foo (200; 4.285675ms)
    Jan 17 16:04:25.473: INFO: (1) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:160/proxy/: foo (200; 4.447446ms)
    Jan 17 16:04:25.473: INFO: (1) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:162/proxy/: bar (200; 4.449439ms)
    Jan 17 16:04:25.474: INFO: (1) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:162/proxy/: bar (200; 5.254927ms)
    Jan 17 16:04:25.474: INFO: (1) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:1080/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:1080/proxy/rewriteme">... (200; 5.340407ms)
    Jan 17 16:04:25.474: INFO: (1) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:460/proxy/: tls baz (200; 5.38878ms)
    Jan 17 16:04:25.474: INFO: (1) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:1080/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:1080/proxy/rewriteme">test<... (200; 5.373455ms)
    Jan 17 16:04:25.476: INFO: (1) /api/v1/namespaces/proxy-4776/services/https:proxy-service-mx2z5:tlsportname1/proxy/: tls baz (200; 6.690432ms)
    Jan 17 16:04:25.476: INFO: (1) /api/v1/namespaces/proxy-4776/services/http:proxy-service-mx2z5:portname1/proxy/: foo (200; 7.408388ms)
    Jan 17 16:04:25.476: INFO: (1) /api/v1/namespaces/proxy-4776/services/proxy-service-mx2z5:portname1/proxy/: foo (200; 7.433345ms)
    Jan 17 16:04:25.477: INFO: (1) /api/v1/namespaces/proxy-4776/services/https:proxy-service-mx2z5:tlsportname2/proxy/: tls qux (200; 7.557818ms)
    Jan 17 16:04:25.478: INFO: (1) /api/v1/namespaces/proxy-4776/services/proxy-service-mx2z5:portname2/proxy/: bar (200; 8.936711ms)
    Jan 17 16:04:25.478: INFO: (1) /api/v1/namespaces/proxy-4776/services/http:proxy-service-mx2z5:portname2/proxy/: bar (200; 9.01177ms)
    Jan 17 16:04:25.482: INFO: (2) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:162/proxy/: bar (200; 4.090366ms)
    Jan 17 16:04:25.482: INFO: (2) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:443/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:443/proxy/tlsrewritem... (200; 4.261168ms)
    Jan 17 16:04:25.483: INFO: (2) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:460/proxy/: tls baz (200; 5.12019ms)
    Jan 17 16:04:25.483: INFO: (2) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76/proxy/rewriteme">test</a> (200; 5.153379ms)
    Jan 17 16:04:25.484: INFO: (2) /api/v1/namespaces/proxy-4776/services/proxy-service-mx2z5:portname2/proxy/: bar (200; 5.962417ms)
    Jan 17 16:04:25.484: INFO: (2) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:1080/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:1080/proxy/rewriteme">test<... (200; 6.100292ms)
    Jan 17 16:04:25.485: INFO: (2) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:160/proxy/: foo (200; 6.452171ms)
    Jan 17 16:04:25.485: INFO: (2) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:1080/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:1080/proxy/rewriteme">... (200; 6.425503ms)
    Jan 17 16:04:25.485: INFO: (2) /api/v1/namespaces/proxy-4776/services/proxy-service-mx2z5:portname1/proxy/: foo (200; 6.646007ms)
    Jan 17 16:04:25.485: INFO: (2) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:162/proxy/: bar (200; 6.600042ms)
    Jan 17 16:04:25.485: INFO: (2) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:462/proxy/: tls qux (200; 6.819506ms)
    Jan 17 16:04:25.485: INFO: (2) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:160/proxy/: foo (200; 7.215658ms)
    Jan 17 16:04:25.485: INFO: (2) /api/v1/namespaces/proxy-4776/services/https:proxy-service-mx2z5:tlsportname1/proxy/: tls baz (200; 7.493065ms)
    Jan 17 16:04:25.486: INFO: (2) /api/v1/namespaces/proxy-4776/services/https:proxy-service-mx2z5:tlsportname2/proxy/: tls qux (200; 7.607833ms)
    Jan 17 16:04:25.486: INFO: (2) /api/v1/namespaces/proxy-4776/services/http:proxy-service-mx2z5:portname1/proxy/: foo (200; 7.682208ms)
    Jan 17 16:04:25.486: INFO: (2) /api/v1/namespaces/proxy-4776/services/http:proxy-service-mx2z5:portname2/proxy/: bar (200; 7.664835ms)
    Jan 17 16:04:25.489: INFO: (3) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76/proxy/rewriteme">test</a> (200; 3.480718ms)
    Jan 17 16:04:25.490: INFO: (3) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:162/proxy/: bar (200; 4.128127ms)
    Jan 17 16:04:25.490: INFO: (3) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:443/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:443/proxy/tlsrewritem... (200; 4.132404ms)
    Jan 17 16:04:25.490: INFO: (3) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:162/proxy/: bar (200; 4.504385ms)
    Jan 17 16:04:25.491: INFO: (3) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:460/proxy/: tls baz (200; 4.697997ms)
    Jan 17 16:04:25.491: INFO: (3) /api/v1/namespaces/proxy-4776/services/http:proxy-service-mx2z5:portname2/proxy/: bar (200; 5.359255ms)
    Jan 17 16:04:25.492: INFO: (3) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:160/proxy/: foo (200; 5.734029ms)
    Jan 17 16:04:25.492: INFO: (3) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:160/proxy/: foo (200; 5.86506ms)
    Jan 17 16:04:25.492: INFO: (3) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:462/proxy/: tls qux (200; 5.840286ms)
    Jan 17 16:04:25.492: INFO: (3) /api/v1/namespaces/proxy-4776/services/https:proxy-service-mx2z5:tlsportname2/proxy/: tls qux (200; 6.100561ms)
    Jan 17 16:04:25.493: INFO: (3) /api/v1/namespaces/proxy-4776/services/proxy-service-mx2z5:portname2/proxy/: bar (200; 7.258084ms)
    Jan 17 16:04:25.493: INFO: (3) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:1080/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:1080/proxy/rewriteme">test<... (200; 7.254329ms)
    Jan 17 16:04:25.493: INFO: (3) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:1080/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:1080/proxy/rewriteme">... (200; 7.294055ms)
    Jan 17 16:04:25.494: INFO: (3) /api/v1/namespaces/proxy-4776/services/http:proxy-service-mx2z5:portname1/proxy/: foo (200; 8.460801ms)
    Jan 17 16:04:25.495: INFO: (3) /api/v1/namespaces/proxy-4776/services/https:proxy-service-mx2z5:tlsportname1/proxy/: tls baz (200; 8.488175ms)
    Jan 17 16:04:25.495: INFO: (3) /api/v1/namespaces/proxy-4776/services/proxy-service-mx2z5:portname1/proxy/: foo (200; 8.523368ms)
    Jan 17 16:04:25.499: INFO: (4) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:160/proxy/: foo (200; 4.500641ms)
    Jan 17 16:04:25.499: INFO: (4) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:462/proxy/: tls qux (200; 4.597808ms)
    Jan 17 16:04:25.499: INFO: (4) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:160/proxy/: foo (200; 4.761876ms)
    Jan 17 16:04:25.499: INFO: (4) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:443/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:443/proxy/tlsrewritem... (200; 4.81394ms)
    Jan 17 16:04:25.499: INFO: (4) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:162/proxy/: bar (200; 4.782829ms)
    Jan 17 16:04:25.500: INFO: (4) /api/v1/namespaces/proxy-4776/services/proxy-service-mx2z5:portname1/proxy/: foo (200; 5.178995ms)
    Jan 17 16:04:25.500: INFO: (4) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:460/proxy/: tls baz (200; 5.237128ms)
    Jan 17 16:04:25.500: INFO: (4) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:162/proxy/: bar (200; 5.60741ms)
    Jan 17 16:04:25.501: INFO: (4) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:1080/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:1080/proxy/rewriteme">... (200; 5.808848ms)
    Jan 17 16:04:25.501: INFO: (4) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:1080/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:1080/proxy/rewriteme">test<... (200; 5.868347ms)
    Jan 17 16:04:25.501: INFO: (4) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76/proxy/rewriteme">test</a> (200; 5.792009ms)
    Jan 17 16:04:25.501: INFO: (4) /api/v1/namespaces/proxy-4776/services/https:proxy-service-mx2z5:tlsportname2/proxy/: tls qux (200; 6.395966ms)
    Jan 17 16:04:25.502: INFO: (4) /api/v1/namespaces/proxy-4776/services/https:proxy-service-mx2z5:tlsportname1/proxy/: tls baz (200; 7.094ms)
    Jan 17 16:04:25.502: INFO: (4) /api/v1/namespaces/proxy-4776/services/http:proxy-service-mx2z5:portname1/proxy/: foo (200; 7.60087ms)
    Jan 17 16:04:25.502: INFO: (4) /api/v1/namespaces/proxy-4776/services/http:proxy-service-mx2z5:portname2/proxy/: bar (200; 7.513218ms)
    Jan 17 16:04:25.502: INFO: (4) /api/v1/namespaces/proxy-4776/services/proxy-service-mx2z5:portname2/proxy/: bar (200; 7.53757ms)
    Jan 17 16:04:25.507: INFO: (5) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:160/proxy/: foo (200; 4.694666ms)
    Jan 17 16:04:25.507: INFO: (5) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:443/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:443/proxy/tlsrewritem... (200; 4.49812ms)
    Jan 17 16:04:25.507: INFO: (5) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:162/proxy/: bar (200; 4.861936ms)
    Jan 17 16:04:25.507: INFO: (5) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:462/proxy/: tls qux (200; 4.797667ms)
    Jan 17 16:04:25.507: INFO: (5) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:1080/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:1080/proxy/rewriteme">test<... (200; 5.103154ms)
    Jan 17 16:04:25.508: INFO: (5) /api/v1/namespaces/proxy-4776/services/proxy-service-mx2z5:portname2/proxy/: bar (200; 5.299919ms)
    Jan 17 16:04:25.508: INFO: (5) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76/proxy/rewriteme">test</a> (200; 5.966741ms)
    Jan 17 16:04:25.508: INFO: (5) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:162/proxy/: bar (200; 5.912324ms)
    Jan 17 16:04:25.508: INFO: (5) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:1080/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:1080/proxy/rewriteme">... (200; 5.826201ms)
    Jan 17 16:04:25.508: INFO: (5) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:160/proxy/: foo (200; 6.00637ms)
    Jan 17 16:04:25.508: INFO: (5) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:460/proxy/: tls baz (200; 6.146798ms)
    Jan 17 16:04:25.509: INFO: (5) /api/v1/namespaces/proxy-4776/services/https:proxy-service-mx2z5:tlsportname2/proxy/: tls qux (200; 6.594775ms)
    Jan 17 16:04:25.509: INFO: (5) /api/v1/namespaces/proxy-4776/services/proxy-service-mx2z5:portname1/proxy/: foo (200; 7.00159ms)
    Jan 17 16:04:25.510: INFO: (5) /api/v1/namespaces/proxy-4776/services/http:proxy-service-mx2z5:portname2/proxy/: bar (200; 7.668612ms)
    Jan 17 16:04:25.510: INFO: (5) /api/v1/namespaces/proxy-4776/services/https:proxy-service-mx2z5:tlsportname1/proxy/: tls baz (200; 7.929281ms)
    Jan 17 16:04:25.510: INFO: (5) /api/v1/namespaces/proxy-4776/services/http:proxy-service-mx2z5:portname1/proxy/: foo (200; 8.102255ms)
    Jan 17 16:04:25.515: INFO: (6) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:160/proxy/: foo (200; 4.465608ms)
    Jan 17 16:04:25.515: INFO: (6) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:460/proxy/: tls baz (200; 4.358385ms)
    Jan 17 16:04:25.515: INFO: (6) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:443/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:443/proxy/tlsrewritem... (200; 4.293756ms)
    Jan 17 16:04:25.515: INFO: (6) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:162/proxy/: bar (200; 4.618733ms)
    Jan 17 16:04:25.515: INFO: (6) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:1080/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:1080/proxy/rewriteme">test<... (200; 4.444567ms)
    Jan 17 16:04:25.516: INFO: (6) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:160/proxy/: foo (200; 4.965604ms)
    Jan 17 16:04:25.516: INFO: (6) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:162/proxy/: bar (200; 5.646394ms)
    Jan 17 16:04:25.516: INFO: (6) /api/v1/namespaces/proxy-4776/services/proxy-service-mx2z5:portname2/proxy/: bar (200; 5.749169ms)
    Jan 17 16:04:25.516: INFO: (6) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76/proxy/rewriteme">test</a> (200; 5.724333ms)
    Jan 17 16:04:25.516: INFO: (6) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:1080/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:1080/proxy/rewriteme">... (200; 5.700082ms)
    Jan 17 16:04:25.517: INFO: (6) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:462/proxy/: tls qux (200; 5.674962ms)
    Jan 17 16:04:25.517: INFO: (6) /api/v1/namespaces/proxy-4776/services/https:proxy-service-mx2z5:tlsportname1/proxy/: tls baz (200; 5.956542ms)
    Jan 17 16:04:25.517: INFO: (6) /api/v1/namespaces/proxy-4776/services/http:proxy-service-mx2z5:portname1/proxy/: foo (200; 6.569649ms)
    Jan 17 16:04:25.518: INFO: (6) /api/v1/namespaces/proxy-4776/services/https:proxy-service-mx2z5:tlsportname2/proxy/: tls qux (200; 7.19745ms)
    Jan 17 16:04:25.518: INFO: (6) /api/v1/namespaces/proxy-4776/services/http:proxy-service-mx2z5:portname2/proxy/: bar (200; 7.483924ms)
    Jan 17 16:04:25.518: INFO: (6) /api/v1/namespaces/proxy-4776/services/proxy-service-mx2z5:portname1/proxy/: foo (200; 7.182176ms)
    Jan 17 16:04:25.523: INFO: (7) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:162/proxy/: bar (200; 4.90209ms)
    Jan 17 16:04:25.523: INFO: (7) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76/proxy/rewriteme">test</a> (200; 4.873692ms)
    Jan 17 16:04:25.523: INFO: (7) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:160/proxy/: foo (200; 4.970507ms)
    Jan 17 16:04:25.523: INFO: (7) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:162/proxy/: bar (200; 5.259784ms)
    Jan 17 16:04:25.524: INFO: (7) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:1080/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:1080/proxy/rewriteme">... (200; 5.2563ms)
    Jan 17 16:04:25.524: INFO: (7) /api/v1/namespaces/proxy-4776/services/http:proxy-service-mx2z5:portname2/proxy/: bar (200; 5.454616ms)
    Jan 17 16:04:25.524: INFO: (7) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:160/proxy/: foo (200; 5.786275ms)
    Jan 17 16:04:25.525: INFO: (7) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:1080/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:1080/proxy/rewriteme">test<... (200; 6.28317ms)
    Jan 17 16:04:25.525: INFO: (7) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:462/proxy/: tls qux (200; 6.258678ms)
    Jan 17 16:04:25.525: INFO: (7) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:460/proxy/: tls baz (200; 6.509073ms)
    Jan 17 16:04:25.525: INFO: (7) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:443/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:443/proxy/tlsrewritem... (200; 6.332782ms)
    Jan 17 16:04:25.525: INFO: (7) /api/v1/namespaces/proxy-4776/services/https:proxy-service-mx2z5:tlsportname2/proxy/: tls qux (200; 6.560915ms)
    Jan 17 16:04:25.525: INFO: (7) /api/v1/namespaces/proxy-4776/services/https:proxy-service-mx2z5:tlsportname1/proxy/: tls baz (200; 7.270078ms)
    Jan 17 16:04:25.526: INFO: (7) /api/v1/namespaces/proxy-4776/services/proxy-service-mx2z5:portname2/proxy/: bar (200; 7.427626ms)
    Jan 17 16:04:25.527: INFO: (7) /api/v1/namespaces/proxy-4776/services/http:proxy-service-mx2z5:portname1/proxy/: foo (200; 8.385498ms)
    Jan 17 16:04:25.527: INFO: (7) /api/v1/namespaces/proxy-4776/services/proxy-service-mx2z5:portname1/proxy/: foo (200; 8.229756ms)
    Jan 17 16:04:25.532: INFO: (8) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76/proxy/rewriteme">test</a> (200; 5.198763ms)
    Jan 17 16:04:25.532: INFO: (8) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:160/proxy/: foo (200; 5.135152ms)
    Jan 17 16:04:25.532: INFO: (8) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:460/proxy/: tls baz (200; 5.450057ms)
    Jan 17 16:04:25.532: INFO: (8) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:160/proxy/: foo (200; 5.419461ms)
    Jan 17 16:04:25.533: INFO: (8) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:162/proxy/: bar (200; 6.176706ms)
    Jan 17 16:04:25.533: INFO: (8) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:1080/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:1080/proxy/rewriteme">... (200; 6.36774ms)
    Jan 17 16:04:25.534: INFO: (8) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:162/proxy/: bar (200; 6.600496ms)
    Jan 17 16:04:25.534: INFO: (8) /api/v1/namespaces/proxy-4776/services/https:proxy-service-mx2z5:tlsportname2/proxy/: tls qux (200; 6.886778ms)
    Jan 17 16:04:25.534: INFO: (8) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:462/proxy/: tls qux (200; 6.857796ms)
    Jan 17 16:04:25.534: INFO: (8) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:1080/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:1080/proxy/rewriteme">test<... (200; 7.477069ms)
    Jan 17 16:04:25.534: INFO: (8) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:443/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:443/proxy/tlsrewritem... (200; 7.455404ms)
    Jan 17 16:04:25.535: INFO: (8) /api/v1/namespaces/proxy-4776/services/http:proxy-service-mx2z5:portname1/proxy/: foo (200; 7.981327ms)
    Jan 17 16:04:25.535: INFO: (8) /api/v1/namespaces/proxy-4776/services/http:proxy-service-mx2z5:portname2/proxy/: bar (200; 8.520415ms)
    Jan 17 16:04:25.536: INFO: (8) /api/v1/namespaces/proxy-4776/services/https:proxy-service-mx2z5:tlsportname1/proxy/: tls baz (200; 8.657707ms)
    Jan 17 16:04:25.536: INFO: (8) /api/v1/namespaces/proxy-4776/services/proxy-service-mx2z5:portname1/proxy/: foo (200; 8.945527ms)
    Jan 17 16:04:25.536: INFO: (8) /api/v1/namespaces/proxy-4776/services/proxy-service-mx2z5:portname2/proxy/: bar (200; 9.003334ms)
    Jan 17 16:04:25.541: INFO: (9) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:162/proxy/: bar (200; 4.729551ms)
    Jan 17 16:04:25.541: INFO: (9) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:460/proxy/: tls baz (200; 4.893135ms)
    Jan 17 16:04:25.541: INFO: (9) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:1080/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:1080/proxy/rewriteme">... (200; 5.222304ms)
    Jan 17 16:04:25.541: INFO: (9) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:160/proxy/: foo (200; 5.452377ms)
    Jan 17 16:04:25.541: INFO: (9) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:162/proxy/: bar (200; 5.365818ms)
    Jan 17 16:04:25.542: INFO: (9) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:160/proxy/: foo (200; 5.436762ms)
    Jan 17 16:04:25.542: INFO: (9) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:1080/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:1080/proxy/rewriteme">test<... (200; 6.401528ms)
    Jan 17 16:04:25.543: INFO: (9) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:443/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:443/proxy/tlsrewritem... (200; 6.375352ms)
    Jan 17 16:04:25.543: INFO: (9) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:462/proxy/: tls qux (200; 6.689612ms)
    Jan 17 16:04:25.543: INFO: (9) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76/proxy/rewriteme">test</a> (200; 6.761282ms)
    Jan 17 16:04:25.544: INFO: (9) /api/v1/namespaces/proxy-4776/services/https:proxy-service-mx2z5:tlsportname2/proxy/: tls qux (200; 7.93089ms)
    Jan 17 16:04:25.544: INFO: (9) /api/v1/namespaces/proxy-4776/services/http:proxy-service-mx2z5:portname1/proxy/: foo (200; 8.452283ms)
    Jan 17 16:04:25.544: INFO: (9) /api/v1/namespaces/proxy-4776/services/http:proxy-service-mx2z5:portname2/proxy/: bar (200; 8.496718ms)
    Jan 17 16:04:25.545: INFO: (9) /api/v1/namespaces/proxy-4776/services/https:proxy-service-mx2z5:tlsportname1/proxy/: tls baz (200; 8.656642ms)
    Jan 17 16:04:25.545: INFO: (9) /api/v1/namespaces/proxy-4776/services/proxy-service-mx2z5:portname1/proxy/: foo (200; 8.88162ms)
    Jan 17 16:04:25.545: INFO: (9) /api/v1/namespaces/proxy-4776/services/proxy-service-mx2z5:portname2/proxy/: bar (200; 8.813878ms)
    Jan 17 16:04:25.550: INFO: (10) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:1080/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:1080/proxy/rewriteme">test<... (200; 5.22973ms)
    Jan 17 16:04:25.550: INFO: (10) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:443/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:443/proxy/tlsrewritem... (200; 5.424456ms)
    Jan 17 16:04:25.551: INFO: (10) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:162/proxy/: bar (200; 6.18279ms)
    Jan 17 16:04:25.551: INFO: (10) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76/proxy/rewriteme">test</a> (200; 6.335304ms)
    Jan 17 16:04:25.551: INFO: (10) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:160/proxy/: foo (200; 6.31237ms)
    Jan 17 16:04:25.551: INFO: (10) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:1080/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:1080/proxy/rewriteme">... (200; 6.414292ms)
    Jan 17 16:04:25.552: INFO: (10) /api/v1/namespaces/proxy-4776/services/proxy-service-mx2z5:portname2/proxy/: bar (200; 6.835941ms)
    Jan 17 16:04:25.552: INFO: (10) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:462/proxy/: tls qux (200; 7.355156ms)
    Jan 17 16:04:25.553: INFO: (10) /api/v1/namespaces/proxy-4776/services/proxy-service-mx2z5:portname1/proxy/: foo (200; 7.760637ms)
    Jan 17 16:04:25.553: INFO: (10) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:162/proxy/: bar (200; 8.288636ms)
    Jan 17 16:04:25.554: INFO: (10) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:460/proxy/: tls baz (200; 8.410678ms)
    Jan 17 16:04:25.554: INFO: (10) /api/v1/namespaces/proxy-4776/services/http:proxy-service-mx2z5:portname2/proxy/: bar (200; 8.585487ms)
    Jan 17 16:04:25.554: INFO: (10) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:160/proxy/: foo (200; 9.010152ms)
    Jan 17 16:04:25.554: INFO: (10) /api/v1/namespaces/proxy-4776/services/https:proxy-service-mx2z5:tlsportname2/proxy/: tls qux (200; 9.032134ms)
    Jan 17 16:04:25.555: INFO: (10) /api/v1/namespaces/proxy-4776/services/https:proxy-service-mx2z5:tlsportname1/proxy/: tls baz (200; 9.459253ms)
    Jan 17 16:04:25.555: INFO: (10) /api/v1/namespaces/proxy-4776/services/http:proxy-service-mx2z5:portname1/proxy/: foo (200; 9.813481ms)
    Jan 17 16:04:25.562: INFO: (11) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:462/proxy/: tls qux (200; 6.243246ms)
    Jan 17 16:04:25.562: INFO: (11) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:443/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:443/proxy/tlsrewritem... (200; 6.704975ms)
    Jan 17 16:04:25.562: INFO: (11) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:1080/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:1080/proxy/rewriteme">... (200; 6.383268ms)
    Jan 17 16:04:25.562: INFO: (11) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:1080/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:1080/proxy/rewriteme">test<... (200; 6.413033ms)
    Jan 17 16:04:25.562: INFO: (11) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:460/proxy/: tls baz (200; 6.643871ms)
    Jan 17 16:04:25.562: INFO: (11) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:162/proxy/: bar (200; 6.694552ms)
    Jan 17 16:04:25.563: INFO: (11) /api/v1/namespaces/proxy-4776/services/proxy-service-mx2z5:portname2/proxy/: bar (200; 7.760278ms)
    Jan 17 16:04:25.563: INFO: (11) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:162/proxy/: bar (200; 8.188796ms)
    Jan 17 16:04:25.564: INFO: (11) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:160/proxy/: foo (200; 8.093763ms)
    Jan 17 16:04:25.564: INFO: (11) /api/v1/namespaces/proxy-4776/services/https:proxy-service-mx2z5:tlsportname2/proxy/: tls qux (200; 8.516573ms)
    Jan 17 16:04:25.564: INFO: (11) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76/proxy/rewriteme">test</a> (200; 8.594014ms)
    Jan 17 16:04:25.564: INFO: (11) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:160/proxy/: foo (200; 8.326486ms)
    Jan 17 16:04:25.565: INFO: (11) /api/v1/namespaces/proxy-4776/services/http:proxy-service-mx2z5:portname1/proxy/: foo (200; 9.793235ms)
    Jan 17 16:04:25.568: INFO: (11) /api/v1/namespaces/proxy-4776/services/https:proxy-service-mx2z5:tlsportname1/proxy/: tls baz (200; 12.559139ms)
    Jan 17 16:04:25.568: INFO: (11) /api/v1/namespaces/proxy-4776/services/http:proxy-service-mx2z5:portname2/proxy/: bar (200; 12.946999ms)
    Jan 17 16:04:25.568: INFO: (11) /api/v1/namespaces/proxy-4776/services/proxy-service-mx2z5:portname1/proxy/: foo (200; 12.697649ms)
    Jan 17 16:04:25.573: INFO: (12) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:162/proxy/: bar (200; 5.074147ms)
    Jan 17 16:04:25.573: INFO: (12) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:160/proxy/: foo (200; 4.864068ms)
    Jan 17 16:04:25.574: INFO: (12) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:1080/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:1080/proxy/rewriteme">... (200; 5.462555ms)
    Jan 17 16:04:25.574: INFO: (12) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:460/proxy/: tls baz (200; 5.470087ms)
    Jan 17 16:04:25.574: INFO: (12) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:162/proxy/: bar (200; 5.755775ms)
    Jan 17 16:04:25.575: INFO: (12) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76/proxy/rewriteme">test</a> (200; 6.702721ms)
    Jan 17 16:04:25.575: INFO: (12) /api/v1/namespaces/proxy-4776/services/proxy-service-mx2z5:portname1/proxy/: foo (200; 7.001062ms)
    Jan 17 16:04:25.575: INFO: (12) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:160/proxy/: foo (200; 7.20708ms)
    Jan 17 16:04:25.577: INFO: (12) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:1080/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:1080/proxy/rewriteme">test<... (200; 8.625807ms)
    Jan 17 16:04:25.577: INFO: (12) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:462/proxy/: tls qux (200; 8.60694ms)
    Jan 17 16:04:25.577: INFO: (12) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:443/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:443/proxy/tlsrewritem... (200; 8.759091ms)
    Jan 17 16:04:25.577: INFO: (12) /api/v1/namespaces/proxy-4776/services/proxy-service-mx2z5:portname2/proxy/: bar (200; 9.157376ms)
    Jan 17 16:04:25.577: INFO: (12) /api/v1/namespaces/proxy-4776/services/http:proxy-service-mx2z5:portname2/proxy/: bar (200; 9.286353ms)
    Jan 17 16:04:25.578: INFO: (12) /api/v1/namespaces/proxy-4776/services/https:proxy-service-mx2z5:tlsportname1/proxy/: tls baz (200; 9.459181ms)
    Jan 17 16:04:25.578: INFO: (12) /api/v1/namespaces/proxy-4776/services/https:proxy-service-mx2z5:tlsportname2/proxy/: tls qux (200; 9.932438ms)
    Jan 17 16:04:25.578: INFO: (12) /api/v1/namespaces/proxy-4776/services/http:proxy-service-mx2z5:portname1/proxy/: foo (200; 10.257047ms)
    Jan 17 16:04:25.584: INFO: (13) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:160/proxy/: foo (200; 4.865841ms)
    Jan 17 16:04:25.585: INFO: (13) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:162/proxy/: bar (200; 6.94589ms)
    Jan 17 16:04:25.585: INFO: (13) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:162/proxy/: bar (200; 6.892653ms)
    Jan 17 16:04:25.585: INFO: (13) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:443/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:443/proxy/tlsrewritem... (200; 6.748758ms)
    Jan 17 16:04:25.586: INFO: (13) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:1080/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:1080/proxy/rewriteme">test<... (200; 6.956202ms)
    Jan 17 16:04:25.587: INFO: (13) /api/v1/namespaces/proxy-4776/services/proxy-service-mx2z5:portname2/proxy/: bar (200; 8.037212ms)
    Jan 17 16:04:25.593: INFO: (13) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:160/proxy/: foo (200; 14.415453ms)
    Jan 17 16:04:25.593: INFO: (13) /api/v1/namespaces/proxy-4776/services/https:proxy-service-mx2z5:tlsportname2/proxy/: tls qux (200; 14.505626ms)
    Jan 17 16:04:25.593: INFO: (13) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:462/proxy/: tls qux (200; 14.669139ms)
    Jan 17 16:04:25.593: INFO: (13) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:1080/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:1080/proxy/rewriteme">... (200; 14.48913ms)
    Jan 17 16:04:25.593: INFO: (13) /api/v1/namespaces/proxy-4776/services/http:proxy-service-mx2z5:portname1/proxy/: foo (200; 14.544512ms)
    Jan 17 16:04:25.593: INFO: (13) /api/v1/namespaces/proxy-4776/services/http:proxy-service-mx2z5:portname2/proxy/: bar (200; 14.710188ms)
    Jan 17 16:04:25.593: INFO: (13) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76/proxy/rewriteme">test</a> (200; 14.582223ms)
    Jan 17 16:04:25.593: INFO: (13) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:460/proxy/: tls baz (200; 14.647469ms)
    Jan 17 16:04:25.593: INFO: (13) /api/v1/namespaces/proxy-4776/services/proxy-service-mx2z5:portname1/proxy/: foo (200; 14.500411ms)
    Jan 17 16:04:25.594: INFO: (13) /api/v1/namespaces/proxy-4776/services/https:proxy-service-mx2z5:tlsportname1/proxy/: tls baz (200; 15.065982ms)
    Jan 17 16:04:25.600: INFO: (14) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:160/proxy/: foo (200; 5.886122ms)
    Jan 17 16:04:25.600: INFO: (14) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:1080/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:1080/proxy/rewriteme">... (200; 6.32548ms)
    Jan 17 16:04:25.600: INFO: (14) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:1080/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:1080/proxy/rewriteme">test<... (200; 6.42817ms)
    Jan 17 16:04:25.601: INFO: (14) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:462/proxy/: tls qux (200; 6.679376ms)
    Jan 17 16:04:25.601: INFO: (14) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:162/proxy/: bar (200; 6.640285ms)
    Jan 17 16:04:25.601: INFO: (14) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76/proxy/rewriteme">test</a> (200; 6.540974ms)
    Jan 17 16:04:25.601: INFO: (14) /api/v1/namespaces/proxy-4776/services/proxy-service-mx2z5:portname1/proxy/: foo (200; 7.265602ms)
    Jan 17 16:04:25.602: INFO: (14) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:162/proxy/: bar (200; 7.878192ms)
    Jan 17 16:04:25.602: INFO: (14) /api/v1/namespaces/proxy-4776/services/http:proxy-service-mx2z5:portname1/proxy/: foo (200; 7.900734ms)
    Jan 17 16:04:25.602: INFO: (14) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:460/proxy/: tls baz (200; 8.159183ms)
    Jan 17 16:04:25.602: INFO: (14) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:160/proxy/: foo (200; 7.928391ms)
    Jan 17 16:04:25.602: INFO: (14) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:443/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:443/proxy/tlsrewritem... (200; 8.136629ms)
    Jan 17 16:04:25.603: INFO: (14) /api/v1/namespaces/proxy-4776/services/https:proxy-service-mx2z5:tlsportname1/proxy/: tls baz (200; 8.834575ms)
    Jan 17 16:04:25.603: INFO: (14) /api/v1/namespaces/proxy-4776/services/http:proxy-service-mx2z5:portname2/proxy/: bar (200; 9.102463ms)
    Jan 17 16:04:25.604: INFO: (14) /api/v1/namespaces/proxy-4776/services/proxy-service-mx2z5:portname2/proxy/: bar (200; 9.682336ms)
    Jan 17 16:04:25.604: INFO: (14) /api/v1/namespaces/proxy-4776/services/https:proxy-service-mx2z5:tlsportname2/proxy/: tls qux (200; 10.081698ms)
    Jan 17 16:04:25.610: INFO: (15) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:443/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:443/proxy/tlsrewritem... (200; 5.19391ms)
    Jan 17 16:04:25.610: INFO: (15) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:160/proxy/: foo (200; 5.561612ms)
    Jan 17 16:04:25.610: INFO: (15) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:160/proxy/: foo (200; 5.72869ms)
    Jan 17 16:04:25.610: INFO: (15) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:462/proxy/: tls qux (200; 5.866495ms)
    Jan 17 16:04:25.611: INFO: (15) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76/proxy/rewriteme">test</a> (200; 6.256263ms)
    Jan 17 16:04:25.611: INFO: (15) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:1080/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:1080/proxy/rewriteme">... (200; 6.79509ms)
    Jan 17 16:04:25.612: INFO: (15) /api/v1/namespaces/proxy-4776/services/proxy-service-mx2z5:portname2/proxy/: bar (200; 7.376019ms)
    Jan 17 16:04:25.612: INFO: (15) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:162/proxy/: bar (200; 7.909994ms)
    Jan 17 16:04:25.612: INFO: (15) /api/v1/namespaces/proxy-4776/services/http:proxy-service-mx2z5:portname1/proxy/: foo (200; 7.859077ms)
    Jan 17 16:04:25.613: INFO: (15) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:1080/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:1080/proxy/rewriteme">test<... (200; 8.044033ms)
    Jan 17 16:04:25.613: INFO: (15) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:162/proxy/: bar (200; 8.395315ms)
    Jan 17 16:04:25.613: INFO: (15) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:460/proxy/: tls baz (200; 8.169909ms)
    Jan 17 16:04:25.614: INFO: (15) /api/v1/namespaces/proxy-4776/services/https:proxy-service-mx2z5:tlsportname2/proxy/: tls qux (200; 9.332764ms)
    Jan 17 16:04:25.614: INFO: (15) /api/v1/namespaces/proxy-4776/services/http:proxy-service-mx2z5:portname2/proxy/: bar (200; 9.797986ms)
    Jan 17 16:04:25.615: INFO: (15) /api/v1/namespaces/proxy-4776/services/proxy-service-mx2z5:portname1/proxy/: foo (200; 10.603151ms)
    Jan 17 16:04:25.615: INFO: (15) /api/v1/namespaces/proxy-4776/services/https:proxy-service-mx2z5:tlsportname1/proxy/: tls baz (200; 10.698069ms)
    Jan 17 16:04:25.620: INFO: (16) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:1080/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:1080/proxy/rewriteme">test<... (200; 4.527218ms)
    Jan 17 16:04:25.620: INFO: (16) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:1080/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:1080/proxy/rewriteme">... (200; 4.658145ms)
    Jan 17 16:04:25.620: INFO: (16) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:443/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:443/proxy/tlsrewritem... (200; 5.001014ms)
    Jan 17 16:04:25.621: INFO: (16) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:160/proxy/: foo (200; 6.063049ms)
    Jan 17 16:04:25.621: INFO: (16) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:162/proxy/: bar (200; 6.129722ms)
    Jan 17 16:04:25.621: INFO: (16) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:162/proxy/: bar (200; 5.967639ms)
    Jan 17 16:04:25.621: INFO: (16) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76/proxy/rewriteme">test</a> (200; 6.484851ms)
    Jan 17 16:04:25.622: INFO: (16) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:160/proxy/: foo (200; 6.334067ms)
    Jan 17 16:04:25.622: INFO: (16) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:460/proxy/: tls baz (200; 6.524142ms)
    Jan 17 16:04:25.622: INFO: (16) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:462/proxy/: tls qux (200; 7.0251ms)
    Jan 17 16:04:25.622: INFO: (16) /api/v1/namespaces/proxy-4776/services/https:proxy-service-mx2z5:tlsportname1/proxy/: tls baz (200; 7.29435ms)
    Jan 17 16:04:25.623: INFO: (16) /api/v1/namespaces/proxy-4776/services/http:proxy-service-mx2z5:portname1/proxy/: foo (200; 7.541209ms)
    Jan 17 16:04:25.623: INFO: (16) /api/v1/namespaces/proxy-4776/services/proxy-service-mx2z5:portname1/proxy/: foo (200; 7.774541ms)
    Jan 17 16:04:25.623: INFO: (16) /api/v1/namespaces/proxy-4776/services/https:proxy-service-mx2z5:tlsportname2/proxy/: tls qux (200; 7.579201ms)
    Jan 17 16:04:25.623: INFO: (16) /api/v1/namespaces/proxy-4776/services/proxy-service-mx2z5:portname2/proxy/: bar (200; 7.675282ms)
    Jan 17 16:04:25.624: INFO: (16) /api/v1/namespaces/proxy-4776/services/http:proxy-service-mx2z5:portname2/proxy/: bar (200; 8.55429ms)
    Jan 17 16:04:25.629: INFO: (17) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:1080/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:1080/proxy/rewriteme">... (200; 4.431862ms)
    Jan 17 16:04:25.629: INFO: (17) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76/proxy/rewriteme">test</a> (200; 5.223814ms)
    Jan 17 16:04:25.629: INFO: (17) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:162/proxy/: bar (200; 5.385548ms)
    Jan 17 16:04:25.629: INFO: (17) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:462/proxy/: tls qux (200; 5.388584ms)
    Jan 17 16:04:25.630: INFO: (17) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:162/proxy/: bar (200; 6.085161ms)
    Jan 17 16:04:25.630: INFO: (17) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:460/proxy/: tls baz (200; 6.568477ms)
    Jan 17 16:04:25.631: INFO: (17) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:443/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:443/proxy/tlsrewritem... (200; 6.549432ms)
    Jan 17 16:04:25.631: INFO: (17) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:160/proxy/: foo (200; 6.789755ms)
    Jan 17 16:04:25.631: INFO: (17) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:160/proxy/: foo (200; 6.945862ms)
    Jan 17 16:04:25.631: INFO: (17) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:1080/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:1080/proxy/rewriteme">test<... (200; 7.009094ms)
    Jan 17 16:04:25.632: INFO: (17) /api/v1/namespaces/proxy-4776/services/https:proxy-service-mx2z5:tlsportname1/proxy/: tls baz (200; 8.301495ms)
    Jan 17 16:04:25.633: INFO: (17) /api/v1/namespaces/proxy-4776/services/http:proxy-service-mx2z5:portname2/proxy/: bar (200; 9.047776ms)
    Jan 17 16:04:25.633: INFO: (17) /api/v1/namespaces/proxy-4776/services/https:proxy-service-mx2z5:tlsportname2/proxy/: tls qux (200; 8.843765ms)
    Jan 17 16:04:25.633: INFO: (17) /api/v1/namespaces/proxy-4776/services/http:proxy-service-mx2z5:portname1/proxy/: foo (200; 9.123114ms)
    Jan 17 16:04:25.633: INFO: (17) /api/v1/namespaces/proxy-4776/services/proxy-service-mx2z5:portname1/proxy/: foo (200; 9.216137ms)
    Jan 17 16:04:25.633: INFO: (17) /api/v1/namespaces/proxy-4776/services/proxy-service-mx2z5:portname2/proxy/: bar (200; 9.404463ms)
    Jan 17 16:04:25.638: INFO: (18) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76/proxy/rewriteme">test</a> (200; 4.60415ms)
    Jan 17 16:04:25.639: INFO: (18) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:162/proxy/: bar (200; 5.68389ms)
    Jan 17 16:04:25.640: INFO: (18) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:1080/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:1080/proxy/rewriteme">test<... (200; 6.450338ms)
    Jan 17 16:04:25.641: INFO: (18) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:443/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:443/proxy/tlsrewritem... (200; 6.768162ms)
    Jan 17 16:04:25.641: INFO: (18) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:1080/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:1080/proxy/rewriteme">... (200; 7.267508ms)
    Jan 17 16:04:25.641: INFO: (18) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:160/proxy/: foo (200; 7.10272ms)
    Jan 17 16:04:25.641: INFO: (18) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:460/proxy/: tls baz (200; 7.404038ms)
    Jan 17 16:04:25.641: INFO: (18) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:162/proxy/: bar (200; 7.437415ms)
    Jan 17 16:04:25.641: INFO: (18) /api/v1/namespaces/proxy-4776/services/proxy-service-mx2z5:portname1/proxy/: foo (200; 7.751838ms)
    Jan 17 16:04:25.642: INFO: (18) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:462/proxy/: tls qux (200; 7.983984ms)
    Jan 17 16:04:25.642: INFO: (18) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:160/proxy/: foo (200; 8.231086ms)
    Jan 17 16:04:25.642: INFO: (18) /api/v1/namespaces/proxy-4776/services/http:proxy-service-mx2z5:portname1/proxy/: foo (200; 8.787343ms)
    Jan 17 16:04:25.642: INFO: (18) /api/v1/namespaces/proxy-4776/services/proxy-service-mx2z5:portname2/proxy/: bar (200; 8.567751ms)
    Jan 17 16:04:25.643: INFO: (18) /api/v1/namespaces/proxy-4776/services/https:proxy-service-mx2z5:tlsportname2/proxy/: tls qux (200; 9.382383ms)
    Jan 17 16:04:25.643: INFO: (18) /api/v1/namespaces/proxy-4776/services/https:proxy-service-mx2z5:tlsportname1/proxy/: tls baz (200; 9.626333ms)
    Jan 17 16:04:25.644: INFO: (18) /api/v1/namespaces/proxy-4776/services/http:proxy-service-mx2z5:portname2/proxy/: bar (200; 9.697284ms)
    Jan 17 16:04:25.649: INFO: (19) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:460/proxy/: tls baz (200; 5.082042ms)
    Jan 17 16:04:25.649: INFO: (19) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76/proxy/rewriteme">test</a> (200; 5.041965ms)
    Jan 17 16:04:25.649: INFO: (19) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:160/proxy/: foo (200; 5.16207ms)
    Jan 17 16:04:25.649: INFO: (19) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:462/proxy/: tls qux (200; 5.37864ms)
    Jan 17 16:04:25.650: INFO: (19) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:1080/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:1080/proxy/rewriteme">... (200; 5.831033ms)
    Jan 17 16:04:25.650: INFO: (19) /api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:443/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/https:proxy-service-mx2z5-srd76:443/proxy/tlsrewritem... (200; 6.498753ms)
    Jan 17 16:04:25.650: INFO: (19) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:160/proxy/: foo (200; 6.515282ms)
    Jan 17 16:04:25.650: INFO: (19) /api/v1/namespaces/proxy-4776/services/https:proxy-service-mx2z5:tlsportname1/proxy/: tls baz (200; 6.495854ms)
    Jan 17 16:04:25.650: INFO: (19) /api/v1/namespaces/proxy-4776/services/proxy-service-mx2z5:portname2/proxy/: bar (200; 6.676422ms)
    Jan 17 16:04:25.650: INFO: (19) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:162/proxy/: bar (200; 6.63847ms)
    Jan 17 16:04:25.652: INFO: (19) /api/v1/namespaces/proxy-4776/pods/http:proxy-service-mx2z5-srd76:162/proxy/: bar (200; 8.524383ms)
    Jan 17 16:04:25.652: INFO: (19) /api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:1080/proxy/: <a href="/api/v1/namespaces/proxy-4776/pods/proxy-service-mx2z5-srd76:1080/proxy/rewriteme">test<... (200; 8.603171ms)
    Jan 17 16:04:25.653: INFO: (19) /api/v1/namespaces/proxy-4776/services/http:proxy-service-mx2z5:portname2/proxy/: bar (200; 9.769402ms)
    Jan 17 16:04:25.654: INFO: (19) /api/v1/namespaces/proxy-4776/services/proxy-service-mx2z5:portname1/proxy/: foo (200; 10.175009ms)
    Jan 17 16:04:25.654: INFO: (19) /api/v1/namespaces/proxy-4776/services/http:proxy-service-mx2z5:portname1/proxy/: foo (200; 10.333046ms)
    Jan 17 16:04:25.654: INFO: (19) /api/v1/namespaces/proxy-4776/services/https:proxy-service-mx2z5:tlsportname2/proxy/: tls qux (200; 10.448732ms)
    STEP: deleting ReplicationController proxy-service-mx2z5 in namespace proxy-4776, will wait for the garbage collector to delete the pods 01/17/23 16:04:25.654
    Jan 17 16:04:25.715: INFO: Deleting ReplicationController proxy-service-mx2z5 took: 6.109275ms
    Jan 17 16:04:25.816: INFO: Terminating ReplicationController proxy-service-mx2z5 pods took: 100.834226ms
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Jan 17 16:04:28.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-4776" for this suite. 01/17/23 16:04:28.627
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:04:28.638
Jan 17 16:04:28.638: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename downward-api 01/17/23 16:04:28.639
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:04:28.663
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:04:28.666
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266
STEP: Creating a pod to test downward api env vars 01/17/23 16:04:28.669
Jan 17 16:04:28.703: INFO: Waiting up to 5m0s for pod "downward-api-c03f85c0-c70c-4074-a488-eee72a98b216" in namespace "downward-api-6903" to be "Succeeded or Failed"
Jan 17 16:04:28.711: INFO: Pod "downward-api-c03f85c0-c70c-4074-a488-eee72a98b216": Phase="Pending", Reason="", readiness=false. Elapsed: 8.323662ms
Jan 17 16:04:30.716: INFO: Pod "downward-api-c03f85c0-c70c-4074-a488-eee72a98b216": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012732771s
Jan 17 16:04:32.715: INFO: Pod "downward-api-c03f85c0-c70c-4074-a488-eee72a98b216": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012407286s
STEP: Saw pod success 01/17/23 16:04:32.715
Jan 17 16:04:32.715: INFO: Pod "downward-api-c03f85c0-c70c-4074-a488-eee72a98b216" satisfied condition "Succeeded or Failed"
Jan 17 16:04:32.718: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod downward-api-c03f85c0-c70c-4074-a488-eee72a98b216 container dapi-container: <nil>
STEP: delete the pod 01/17/23 16:04:32.724
Jan 17 16:04:32.736: INFO: Waiting for pod downward-api-c03f85c0-c70c-4074-a488-eee72a98b216 to disappear
Jan 17 16:04:32.740: INFO: Pod downward-api-c03f85c0-c70c-4074-a488-eee72a98b216 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Jan 17 16:04:32.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6903" for this suite. 01/17/23 16:04:32.745
{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","completed":307,"skipped":5637,"failed":0}
------------------------------
• [4.112 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:04:28.638
    Jan 17 16:04:28.638: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename downward-api 01/17/23 16:04:28.639
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:04:28.663
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:04:28.666
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:266
    STEP: Creating a pod to test downward api env vars 01/17/23 16:04:28.669
    Jan 17 16:04:28.703: INFO: Waiting up to 5m0s for pod "downward-api-c03f85c0-c70c-4074-a488-eee72a98b216" in namespace "downward-api-6903" to be "Succeeded or Failed"
    Jan 17 16:04:28.711: INFO: Pod "downward-api-c03f85c0-c70c-4074-a488-eee72a98b216": Phase="Pending", Reason="", readiness=false. Elapsed: 8.323662ms
    Jan 17 16:04:30.716: INFO: Pod "downward-api-c03f85c0-c70c-4074-a488-eee72a98b216": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012732771s
    Jan 17 16:04:32.715: INFO: Pod "downward-api-c03f85c0-c70c-4074-a488-eee72a98b216": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012407286s
    STEP: Saw pod success 01/17/23 16:04:32.715
    Jan 17 16:04:32.715: INFO: Pod "downward-api-c03f85c0-c70c-4074-a488-eee72a98b216" satisfied condition "Succeeded or Failed"
    Jan 17 16:04:32.718: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod downward-api-c03f85c0-c70c-4074-a488-eee72a98b216 container dapi-container: <nil>
    STEP: delete the pod 01/17/23 16:04:32.724
    Jan 17 16:04:32.736: INFO: Waiting for pod downward-api-c03f85c0-c70c-4074-a488-eee72a98b216 to disappear
    Jan 17 16:04:32.740: INFO: Pod downward-api-c03f85c0-c70c-4074-a488-eee72a98b216 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Jan 17 16:04:32.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-6903" for this suite. 01/17/23 16:04:32.745
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:04:32.75
Jan 17 16:04:32.750: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename webhook 01/17/23 16:04:32.751
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:04:32.768
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:04:32.772
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/17/23 16:04:32.796
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/17/23 16:04:33.125
STEP: Deploying the webhook pod 01/17/23 16:04:33.136
STEP: Wait for the deployment to be ready 01/17/23 16:04:33.146
Jan 17 16:04:33.157: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/17/23 16:04:35.167
STEP: Verifying the service has paired with the endpoint 01/17/23 16:04:35.176
Jan 17 16:04:36.177: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322
Jan 17 16:04:36.181: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5722-crds.webhook.example.com via the AdmissionRegistration API 01/17/23 16:04:36.69
STEP: Creating a custom resource while v1 is storage version 01/17/23 16:04:36.714
STEP: Patching Custom Resource Definition to set v2 as storage 01/17/23 16:04:38.762
STEP: Patching the custom resource while v2 is storage version 01/17/23 16:04:38.774
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 17 16:04:39.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5163" for this suite. 01/17/23 16:04:39.346
STEP: Destroying namespace "webhook-5163-markers" for this suite. 01/17/23 16:04:39.352
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","completed":308,"skipped":5646,"failed":0}
------------------------------
• [SLOW TEST] [6.685 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:04:32.75
    Jan 17 16:04:32.750: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename webhook 01/17/23 16:04:32.751
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:04:32.768
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:04:32.772
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/17/23 16:04:32.796
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/17/23 16:04:33.125
    STEP: Deploying the webhook pod 01/17/23 16:04:33.136
    STEP: Wait for the deployment to be ready 01/17/23 16:04:33.146
    Jan 17 16:04:33.157: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/17/23 16:04:35.167
    STEP: Verifying the service has paired with the endpoint 01/17/23 16:04:35.176
    Jan 17 16:04:36.177: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:322
    Jan 17 16:04:36.181: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5722-crds.webhook.example.com via the AdmissionRegistration API 01/17/23 16:04:36.69
    STEP: Creating a custom resource while v1 is storage version 01/17/23 16:04:36.714
    STEP: Patching Custom Resource Definition to set v2 as storage 01/17/23 16:04:38.762
    STEP: Patching the custom resource while v2 is storage version 01/17/23 16:04:38.774
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 17 16:04:39.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-5163" for this suite. 01/17/23 16:04:39.346
    STEP: Destroying namespace "webhook-5163-markers" for this suite. 01/17/23 16:04:39.352
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:04:39.436
Jan 17 16:04:39.436: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename configmap 01/17/23 16:04:39.436
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:04:39.475
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:04:39.478
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73
STEP: Creating configMap with name configmap-test-volume-8cc111b8-56b9-40e9-b3bd-954824ef2bd8 01/17/23 16:04:39.484
STEP: Creating a pod to test consume configMaps 01/17/23 16:04:39.499
Jan 17 16:04:39.565: INFO: Waiting up to 5m0s for pod "pod-configmaps-287619c2-5ad8-45a5-9185-19dd193b794a" in namespace "configmap-3593" to be "Succeeded or Failed"
Jan 17 16:04:39.570: INFO: Pod "pod-configmaps-287619c2-5ad8-45a5-9185-19dd193b794a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.333764ms
Jan 17 16:04:41.574: INFO: Pod "pod-configmaps-287619c2-5ad8-45a5-9185-19dd193b794a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008884629s
Jan 17 16:04:43.574: INFO: Pod "pod-configmaps-287619c2-5ad8-45a5-9185-19dd193b794a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009504178s
STEP: Saw pod success 01/17/23 16:04:43.574
Jan 17 16:04:43.574: INFO: Pod "pod-configmaps-287619c2-5ad8-45a5-9185-19dd193b794a" satisfied condition "Succeeded or Failed"
Jan 17 16:04:43.578: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-configmaps-287619c2-5ad8-45a5-9185-19dd193b794a container agnhost-container: <nil>
STEP: delete the pod 01/17/23 16:04:43.583
Jan 17 16:04:43.596: INFO: Waiting for pod pod-configmaps-287619c2-5ad8-45a5-9185-19dd193b794a to disappear
Jan 17 16:04:43.600: INFO: Pod pod-configmaps-287619c2-5ad8-45a5-9185-19dd193b794a no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 17 16:04:43.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3593" for this suite. 01/17/23 16:04:43.604
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","completed":309,"skipped":5655,"failed":0}
------------------------------
• [4.174 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:04:39.436
    Jan 17 16:04:39.436: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename configmap 01/17/23 16:04:39.436
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:04:39.475
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:04:39.478
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:73
    STEP: Creating configMap with name configmap-test-volume-8cc111b8-56b9-40e9-b3bd-954824ef2bd8 01/17/23 16:04:39.484
    STEP: Creating a pod to test consume configMaps 01/17/23 16:04:39.499
    Jan 17 16:04:39.565: INFO: Waiting up to 5m0s for pod "pod-configmaps-287619c2-5ad8-45a5-9185-19dd193b794a" in namespace "configmap-3593" to be "Succeeded or Failed"
    Jan 17 16:04:39.570: INFO: Pod "pod-configmaps-287619c2-5ad8-45a5-9185-19dd193b794a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.333764ms
    Jan 17 16:04:41.574: INFO: Pod "pod-configmaps-287619c2-5ad8-45a5-9185-19dd193b794a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008884629s
    Jan 17 16:04:43.574: INFO: Pod "pod-configmaps-287619c2-5ad8-45a5-9185-19dd193b794a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009504178s
    STEP: Saw pod success 01/17/23 16:04:43.574
    Jan 17 16:04:43.574: INFO: Pod "pod-configmaps-287619c2-5ad8-45a5-9185-19dd193b794a" satisfied condition "Succeeded or Failed"
    Jan 17 16:04:43.578: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-configmaps-287619c2-5ad8-45a5-9185-19dd193b794a container agnhost-container: <nil>
    STEP: delete the pod 01/17/23 16:04:43.583
    Jan 17 16:04:43.596: INFO: Waiting for pod pod-configmaps-287619c2-5ad8-45a5-9185-19dd193b794a to disappear
    Jan 17 16:04:43.600: INFO: Pod pod-configmaps-287619c2-5ad8-45a5-9185-19dd193b794a no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 17 16:04:43.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-3593" for this suite. 01/17/23 16:04:43.604
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:04:43.61
Jan 17 16:04:43.610: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename resourcequota 01/17/23 16:04:43.611
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:04:43.629
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:04:43.632
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220
STEP: Counting existing ResourceQuota 01/17/23 16:04:43.638
STEP: Creating a ResourceQuota 01/17/23 16:04:48.643
STEP: Ensuring resource quota status is calculated 01/17/23 16:04:48.651
STEP: Creating a Pod that fits quota 01/17/23 16:04:50.654
STEP: Ensuring ResourceQuota status captures the pod usage 01/17/23 16:04:50.677
STEP: Not allowing a pod to be created that exceeds remaining quota 01/17/23 16:04:52.682
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 01/17/23 16:04:52.691
STEP: Ensuring a pod cannot update its resource requirements 01/17/23 16:04:52.699
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 01/17/23 16:04:52.704
STEP: Deleting the pod 01/17/23 16:04:54.709
STEP: Ensuring resource quota status released the pod usage 01/17/23 16:04:54.719
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 17 16:04:56.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6696" for this suite. 01/17/23 16:04:56.73
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","completed":310,"skipped":5660,"failed":0}
------------------------------
• [SLOW TEST] [13.127 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:04:43.61
    Jan 17 16:04:43.610: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename resourcequota 01/17/23 16:04:43.611
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:04:43.629
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:04:43.632
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:220
    STEP: Counting existing ResourceQuota 01/17/23 16:04:43.638
    STEP: Creating a ResourceQuota 01/17/23 16:04:48.643
    STEP: Ensuring resource quota status is calculated 01/17/23 16:04:48.651
    STEP: Creating a Pod that fits quota 01/17/23 16:04:50.654
    STEP: Ensuring ResourceQuota status captures the pod usage 01/17/23 16:04:50.677
    STEP: Not allowing a pod to be created that exceeds remaining quota 01/17/23 16:04:52.682
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 01/17/23 16:04:52.691
    STEP: Ensuring a pod cannot update its resource requirements 01/17/23 16:04:52.699
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 01/17/23 16:04:52.704
    STEP: Deleting the pod 01/17/23 16:04:54.709
    STEP: Ensuring resource quota status released the pod usage 01/17/23 16:04:54.719
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 17 16:04:56.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-6696" for this suite. 01/17/23 16:04:56.73
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:04:56.737
Jan 17 16:04:56.737: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename init-container 01/17/23 16:04:56.738
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:04:56.756
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:04:56.762
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457
STEP: creating the pod 01/17/23 16:04:56.765
Jan 17 16:04:56.765: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Jan 17 16:05:00.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5491" for this suite. 01/17/23 16:05:00.633
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","completed":311,"skipped":5663,"failed":0}
------------------------------
• [3.904 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:04:56.737
    Jan 17 16:04:56.737: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename init-container 01/17/23 16:04:56.738
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:04:56.756
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:04:56.762
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:457
    STEP: creating the pod 01/17/23 16:04:56.765
    Jan 17 16:04:56.765: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan 17 16:05:00.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-5491" for this suite. 01/17/23 16:05:00.633
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:05:00.643
Jan 17 16:05:00.643: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename job 01/17/23 16:05:00.643
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:05:00.677
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:05:00.684
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464
STEP: Creating a job 01/17/23 16:05:00.687
W0117 16:05:00.698407      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensure pods equal to paralellism count is attached to the job 01/17/23 16:05:00.698
STEP: patching /status 01/17/23 16:05:02.702
STEP: updating /status 01/17/23 16:05:02.708
STEP: get /status 01/17/23 16:05:02.715
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jan 17 16:05:02.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-8310" for this suite. 01/17/23 16:05:02.722
{"msg":"PASSED [sig-apps] Job should apply changes to a job status [Conformance]","completed":312,"skipped":5700,"failed":0}
------------------------------
• [2.085 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:05:00.643
    Jan 17 16:05:00.643: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename job 01/17/23 16:05:00.643
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:05:00.677
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:05:00.684
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:464
    STEP: Creating a job 01/17/23 16:05:00.687
    W0117 16:05:00.698407      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Ensure pods equal to paralellism count is attached to the job 01/17/23 16:05:00.698
    STEP: patching /status 01/17/23 16:05:02.702
    STEP: updating /status 01/17/23 16:05:02.708
    STEP: get /status 01/17/23 16:05:02.715
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jan 17 16:05:02.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-8310" for this suite. 01/17/23 16:05:02.722
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:05:02.728
Jan 17 16:05:02.728: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename ephemeral-containers-test 01/17/23 16:05:02.729
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:05:02.748
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:05:02.751
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 01/17/23 16:05:02.759
Jan 17 16:05:02.795: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-5545" to be "running and ready"
Jan 17 16:05:02.803: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.283645ms
Jan 17 16:05:02.803: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 17 16:05:04.807: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011734072s
Jan 17 16:05:04.807: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
Jan 17 16:05:04.807: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 01/17/23 16:05:04.81
Jan 17 16:05:04.819: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-5545" to be "container debugger running"
Jan 17 16:05:04.822: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.291097ms
Jan 17 16:05:06.826: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007252536s
Jan 17 16:05:06.826: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 01/17/23 16:05:06.826
Jan 17 16:05:06.826: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-5545 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 16:05:06.826: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
Jan 17 16:05:06.827: INFO: ExecWithOptions: Clientset creation
Jan 17 16:05:06.827: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/ephemeral-containers-test-5545/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
Jan 17 16:05:06.903: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/framework.go:187
Jan 17 16:05:06.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ephemeral-containers-test-5545" for this suite. 01/17/23 16:05:06.914
{"msg":"PASSED [sig-node] Ephemeral Containers [NodeConformance] will start an ephemeral container in an existing pod [Conformance]","completed":313,"skipped":5704,"failed":0}
------------------------------
• [4.190 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:05:02.728
    Jan 17 16:05:02.728: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename ephemeral-containers-test 01/17/23 16:05:02.729
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:05:02.748
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:05:02.751
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 01/17/23 16:05:02.759
    Jan 17 16:05:02.795: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-5545" to be "running and ready"
    Jan 17 16:05:02.803: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.283645ms
    Jan 17 16:05:02.803: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 16:05:04.807: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011734072s
    Jan 17 16:05:04.807: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    Jan 17 16:05:04.807: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 01/17/23 16:05:04.81
    Jan 17 16:05:04.819: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-5545" to be "container debugger running"
    Jan 17 16:05:04.822: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.291097ms
    Jan 17 16:05:06.826: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007252536s
    Jan 17 16:05:06.826: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 01/17/23 16:05:06.826
    Jan 17 16:05:06.826: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-5545 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 17 16:05:06.826: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    Jan 17 16:05:06.827: INFO: ExecWithOptions: Clientset creation
    Jan 17 16:05:06.827: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/ephemeral-containers-test-5545/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    Jan 17 16:05:06.903: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan 17 16:05:06.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ephemeral-containers-test-5545" for this suite. 01/17/23 16:05:06.914
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:392
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:05:06.919
Jan 17 16:05:06.919: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename kubectl 01/17/23 16:05:06.92
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:05:06.938
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:05:06.941
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:392
STEP: creating all guestbook components 01/17/23 16:05:06.958
Jan 17 16:05:06.959: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Jan 17 16:05:06.959: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-2530 create -f -'
Jan 17 16:05:08.619: INFO: stderr: ""
Jan 17 16:05:08.619: INFO: stdout: "service/agnhost-replica created\n"
Jan 17 16:05:08.619: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Jan 17 16:05:08.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-2530 create -f -'
Jan 17 16:05:08.874: INFO: stderr: ""
Jan 17 16:05:08.874: INFO: stdout: "service/agnhost-primary created\n"
Jan 17 16:05:08.874: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jan 17 16:05:08.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-2530 create -f -'
Jan 17 16:05:10.533: INFO: stderr: ""
Jan 17 16:05:10.533: INFO: stdout: "service/frontend created\n"
Jan 17 16:05:10.533: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Jan 17 16:05:10.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-2530 create -f -'
Jan 17 16:05:12.100: INFO: stderr: ""
Jan 17 16:05:12.100: INFO: stdout: "deployment.apps/frontend created\n"
Jan 17 16:05:12.100: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jan 17 16:05:12.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-2530 create -f -'
Jan 17 16:05:12.360: INFO: stderr: ""
Jan 17 16:05:12.360: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Jan 17 16:05:12.360: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jan 17 16:05:12.360: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-2530 create -f -'
Jan 17 16:05:12.645: INFO: stderr: ""
Jan 17 16:05:12.645: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 01/17/23 16:05:12.645
Jan 17 16:05:12.645: INFO: Waiting for all frontend pods to be Running.
Jan 17 16:05:17.696: INFO: Waiting for frontend to serve content.
Jan 17 16:05:17.709: INFO: Trying to add a new entry to the guestbook.
Jan 17 16:05:17.721: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources 01/17/23 16:05:17.729
Jan 17 16:05:17.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-2530 delete --grace-period=0 --force -f -'
Jan 17 16:05:17.803: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 17 16:05:17.803: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 01/17/23 16:05:17.803
Jan 17 16:05:17.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-2530 delete --grace-period=0 --force -f -'
Jan 17 16:05:17.873: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 17 16:05:17.873: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 01/17/23 16:05:17.873
Jan 17 16:05:17.873: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-2530 delete --grace-period=0 --force -f -'
Jan 17 16:05:17.963: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 17 16:05:17.963: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 01/17/23 16:05:17.963
Jan 17 16:05:17.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-2530 delete --grace-period=0 --force -f -'
Jan 17 16:05:18.011: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 17 16:05:18.012: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 01/17/23 16:05:18.012
Jan 17 16:05:18.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-2530 delete --grace-period=0 --force -f -'
Jan 17 16:05:18.070: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 17 16:05:18.070: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 01/17/23 16:05:18.07
Jan 17 16:05:18.070: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-2530 delete --grace-period=0 --force -f -'
Jan 17 16:05:18.121: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 17 16:05:18.121: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 17 16:05:18.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2530" for this suite. 01/17/23 16:05:18.125
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","completed":314,"skipped":5734,"failed":0}
------------------------------
• [SLOW TEST] [11.212 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:367
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:05:06.919
    Jan 17 16:05:06.919: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename kubectl 01/17/23 16:05:06.92
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:05:06.938
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:05:06.941
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:392
    STEP: creating all guestbook components 01/17/23 16:05:06.958
    Jan 17 16:05:06.959: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    Jan 17 16:05:06.959: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-2530 create -f -'
    Jan 17 16:05:08.619: INFO: stderr: ""
    Jan 17 16:05:08.619: INFO: stdout: "service/agnhost-replica created\n"
    Jan 17 16:05:08.619: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    Jan 17 16:05:08.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-2530 create -f -'
    Jan 17 16:05:08.874: INFO: stderr: ""
    Jan 17 16:05:08.874: INFO: stdout: "service/agnhost-primary created\n"
    Jan 17 16:05:08.874: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    Jan 17 16:05:08.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-2530 create -f -'
    Jan 17 16:05:10.533: INFO: stderr: ""
    Jan 17 16:05:10.533: INFO: stdout: "service/frontend created\n"
    Jan 17 16:05:10.533: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    Jan 17 16:05:10.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-2530 create -f -'
    Jan 17 16:05:12.100: INFO: stderr: ""
    Jan 17 16:05:12.100: INFO: stdout: "deployment.apps/frontend created\n"
    Jan 17 16:05:12.100: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Jan 17 16:05:12.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-2530 create -f -'
    Jan 17 16:05:12.360: INFO: stderr: ""
    Jan 17 16:05:12.360: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    Jan 17 16:05:12.360: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Jan 17 16:05:12.360: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-2530 create -f -'
    Jan 17 16:05:12.645: INFO: stderr: ""
    Jan 17 16:05:12.645: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 01/17/23 16:05:12.645
    Jan 17 16:05:12.645: INFO: Waiting for all frontend pods to be Running.
    Jan 17 16:05:17.696: INFO: Waiting for frontend to serve content.
    Jan 17 16:05:17.709: INFO: Trying to add a new entry to the guestbook.
    Jan 17 16:05:17.721: INFO: Verifying that added entry can be retrieved.
    STEP: using delete to clean up resources 01/17/23 16:05:17.729
    Jan 17 16:05:17.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-2530 delete --grace-period=0 --force -f -'
    Jan 17 16:05:17.803: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 17 16:05:17.803: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 01/17/23 16:05:17.803
    Jan 17 16:05:17.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-2530 delete --grace-period=0 --force -f -'
    Jan 17 16:05:17.873: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 17 16:05:17.873: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 01/17/23 16:05:17.873
    Jan 17 16:05:17.873: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-2530 delete --grace-period=0 --force -f -'
    Jan 17 16:05:17.963: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 17 16:05:17.963: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 01/17/23 16:05:17.963
    Jan 17 16:05:17.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-2530 delete --grace-period=0 --force -f -'
    Jan 17 16:05:18.011: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 17 16:05:18.012: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 01/17/23 16:05:18.012
    Jan 17 16:05:18.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-2530 delete --grace-period=0 --force -f -'
    Jan 17 16:05:18.070: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 17 16:05:18.070: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 01/17/23 16:05:18.07
    Jan 17 16:05:18.070: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=kubectl-2530 delete --grace-period=0 --force -f -'
    Jan 17 16:05:18.121: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 17 16:05:18.121: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 17 16:05:18.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-2530" for this suite. 01/17/23 16:05:18.125
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:05:18.134
Jan 17 16:05:18.134: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename pods 01/17/23 16:05:18.134
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:05:18.162
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:05:18.166
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535
Jan 17 16:05:18.169: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: creating the pod 01/17/23 16:05:18.169
STEP: submitting the pod to kubernetes 01/17/23 16:05:18.169
Jan 17 16:05:18.211: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-8b1f3672-5d24-4d58-b856-96fd32d6952f" in namespace "pods-8059" to be "running and ready"
Jan 17 16:05:18.216: INFO: Pod "pod-exec-websocket-8b1f3672-5d24-4d58-b856-96fd32d6952f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.140088ms
Jan 17 16:05:18.216: INFO: The phase of Pod pod-exec-websocket-8b1f3672-5d24-4d58-b856-96fd32d6952f is Pending, waiting for it to be Running (with Ready = true)
Jan 17 16:05:20.219: INFO: Pod "pod-exec-websocket-8b1f3672-5d24-4d58-b856-96fd32d6952f": Phase="Running", Reason="", readiness=true. Elapsed: 2.008508928s
Jan 17 16:05:20.219: INFO: The phase of Pod pod-exec-websocket-8b1f3672-5d24-4d58-b856-96fd32d6952f is Running (Ready = true)
Jan 17 16:05:20.219: INFO: Pod "pod-exec-websocket-8b1f3672-5d24-4d58-b856-96fd32d6952f" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 17 16:05:20.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8059" for this suite. 01/17/23 16:05:20.326
{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","completed":315,"skipped":5814,"failed":0}
------------------------------
• [2.199 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:05:18.134
    Jan 17 16:05:18.134: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename pods 01/17/23 16:05:18.134
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:05:18.162
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:05:18.166
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:535
    Jan 17 16:05:18.169: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: creating the pod 01/17/23 16:05:18.169
    STEP: submitting the pod to kubernetes 01/17/23 16:05:18.169
    Jan 17 16:05:18.211: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-8b1f3672-5d24-4d58-b856-96fd32d6952f" in namespace "pods-8059" to be "running and ready"
    Jan 17 16:05:18.216: INFO: Pod "pod-exec-websocket-8b1f3672-5d24-4d58-b856-96fd32d6952f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.140088ms
    Jan 17 16:05:18.216: INFO: The phase of Pod pod-exec-websocket-8b1f3672-5d24-4d58-b856-96fd32d6952f is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 16:05:20.219: INFO: Pod "pod-exec-websocket-8b1f3672-5d24-4d58-b856-96fd32d6952f": Phase="Running", Reason="", readiness=true. Elapsed: 2.008508928s
    Jan 17 16:05:20.219: INFO: The phase of Pod pod-exec-websocket-8b1f3672-5d24-4d58-b856-96fd32d6952f is Running (Ready = true)
    Jan 17 16:05:20.219: INFO: Pod "pod-exec-websocket-8b1f3672-5d24-4d58-b856-96fd32d6952f" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 17 16:05:20.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-8059" for this suite. 01/17/23 16:05:20.326
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:05:20.333
Jan 17 16:05:20.333: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename webhook 01/17/23 16:05:20.334
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:05:20.349
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:05:20.352
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/17/23 16:05:20.385
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/17/23 16:05:20.589
STEP: Deploying the webhook pod 01/17/23 16:05:20.601
STEP: Wait for the deployment to be ready 01/17/23 16:05:20.615
Jan 17 16:05:20.623: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/17/23 16:05:22.633
STEP: Verifying the service has paired with the endpoint 01/17/23 16:05:22.645
Jan 17 16:05:23.646: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412
STEP: Creating a validating webhook configuration 01/17/23 16:05:23.65
STEP: Creating a configMap that does not comply to the validation webhook rules 01/17/23 16:05:23.664
STEP: Updating a validating webhook configuration's rules to not include the create operation 01/17/23 16:05:23.669
STEP: Creating a configMap that does not comply to the validation webhook rules 01/17/23 16:05:23.677
STEP: Patching a validating webhook configuration's rules to include the create operation 01/17/23 16:05:23.689
STEP: Creating a configMap that does not comply to the validation webhook rules 01/17/23 16:05:23.697
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 17 16:05:23.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7969" for this suite. 01/17/23 16:05:23.712
STEP: Destroying namespace "webhook-7969-markers" for this suite. 01/17/23 16:05:23.718
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","completed":316,"skipped":5847,"failed":0}
------------------------------
• [3.503 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:05:20.333
    Jan 17 16:05:20.333: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename webhook 01/17/23 16:05:20.334
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:05:20.349
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:05:20.352
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/17/23 16:05:20.385
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/17/23 16:05:20.589
    STEP: Deploying the webhook pod 01/17/23 16:05:20.601
    STEP: Wait for the deployment to be ready 01/17/23 16:05:20.615
    Jan 17 16:05:20.623: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/17/23 16:05:22.633
    STEP: Verifying the service has paired with the endpoint 01/17/23 16:05:22.645
    Jan 17 16:05:23.646: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:412
    STEP: Creating a validating webhook configuration 01/17/23 16:05:23.65
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/17/23 16:05:23.664
    STEP: Updating a validating webhook configuration's rules to not include the create operation 01/17/23 16:05:23.669
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/17/23 16:05:23.677
    STEP: Patching a validating webhook configuration's rules to include the create operation 01/17/23 16:05:23.689
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/17/23 16:05:23.697
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 17 16:05:23.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-7969" for this suite. 01/17/23 16:05:23.712
    STEP: Destroying namespace "webhook-7969-markers" for this suite. 01/17/23 16:05:23.718
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:05:23.837
Jan 17 16:05:23.837: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename init-container 01/17/23 16:05:23.838
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:05:23.858
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:05:23.868
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176
STEP: creating the pod 01/17/23 16:05:23.872
Jan 17 16:05:23.872: INFO: PodSpec: initContainers in spec.initContainers
W0117 16:05:23.911763      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (containers "init1", "init2", "run1" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "init1", "init2", "run1" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "init1", "init2", "run1" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "init1", "init2", "run1" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Jan 17 16:05:29.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8108" for this suite. 01/17/23 16:05:29.173
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","completed":317,"skipped":5859,"failed":0}
------------------------------
• [SLOW TEST] [5.343 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:05:23.837
    Jan 17 16:05:23.837: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename init-container 01/17/23 16:05:23.838
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:05:23.858
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:05:23.868
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:176
    STEP: creating the pod 01/17/23 16:05:23.872
    Jan 17 16:05:23.872: INFO: PodSpec: initContainers in spec.initContainers
    W0117 16:05:23.911763      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (containers "init1", "init2", "run1" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "init1", "init2", "run1" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "init1", "init2", "run1" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "init1", "init2", "run1" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan 17 16:05:29.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-8108" for this suite. 01/17/23 16:05:29.173
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:05:29.18
Jan 17 16:05:29.181: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename pods 01/17/23 16:05:29.181
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:05:29.201
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:05:29.204
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 01/17/23 16:05:29.206
STEP: submitting the pod to kubernetes 01/17/23 16:05:29.206
STEP: verifying QOS class is set on the pod 01/17/23 16:05:29.247
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:187
Jan 17 16:05:29.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7872" for this suite. 01/17/23 16:05:29.261
{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","completed":318,"skipped":5880,"failed":0}
------------------------------
• [0.095 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:05:29.18
    Jan 17 16:05:29.181: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename pods 01/17/23 16:05:29.181
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:05:29.201
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:05:29.204
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 01/17/23 16:05:29.206
    STEP: submitting the pod to kubernetes 01/17/23 16:05:29.206
    STEP: verifying QOS class is set on the pod 01/17/23 16:05:29.247
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/framework.go:187
    Jan 17 16:05:29.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-7872" for this suite. 01/17/23 16:05:29.261
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:05:29.276
Jan 17 16:05:29.276: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename webhook 01/17/23 16:05:29.276
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:05:29.3
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:05:29.305
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/17/23 16:05:29.32
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/17/23 16:05:29.844
STEP: Deploying the webhook pod 01/17/23 16:05:29.854
STEP: Wait for the deployment to be ready 01/17/23 16:05:29.865
Jan 17 16:05:29.873: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/17/23 16:05:31.885
STEP: Verifying the service has paired with the endpoint 01/17/23 16:05:31.896
Jan 17 16:05:32.896: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 01/17/23 16:05:32.9
STEP: create a namespace for the webhook 01/17/23 16:05:32.913
STEP: create a configmap should be unconditionally rejected by the webhook 01/17/23 16:05:32.92
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 17 16:05:32.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1074" for this suite. 01/17/23 16:05:32.956
STEP: Destroying namespace "webhook-1074-markers" for this suite. 01/17/23 16:05:32.963
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","completed":319,"skipped":5899,"failed":0}
------------------------------
• [3.784 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:05:29.276
    Jan 17 16:05:29.276: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename webhook 01/17/23 16:05:29.276
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:05:29.3
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:05:29.305
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/17/23 16:05:29.32
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/17/23 16:05:29.844
    STEP: Deploying the webhook pod 01/17/23 16:05:29.854
    STEP: Wait for the deployment to be ready 01/17/23 16:05:29.865
    Jan 17 16:05:29.873: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/17/23 16:05:31.885
    STEP: Verifying the service has paired with the endpoint 01/17/23 16:05:31.896
    Jan 17 16:05:32.896: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:238
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 01/17/23 16:05:32.9
    STEP: create a namespace for the webhook 01/17/23 16:05:32.913
    STEP: create a configmap should be unconditionally rejected by the webhook 01/17/23 16:05:32.92
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 17 16:05:32.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-1074" for this suite. 01/17/23 16:05:32.956
    STEP: Destroying namespace "webhook-1074-markers" for this suite. 01/17/23 16:05:32.963
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:695
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:05:33.06
Jan 17 16:05:33.061: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename statefulset 01/17/23 16:05:33.061
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:05:33.082
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:05:33.084
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-7797 01/17/23 16:05:33.093
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:695
STEP: Creating stateful set ss in namespace statefulset-7797 01/17/23 16:05:33.104
W0117 16:05:33.114175      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7797 01/17/23 16:05:33.114
Jan 17 16:05:33.130: INFO: Found 0 stateful pods, waiting for 1
Jan 17 16:05:43.135: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 01/17/23 16:05:43.135
Jan 17 16:05:43.140: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=statefulset-7797 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 17 16:05:43.327: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 17 16:05:43.327: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 17 16:05:43.327: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 17 16:05:43.330: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan 17 16:05:53.335: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 17 16:05:53.335: INFO: Waiting for statefulset status.replicas updated to 0
Jan 17 16:05:53.349: INFO: POD   NODE                          PHASE    GRACE  CONDITIONS
Jan 17 16:05:53.349: INFO: ss-0  ip-10-0-139-213.ec2.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 16:05:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 16:05:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 16:05:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 16:05:33 +0000 UTC  }]
Jan 17 16:05:53.349: INFO: 
Jan 17 16:05:53.349: INFO: StatefulSet ss has not reached scale 3, at 1
Jan 17 16:05:54.357: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997420323s
Jan 17 16:05:55.360: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.989210483s
Jan 17 16:05:56.363: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.9859923s
Jan 17 16:05:57.368: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.982786892s
Jan 17 16:05:58.372: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.977553408s
Jan 17 16:05:59.375: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.973933751s
Jan 17 16:06:00.379: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.9706113s
Jan 17 16:06:01.384: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.966627197s
Jan 17 16:06:02.387: INFO: Verifying statefulset ss doesn't scale past 3 for another 962.054039ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7797 01/17/23 16:06:03.388
Jan 17 16:06:03.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=statefulset-7797 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 17 16:06:03.499: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 17 16:06:03.499: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 17 16:06:03.499: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 17 16:06:03.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=statefulset-7797 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 17 16:06:03.602: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jan 17 16:06:03.602: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 17 16:06:03.602: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 17 16:06:03.602: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=statefulset-7797 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 17 16:06:03.699: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jan 17 16:06:03.699: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 17 16:06:03.699: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 17 16:06:03.702: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Jan 17 16:06:13.706: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 17 16:06:13.706: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 17 16:06:13.706: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 01/17/23 16:06:13.706
Jan 17 16:06:13.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=statefulset-7797 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 17 16:06:13.814: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 17 16:06:13.814: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 17 16:06:13.814: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 17 16:06:13.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=statefulset-7797 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 17 16:06:13.938: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 17 16:06:13.938: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 17 16:06:13.938: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 17 16:06:13.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=statefulset-7797 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 17 16:06:14.063: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 17 16:06:14.063: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 17 16:06:14.063: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 17 16:06:14.063: INFO: Waiting for statefulset status.replicas updated to 0
Jan 17 16:06:14.067: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Jan 17 16:06:24.075: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 17 16:06:24.075: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan 17 16:06:24.075: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan 17 16:06:24.085: INFO: POD   NODE                          PHASE    GRACE  CONDITIONS
Jan 17 16:06:24.085: INFO: ss-0  ip-10-0-139-213.ec2.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 16:05:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 16:06:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 16:06:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 16:05:33 +0000 UTC  }]
Jan 17 16:06:24.085: INFO: ss-1  ip-10-0-151-22.ec2.internal   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 16:05:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 16:06:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 16:06:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 16:05:53 +0000 UTC  }]
Jan 17 16:06:24.085: INFO: ss-2  ip-10-0-165-14.ec2.internal   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 16:05:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 16:06:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 16:06:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 16:05:53 +0000 UTC  }]
Jan 17 16:06:24.085: INFO: 
Jan 17 16:06:24.085: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 17 16:06:25.090: INFO: POD   NODE                          PHASE    GRACE  CONDITIONS
Jan 17 16:06:25.090: INFO: ss-0  ip-10-0-139-213.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 16:05:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 16:06:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 16:06:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 16:05:33 +0000 UTC  }]
Jan 17 16:06:25.090: INFO: ss-2  ip-10-0-165-14.ec2.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 16:05:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 16:06:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 16:06:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 16:05:53 +0000 UTC  }]
Jan 17 16:06:25.090: INFO: 
Jan 17 16:06:25.090: INFO: StatefulSet ss has not reached scale 0, at 2
Jan 17 16:06:26.094: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.992492424s
Jan 17 16:06:27.097: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.988432313s
Jan 17 16:06:28.102: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.985385495s
Jan 17 16:06:29.106: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.979732693s
Jan 17 16:06:30.111: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.974979419s
Jan 17 16:06:31.114: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.971035798s
Jan 17 16:06:32.117: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.968096248s
Jan 17 16:06:33.121: INFO: Verifying statefulset ss doesn't scale past 0 for another 964.901653ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7797 01/17/23 16:06:34.121
Jan 17 16:06:34.130: INFO: Scaling statefulset ss to 0
Jan 17 16:06:34.142: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 17 16:06:34.144: INFO: Deleting all statefulset in ns statefulset-7797
Jan 17 16:06:34.146: INFO: Scaling statefulset ss to 0
Jan 17 16:06:34.157: INFO: Waiting for statefulset status.replicas updated to 0
Jan 17 16:06:34.159: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan 17 16:06:34.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7797" for this suite. 01/17/23 16:06:34.189
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","completed":320,"skipped":5918,"failed":0}
------------------------------
• [SLOW TEST] [61.140 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:695

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:05:33.06
    Jan 17 16:05:33.061: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename statefulset 01/17/23 16:05:33.061
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:05:33.082
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:05:33.084
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-7797 01/17/23 16:05:33.093
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:695
    STEP: Creating stateful set ss in namespace statefulset-7797 01/17/23 16:05:33.104
    W0117 16:05:33.114175      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7797 01/17/23 16:05:33.114
    Jan 17 16:05:33.130: INFO: Found 0 stateful pods, waiting for 1
    Jan 17 16:05:43.135: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 01/17/23 16:05:43.135
    Jan 17 16:05:43.140: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=statefulset-7797 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 17 16:05:43.327: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 17 16:05:43.327: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 17 16:05:43.327: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 17 16:05:43.330: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Jan 17 16:05:53.335: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan 17 16:05:53.335: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 17 16:05:53.349: INFO: POD   NODE                          PHASE    GRACE  CONDITIONS
    Jan 17 16:05:53.349: INFO: ss-0  ip-10-0-139-213.ec2.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 16:05:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 16:05:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 16:05:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 16:05:33 +0000 UTC  }]
    Jan 17 16:05:53.349: INFO: 
    Jan 17 16:05:53.349: INFO: StatefulSet ss has not reached scale 3, at 1
    Jan 17 16:05:54.357: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997420323s
    Jan 17 16:05:55.360: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.989210483s
    Jan 17 16:05:56.363: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.9859923s
    Jan 17 16:05:57.368: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.982786892s
    Jan 17 16:05:58.372: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.977553408s
    Jan 17 16:05:59.375: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.973933751s
    Jan 17 16:06:00.379: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.9706113s
    Jan 17 16:06:01.384: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.966627197s
    Jan 17 16:06:02.387: INFO: Verifying statefulset ss doesn't scale past 3 for another 962.054039ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7797 01/17/23 16:06:03.388
    Jan 17 16:06:03.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=statefulset-7797 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 17 16:06:03.499: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 17 16:06:03.499: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 17 16:06:03.499: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 17 16:06:03.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=statefulset-7797 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 17 16:06:03.602: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Jan 17 16:06:03.602: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 17 16:06:03.602: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 17 16:06:03.602: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=statefulset-7797 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 17 16:06:03.699: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Jan 17 16:06:03.699: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 17 16:06:03.699: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 17 16:06:03.702: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
    Jan 17 16:06:13.706: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 17 16:06:13.706: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 17 16:06:13.706: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 01/17/23 16:06:13.706
    Jan 17 16:06:13.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=statefulset-7797 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 17 16:06:13.814: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 17 16:06:13.814: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 17 16:06:13.814: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 17 16:06:13.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=statefulset-7797 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 17 16:06:13.938: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 17 16:06:13.938: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 17 16:06:13.938: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 17 16:06:13.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=statefulset-7797 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 17 16:06:14.063: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 17 16:06:14.063: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 17 16:06:14.063: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 17 16:06:14.063: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 17 16:06:14.067: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Jan 17 16:06:24.075: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan 17 16:06:24.075: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Jan 17 16:06:24.075: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Jan 17 16:06:24.085: INFO: POD   NODE                          PHASE    GRACE  CONDITIONS
    Jan 17 16:06:24.085: INFO: ss-0  ip-10-0-139-213.ec2.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 16:05:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 16:06:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 16:06:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 16:05:33 +0000 UTC  }]
    Jan 17 16:06:24.085: INFO: ss-1  ip-10-0-151-22.ec2.internal   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 16:05:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 16:06:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 16:06:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 16:05:53 +0000 UTC  }]
    Jan 17 16:06:24.085: INFO: ss-2  ip-10-0-165-14.ec2.internal   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 16:05:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 16:06:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 16:06:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 16:05:53 +0000 UTC  }]
    Jan 17 16:06:24.085: INFO: 
    Jan 17 16:06:24.085: INFO: StatefulSet ss has not reached scale 0, at 3
    Jan 17 16:06:25.090: INFO: POD   NODE                          PHASE    GRACE  CONDITIONS
    Jan 17 16:06:25.090: INFO: ss-0  ip-10-0-139-213.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 16:05:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 16:06:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 16:06:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 16:05:33 +0000 UTC  }]
    Jan 17 16:06:25.090: INFO: ss-2  ip-10-0-165-14.ec2.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 16:05:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 16:06:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 16:06:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 16:05:53 +0000 UTC  }]
    Jan 17 16:06:25.090: INFO: 
    Jan 17 16:06:25.090: INFO: StatefulSet ss has not reached scale 0, at 2
    Jan 17 16:06:26.094: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.992492424s
    Jan 17 16:06:27.097: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.988432313s
    Jan 17 16:06:28.102: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.985385495s
    Jan 17 16:06:29.106: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.979732693s
    Jan 17 16:06:30.111: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.974979419s
    Jan 17 16:06:31.114: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.971035798s
    Jan 17 16:06:32.117: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.968096248s
    Jan 17 16:06:33.121: INFO: Verifying statefulset ss doesn't scale past 0 for another 964.901653ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7797 01/17/23 16:06:34.121
    Jan 17 16:06:34.130: INFO: Scaling statefulset ss to 0
    Jan 17 16:06:34.142: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan 17 16:06:34.144: INFO: Deleting all statefulset in ns statefulset-7797
    Jan 17 16:06:34.146: INFO: Scaling statefulset ss to 0
    Jan 17 16:06:34.157: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 17 16:06:34.159: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan 17 16:06:34.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-7797" for this suite. 01/17/23 16:06:34.189
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:06:34.201
Jan 17 16:06:34.201: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename container-probe 01/17/23 16:06:34.202
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:06:34.223
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:06:34.226
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180
STEP: Creating pod liveness-1f5db9ae-a7fc-44e3-b4a2-0bc59d0ad65e in namespace container-probe-6697 01/17/23 16:06:34.231
Jan 17 16:06:34.252: INFO: Waiting up to 5m0s for pod "liveness-1f5db9ae-a7fc-44e3-b4a2-0bc59d0ad65e" in namespace "container-probe-6697" to be "not pending"
Jan 17 16:06:34.259: INFO: Pod "liveness-1f5db9ae-a7fc-44e3-b4a2-0bc59d0ad65e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.417907ms
Jan 17 16:06:36.262: INFO: Pod "liveness-1f5db9ae-a7fc-44e3-b4a2-0bc59d0ad65e": Phase="Running", Reason="", readiness=true. Elapsed: 2.010377778s
Jan 17 16:06:36.262: INFO: Pod "liveness-1f5db9ae-a7fc-44e3-b4a2-0bc59d0ad65e" satisfied condition "not pending"
Jan 17 16:06:36.262: INFO: Started pod liveness-1f5db9ae-a7fc-44e3-b4a2-0bc59d0ad65e in namespace container-probe-6697
STEP: checking the pod's current state and verifying that restartCount is present 01/17/23 16:06:36.262
Jan 17 16:06:36.265: INFO: Initial restart count of pod liveness-1f5db9ae-a7fc-44e3-b4a2-0bc59d0ad65e is 0
STEP: deleting the pod 01/17/23 16:10:36.815
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan 17 16:10:36.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6697" for this suite. 01/17/23 16:10:36.836
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","completed":321,"skipped":5928,"failed":0}
------------------------------
• [SLOW TEST] [242.642 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:06:34.201
    Jan 17 16:06:34.201: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename container-probe 01/17/23 16:06:34.202
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:06:34.223
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:06:34.226
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:180
    STEP: Creating pod liveness-1f5db9ae-a7fc-44e3-b4a2-0bc59d0ad65e in namespace container-probe-6697 01/17/23 16:06:34.231
    Jan 17 16:06:34.252: INFO: Waiting up to 5m0s for pod "liveness-1f5db9ae-a7fc-44e3-b4a2-0bc59d0ad65e" in namespace "container-probe-6697" to be "not pending"
    Jan 17 16:06:34.259: INFO: Pod "liveness-1f5db9ae-a7fc-44e3-b4a2-0bc59d0ad65e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.417907ms
    Jan 17 16:06:36.262: INFO: Pod "liveness-1f5db9ae-a7fc-44e3-b4a2-0bc59d0ad65e": Phase="Running", Reason="", readiness=true. Elapsed: 2.010377778s
    Jan 17 16:06:36.262: INFO: Pod "liveness-1f5db9ae-a7fc-44e3-b4a2-0bc59d0ad65e" satisfied condition "not pending"
    Jan 17 16:06:36.262: INFO: Started pod liveness-1f5db9ae-a7fc-44e3-b4a2-0bc59d0ad65e in namespace container-probe-6697
    STEP: checking the pod's current state and verifying that restartCount is present 01/17/23 16:06:36.262
    Jan 17 16:06:36.265: INFO: Initial restart count of pod liveness-1f5db9ae-a7fc-44e3-b4a2-0bc59d0ad65e is 0
    STEP: deleting the pod 01/17/23 16:10:36.815
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan 17 16:10:36.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-6697" for this suite. 01/17/23 16:10:36.836
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:10:36.844
Jan 17 16:10:36.844: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename crd-webhook 01/17/23 16:10:36.844
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:10:36.873
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:10:36.876
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 01/17/23 16:10:36.881
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/17/23 16:10:37.246
STEP: Deploying the custom resource conversion webhook pod 01/17/23 16:10:37.255
STEP: Wait for the deployment to be ready 01/17/23 16:10:37.264
Jan 17 16:10:37.273: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/17/23 16:10:39.282
STEP: Verifying the service has paired with the endpoint 01/17/23 16:10:39.293
Jan 17 16:10:40.293: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
Jan 17 16:10:40.295: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Creating a v1 custom resource 01/17/23 16:10:42.874
STEP: v2 custom resource should be converted 01/17/23 16:10:42.879
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 17 16:10:43.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-8330" for this suite. 01/17/23 16:10:43.399
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","completed":322,"skipped":5942,"failed":0}
------------------------------
• [SLOW TEST] [6.638 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:10:36.844
    Jan 17 16:10:36.844: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename crd-webhook 01/17/23 16:10:36.844
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:10:36.873
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:10:36.876
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 01/17/23 16:10:36.881
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/17/23 16:10:37.246
    STEP: Deploying the custom resource conversion webhook pod 01/17/23 16:10:37.255
    STEP: Wait for the deployment to be ready 01/17/23 16:10:37.264
    Jan 17 16:10:37.273: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/17/23 16:10:39.282
    STEP: Verifying the service has paired with the endpoint 01/17/23 16:10:39.293
    Jan 17 16:10:40.293: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    Jan 17 16:10:40.295: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Creating a v1 custom resource 01/17/23 16:10:42.874
    STEP: v2 custom resource should be converted 01/17/23 16:10:42.879
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 17 16:10:43.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-webhook-8330" for this suite. 01/17/23 16:10:43.399
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:10:43.483
Jan 17 16:10:43.483: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename projected 01/17/23 16:10:43.483
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:10:43.55
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:10:43.564
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234
STEP: Creating a pod to test downward API volume plugin 01/17/23 16:10:43.567
Jan 17 16:10:43.647: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2d9af2ca-db75-4dd9-8966-b16ae928cbcf" in namespace "projected-3151" to be "Succeeded or Failed"
Jan 17 16:10:43.657: INFO: Pod "downwardapi-volume-2d9af2ca-db75-4dd9-8966-b16ae928cbcf": Phase="Pending", Reason="", readiness=false. Elapsed: 9.750497ms
Jan 17 16:10:45.661: INFO: Pod "downwardapi-volume-2d9af2ca-db75-4dd9-8966-b16ae928cbcf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014053239s
Jan 17 16:10:47.661: INFO: Pod "downwardapi-volume-2d9af2ca-db75-4dd9-8966-b16ae928cbcf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013671477s
STEP: Saw pod success 01/17/23 16:10:47.661
Jan 17 16:10:47.661: INFO: Pod "downwardapi-volume-2d9af2ca-db75-4dd9-8966-b16ae928cbcf" satisfied condition "Succeeded or Failed"
Jan 17 16:10:47.664: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod downwardapi-volume-2d9af2ca-db75-4dd9-8966-b16ae928cbcf container client-container: <nil>
STEP: delete the pod 01/17/23 16:10:47.676
Jan 17 16:10:47.688: INFO: Waiting for pod downwardapi-volume-2d9af2ca-db75-4dd9-8966-b16ae928cbcf to disappear
Jan 17 16:10:47.691: INFO: Pod downwardapi-volume-2d9af2ca-db75-4dd9-8966-b16ae928cbcf no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 17 16:10:47.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3151" for this suite. 01/17/23 16:10:47.696
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","completed":323,"skipped":5962,"failed":0}
------------------------------
• [4.219 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:10:43.483
    Jan 17 16:10:43.483: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename projected 01/17/23 16:10:43.483
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:10:43.55
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:10:43.564
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:234
    STEP: Creating a pod to test downward API volume plugin 01/17/23 16:10:43.567
    Jan 17 16:10:43.647: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2d9af2ca-db75-4dd9-8966-b16ae928cbcf" in namespace "projected-3151" to be "Succeeded or Failed"
    Jan 17 16:10:43.657: INFO: Pod "downwardapi-volume-2d9af2ca-db75-4dd9-8966-b16ae928cbcf": Phase="Pending", Reason="", readiness=false. Elapsed: 9.750497ms
    Jan 17 16:10:45.661: INFO: Pod "downwardapi-volume-2d9af2ca-db75-4dd9-8966-b16ae928cbcf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014053239s
    Jan 17 16:10:47.661: INFO: Pod "downwardapi-volume-2d9af2ca-db75-4dd9-8966-b16ae928cbcf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013671477s
    STEP: Saw pod success 01/17/23 16:10:47.661
    Jan 17 16:10:47.661: INFO: Pod "downwardapi-volume-2d9af2ca-db75-4dd9-8966-b16ae928cbcf" satisfied condition "Succeeded or Failed"
    Jan 17 16:10:47.664: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod downwardapi-volume-2d9af2ca-db75-4dd9-8966-b16ae928cbcf container client-container: <nil>
    STEP: delete the pod 01/17/23 16:10:47.676
    Jan 17 16:10:47.688: INFO: Waiting for pod downwardapi-volume-2d9af2ca-db75-4dd9-8966-b16ae928cbcf to disappear
    Jan 17 16:10:47.691: INFO: Pod downwardapi-volume-2d9af2ca-db75-4dd9-8966-b16ae928cbcf no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 17 16:10:47.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3151" for this suite. 01/17/23 16:10:47.696
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:10:47.702
Jan 17 16:10:47.702: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename container-probe 01/17/23 16:10:47.703
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:10:47.716
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:10:47.718
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68
Jan 17 16:10:48.742: INFO: Waiting up to 5m0s for pod "test-webserver-f3888b33-29fa-4255-b440-fe8f47fb7d9a" in namespace "container-probe-1571" to be "running and ready"
Jan 17 16:10:48.745: INFO: Pod "test-webserver-f3888b33-29fa-4255-b440-fe8f47fb7d9a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.632832ms
Jan 17 16:10:48.745: INFO: The phase of Pod test-webserver-f3888b33-29fa-4255-b440-fe8f47fb7d9a is Pending, waiting for it to be Running (with Ready = true)
Jan 17 16:10:50.749: INFO: Pod "test-webserver-f3888b33-29fa-4255-b440-fe8f47fb7d9a": Phase="Running", Reason="", readiness=false. Elapsed: 2.007197498s
Jan 17 16:10:50.749: INFO: The phase of Pod test-webserver-f3888b33-29fa-4255-b440-fe8f47fb7d9a is Running (Ready = false)
Jan 17 16:10:52.750: INFO: Pod "test-webserver-f3888b33-29fa-4255-b440-fe8f47fb7d9a": Phase="Running", Reason="", readiness=false. Elapsed: 4.007954357s
Jan 17 16:10:52.750: INFO: The phase of Pod test-webserver-f3888b33-29fa-4255-b440-fe8f47fb7d9a is Running (Ready = false)
Jan 17 16:10:54.750: INFO: Pod "test-webserver-f3888b33-29fa-4255-b440-fe8f47fb7d9a": Phase="Running", Reason="", readiness=false. Elapsed: 6.00795078s
Jan 17 16:10:54.750: INFO: The phase of Pod test-webserver-f3888b33-29fa-4255-b440-fe8f47fb7d9a is Running (Ready = false)
Jan 17 16:10:56.748: INFO: Pod "test-webserver-f3888b33-29fa-4255-b440-fe8f47fb7d9a": Phase="Running", Reason="", readiness=false. Elapsed: 8.006106826s
Jan 17 16:10:56.748: INFO: The phase of Pod test-webserver-f3888b33-29fa-4255-b440-fe8f47fb7d9a is Running (Ready = false)
Jan 17 16:10:58.748: INFO: Pod "test-webserver-f3888b33-29fa-4255-b440-fe8f47fb7d9a": Phase="Running", Reason="", readiness=false. Elapsed: 10.005933874s
Jan 17 16:10:58.748: INFO: The phase of Pod test-webserver-f3888b33-29fa-4255-b440-fe8f47fb7d9a is Running (Ready = false)
Jan 17 16:11:00.749: INFO: Pod "test-webserver-f3888b33-29fa-4255-b440-fe8f47fb7d9a": Phase="Running", Reason="", readiness=false. Elapsed: 12.007107623s
Jan 17 16:11:00.749: INFO: The phase of Pod test-webserver-f3888b33-29fa-4255-b440-fe8f47fb7d9a is Running (Ready = false)
Jan 17 16:11:02.749: INFO: Pod "test-webserver-f3888b33-29fa-4255-b440-fe8f47fb7d9a": Phase="Running", Reason="", readiness=false. Elapsed: 14.007340409s
Jan 17 16:11:02.749: INFO: The phase of Pod test-webserver-f3888b33-29fa-4255-b440-fe8f47fb7d9a is Running (Ready = false)
Jan 17 16:11:04.749: INFO: Pod "test-webserver-f3888b33-29fa-4255-b440-fe8f47fb7d9a": Phase="Running", Reason="", readiness=false. Elapsed: 16.006616095s
Jan 17 16:11:04.749: INFO: The phase of Pod test-webserver-f3888b33-29fa-4255-b440-fe8f47fb7d9a is Running (Ready = false)
Jan 17 16:11:06.749: INFO: Pod "test-webserver-f3888b33-29fa-4255-b440-fe8f47fb7d9a": Phase="Running", Reason="", readiness=false. Elapsed: 18.007186523s
Jan 17 16:11:06.749: INFO: The phase of Pod test-webserver-f3888b33-29fa-4255-b440-fe8f47fb7d9a is Running (Ready = false)
Jan 17 16:11:08.749: INFO: Pod "test-webserver-f3888b33-29fa-4255-b440-fe8f47fb7d9a": Phase="Running", Reason="", readiness=false. Elapsed: 20.007173098s
Jan 17 16:11:08.749: INFO: The phase of Pod test-webserver-f3888b33-29fa-4255-b440-fe8f47fb7d9a is Running (Ready = false)
Jan 17 16:11:10.750: INFO: Pod "test-webserver-f3888b33-29fa-4255-b440-fe8f47fb7d9a": Phase="Running", Reason="", readiness=true. Elapsed: 22.008557264s
Jan 17 16:11:10.751: INFO: The phase of Pod test-webserver-f3888b33-29fa-4255-b440-fe8f47fb7d9a is Running (Ready = true)
Jan 17 16:11:10.751: INFO: Pod "test-webserver-f3888b33-29fa-4255-b440-fe8f47fb7d9a" satisfied condition "running and ready"
Jan 17 16:11:10.753: INFO: Container started at 2023-01-17 16:10:49 +0000 UTC, pod became ready at 2023-01-17 16:11:09 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan 17 16:11:10.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1571" for this suite. 01/17/23 16:11:10.757
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","completed":324,"skipped":5965,"failed":0}
------------------------------
• [SLOW TEST] [23.060 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:10:47.702
    Jan 17 16:10:47.702: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename container-probe 01/17/23 16:10:47.703
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:10:47.716
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:10:47.718
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:68
    Jan 17 16:10:48.742: INFO: Waiting up to 5m0s for pod "test-webserver-f3888b33-29fa-4255-b440-fe8f47fb7d9a" in namespace "container-probe-1571" to be "running and ready"
    Jan 17 16:10:48.745: INFO: Pod "test-webserver-f3888b33-29fa-4255-b440-fe8f47fb7d9a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.632832ms
    Jan 17 16:10:48.745: INFO: The phase of Pod test-webserver-f3888b33-29fa-4255-b440-fe8f47fb7d9a is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 16:10:50.749: INFO: Pod "test-webserver-f3888b33-29fa-4255-b440-fe8f47fb7d9a": Phase="Running", Reason="", readiness=false. Elapsed: 2.007197498s
    Jan 17 16:10:50.749: INFO: The phase of Pod test-webserver-f3888b33-29fa-4255-b440-fe8f47fb7d9a is Running (Ready = false)
    Jan 17 16:10:52.750: INFO: Pod "test-webserver-f3888b33-29fa-4255-b440-fe8f47fb7d9a": Phase="Running", Reason="", readiness=false. Elapsed: 4.007954357s
    Jan 17 16:10:52.750: INFO: The phase of Pod test-webserver-f3888b33-29fa-4255-b440-fe8f47fb7d9a is Running (Ready = false)
    Jan 17 16:10:54.750: INFO: Pod "test-webserver-f3888b33-29fa-4255-b440-fe8f47fb7d9a": Phase="Running", Reason="", readiness=false. Elapsed: 6.00795078s
    Jan 17 16:10:54.750: INFO: The phase of Pod test-webserver-f3888b33-29fa-4255-b440-fe8f47fb7d9a is Running (Ready = false)
    Jan 17 16:10:56.748: INFO: Pod "test-webserver-f3888b33-29fa-4255-b440-fe8f47fb7d9a": Phase="Running", Reason="", readiness=false. Elapsed: 8.006106826s
    Jan 17 16:10:56.748: INFO: The phase of Pod test-webserver-f3888b33-29fa-4255-b440-fe8f47fb7d9a is Running (Ready = false)
    Jan 17 16:10:58.748: INFO: Pod "test-webserver-f3888b33-29fa-4255-b440-fe8f47fb7d9a": Phase="Running", Reason="", readiness=false. Elapsed: 10.005933874s
    Jan 17 16:10:58.748: INFO: The phase of Pod test-webserver-f3888b33-29fa-4255-b440-fe8f47fb7d9a is Running (Ready = false)
    Jan 17 16:11:00.749: INFO: Pod "test-webserver-f3888b33-29fa-4255-b440-fe8f47fb7d9a": Phase="Running", Reason="", readiness=false. Elapsed: 12.007107623s
    Jan 17 16:11:00.749: INFO: The phase of Pod test-webserver-f3888b33-29fa-4255-b440-fe8f47fb7d9a is Running (Ready = false)
    Jan 17 16:11:02.749: INFO: Pod "test-webserver-f3888b33-29fa-4255-b440-fe8f47fb7d9a": Phase="Running", Reason="", readiness=false. Elapsed: 14.007340409s
    Jan 17 16:11:02.749: INFO: The phase of Pod test-webserver-f3888b33-29fa-4255-b440-fe8f47fb7d9a is Running (Ready = false)
    Jan 17 16:11:04.749: INFO: Pod "test-webserver-f3888b33-29fa-4255-b440-fe8f47fb7d9a": Phase="Running", Reason="", readiness=false. Elapsed: 16.006616095s
    Jan 17 16:11:04.749: INFO: The phase of Pod test-webserver-f3888b33-29fa-4255-b440-fe8f47fb7d9a is Running (Ready = false)
    Jan 17 16:11:06.749: INFO: Pod "test-webserver-f3888b33-29fa-4255-b440-fe8f47fb7d9a": Phase="Running", Reason="", readiness=false. Elapsed: 18.007186523s
    Jan 17 16:11:06.749: INFO: The phase of Pod test-webserver-f3888b33-29fa-4255-b440-fe8f47fb7d9a is Running (Ready = false)
    Jan 17 16:11:08.749: INFO: Pod "test-webserver-f3888b33-29fa-4255-b440-fe8f47fb7d9a": Phase="Running", Reason="", readiness=false. Elapsed: 20.007173098s
    Jan 17 16:11:08.749: INFO: The phase of Pod test-webserver-f3888b33-29fa-4255-b440-fe8f47fb7d9a is Running (Ready = false)
    Jan 17 16:11:10.750: INFO: Pod "test-webserver-f3888b33-29fa-4255-b440-fe8f47fb7d9a": Phase="Running", Reason="", readiness=true. Elapsed: 22.008557264s
    Jan 17 16:11:10.751: INFO: The phase of Pod test-webserver-f3888b33-29fa-4255-b440-fe8f47fb7d9a is Running (Ready = true)
    Jan 17 16:11:10.751: INFO: Pod "test-webserver-f3888b33-29fa-4255-b440-fe8f47fb7d9a" satisfied condition "running and ready"
    Jan 17 16:11:10.753: INFO: Container started at 2023-01-17 16:10:49 +0000 UTC, pod became ready at 2023-01-17 16:11:09 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan 17 16:11:10.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-1571" for this suite. 01/17/23 16:11:10.757
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:11:10.763
Jan 17 16:11:10.763: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename emptydir-wrapper 01/17/23 16:11:10.764
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:11:10.79
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:11:10.793
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
Jan 17 16:11:10.861: INFO: Waiting up to 5m0s for pod "pod-secrets-dd196038-1e76-4199-9510-4c369802398e" in namespace "emptydir-wrapper-984" to be "running and ready"
Jan 17 16:11:10.865: INFO: Pod "pod-secrets-dd196038-1e76-4199-9510-4c369802398e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.638383ms
Jan 17 16:11:10.865: INFO: The phase of Pod pod-secrets-dd196038-1e76-4199-9510-4c369802398e is Pending, waiting for it to be Running (with Ready = true)
Jan 17 16:11:12.870: INFO: Pod "pod-secrets-dd196038-1e76-4199-9510-4c369802398e": Phase="Running", Reason="", readiness=true. Elapsed: 2.008950306s
Jan 17 16:11:12.870: INFO: The phase of Pod pod-secrets-dd196038-1e76-4199-9510-4c369802398e is Running (Ready = true)
Jan 17 16:11:12.870: INFO: Pod "pod-secrets-dd196038-1e76-4199-9510-4c369802398e" satisfied condition "running and ready"
STEP: Cleaning up the secret 01/17/23 16:11:12.873
STEP: Cleaning up the configmap 01/17/23 16:11:12.878
STEP: Cleaning up the pod 01/17/23 16:11:12.883
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
Jan 17 16:11:12.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-984" for this suite. 01/17/23 16:11:12.899
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","completed":325,"skipped":5974,"failed":0}
------------------------------
• [2.145 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:11:10.763
    Jan 17 16:11:10.763: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename emptydir-wrapper 01/17/23 16:11:10.764
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:11:10.79
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:11:10.793
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    Jan 17 16:11:10.861: INFO: Waiting up to 5m0s for pod "pod-secrets-dd196038-1e76-4199-9510-4c369802398e" in namespace "emptydir-wrapper-984" to be "running and ready"
    Jan 17 16:11:10.865: INFO: Pod "pod-secrets-dd196038-1e76-4199-9510-4c369802398e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.638383ms
    Jan 17 16:11:10.865: INFO: The phase of Pod pod-secrets-dd196038-1e76-4199-9510-4c369802398e is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 16:11:12.870: INFO: Pod "pod-secrets-dd196038-1e76-4199-9510-4c369802398e": Phase="Running", Reason="", readiness=true. Elapsed: 2.008950306s
    Jan 17 16:11:12.870: INFO: The phase of Pod pod-secrets-dd196038-1e76-4199-9510-4c369802398e is Running (Ready = true)
    Jan 17 16:11:12.870: INFO: Pod "pod-secrets-dd196038-1e76-4199-9510-4c369802398e" satisfied condition "running and ready"
    STEP: Cleaning up the secret 01/17/23 16:11:12.873
    STEP: Cleaning up the configmap 01/17/23 16:11:12.878
    STEP: Cleaning up the pod 01/17/23 16:11:12.883
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:187
    Jan 17 16:11:12.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-wrapper-984" for this suite. 01/17/23 16:11:12.899
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:11:12.909
Jan 17 16:11:12.909: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename discovery 01/17/23 16:11:12.91
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:11:12.938
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:11:12.942
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 01/17/23 16:11:12.947
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
Jan 17 16:11:13.244: INFO: Checking APIGroup: apiregistration.k8s.io
Jan 17 16:11:13.245: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Jan 17 16:11:13.245: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Jan 17 16:11:13.245: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Jan 17 16:11:13.245: INFO: Checking APIGroup: apps
Jan 17 16:11:13.246: INFO: PreferredVersion.GroupVersion: apps/v1
Jan 17 16:11:13.246: INFO: Versions found [{apps/v1 v1}]
Jan 17 16:11:13.246: INFO: apps/v1 matches apps/v1
Jan 17 16:11:13.246: INFO: Checking APIGroup: events.k8s.io
Jan 17 16:11:13.247: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Jan 17 16:11:13.247: INFO: Versions found [{events.k8s.io/v1 v1}]
Jan 17 16:11:13.247: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Jan 17 16:11:13.247: INFO: Checking APIGroup: authentication.k8s.io
Jan 17 16:11:13.247: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Jan 17 16:11:13.247: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Jan 17 16:11:13.247: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Jan 17 16:11:13.247: INFO: Checking APIGroup: authorization.k8s.io
Jan 17 16:11:13.248: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Jan 17 16:11:13.248: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Jan 17 16:11:13.248: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Jan 17 16:11:13.248: INFO: Checking APIGroup: autoscaling
Jan 17 16:11:13.249: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Jan 17 16:11:13.249: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta2 v2beta2}]
Jan 17 16:11:13.249: INFO: autoscaling/v2 matches autoscaling/v2
Jan 17 16:11:13.249: INFO: Checking APIGroup: batch
Jan 17 16:11:13.250: INFO: PreferredVersion.GroupVersion: batch/v1
Jan 17 16:11:13.250: INFO: Versions found [{batch/v1 v1}]
Jan 17 16:11:13.250: INFO: batch/v1 matches batch/v1
Jan 17 16:11:13.250: INFO: Checking APIGroup: certificates.k8s.io
Jan 17 16:11:13.251: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Jan 17 16:11:13.251: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Jan 17 16:11:13.251: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Jan 17 16:11:13.251: INFO: Checking APIGroup: networking.k8s.io
Jan 17 16:11:13.251: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Jan 17 16:11:13.251: INFO: Versions found [{networking.k8s.io/v1 v1}]
Jan 17 16:11:13.251: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Jan 17 16:11:13.251: INFO: Checking APIGroup: policy
Jan 17 16:11:13.252: INFO: PreferredVersion.GroupVersion: policy/v1
Jan 17 16:11:13.252: INFO: Versions found [{policy/v1 v1}]
Jan 17 16:11:13.252: INFO: policy/v1 matches policy/v1
Jan 17 16:11:13.252: INFO: Checking APIGroup: rbac.authorization.k8s.io
Jan 17 16:11:13.253: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Jan 17 16:11:13.253: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Jan 17 16:11:13.253: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Jan 17 16:11:13.253: INFO: Checking APIGroup: storage.k8s.io
Jan 17 16:11:13.254: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Jan 17 16:11:13.254: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Jan 17 16:11:13.254: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Jan 17 16:11:13.254: INFO: Checking APIGroup: admissionregistration.k8s.io
Jan 17 16:11:13.254: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Jan 17 16:11:13.254: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Jan 17 16:11:13.255: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Jan 17 16:11:13.255: INFO: Checking APIGroup: apiextensions.k8s.io
Jan 17 16:11:13.255: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Jan 17 16:11:13.255: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Jan 17 16:11:13.255: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Jan 17 16:11:13.255: INFO: Checking APIGroup: scheduling.k8s.io
Jan 17 16:11:13.256: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Jan 17 16:11:13.256: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Jan 17 16:11:13.256: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Jan 17 16:11:13.256: INFO: Checking APIGroup: coordination.k8s.io
Jan 17 16:11:13.257: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Jan 17 16:11:13.257: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Jan 17 16:11:13.257: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Jan 17 16:11:13.257: INFO: Checking APIGroup: node.k8s.io
Jan 17 16:11:13.258: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Jan 17 16:11:13.258: INFO: Versions found [{node.k8s.io/v1 v1}]
Jan 17 16:11:13.258: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Jan 17 16:11:13.258: INFO: Checking APIGroup: discovery.k8s.io
Jan 17 16:11:13.258: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Jan 17 16:11:13.258: INFO: Versions found [{discovery.k8s.io/v1 v1}]
Jan 17 16:11:13.258: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Jan 17 16:11:13.258: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Jan 17 16:11:13.259: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
Jan 17 16:11:13.259: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Jan 17 16:11:13.259: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
Jan 17 16:11:13.259: INFO: Checking APIGroup: apps.openshift.io
Jan 17 16:11:13.260: INFO: PreferredVersion.GroupVersion: apps.openshift.io/v1
Jan 17 16:11:13.260: INFO: Versions found [{apps.openshift.io/v1 v1}]
Jan 17 16:11:13.260: INFO: apps.openshift.io/v1 matches apps.openshift.io/v1
Jan 17 16:11:13.260: INFO: Checking APIGroup: authorization.openshift.io
Jan 17 16:11:13.261: INFO: PreferredVersion.GroupVersion: authorization.openshift.io/v1
Jan 17 16:11:13.261: INFO: Versions found [{authorization.openshift.io/v1 v1}]
Jan 17 16:11:13.261: INFO: authorization.openshift.io/v1 matches authorization.openshift.io/v1
Jan 17 16:11:13.261: INFO: Checking APIGroup: build.openshift.io
Jan 17 16:11:13.261: INFO: PreferredVersion.GroupVersion: build.openshift.io/v1
Jan 17 16:11:13.261: INFO: Versions found [{build.openshift.io/v1 v1}]
Jan 17 16:11:13.261: INFO: build.openshift.io/v1 matches build.openshift.io/v1
Jan 17 16:11:13.261: INFO: Checking APIGroup: image.openshift.io
Jan 17 16:11:13.262: INFO: PreferredVersion.GroupVersion: image.openshift.io/v1
Jan 17 16:11:13.262: INFO: Versions found [{image.openshift.io/v1 v1}]
Jan 17 16:11:13.262: INFO: image.openshift.io/v1 matches image.openshift.io/v1
Jan 17 16:11:13.262: INFO: Checking APIGroup: oauth.openshift.io
Jan 17 16:11:13.263: INFO: PreferredVersion.GroupVersion: oauth.openshift.io/v1
Jan 17 16:11:13.263: INFO: Versions found [{oauth.openshift.io/v1 v1}]
Jan 17 16:11:13.263: INFO: oauth.openshift.io/v1 matches oauth.openshift.io/v1
Jan 17 16:11:13.263: INFO: Checking APIGroup: project.openshift.io
Jan 17 16:11:13.264: INFO: PreferredVersion.GroupVersion: project.openshift.io/v1
Jan 17 16:11:13.264: INFO: Versions found [{project.openshift.io/v1 v1}]
Jan 17 16:11:13.264: INFO: project.openshift.io/v1 matches project.openshift.io/v1
Jan 17 16:11:13.264: INFO: Checking APIGroup: quota.openshift.io
Jan 17 16:11:13.264: INFO: PreferredVersion.GroupVersion: quota.openshift.io/v1
Jan 17 16:11:13.264: INFO: Versions found [{quota.openshift.io/v1 v1}]
Jan 17 16:11:13.264: INFO: quota.openshift.io/v1 matches quota.openshift.io/v1
Jan 17 16:11:13.264: INFO: Checking APIGroup: route.openshift.io
Jan 17 16:11:13.265: INFO: PreferredVersion.GroupVersion: route.openshift.io/v1
Jan 17 16:11:13.265: INFO: Versions found [{route.openshift.io/v1 v1}]
Jan 17 16:11:13.265: INFO: route.openshift.io/v1 matches route.openshift.io/v1
Jan 17 16:11:13.265: INFO: Checking APIGroup: security.openshift.io
Jan 17 16:11:13.266: INFO: PreferredVersion.GroupVersion: security.openshift.io/v1
Jan 17 16:11:13.266: INFO: Versions found [{security.openshift.io/v1 v1}]
Jan 17 16:11:13.266: INFO: security.openshift.io/v1 matches security.openshift.io/v1
Jan 17 16:11:13.266: INFO: Checking APIGroup: template.openshift.io
Jan 17 16:11:13.267: INFO: PreferredVersion.GroupVersion: template.openshift.io/v1
Jan 17 16:11:13.267: INFO: Versions found [{template.openshift.io/v1 v1}]
Jan 17 16:11:13.267: INFO: template.openshift.io/v1 matches template.openshift.io/v1
Jan 17 16:11:13.267: INFO: Checking APIGroup: user.openshift.io
Jan 17 16:11:13.267: INFO: PreferredVersion.GroupVersion: user.openshift.io/v1
Jan 17 16:11:13.267: INFO: Versions found [{user.openshift.io/v1 v1}]
Jan 17 16:11:13.267: INFO: user.openshift.io/v1 matches user.openshift.io/v1
Jan 17 16:11:13.267: INFO: Checking APIGroup: packages.operators.coreos.com
Jan 17 16:11:13.268: INFO: PreferredVersion.GroupVersion: packages.operators.coreos.com/v1
Jan 17 16:11:13.268: INFO: Versions found [{packages.operators.coreos.com/v1 v1}]
Jan 17 16:11:13.268: INFO: packages.operators.coreos.com/v1 matches packages.operators.coreos.com/v1
Jan 17 16:11:13.268: INFO: Checking APIGroup: config.openshift.io
Jan 17 16:11:13.269: INFO: PreferredVersion.GroupVersion: config.openshift.io/v1
Jan 17 16:11:13.269: INFO: Versions found [{config.openshift.io/v1 v1}]
Jan 17 16:11:13.269: INFO: config.openshift.io/v1 matches config.openshift.io/v1
Jan 17 16:11:13.269: INFO: Checking APIGroup: operator.openshift.io
Jan 17 16:11:13.270: INFO: PreferredVersion.GroupVersion: operator.openshift.io/v1
Jan 17 16:11:13.270: INFO: Versions found [{operator.openshift.io/v1 v1} {operator.openshift.io/v1alpha1 v1alpha1}]
Jan 17 16:11:13.270: INFO: operator.openshift.io/v1 matches operator.openshift.io/v1
Jan 17 16:11:13.270: INFO: Checking APIGroup: apiserver.openshift.io
Jan 17 16:11:13.270: INFO: PreferredVersion.GroupVersion: apiserver.openshift.io/v1
Jan 17 16:11:13.270: INFO: Versions found [{apiserver.openshift.io/v1 v1}]
Jan 17 16:11:13.270: INFO: apiserver.openshift.io/v1 matches apiserver.openshift.io/v1
Jan 17 16:11:13.270: INFO: Checking APIGroup: autoscaling.openshift.io
Jan 17 16:11:13.271: INFO: PreferredVersion.GroupVersion: autoscaling.openshift.io/v1
Jan 17 16:11:13.271: INFO: Versions found [{autoscaling.openshift.io/v1 v1} {autoscaling.openshift.io/v1beta1 v1beta1}]
Jan 17 16:11:13.271: INFO: autoscaling.openshift.io/v1 matches autoscaling.openshift.io/v1
Jan 17 16:11:13.271: INFO: Checking APIGroup: cloud.network.openshift.io
Jan 17 16:11:13.272: INFO: PreferredVersion.GroupVersion: cloud.network.openshift.io/v1
Jan 17 16:11:13.272: INFO: Versions found [{cloud.network.openshift.io/v1 v1}]
Jan 17 16:11:13.272: INFO: cloud.network.openshift.io/v1 matches cloud.network.openshift.io/v1
Jan 17 16:11:13.272: INFO: Checking APIGroup: cloudcredential.openshift.io
Jan 17 16:11:13.273: INFO: PreferredVersion.GroupVersion: cloudcredential.openshift.io/v1
Jan 17 16:11:13.273: INFO: Versions found [{cloudcredential.openshift.io/v1 v1}]
Jan 17 16:11:13.273: INFO: cloudcredential.openshift.io/v1 matches cloudcredential.openshift.io/v1
Jan 17 16:11:13.273: INFO: Checking APIGroup: console.openshift.io
Jan 17 16:11:13.274: INFO: PreferredVersion.GroupVersion: console.openshift.io/v1
Jan 17 16:11:13.274: INFO: Versions found [{console.openshift.io/v1 v1} {console.openshift.io/v1alpha1 v1alpha1}]
Jan 17 16:11:13.274: INFO: console.openshift.io/v1 matches console.openshift.io/v1
Jan 17 16:11:13.274: INFO: Checking APIGroup: imageregistry.operator.openshift.io
Jan 17 16:11:13.274: INFO: PreferredVersion.GroupVersion: imageregistry.operator.openshift.io/v1
Jan 17 16:11:13.274: INFO: Versions found [{imageregistry.operator.openshift.io/v1 v1}]
Jan 17 16:11:13.274: INFO: imageregistry.operator.openshift.io/v1 matches imageregistry.operator.openshift.io/v1
Jan 17 16:11:13.274: INFO: Checking APIGroup: ingress.operator.openshift.io
Jan 17 16:11:13.275: INFO: PreferredVersion.GroupVersion: ingress.operator.openshift.io/v1
Jan 17 16:11:13.275: INFO: Versions found [{ingress.operator.openshift.io/v1 v1}]
Jan 17 16:11:13.275: INFO: ingress.operator.openshift.io/v1 matches ingress.operator.openshift.io/v1
Jan 17 16:11:13.275: INFO: Checking APIGroup: k8s.cni.cncf.io
Jan 17 16:11:13.276: INFO: PreferredVersion.GroupVersion: k8s.cni.cncf.io/v1
Jan 17 16:11:13.276: INFO: Versions found [{k8s.cni.cncf.io/v1 v1}]
Jan 17 16:11:13.276: INFO: k8s.cni.cncf.io/v1 matches k8s.cni.cncf.io/v1
Jan 17 16:11:13.276: INFO: Checking APIGroup: k8s.ovn.org
Jan 17 16:11:13.277: INFO: PreferredVersion.GroupVersion: k8s.ovn.org/v1
Jan 17 16:11:13.277: INFO: Versions found [{k8s.ovn.org/v1 v1}]
Jan 17 16:11:13.277: INFO: k8s.ovn.org/v1 matches k8s.ovn.org/v1
Jan 17 16:11:13.277: INFO: Checking APIGroup: machine.openshift.io
Jan 17 16:11:13.277: INFO: PreferredVersion.GroupVersion: machine.openshift.io/v1
Jan 17 16:11:13.277: INFO: Versions found [{machine.openshift.io/v1 v1} {machine.openshift.io/v1beta1 v1beta1}]
Jan 17 16:11:13.277: INFO: machine.openshift.io/v1 matches machine.openshift.io/v1
Jan 17 16:11:13.277: INFO: Checking APIGroup: machineconfiguration.openshift.io
Jan 17 16:11:13.278: INFO: PreferredVersion.GroupVersion: machineconfiguration.openshift.io/v1
Jan 17 16:11:13.278: INFO: Versions found [{machineconfiguration.openshift.io/v1 v1}]
Jan 17 16:11:13.278: INFO: machineconfiguration.openshift.io/v1 matches machineconfiguration.openshift.io/v1
Jan 17 16:11:13.278: INFO: Checking APIGroup: monitoring.coreos.com
Jan 17 16:11:13.279: INFO: PreferredVersion.GroupVersion: monitoring.coreos.com/v1
Jan 17 16:11:13.279: INFO: Versions found [{monitoring.coreos.com/v1 v1} {monitoring.coreos.com/v1beta1 v1beta1} {monitoring.coreos.com/v1alpha1 v1alpha1}]
Jan 17 16:11:13.279: INFO: monitoring.coreos.com/v1 matches monitoring.coreos.com/v1
Jan 17 16:11:13.279: INFO: Checking APIGroup: network.operator.openshift.io
Jan 17 16:11:13.280: INFO: PreferredVersion.GroupVersion: network.operator.openshift.io/v1
Jan 17 16:11:13.280: INFO: Versions found [{network.operator.openshift.io/v1 v1}]
Jan 17 16:11:13.280: INFO: network.operator.openshift.io/v1 matches network.operator.openshift.io/v1
Jan 17 16:11:13.280: INFO: Checking APIGroup: operators.coreos.com
Jan 17 16:11:13.281: INFO: PreferredVersion.GroupVersion: operators.coreos.com/v2
Jan 17 16:11:13.281: INFO: Versions found [{operators.coreos.com/v2 v2} {operators.coreos.com/v1 v1} {operators.coreos.com/v1alpha2 v1alpha2} {operators.coreos.com/v1alpha1 v1alpha1}]
Jan 17 16:11:13.281: INFO: operators.coreos.com/v2 matches operators.coreos.com/v2
Jan 17 16:11:13.281: INFO: Checking APIGroup: performance.openshift.io
Jan 17 16:11:13.281: INFO: PreferredVersion.GroupVersion: performance.openshift.io/v2
Jan 17 16:11:13.281: INFO: Versions found [{performance.openshift.io/v2 v2} {performance.openshift.io/v1 v1} {performance.openshift.io/v1alpha1 v1alpha1}]
Jan 17 16:11:13.281: INFO: performance.openshift.io/v2 matches performance.openshift.io/v2
Jan 17 16:11:13.281: INFO: Checking APIGroup: samples.operator.openshift.io
Jan 17 16:11:13.282: INFO: PreferredVersion.GroupVersion: samples.operator.openshift.io/v1
Jan 17 16:11:13.282: INFO: Versions found [{samples.operator.openshift.io/v1 v1}]
Jan 17 16:11:13.282: INFO: samples.operator.openshift.io/v1 matches samples.operator.openshift.io/v1
Jan 17 16:11:13.282: INFO: Checking APIGroup: security.internal.openshift.io
Jan 17 16:11:13.294: INFO: PreferredVersion.GroupVersion: security.internal.openshift.io/v1
Jan 17 16:11:13.294: INFO: Versions found [{security.internal.openshift.io/v1 v1}]
Jan 17 16:11:13.294: INFO: security.internal.openshift.io/v1 matches security.internal.openshift.io/v1
Jan 17 16:11:13.294: INFO: Checking APIGroup: snapshot.storage.k8s.io
Jan 17 16:11:13.344: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
Jan 17 16:11:13.344: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1}]
Jan 17 16:11:13.344: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
Jan 17 16:11:13.344: INFO: Checking APIGroup: tuned.openshift.io
Jan 17 16:11:13.394: INFO: PreferredVersion.GroupVersion: tuned.openshift.io/v1
Jan 17 16:11:13.394: INFO: Versions found [{tuned.openshift.io/v1 v1}]
Jan 17 16:11:13.394: INFO: tuned.openshift.io/v1 matches tuned.openshift.io/v1
Jan 17 16:11:13.394: INFO: Checking APIGroup: controlplane.operator.openshift.io
Jan 17 16:11:13.445: INFO: PreferredVersion.GroupVersion: controlplane.operator.openshift.io/v1alpha1
Jan 17 16:11:13.445: INFO: Versions found [{controlplane.operator.openshift.io/v1alpha1 v1alpha1}]
Jan 17 16:11:13.445: INFO: controlplane.operator.openshift.io/v1alpha1 matches controlplane.operator.openshift.io/v1alpha1
Jan 17 16:11:13.445: INFO: Checking APIGroup: metal3.io
Jan 17 16:11:13.494: INFO: PreferredVersion.GroupVersion: metal3.io/v1alpha1
Jan 17 16:11:13.494: INFO: Versions found [{metal3.io/v1alpha1 v1alpha1}]
Jan 17 16:11:13.494: INFO: metal3.io/v1alpha1 matches metal3.io/v1alpha1
Jan 17 16:11:13.494: INFO: Checking APIGroup: migration.k8s.io
Jan 17 16:11:13.545: INFO: PreferredVersion.GroupVersion: migration.k8s.io/v1alpha1
Jan 17 16:11:13.545: INFO: Versions found [{migration.k8s.io/v1alpha1 v1alpha1}]
Jan 17 16:11:13.545: INFO: migration.k8s.io/v1alpha1 matches migration.k8s.io/v1alpha1
Jan 17 16:11:13.545: INFO: Checking APIGroup: whereabouts.cni.cncf.io
Jan 17 16:11:13.594: INFO: PreferredVersion.GroupVersion: whereabouts.cni.cncf.io/v1alpha1
Jan 17 16:11:13.594: INFO: Versions found [{whereabouts.cni.cncf.io/v1alpha1 v1alpha1}]
Jan 17 16:11:13.594: INFO: whereabouts.cni.cncf.io/v1alpha1 matches whereabouts.cni.cncf.io/v1alpha1
Jan 17 16:11:13.594: INFO: Checking APIGroup: helm.openshift.io
Jan 17 16:11:13.645: INFO: PreferredVersion.GroupVersion: helm.openshift.io/v1beta1
Jan 17 16:11:13.645: INFO: Versions found [{helm.openshift.io/v1beta1 v1beta1}]
Jan 17 16:11:13.645: INFO: helm.openshift.io/v1beta1 matches helm.openshift.io/v1beta1
Jan 17 16:11:13.645: INFO: Checking APIGroup: metrics.k8s.io
Jan 17 16:11:13.694: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Jan 17 16:11:13.694: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Jan 17 16:11:13.694: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:187
Jan 17 16:11:13.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-4457" for this suite. 01/17/23 16:11:13.748
{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","completed":326,"skipped":6022,"failed":0}
------------------------------
• [0.891 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:11:12.909
    Jan 17 16:11:12.909: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename discovery 01/17/23 16:11:12.91
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:11:12.938
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:11:12.942
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 01/17/23 16:11:12.947
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    Jan 17 16:11:13.244: INFO: Checking APIGroup: apiregistration.k8s.io
    Jan 17 16:11:13.245: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    Jan 17 16:11:13.245: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    Jan 17 16:11:13.245: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    Jan 17 16:11:13.245: INFO: Checking APIGroup: apps
    Jan 17 16:11:13.246: INFO: PreferredVersion.GroupVersion: apps/v1
    Jan 17 16:11:13.246: INFO: Versions found [{apps/v1 v1}]
    Jan 17 16:11:13.246: INFO: apps/v1 matches apps/v1
    Jan 17 16:11:13.246: INFO: Checking APIGroup: events.k8s.io
    Jan 17 16:11:13.247: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    Jan 17 16:11:13.247: INFO: Versions found [{events.k8s.io/v1 v1}]
    Jan 17 16:11:13.247: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    Jan 17 16:11:13.247: INFO: Checking APIGroup: authentication.k8s.io
    Jan 17 16:11:13.247: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    Jan 17 16:11:13.247: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    Jan 17 16:11:13.247: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    Jan 17 16:11:13.247: INFO: Checking APIGroup: authorization.k8s.io
    Jan 17 16:11:13.248: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    Jan 17 16:11:13.248: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    Jan 17 16:11:13.248: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    Jan 17 16:11:13.248: INFO: Checking APIGroup: autoscaling
    Jan 17 16:11:13.249: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    Jan 17 16:11:13.249: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta2 v2beta2}]
    Jan 17 16:11:13.249: INFO: autoscaling/v2 matches autoscaling/v2
    Jan 17 16:11:13.249: INFO: Checking APIGroup: batch
    Jan 17 16:11:13.250: INFO: PreferredVersion.GroupVersion: batch/v1
    Jan 17 16:11:13.250: INFO: Versions found [{batch/v1 v1}]
    Jan 17 16:11:13.250: INFO: batch/v1 matches batch/v1
    Jan 17 16:11:13.250: INFO: Checking APIGroup: certificates.k8s.io
    Jan 17 16:11:13.251: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    Jan 17 16:11:13.251: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    Jan 17 16:11:13.251: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    Jan 17 16:11:13.251: INFO: Checking APIGroup: networking.k8s.io
    Jan 17 16:11:13.251: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    Jan 17 16:11:13.251: INFO: Versions found [{networking.k8s.io/v1 v1}]
    Jan 17 16:11:13.251: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    Jan 17 16:11:13.251: INFO: Checking APIGroup: policy
    Jan 17 16:11:13.252: INFO: PreferredVersion.GroupVersion: policy/v1
    Jan 17 16:11:13.252: INFO: Versions found [{policy/v1 v1}]
    Jan 17 16:11:13.252: INFO: policy/v1 matches policy/v1
    Jan 17 16:11:13.252: INFO: Checking APIGroup: rbac.authorization.k8s.io
    Jan 17 16:11:13.253: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    Jan 17 16:11:13.253: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    Jan 17 16:11:13.253: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    Jan 17 16:11:13.253: INFO: Checking APIGroup: storage.k8s.io
    Jan 17 16:11:13.254: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    Jan 17 16:11:13.254: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    Jan 17 16:11:13.254: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    Jan 17 16:11:13.254: INFO: Checking APIGroup: admissionregistration.k8s.io
    Jan 17 16:11:13.254: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    Jan 17 16:11:13.254: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    Jan 17 16:11:13.255: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    Jan 17 16:11:13.255: INFO: Checking APIGroup: apiextensions.k8s.io
    Jan 17 16:11:13.255: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    Jan 17 16:11:13.255: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    Jan 17 16:11:13.255: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    Jan 17 16:11:13.255: INFO: Checking APIGroup: scheduling.k8s.io
    Jan 17 16:11:13.256: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    Jan 17 16:11:13.256: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    Jan 17 16:11:13.256: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    Jan 17 16:11:13.256: INFO: Checking APIGroup: coordination.k8s.io
    Jan 17 16:11:13.257: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    Jan 17 16:11:13.257: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    Jan 17 16:11:13.257: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    Jan 17 16:11:13.257: INFO: Checking APIGroup: node.k8s.io
    Jan 17 16:11:13.258: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    Jan 17 16:11:13.258: INFO: Versions found [{node.k8s.io/v1 v1}]
    Jan 17 16:11:13.258: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    Jan 17 16:11:13.258: INFO: Checking APIGroup: discovery.k8s.io
    Jan 17 16:11:13.258: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    Jan 17 16:11:13.258: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    Jan 17 16:11:13.258: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    Jan 17 16:11:13.258: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    Jan 17 16:11:13.259: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
    Jan 17 16:11:13.259: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
    Jan 17 16:11:13.259: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
    Jan 17 16:11:13.259: INFO: Checking APIGroup: apps.openshift.io
    Jan 17 16:11:13.260: INFO: PreferredVersion.GroupVersion: apps.openshift.io/v1
    Jan 17 16:11:13.260: INFO: Versions found [{apps.openshift.io/v1 v1}]
    Jan 17 16:11:13.260: INFO: apps.openshift.io/v1 matches apps.openshift.io/v1
    Jan 17 16:11:13.260: INFO: Checking APIGroup: authorization.openshift.io
    Jan 17 16:11:13.261: INFO: PreferredVersion.GroupVersion: authorization.openshift.io/v1
    Jan 17 16:11:13.261: INFO: Versions found [{authorization.openshift.io/v1 v1}]
    Jan 17 16:11:13.261: INFO: authorization.openshift.io/v1 matches authorization.openshift.io/v1
    Jan 17 16:11:13.261: INFO: Checking APIGroup: build.openshift.io
    Jan 17 16:11:13.261: INFO: PreferredVersion.GroupVersion: build.openshift.io/v1
    Jan 17 16:11:13.261: INFO: Versions found [{build.openshift.io/v1 v1}]
    Jan 17 16:11:13.261: INFO: build.openshift.io/v1 matches build.openshift.io/v1
    Jan 17 16:11:13.261: INFO: Checking APIGroup: image.openshift.io
    Jan 17 16:11:13.262: INFO: PreferredVersion.GroupVersion: image.openshift.io/v1
    Jan 17 16:11:13.262: INFO: Versions found [{image.openshift.io/v1 v1}]
    Jan 17 16:11:13.262: INFO: image.openshift.io/v1 matches image.openshift.io/v1
    Jan 17 16:11:13.262: INFO: Checking APIGroup: oauth.openshift.io
    Jan 17 16:11:13.263: INFO: PreferredVersion.GroupVersion: oauth.openshift.io/v1
    Jan 17 16:11:13.263: INFO: Versions found [{oauth.openshift.io/v1 v1}]
    Jan 17 16:11:13.263: INFO: oauth.openshift.io/v1 matches oauth.openshift.io/v1
    Jan 17 16:11:13.263: INFO: Checking APIGroup: project.openshift.io
    Jan 17 16:11:13.264: INFO: PreferredVersion.GroupVersion: project.openshift.io/v1
    Jan 17 16:11:13.264: INFO: Versions found [{project.openshift.io/v1 v1}]
    Jan 17 16:11:13.264: INFO: project.openshift.io/v1 matches project.openshift.io/v1
    Jan 17 16:11:13.264: INFO: Checking APIGroup: quota.openshift.io
    Jan 17 16:11:13.264: INFO: PreferredVersion.GroupVersion: quota.openshift.io/v1
    Jan 17 16:11:13.264: INFO: Versions found [{quota.openshift.io/v1 v1}]
    Jan 17 16:11:13.264: INFO: quota.openshift.io/v1 matches quota.openshift.io/v1
    Jan 17 16:11:13.264: INFO: Checking APIGroup: route.openshift.io
    Jan 17 16:11:13.265: INFO: PreferredVersion.GroupVersion: route.openshift.io/v1
    Jan 17 16:11:13.265: INFO: Versions found [{route.openshift.io/v1 v1}]
    Jan 17 16:11:13.265: INFO: route.openshift.io/v1 matches route.openshift.io/v1
    Jan 17 16:11:13.265: INFO: Checking APIGroup: security.openshift.io
    Jan 17 16:11:13.266: INFO: PreferredVersion.GroupVersion: security.openshift.io/v1
    Jan 17 16:11:13.266: INFO: Versions found [{security.openshift.io/v1 v1}]
    Jan 17 16:11:13.266: INFO: security.openshift.io/v1 matches security.openshift.io/v1
    Jan 17 16:11:13.266: INFO: Checking APIGroup: template.openshift.io
    Jan 17 16:11:13.267: INFO: PreferredVersion.GroupVersion: template.openshift.io/v1
    Jan 17 16:11:13.267: INFO: Versions found [{template.openshift.io/v1 v1}]
    Jan 17 16:11:13.267: INFO: template.openshift.io/v1 matches template.openshift.io/v1
    Jan 17 16:11:13.267: INFO: Checking APIGroup: user.openshift.io
    Jan 17 16:11:13.267: INFO: PreferredVersion.GroupVersion: user.openshift.io/v1
    Jan 17 16:11:13.267: INFO: Versions found [{user.openshift.io/v1 v1}]
    Jan 17 16:11:13.267: INFO: user.openshift.io/v1 matches user.openshift.io/v1
    Jan 17 16:11:13.267: INFO: Checking APIGroup: packages.operators.coreos.com
    Jan 17 16:11:13.268: INFO: PreferredVersion.GroupVersion: packages.operators.coreos.com/v1
    Jan 17 16:11:13.268: INFO: Versions found [{packages.operators.coreos.com/v1 v1}]
    Jan 17 16:11:13.268: INFO: packages.operators.coreos.com/v1 matches packages.operators.coreos.com/v1
    Jan 17 16:11:13.268: INFO: Checking APIGroup: config.openshift.io
    Jan 17 16:11:13.269: INFO: PreferredVersion.GroupVersion: config.openshift.io/v1
    Jan 17 16:11:13.269: INFO: Versions found [{config.openshift.io/v1 v1}]
    Jan 17 16:11:13.269: INFO: config.openshift.io/v1 matches config.openshift.io/v1
    Jan 17 16:11:13.269: INFO: Checking APIGroup: operator.openshift.io
    Jan 17 16:11:13.270: INFO: PreferredVersion.GroupVersion: operator.openshift.io/v1
    Jan 17 16:11:13.270: INFO: Versions found [{operator.openshift.io/v1 v1} {operator.openshift.io/v1alpha1 v1alpha1}]
    Jan 17 16:11:13.270: INFO: operator.openshift.io/v1 matches operator.openshift.io/v1
    Jan 17 16:11:13.270: INFO: Checking APIGroup: apiserver.openshift.io
    Jan 17 16:11:13.270: INFO: PreferredVersion.GroupVersion: apiserver.openshift.io/v1
    Jan 17 16:11:13.270: INFO: Versions found [{apiserver.openshift.io/v1 v1}]
    Jan 17 16:11:13.270: INFO: apiserver.openshift.io/v1 matches apiserver.openshift.io/v1
    Jan 17 16:11:13.270: INFO: Checking APIGroup: autoscaling.openshift.io
    Jan 17 16:11:13.271: INFO: PreferredVersion.GroupVersion: autoscaling.openshift.io/v1
    Jan 17 16:11:13.271: INFO: Versions found [{autoscaling.openshift.io/v1 v1} {autoscaling.openshift.io/v1beta1 v1beta1}]
    Jan 17 16:11:13.271: INFO: autoscaling.openshift.io/v1 matches autoscaling.openshift.io/v1
    Jan 17 16:11:13.271: INFO: Checking APIGroup: cloud.network.openshift.io
    Jan 17 16:11:13.272: INFO: PreferredVersion.GroupVersion: cloud.network.openshift.io/v1
    Jan 17 16:11:13.272: INFO: Versions found [{cloud.network.openshift.io/v1 v1}]
    Jan 17 16:11:13.272: INFO: cloud.network.openshift.io/v1 matches cloud.network.openshift.io/v1
    Jan 17 16:11:13.272: INFO: Checking APIGroup: cloudcredential.openshift.io
    Jan 17 16:11:13.273: INFO: PreferredVersion.GroupVersion: cloudcredential.openshift.io/v1
    Jan 17 16:11:13.273: INFO: Versions found [{cloudcredential.openshift.io/v1 v1}]
    Jan 17 16:11:13.273: INFO: cloudcredential.openshift.io/v1 matches cloudcredential.openshift.io/v1
    Jan 17 16:11:13.273: INFO: Checking APIGroup: console.openshift.io
    Jan 17 16:11:13.274: INFO: PreferredVersion.GroupVersion: console.openshift.io/v1
    Jan 17 16:11:13.274: INFO: Versions found [{console.openshift.io/v1 v1} {console.openshift.io/v1alpha1 v1alpha1}]
    Jan 17 16:11:13.274: INFO: console.openshift.io/v1 matches console.openshift.io/v1
    Jan 17 16:11:13.274: INFO: Checking APIGroup: imageregistry.operator.openshift.io
    Jan 17 16:11:13.274: INFO: PreferredVersion.GroupVersion: imageregistry.operator.openshift.io/v1
    Jan 17 16:11:13.274: INFO: Versions found [{imageregistry.operator.openshift.io/v1 v1}]
    Jan 17 16:11:13.274: INFO: imageregistry.operator.openshift.io/v1 matches imageregistry.operator.openshift.io/v1
    Jan 17 16:11:13.274: INFO: Checking APIGroup: ingress.operator.openshift.io
    Jan 17 16:11:13.275: INFO: PreferredVersion.GroupVersion: ingress.operator.openshift.io/v1
    Jan 17 16:11:13.275: INFO: Versions found [{ingress.operator.openshift.io/v1 v1}]
    Jan 17 16:11:13.275: INFO: ingress.operator.openshift.io/v1 matches ingress.operator.openshift.io/v1
    Jan 17 16:11:13.275: INFO: Checking APIGroup: k8s.cni.cncf.io
    Jan 17 16:11:13.276: INFO: PreferredVersion.GroupVersion: k8s.cni.cncf.io/v1
    Jan 17 16:11:13.276: INFO: Versions found [{k8s.cni.cncf.io/v1 v1}]
    Jan 17 16:11:13.276: INFO: k8s.cni.cncf.io/v1 matches k8s.cni.cncf.io/v1
    Jan 17 16:11:13.276: INFO: Checking APIGroup: k8s.ovn.org
    Jan 17 16:11:13.277: INFO: PreferredVersion.GroupVersion: k8s.ovn.org/v1
    Jan 17 16:11:13.277: INFO: Versions found [{k8s.ovn.org/v1 v1}]
    Jan 17 16:11:13.277: INFO: k8s.ovn.org/v1 matches k8s.ovn.org/v1
    Jan 17 16:11:13.277: INFO: Checking APIGroup: machine.openshift.io
    Jan 17 16:11:13.277: INFO: PreferredVersion.GroupVersion: machine.openshift.io/v1
    Jan 17 16:11:13.277: INFO: Versions found [{machine.openshift.io/v1 v1} {machine.openshift.io/v1beta1 v1beta1}]
    Jan 17 16:11:13.277: INFO: machine.openshift.io/v1 matches machine.openshift.io/v1
    Jan 17 16:11:13.277: INFO: Checking APIGroup: machineconfiguration.openshift.io
    Jan 17 16:11:13.278: INFO: PreferredVersion.GroupVersion: machineconfiguration.openshift.io/v1
    Jan 17 16:11:13.278: INFO: Versions found [{machineconfiguration.openshift.io/v1 v1}]
    Jan 17 16:11:13.278: INFO: machineconfiguration.openshift.io/v1 matches machineconfiguration.openshift.io/v1
    Jan 17 16:11:13.278: INFO: Checking APIGroup: monitoring.coreos.com
    Jan 17 16:11:13.279: INFO: PreferredVersion.GroupVersion: monitoring.coreos.com/v1
    Jan 17 16:11:13.279: INFO: Versions found [{monitoring.coreos.com/v1 v1} {monitoring.coreos.com/v1beta1 v1beta1} {monitoring.coreos.com/v1alpha1 v1alpha1}]
    Jan 17 16:11:13.279: INFO: monitoring.coreos.com/v1 matches monitoring.coreos.com/v1
    Jan 17 16:11:13.279: INFO: Checking APIGroup: network.operator.openshift.io
    Jan 17 16:11:13.280: INFO: PreferredVersion.GroupVersion: network.operator.openshift.io/v1
    Jan 17 16:11:13.280: INFO: Versions found [{network.operator.openshift.io/v1 v1}]
    Jan 17 16:11:13.280: INFO: network.operator.openshift.io/v1 matches network.operator.openshift.io/v1
    Jan 17 16:11:13.280: INFO: Checking APIGroup: operators.coreos.com
    Jan 17 16:11:13.281: INFO: PreferredVersion.GroupVersion: operators.coreos.com/v2
    Jan 17 16:11:13.281: INFO: Versions found [{operators.coreos.com/v2 v2} {operators.coreos.com/v1 v1} {operators.coreos.com/v1alpha2 v1alpha2} {operators.coreos.com/v1alpha1 v1alpha1}]
    Jan 17 16:11:13.281: INFO: operators.coreos.com/v2 matches operators.coreos.com/v2
    Jan 17 16:11:13.281: INFO: Checking APIGroup: performance.openshift.io
    Jan 17 16:11:13.281: INFO: PreferredVersion.GroupVersion: performance.openshift.io/v2
    Jan 17 16:11:13.281: INFO: Versions found [{performance.openshift.io/v2 v2} {performance.openshift.io/v1 v1} {performance.openshift.io/v1alpha1 v1alpha1}]
    Jan 17 16:11:13.281: INFO: performance.openshift.io/v2 matches performance.openshift.io/v2
    Jan 17 16:11:13.281: INFO: Checking APIGroup: samples.operator.openshift.io
    Jan 17 16:11:13.282: INFO: PreferredVersion.GroupVersion: samples.operator.openshift.io/v1
    Jan 17 16:11:13.282: INFO: Versions found [{samples.operator.openshift.io/v1 v1}]
    Jan 17 16:11:13.282: INFO: samples.operator.openshift.io/v1 matches samples.operator.openshift.io/v1
    Jan 17 16:11:13.282: INFO: Checking APIGroup: security.internal.openshift.io
    Jan 17 16:11:13.294: INFO: PreferredVersion.GroupVersion: security.internal.openshift.io/v1
    Jan 17 16:11:13.294: INFO: Versions found [{security.internal.openshift.io/v1 v1}]
    Jan 17 16:11:13.294: INFO: security.internal.openshift.io/v1 matches security.internal.openshift.io/v1
    Jan 17 16:11:13.294: INFO: Checking APIGroup: snapshot.storage.k8s.io
    Jan 17 16:11:13.344: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
    Jan 17 16:11:13.344: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1}]
    Jan 17 16:11:13.344: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
    Jan 17 16:11:13.344: INFO: Checking APIGroup: tuned.openshift.io
    Jan 17 16:11:13.394: INFO: PreferredVersion.GroupVersion: tuned.openshift.io/v1
    Jan 17 16:11:13.394: INFO: Versions found [{tuned.openshift.io/v1 v1}]
    Jan 17 16:11:13.394: INFO: tuned.openshift.io/v1 matches tuned.openshift.io/v1
    Jan 17 16:11:13.394: INFO: Checking APIGroup: controlplane.operator.openshift.io
    Jan 17 16:11:13.445: INFO: PreferredVersion.GroupVersion: controlplane.operator.openshift.io/v1alpha1
    Jan 17 16:11:13.445: INFO: Versions found [{controlplane.operator.openshift.io/v1alpha1 v1alpha1}]
    Jan 17 16:11:13.445: INFO: controlplane.operator.openshift.io/v1alpha1 matches controlplane.operator.openshift.io/v1alpha1
    Jan 17 16:11:13.445: INFO: Checking APIGroup: metal3.io
    Jan 17 16:11:13.494: INFO: PreferredVersion.GroupVersion: metal3.io/v1alpha1
    Jan 17 16:11:13.494: INFO: Versions found [{metal3.io/v1alpha1 v1alpha1}]
    Jan 17 16:11:13.494: INFO: metal3.io/v1alpha1 matches metal3.io/v1alpha1
    Jan 17 16:11:13.494: INFO: Checking APIGroup: migration.k8s.io
    Jan 17 16:11:13.545: INFO: PreferredVersion.GroupVersion: migration.k8s.io/v1alpha1
    Jan 17 16:11:13.545: INFO: Versions found [{migration.k8s.io/v1alpha1 v1alpha1}]
    Jan 17 16:11:13.545: INFO: migration.k8s.io/v1alpha1 matches migration.k8s.io/v1alpha1
    Jan 17 16:11:13.545: INFO: Checking APIGroup: whereabouts.cni.cncf.io
    Jan 17 16:11:13.594: INFO: PreferredVersion.GroupVersion: whereabouts.cni.cncf.io/v1alpha1
    Jan 17 16:11:13.594: INFO: Versions found [{whereabouts.cni.cncf.io/v1alpha1 v1alpha1}]
    Jan 17 16:11:13.594: INFO: whereabouts.cni.cncf.io/v1alpha1 matches whereabouts.cni.cncf.io/v1alpha1
    Jan 17 16:11:13.594: INFO: Checking APIGroup: helm.openshift.io
    Jan 17 16:11:13.645: INFO: PreferredVersion.GroupVersion: helm.openshift.io/v1beta1
    Jan 17 16:11:13.645: INFO: Versions found [{helm.openshift.io/v1beta1 v1beta1}]
    Jan 17 16:11:13.645: INFO: helm.openshift.io/v1beta1 matches helm.openshift.io/v1beta1
    Jan 17 16:11:13.645: INFO: Checking APIGroup: metrics.k8s.io
    Jan 17 16:11:13.694: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
    Jan 17 16:11:13.694: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
    Jan 17 16:11:13.694: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/framework.go:187
    Jan 17 16:11:13.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "discovery-4457" for this suite. 01/17/23 16:11:13.748
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:11:13.802
Jan 17 16:11:13.802: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename namespaces 01/17/23 16:11:13.803
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:11:13.826
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:11:13.829
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242
STEP: Creating a test namespace 01/17/23 16:11:13.832
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:11:13.861
STEP: Creating a pod in the namespace 01/17/23 16:11:13.863
STEP: Waiting for the pod to have running status 01/17/23 16:11:13.929
Jan 17 16:11:13.929: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-866" to be "running"
Jan 17 16:11:13.938: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.000092ms
Jan 17 16:11:15.941: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.012237172s
Jan 17 16:11:15.941: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 01/17/23 16:11:15.941
STEP: Waiting for the namespace to be removed. 01/17/23 16:11:15.947
STEP: Recreating the namespace 01/17/23 16:11:27.952
STEP: Verifying there are no pods in the namespace 01/17/23 16:11:27.971
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Jan 17 16:11:27.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-2691" for this suite. 01/17/23 16:11:27.98
STEP: Destroying namespace "nsdeletetest-866" for this suite. 01/17/23 16:11:27.991
Jan 17 16:11:27.995: INFO: Namespace nsdeletetest-866 was already deleted
STEP: Destroying namespace "nsdeletetest-2196" for this suite. 01/17/23 16:11:27.995
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","completed":327,"skipped":6093,"failed":0}
------------------------------
• [SLOW TEST] [14.214 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:11:13.802
    Jan 17 16:11:13.802: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename namespaces 01/17/23 16:11:13.803
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:11:13.826
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:11:13.829
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:242
    STEP: Creating a test namespace 01/17/23 16:11:13.832
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:11:13.861
    STEP: Creating a pod in the namespace 01/17/23 16:11:13.863
    STEP: Waiting for the pod to have running status 01/17/23 16:11:13.929
    Jan 17 16:11:13.929: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-866" to be "running"
    Jan 17 16:11:13.938: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.000092ms
    Jan 17 16:11:15.941: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.012237172s
    Jan 17 16:11:15.941: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 01/17/23 16:11:15.941
    STEP: Waiting for the namespace to be removed. 01/17/23 16:11:15.947
    STEP: Recreating the namespace 01/17/23 16:11:27.952
    STEP: Verifying there are no pods in the namespace 01/17/23 16:11:27.971
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Jan 17 16:11:27.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-2691" for this suite. 01/17/23 16:11:27.98
    STEP: Destroying namespace "nsdeletetest-866" for this suite. 01/17/23 16:11:27.991
    Jan 17 16:11:27.995: INFO: Namespace nsdeletetest-866 was already deleted
    STEP: Destroying namespace "nsdeletetest-2196" for this suite. 01/17/23 16:11:27.995
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:11:28.016
Jan 17 16:11:28.017: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename projected 01/17/23 16:11:28.017
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:11:28.046
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:11:28.05
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66
STEP: Creating projection with secret that has name projected-secret-test-6decf62a-338a-4a9f-9e3a-6033efe2d89d 01/17/23 16:11:28.054
STEP: Creating a pod to test consume secrets 01/17/23 16:11:28.063
Jan 17 16:11:28.091: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-31c0cde8-52c1-40e0-a9a5-b4c5c6355673" in namespace "projected-5879" to be "Succeeded or Failed"
Jan 17 16:11:28.097: INFO: Pod "pod-projected-secrets-31c0cde8-52c1-40e0-a9a5-b4c5c6355673": Phase="Pending", Reason="", readiness=false. Elapsed: 5.355955ms
Jan 17 16:11:30.101: INFO: Pod "pod-projected-secrets-31c0cde8-52c1-40e0-a9a5-b4c5c6355673": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00971022s
Jan 17 16:11:32.102: INFO: Pod "pod-projected-secrets-31c0cde8-52c1-40e0-a9a5-b4c5c6355673": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011251387s
STEP: Saw pod success 01/17/23 16:11:32.102
Jan 17 16:11:32.103: INFO: Pod "pod-projected-secrets-31c0cde8-52c1-40e0-a9a5-b4c5c6355673" satisfied condition "Succeeded or Failed"
Jan 17 16:11:32.106: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-projected-secrets-31c0cde8-52c1-40e0-a9a5-b4c5c6355673 container projected-secret-volume-test: <nil>
STEP: delete the pod 01/17/23 16:11:32.112
Jan 17 16:11:32.124: INFO: Waiting for pod pod-projected-secrets-31c0cde8-52c1-40e0-a9a5-b4c5c6355673 to disappear
Jan 17 16:11:32.126: INFO: Pod pod-projected-secrets-31c0cde8-52c1-40e0-a9a5-b4c5c6355673 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan 17 16:11:32.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5879" for this suite. 01/17/23 16:11:32.13
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","completed":328,"skipped":6098,"failed":0}
------------------------------
• [4.119 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:11:28.016
    Jan 17 16:11:28.017: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename projected 01/17/23 16:11:28.017
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:11:28.046
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:11:28.05
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:66
    STEP: Creating projection with secret that has name projected-secret-test-6decf62a-338a-4a9f-9e3a-6033efe2d89d 01/17/23 16:11:28.054
    STEP: Creating a pod to test consume secrets 01/17/23 16:11:28.063
    Jan 17 16:11:28.091: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-31c0cde8-52c1-40e0-a9a5-b4c5c6355673" in namespace "projected-5879" to be "Succeeded or Failed"
    Jan 17 16:11:28.097: INFO: Pod "pod-projected-secrets-31c0cde8-52c1-40e0-a9a5-b4c5c6355673": Phase="Pending", Reason="", readiness=false. Elapsed: 5.355955ms
    Jan 17 16:11:30.101: INFO: Pod "pod-projected-secrets-31c0cde8-52c1-40e0-a9a5-b4c5c6355673": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00971022s
    Jan 17 16:11:32.102: INFO: Pod "pod-projected-secrets-31c0cde8-52c1-40e0-a9a5-b4c5c6355673": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011251387s
    STEP: Saw pod success 01/17/23 16:11:32.102
    Jan 17 16:11:32.103: INFO: Pod "pod-projected-secrets-31c0cde8-52c1-40e0-a9a5-b4c5c6355673" satisfied condition "Succeeded or Failed"
    Jan 17 16:11:32.106: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-projected-secrets-31c0cde8-52c1-40e0-a9a5-b4c5c6355673 container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/17/23 16:11:32.112
    Jan 17 16:11:32.124: INFO: Waiting for pod pod-projected-secrets-31c0cde8-52c1-40e0-a9a5-b4c5c6355673 to disappear
    Jan 17 16:11:32.126: INFO: Pod pod-projected-secrets-31c0cde8-52c1-40e0-a9a5-b4c5c6355673 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan 17 16:11:32.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5879" for this suite. 01/17/23 16:11:32.13
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:11:32.136
Jan 17 16:11:32.136: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename crd-publish-openapi 01/17/23 16:11:32.136
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:11:32.155
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:11:32.158
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 01/17/23 16:11:32.162
Jan 17 16:11:32.163: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
Jan 17 16:11:40.020: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 17 16:12:03.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8658" for this suite. 01/17/23 16:12:03.579
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","completed":329,"skipped":6101,"failed":0}
------------------------------
• [SLOW TEST] [31.450 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:11:32.136
    Jan 17 16:11:32.136: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename crd-publish-openapi 01/17/23 16:11:32.136
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:11:32.155
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:11:32.158
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:275
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 01/17/23 16:11:32.162
    Jan 17 16:11:32.163: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    Jan 17 16:11:40.020: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 17 16:12:03.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-8658" for this suite. 01/17/23 16:12:03.579
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:12:03.586
Jan 17 16:12:03.586: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename services 01/17/23 16:12:03.586
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:12:03.625
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:12:03.628
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443
STEP: creating a service externalname-service with the type=ExternalName in namespace services-3900 01/17/23 16:12:03.63
STEP: changing the ExternalName service to type=NodePort 01/17/23 16:12:03.645
STEP: creating replication controller externalname-service in namespace services-3900 01/17/23 16:12:03.7
I0117 16:12:03.706040      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-3900, replica count: 2
I0117 16:12:06.756958      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 17 16:12:06.757: INFO: Creating new exec pod
Jan 17 16:12:06.794: INFO: Waiting up to 5m0s for pod "execpodkkgpj" in namespace "services-3900" to be "running"
Jan 17 16:12:06.797: INFO: Pod "execpodkkgpj": Phase="Pending", Reason="", readiness=false. Elapsed: 3.452606ms
Jan 17 16:12:08.801: INFO: Pod "execpodkkgpj": Phase="Running", Reason="", readiness=true. Elapsed: 2.007573005s
Jan 17 16:12:08.801: INFO: Pod "execpodkkgpj" satisfied condition "running"
Jan 17 16:12:09.807: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-3900 exec execpodkkgpj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jan 17 16:12:09.924: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan 17 16:12:09.924: INFO: stdout: "externalname-service-58zjs"
Jan 17 16:12:09.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-3900 exec execpodkkgpj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.38.145 80'
Jan 17 16:12:10.020: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.38.145 80\nConnection to 172.30.38.145 80 port [tcp/http] succeeded!\n"
Jan 17 16:12:10.020: INFO: stdout: "externalname-service-58zjs"
Jan 17 16:12:10.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-3900 exec execpodkkgpj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.139.213 30045'
Jan 17 16:12:10.116: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.139.213 30045\nConnection to 10.0.139.213 30045 port [tcp/*] succeeded!\n"
Jan 17 16:12:10.116: INFO: stdout: ""
Jan 17 16:12:11.116: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-3900 exec execpodkkgpj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.139.213 30045'
Jan 17 16:12:11.222: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.139.213 30045\nConnection to 10.0.139.213 30045 port [tcp/*] succeeded!\n"
Jan 17 16:12:11.222: INFO: stdout: ""
Jan 17 16:12:12.116: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-3900 exec execpodkkgpj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.139.213 30045'
Jan 17 16:12:12.231: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.139.213 30045\nConnection to 10.0.139.213 30045 port [tcp/*] succeeded!\n"
Jan 17 16:12:12.231: INFO: stdout: "externalname-service-w7hgw"
Jan 17 16:12:12.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-3900 exec execpodkkgpj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.151.22 30045'
Jan 17 16:12:12.332: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.151.22 30045\nConnection to 10.0.151.22 30045 port [tcp/*] succeeded!\n"
Jan 17 16:12:12.332: INFO: stdout: "externalname-service-58zjs"
Jan 17 16:12:12.332: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 17 16:12:12.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3900" for this suite. 01/17/23 16:12:12.392
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","completed":330,"skipped":6101,"failed":0}
------------------------------
• [SLOW TEST] [8.816 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:12:03.586
    Jan 17 16:12:03.586: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename services 01/17/23 16:12:03.586
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:12:03.625
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:12:03.628
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1443
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-3900 01/17/23 16:12:03.63
    STEP: changing the ExternalName service to type=NodePort 01/17/23 16:12:03.645
    STEP: creating replication controller externalname-service in namespace services-3900 01/17/23 16:12:03.7
    I0117 16:12:03.706040      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-3900, replica count: 2
    I0117 16:12:06.756958      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 17 16:12:06.757: INFO: Creating new exec pod
    Jan 17 16:12:06.794: INFO: Waiting up to 5m0s for pod "execpodkkgpj" in namespace "services-3900" to be "running"
    Jan 17 16:12:06.797: INFO: Pod "execpodkkgpj": Phase="Pending", Reason="", readiness=false. Elapsed: 3.452606ms
    Jan 17 16:12:08.801: INFO: Pod "execpodkkgpj": Phase="Running", Reason="", readiness=true. Elapsed: 2.007573005s
    Jan 17 16:12:08.801: INFO: Pod "execpodkkgpj" satisfied condition "running"
    Jan 17 16:12:09.807: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-3900 exec execpodkkgpj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Jan 17 16:12:09.924: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jan 17 16:12:09.924: INFO: stdout: "externalname-service-58zjs"
    Jan 17 16:12:09.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-3900 exec execpodkkgpj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.38.145 80'
    Jan 17 16:12:10.020: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.38.145 80\nConnection to 172.30.38.145 80 port [tcp/http] succeeded!\n"
    Jan 17 16:12:10.020: INFO: stdout: "externalname-service-58zjs"
    Jan 17 16:12:10.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-3900 exec execpodkkgpj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.139.213 30045'
    Jan 17 16:12:10.116: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.139.213 30045\nConnection to 10.0.139.213 30045 port [tcp/*] succeeded!\n"
    Jan 17 16:12:10.116: INFO: stdout: ""
    Jan 17 16:12:11.116: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-3900 exec execpodkkgpj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.139.213 30045'
    Jan 17 16:12:11.222: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.139.213 30045\nConnection to 10.0.139.213 30045 port [tcp/*] succeeded!\n"
    Jan 17 16:12:11.222: INFO: stdout: ""
    Jan 17 16:12:12.116: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-3900 exec execpodkkgpj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.139.213 30045'
    Jan 17 16:12:12.231: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.139.213 30045\nConnection to 10.0.139.213 30045 port [tcp/*] succeeded!\n"
    Jan 17 16:12:12.231: INFO: stdout: "externalname-service-w7hgw"
    Jan 17 16:12:12.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-3900 exec execpodkkgpj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.151.22 30045'
    Jan 17 16:12:12.332: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.151.22 30045\nConnection to 10.0.151.22 30045 port [tcp/*] succeeded!\n"
    Jan 17 16:12:12.332: INFO: stdout: "externalname-service-58zjs"
    Jan 17 16:12:12.332: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 17 16:12:12.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-3900" for this suite. 01/17/23 16:12:12.392
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:12:12.403
Jan 17 16:12:12.403: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename watch 01/17/23 16:12:12.404
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:12:12.439
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:12:12.443
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 01/17/23 16:12:12.446
STEP: creating a new configmap 01/17/23 16:12:12.448
STEP: modifying the configmap once 01/17/23 16:12:12.46
STEP: changing the label value of the configmap 01/17/23 16:12:12.471
STEP: Expecting to observe a delete notification for the watched object 01/17/23 16:12:12.482
Jan 17 16:12:12.482: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6026  f15f6e5b-651d-4ba0-8f7c-96bbc43461a4 134259 0 2023-01-17 16:12:12 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-17 16:12:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 17 16:12:12.482: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6026  f15f6e5b-651d-4ba0-8f7c-96bbc43461a4 134261 0 2023-01-17 16:12:12 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-17 16:12:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 17 16:12:12.482: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6026  f15f6e5b-651d-4ba0-8f7c-96bbc43461a4 134262 0 2023-01-17 16:12:12 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-17 16:12:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 01/17/23 16:12:12.482
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 01/17/23 16:12:12.491
STEP: changing the label value of the configmap back 01/17/23 16:12:22.491
STEP: modifying the configmap a third time 01/17/23 16:12:22.5
STEP: deleting the configmap 01/17/23 16:12:22.507
STEP: Expecting to observe an add notification for the watched object when the label value was restored 01/17/23 16:12:22.512
Jan 17 16:12:22.512: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6026  f15f6e5b-651d-4ba0-8f7c-96bbc43461a4 134391 0 2023-01-17 16:12:12 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-17 16:12:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 17 16:12:22.513: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6026  f15f6e5b-651d-4ba0-8f7c-96bbc43461a4 134392 0 2023-01-17 16:12:12 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-17 16:12:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 17 16:12:22.513: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6026  f15f6e5b-651d-4ba0-8f7c-96bbc43461a4 134393 0 2023-01-17 16:12:12 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-17 16:12:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Jan 17 16:12:22.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6026" for this suite. 01/17/23 16:12:22.518
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","completed":331,"skipped":6115,"failed":0}
------------------------------
• [SLOW TEST] [10.121 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:12:12.403
    Jan 17 16:12:12.403: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename watch 01/17/23 16:12:12.404
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:12:12.439
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:12:12.443
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 01/17/23 16:12:12.446
    STEP: creating a new configmap 01/17/23 16:12:12.448
    STEP: modifying the configmap once 01/17/23 16:12:12.46
    STEP: changing the label value of the configmap 01/17/23 16:12:12.471
    STEP: Expecting to observe a delete notification for the watched object 01/17/23 16:12:12.482
    Jan 17 16:12:12.482: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6026  f15f6e5b-651d-4ba0-8f7c-96bbc43461a4 134259 0 2023-01-17 16:12:12 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-17 16:12:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 17 16:12:12.482: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6026  f15f6e5b-651d-4ba0-8f7c-96bbc43461a4 134261 0 2023-01-17 16:12:12 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-17 16:12:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 17 16:12:12.482: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6026  f15f6e5b-651d-4ba0-8f7c-96bbc43461a4 134262 0 2023-01-17 16:12:12 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-17 16:12:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 01/17/23 16:12:12.482
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 01/17/23 16:12:12.491
    STEP: changing the label value of the configmap back 01/17/23 16:12:22.491
    STEP: modifying the configmap a third time 01/17/23 16:12:22.5
    STEP: deleting the configmap 01/17/23 16:12:22.507
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 01/17/23 16:12:22.512
    Jan 17 16:12:22.512: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6026  f15f6e5b-651d-4ba0-8f7c-96bbc43461a4 134391 0 2023-01-17 16:12:12 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-17 16:12:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 17 16:12:22.513: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6026  f15f6e5b-651d-4ba0-8f7c-96bbc43461a4 134392 0 2023-01-17 16:12:12 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-17 16:12:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 17 16:12:22.513: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6026  f15f6e5b-651d-4ba0-8f7c-96bbc43461a4 134393 0 2023-01-17 16:12:12 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-17 16:12:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Jan 17 16:12:22.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-6026" for this suite. 01/17/23 16:12:22.518
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:12:22.524
Jan 17 16:12:22.524: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename ingressclass 01/17/23 16:12:22.525
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:12:22.55
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:12:22.553
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 01/17/23 16:12:22.555
STEP: getting /apis/networking.k8s.io 01/17/23 16:12:22.557
STEP: getting /apis/networking.k8s.iov1 01/17/23 16:12:22.558
STEP: creating 01/17/23 16:12:22.558
STEP: getting 01/17/23 16:12:22.58
STEP: listing 01/17/23 16:12:22.587
STEP: watching 01/17/23 16:12:22.593
Jan 17 16:12:22.593: INFO: starting watch
STEP: patching 01/17/23 16:12:22.594
STEP: updating 01/17/23 16:12:22.6
Jan 17 16:12:22.605: INFO: waiting for watch events with expected annotations
Jan 17 16:12:22.605: INFO: saw patched and updated annotations
STEP: deleting 01/17/23 16:12:22.605
STEP: deleting a collection 01/17/23 16:12:22.619
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:187
Jan 17 16:12:22.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-3376" for this suite. 01/17/23 16:12:22.637
{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","completed":332,"skipped":6136,"failed":0}
------------------------------
• [0.120 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:12:22.524
    Jan 17 16:12:22.524: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename ingressclass 01/17/23 16:12:22.525
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:12:22.55
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:12:22.553
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 01/17/23 16:12:22.555
    STEP: getting /apis/networking.k8s.io 01/17/23 16:12:22.557
    STEP: getting /apis/networking.k8s.iov1 01/17/23 16:12:22.558
    STEP: creating 01/17/23 16:12:22.558
    STEP: getting 01/17/23 16:12:22.58
    STEP: listing 01/17/23 16:12:22.587
    STEP: watching 01/17/23 16:12:22.593
    Jan 17 16:12:22.593: INFO: starting watch
    STEP: patching 01/17/23 16:12:22.594
    STEP: updating 01/17/23 16:12:22.6
    Jan 17 16:12:22.605: INFO: waiting for watch events with expected annotations
    Jan 17 16:12:22.605: INFO: saw patched and updated annotations
    STEP: deleting 01/17/23 16:12:22.605
    STEP: deleting a collection 01/17/23 16:12:22.619
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/framework.go:187
    Jan 17 16:12:22.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ingressclass-3376" for this suite. 01/17/23 16:12:22.637
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:12:22.645
Jan 17 16:12:22.645: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename downward-api 01/17/23 16:12:22.645
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:12:22.677
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:12:22.68
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234
STEP: Creating a pod to test downward API volume plugin 01/17/23 16:12:22.682
Jan 17 16:12:22.713: INFO: Waiting up to 5m0s for pod "downwardapi-volume-da3280eb-d50f-4d3c-bc88-e5d9448c9375" in namespace "downward-api-2113" to be "Succeeded or Failed"
Jan 17 16:12:22.719: INFO: Pod "downwardapi-volume-da3280eb-d50f-4d3c-bc88-e5d9448c9375": Phase="Pending", Reason="", readiness=false. Elapsed: 6.500929ms
Jan 17 16:12:24.726: INFO: Pod "downwardapi-volume-da3280eb-d50f-4d3c-bc88-e5d9448c9375": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012671737s
Jan 17 16:12:26.723: INFO: Pod "downwardapi-volume-da3280eb-d50f-4d3c-bc88-e5d9448c9375": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010595462s
STEP: Saw pod success 01/17/23 16:12:26.723
Jan 17 16:12:26.724: INFO: Pod "downwardapi-volume-da3280eb-d50f-4d3c-bc88-e5d9448c9375" satisfied condition "Succeeded or Failed"
Jan 17 16:12:26.727: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod downwardapi-volume-da3280eb-d50f-4d3c-bc88-e5d9448c9375 container client-container: <nil>
STEP: delete the pod 01/17/23 16:12:26.735
Jan 17 16:12:26.746: INFO: Waiting for pod downwardapi-volume-da3280eb-d50f-4d3c-bc88-e5d9448c9375 to disappear
Jan 17 16:12:26.749: INFO: Pod downwardapi-volume-da3280eb-d50f-4d3c-bc88-e5d9448c9375 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 17 16:12:26.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2113" for this suite. 01/17/23 16:12:26.754
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","completed":333,"skipped":6141,"failed":0}
------------------------------
• [4.115 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:12:22.645
    Jan 17 16:12:22.645: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename downward-api 01/17/23 16:12:22.645
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:12:22.677
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:12:22.68
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:234
    STEP: Creating a pod to test downward API volume plugin 01/17/23 16:12:22.682
    Jan 17 16:12:22.713: INFO: Waiting up to 5m0s for pod "downwardapi-volume-da3280eb-d50f-4d3c-bc88-e5d9448c9375" in namespace "downward-api-2113" to be "Succeeded or Failed"
    Jan 17 16:12:22.719: INFO: Pod "downwardapi-volume-da3280eb-d50f-4d3c-bc88-e5d9448c9375": Phase="Pending", Reason="", readiness=false. Elapsed: 6.500929ms
    Jan 17 16:12:24.726: INFO: Pod "downwardapi-volume-da3280eb-d50f-4d3c-bc88-e5d9448c9375": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012671737s
    Jan 17 16:12:26.723: INFO: Pod "downwardapi-volume-da3280eb-d50f-4d3c-bc88-e5d9448c9375": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010595462s
    STEP: Saw pod success 01/17/23 16:12:26.723
    Jan 17 16:12:26.724: INFO: Pod "downwardapi-volume-da3280eb-d50f-4d3c-bc88-e5d9448c9375" satisfied condition "Succeeded or Failed"
    Jan 17 16:12:26.727: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod downwardapi-volume-da3280eb-d50f-4d3c-bc88-e5d9448c9375 container client-container: <nil>
    STEP: delete the pod 01/17/23 16:12:26.735
    Jan 17 16:12:26.746: INFO: Waiting for pod downwardapi-volume-da3280eb-d50f-4d3c-bc88-e5d9448c9375 to disappear
    Jan 17 16:12:26.749: INFO: Pod downwardapi-volume-da3280eb-d50f-4d3c-bc88-e5d9448c9375 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 17 16:12:26.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-2113" for this suite. 01/17/23 16:12:26.754
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:975
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:12:26.761
Jan 17 16:12:26.761: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename statefulset 01/17/23 16:12:26.761
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:12:26.782
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:12:26.785
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-7037 01/17/23 16:12:26.788
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:975
STEP: Creating statefulset ss in namespace statefulset-7037 01/17/23 16:12:26.817
Jan 17 16:12:26.833: INFO: Found 0 stateful pods, waiting for 1
Jan 17 16:12:36.837: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 01/17/23 16:12:36.844
STEP: Getting /status 01/17/23 16:12:36.849
Jan 17 16:12:36.853: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 01/17/23 16:12:36.853
Jan 17 16:12:36.861: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 01/17/23 16:12:36.861
Jan 17 16:12:36.862: INFO: Observed &StatefulSet event: ADDED
Jan 17 16:12:36.862: INFO: Found Statefulset ss in namespace statefulset-7037 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 17 16:12:36.862: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 01/17/23 16:12:36.862
Jan 17 16:12:36.862: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan 17 16:12:36.868: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 01/17/23 16:12:36.868
Jan 17 16:12:36.869: INFO: Observed &StatefulSet event: ADDED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 17 16:12:36.869: INFO: Deleting all statefulset in ns statefulset-7037
Jan 17 16:12:36.872: INFO: Scaling statefulset ss to 0
Jan 17 16:12:46.893: INFO: Waiting for statefulset status.replicas updated to 0
Jan 17 16:12:46.895: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan 17 16:12:46.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7037" for this suite. 01/17/23 16:12:46.914
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]","completed":334,"skipped":6151,"failed":0}
------------------------------
• [SLOW TEST] [20.167 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:975

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:12:26.761
    Jan 17 16:12:26.761: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename statefulset 01/17/23 16:12:26.761
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:12:26.782
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:12:26.785
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-7037 01/17/23 16:12:26.788
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:975
    STEP: Creating statefulset ss in namespace statefulset-7037 01/17/23 16:12:26.817
    Jan 17 16:12:26.833: INFO: Found 0 stateful pods, waiting for 1
    Jan 17 16:12:36.837: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 01/17/23 16:12:36.844
    STEP: Getting /status 01/17/23 16:12:36.849
    Jan 17 16:12:36.853: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 01/17/23 16:12:36.853
    Jan 17 16:12:36.861: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 01/17/23 16:12:36.861
    Jan 17 16:12:36.862: INFO: Observed &StatefulSet event: ADDED
    Jan 17 16:12:36.862: INFO: Found Statefulset ss in namespace statefulset-7037 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan 17 16:12:36.862: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 01/17/23 16:12:36.862
    Jan 17 16:12:36.862: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jan 17 16:12:36.868: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 01/17/23 16:12:36.868
    Jan 17 16:12:36.869: INFO: Observed &StatefulSet event: ADDED
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan 17 16:12:36.869: INFO: Deleting all statefulset in ns statefulset-7037
    Jan 17 16:12:36.872: INFO: Scaling statefulset ss to 0
    Jan 17 16:12:46.893: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 17 16:12:46.895: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan 17 16:12:46.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-7037" for this suite. 01/17/23 16:12:46.914
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:309
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:12:46.928
Jan 17 16:12:46.928: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename job 01/17/23 16:12:46.929
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:12:46.956
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:12:46.961
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:309
STEP: Creating a job 01/17/23 16:12:46.963
STEP: Ensuring active pods == parallelism 01/17/23 16:12:46.989
STEP: delete a job 01/17/23 16:12:48.993
STEP: deleting Job.batch foo in namespace job-5288, will wait for the garbage collector to delete the pods 01/17/23 16:12:48.993
Jan 17 16:12:49.054: INFO: Deleting Job.batch foo took: 5.55582ms
Jan 17 16:12:49.155: INFO: Terminating Job.batch foo pods took: 100.699618ms
STEP: Ensuring job was deleted 01/17/23 16:13:21.756
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jan 17 16:13:21.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-5288" for this suite. 01/17/23 16:13:21.764
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","completed":335,"skipped":6158,"failed":0}
------------------------------
• [SLOW TEST] [34.841 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:12:46.928
    Jan 17 16:12:46.928: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename job 01/17/23 16:12:46.929
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:12:46.956
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:12:46.961
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:309
    STEP: Creating a job 01/17/23 16:12:46.963
    STEP: Ensuring active pods == parallelism 01/17/23 16:12:46.989
    STEP: delete a job 01/17/23 16:12:48.993
    STEP: deleting Job.batch foo in namespace job-5288, will wait for the garbage collector to delete the pods 01/17/23 16:12:48.993
    Jan 17 16:12:49.054: INFO: Deleting Job.batch foo took: 5.55582ms
    Jan 17 16:12:49.155: INFO: Terminating Job.batch foo pods took: 100.699618ms
    STEP: Ensuring job was deleted 01/17/23 16:13:21.756
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jan 17 16:13:21.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-5288" for this suite. 01/17/23 16:13:21.764
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:13:21.77
Jan 17 16:13:21.770: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename configmap 01/17/23 16:13:21.77
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:13:21.794
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:13:21.796
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98
STEP: Creating configMap with name configmap-test-volume-map-dcc3aa66-034e-42b6-a2fb-af5dffcb2084 01/17/23 16:13:21.798
STEP: Creating a pod to test consume configMaps 01/17/23 16:13:21.806
Jan 17 16:13:21.834: INFO: Waiting up to 5m0s for pod "pod-configmaps-274e50d7-db97-4dda-8f41-9f443f62458d" in namespace "configmap-8239" to be "Succeeded or Failed"
Jan 17 16:13:21.840: INFO: Pod "pod-configmaps-274e50d7-db97-4dda-8f41-9f443f62458d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.73168ms
Jan 17 16:13:23.845: INFO: Pod "pod-configmaps-274e50d7-db97-4dda-8f41-9f443f62458d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011172533s
Jan 17 16:13:25.843: INFO: Pod "pod-configmaps-274e50d7-db97-4dda-8f41-9f443f62458d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009034526s
STEP: Saw pod success 01/17/23 16:13:25.843
Jan 17 16:13:25.843: INFO: Pod "pod-configmaps-274e50d7-db97-4dda-8f41-9f443f62458d" satisfied condition "Succeeded or Failed"
Jan 17 16:13:25.846: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-configmaps-274e50d7-db97-4dda-8f41-9f443f62458d container agnhost-container: <nil>
STEP: delete the pod 01/17/23 16:13:25.855
Jan 17 16:13:25.869: INFO: Waiting for pod pod-configmaps-274e50d7-db97-4dda-8f41-9f443f62458d to disappear
Jan 17 16:13:25.872: INFO: Pod pod-configmaps-274e50d7-db97-4dda-8f41-9f443f62458d no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 17 16:13:25.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8239" for this suite. 01/17/23 16:13:25.876
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":336,"skipped":6177,"failed":0}
------------------------------
• [4.111 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:13:21.77
    Jan 17 16:13:21.770: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename configmap 01/17/23 16:13:21.77
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:13:21.794
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:13:21.796
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:98
    STEP: Creating configMap with name configmap-test-volume-map-dcc3aa66-034e-42b6-a2fb-af5dffcb2084 01/17/23 16:13:21.798
    STEP: Creating a pod to test consume configMaps 01/17/23 16:13:21.806
    Jan 17 16:13:21.834: INFO: Waiting up to 5m0s for pod "pod-configmaps-274e50d7-db97-4dda-8f41-9f443f62458d" in namespace "configmap-8239" to be "Succeeded or Failed"
    Jan 17 16:13:21.840: INFO: Pod "pod-configmaps-274e50d7-db97-4dda-8f41-9f443f62458d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.73168ms
    Jan 17 16:13:23.845: INFO: Pod "pod-configmaps-274e50d7-db97-4dda-8f41-9f443f62458d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011172533s
    Jan 17 16:13:25.843: INFO: Pod "pod-configmaps-274e50d7-db97-4dda-8f41-9f443f62458d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009034526s
    STEP: Saw pod success 01/17/23 16:13:25.843
    Jan 17 16:13:25.843: INFO: Pod "pod-configmaps-274e50d7-db97-4dda-8f41-9f443f62458d" satisfied condition "Succeeded or Failed"
    Jan 17 16:13:25.846: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-configmaps-274e50d7-db97-4dda-8f41-9f443f62458d container agnhost-container: <nil>
    STEP: delete the pod 01/17/23 16:13:25.855
    Jan 17 16:13:25.869: INFO: Waiting for pod pod-configmaps-274e50d7-db97-4dda-8f41-9f443f62458d to disappear
    Jan 17 16:13:25.872: INFO: Pod pod-configmaps-274e50d7-db97-4dda-8f41-9f443f62458d no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 17 16:13:25.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-8239" for this suite. 01/17/23 16:13:25.876
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:13:25.882
Jan 17 16:13:25.882: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename services 01/17/23 16:13:25.883
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:13:25.907
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:13:25.91
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404
STEP: creating a service externalname-service with the type=ExternalName in namespace services-4802 01/17/23 16:13:25.912
STEP: changing the ExternalName service to type=ClusterIP 01/17/23 16:13:25.922
STEP: creating replication controller externalname-service in namespace services-4802 01/17/23 16:13:25.949
I0117 16:13:25.960981      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-4802, replica count: 2
I0117 16:13:29.012137      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 17 16:13:29.012: INFO: Creating new exec pod
Jan 17 16:13:29.037: INFO: Waiting up to 5m0s for pod "execpodjf2gh" in namespace "services-4802" to be "running"
Jan 17 16:13:29.040: INFO: Pod "execpodjf2gh": Phase="Pending", Reason="", readiness=false. Elapsed: 3.052682ms
Jan 17 16:13:31.044: INFO: Pod "execpodjf2gh": Phase="Running", Reason="", readiness=true. Elapsed: 2.00657351s
Jan 17 16:13:31.044: INFO: Pod "execpodjf2gh" satisfied condition "running"
Jan 17 16:13:32.044: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-4802 exec execpodjf2gh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jan 17 16:13:32.173: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan 17 16:13:32.173: INFO: stdout: "externalname-service-8jcjp"
Jan 17 16:13:32.173: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-4802 exec execpodjf2gh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.120.249 80'
Jan 17 16:13:32.286: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.120.249 80\nConnection to 172.30.120.249 80 port [tcp/http] succeeded!\n"
Jan 17 16:13:32.286: INFO: stdout: "externalname-service-558h9"
Jan 17 16:13:32.286: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 17 16:13:32.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4802" for this suite. 01/17/23 16:13:32.323
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","completed":337,"skipped":6226,"failed":0}
------------------------------
• [SLOW TEST] [6.449 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:13:25.882
    Jan 17 16:13:25.882: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename services 01/17/23 16:13:25.883
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:13:25.907
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:13:25.91
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1404
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-4802 01/17/23 16:13:25.912
    STEP: changing the ExternalName service to type=ClusterIP 01/17/23 16:13:25.922
    STEP: creating replication controller externalname-service in namespace services-4802 01/17/23 16:13:25.949
    I0117 16:13:25.960981      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-4802, replica count: 2
    I0117 16:13:29.012137      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 17 16:13:29.012: INFO: Creating new exec pod
    Jan 17 16:13:29.037: INFO: Waiting up to 5m0s for pod "execpodjf2gh" in namespace "services-4802" to be "running"
    Jan 17 16:13:29.040: INFO: Pod "execpodjf2gh": Phase="Pending", Reason="", readiness=false. Elapsed: 3.052682ms
    Jan 17 16:13:31.044: INFO: Pod "execpodjf2gh": Phase="Running", Reason="", readiness=true. Elapsed: 2.00657351s
    Jan 17 16:13:31.044: INFO: Pod "execpodjf2gh" satisfied condition "running"
    Jan 17 16:13:32.044: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-4802 exec execpodjf2gh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Jan 17 16:13:32.173: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jan 17 16:13:32.173: INFO: stdout: "externalname-service-8jcjp"
    Jan 17 16:13:32.173: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=services-4802 exec execpodjf2gh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.120.249 80'
    Jan 17 16:13:32.286: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.120.249 80\nConnection to 172.30.120.249 80 port [tcp/http] succeeded!\n"
    Jan 17 16:13:32.286: INFO: stdout: "externalname-service-558h9"
    Jan 17 16:13:32.286: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 17 16:13:32.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-4802" for this suite. 01/17/23 16:13:32.323
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:13:32.331
Jan 17 16:13:32.331: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename tables 01/17/23 16:13:32.332
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:13:32.36
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:13:32.364
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:187
Jan 17 16:13:32.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-1227" for this suite. 01/17/23 16:13:32.373
{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","completed":338,"skipped":6237,"failed":0}
------------------------------
• [0.080 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:13:32.331
    Jan 17 16:13:32.331: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename tables 01/17/23 16:13:32.332
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:13:32.36
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:13:32.364
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/framework.go:187
    Jan 17 16:13:32.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "tables-1227" for this suite. 01/17/23 16:13:32.373
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:13:32.412
Jan 17 16:13:32.412: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename dns 01/17/23 16:13:32.413
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:13:32.439
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:13:32.442
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 01/17/23 16:13:32.445
Jan 17 16:13:32.477: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-3441  7dd3fe44-9950-401d-bd53-90bdc2ff5f3b 135362 0 2023-01-17 16:13:32 +0000 UTC <nil> <nil> map[] map[openshift.io/scc:anyuid] [] [] [{e2e.test Update v1 2023-01-17 16:13:32 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cr7dj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cr7dj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c66,c20,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 16:13:32.477: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-3441" to be "running and ready"
Jan 17 16:13:32.482: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 4.778706ms
Jan 17 16:13:32.482: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Jan 17 16:13:34.491: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.014134095s
Jan 17 16:13:34.491: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
Jan 17 16:13:34.491: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 01/17/23 16:13:34.491
Jan 17 16:13:34.491: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-3441 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 16:13:34.491: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
Jan 17 16:13:34.491: INFO: ExecWithOptions: Clientset creation
Jan 17 16:13:34.491: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/dns-3441/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 01/17/23 16:13:34.575
Jan 17 16:13:34.576: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-3441 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 16:13:34.576: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
Jan 17 16:13:34.576: INFO: ExecWithOptions: Clientset creation
Jan 17 16:13:34.576: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/dns-3441/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 17 16:13:34.652: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan 17 16:13:34.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3441" for this suite. 01/17/23 16:13:34.668
{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","completed":339,"skipped":6239,"failed":0}
------------------------------
• [2.267 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:13:32.412
    Jan 17 16:13:32.412: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename dns 01/17/23 16:13:32.413
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:13:32.439
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:13:32.442
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 01/17/23 16:13:32.445
    Jan 17 16:13:32.477: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-3441  7dd3fe44-9950-401d-bd53-90bdc2ff5f3b 135362 0 2023-01-17 16:13:32 +0000 UTC <nil> <nil> map[] map[openshift.io/scc:anyuid] [] [] [{e2e.test Update v1 2023-01-17 16:13:32 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cr7dj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cr7dj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c66,c20,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 17 16:13:32.477: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-3441" to be "running and ready"
    Jan 17 16:13:32.482: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 4.778706ms
    Jan 17 16:13:32.482: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 16:13:34.491: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.014134095s
    Jan 17 16:13:34.491: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    Jan 17 16:13:34.491: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 01/17/23 16:13:34.491
    Jan 17 16:13:34.491: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-3441 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 17 16:13:34.491: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    Jan 17 16:13:34.491: INFO: ExecWithOptions: Clientset creation
    Jan 17 16:13:34.491: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/dns-3441/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 01/17/23 16:13:34.575
    Jan 17 16:13:34.576: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-3441 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 17 16:13:34.576: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    Jan 17 16:13:34.576: INFO: ExecWithOptions: Clientset creation
    Jan 17 16:13:34.576: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/dns-3441/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 17 16:13:34.652: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan 17 16:13:34.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-3441" for this suite. 01/17/23 16:13:34.668
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:13:34.681
Jan 17 16:13:34.681: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename emptydir 01/17/23 16:13:34.682
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:13:34.719
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:13:34.722
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216
STEP: Creating a pod to test emptydir 0777 on node default medium 01/17/23 16:13:34.724
Jan 17 16:13:34.769: INFO: Waiting up to 5m0s for pod "pod-7b268bfd-51d2-463d-b1a4-701381d8b5cd" in namespace "emptydir-3813" to be "Succeeded or Failed"
Jan 17 16:13:34.774: INFO: Pod "pod-7b268bfd-51d2-463d-b1a4-701381d8b5cd": Phase="Pending", Reason="", readiness=false. Elapsed: 5.448793ms
Jan 17 16:13:36.778: INFO: Pod "pod-7b268bfd-51d2-463d-b1a4-701381d8b5cd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00952517s
Jan 17 16:13:38.779: INFO: Pod "pod-7b268bfd-51d2-463d-b1a4-701381d8b5cd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009766811s
STEP: Saw pod success 01/17/23 16:13:38.779
Jan 17 16:13:38.779: INFO: Pod "pod-7b268bfd-51d2-463d-b1a4-701381d8b5cd" satisfied condition "Succeeded or Failed"
Jan 17 16:13:38.782: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-7b268bfd-51d2-463d-b1a4-701381d8b5cd container test-container: <nil>
STEP: delete the pod 01/17/23 16:13:38.787
Jan 17 16:13:38.801: INFO: Waiting for pod pod-7b268bfd-51d2-463d-b1a4-701381d8b5cd to disappear
Jan 17 16:13:38.803: INFO: Pod pod-7b268bfd-51d2-463d-b1a4-701381d8b5cd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 17 16:13:38.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3813" for this suite. 01/17/23 16:13:38.808
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":340,"skipped":6355,"failed":0}
------------------------------
• [4.133 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:13:34.681
    Jan 17 16:13:34.681: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename emptydir 01/17/23 16:13:34.682
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:13:34.719
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:13:34.722
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:216
    STEP: Creating a pod to test emptydir 0777 on node default medium 01/17/23 16:13:34.724
    Jan 17 16:13:34.769: INFO: Waiting up to 5m0s for pod "pod-7b268bfd-51d2-463d-b1a4-701381d8b5cd" in namespace "emptydir-3813" to be "Succeeded or Failed"
    Jan 17 16:13:34.774: INFO: Pod "pod-7b268bfd-51d2-463d-b1a4-701381d8b5cd": Phase="Pending", Reason="", readiness=false. Elapsed: 5.448793ms
    Jan 17 16:13:36.778: INFO: Pod "pod-7b268bfd-51d2-463d-b1a4-701381d8b5cd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00952517s
    Jan 17 16:13:38.779: INFO: Pod "pod-7b268bfd-51d2-463d-b1a4-701381d8b5cd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009766811s
    STEP: Saw pod success 01/17/23 16:13:38.779
    Jan 17 16:13:38.779: INFO: Pod "pod-7b268bfd-51d2-463d-b1a4-701381d8b5cd" satisfied condition "Succeeded or Failed"
    Jan 17 16:13:38.782: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-7b268bfd-51d2-463d-b1a4-701381d8b5cd container test-container: <nil>
    STEP: delete the pod 01/17/23 16:13:38.787
    Jan 17 16:13:38.801: INFO: Waiting for pod pod-7b268bfd-51d2-463d-b1a4-701381d8b5cd to disappear
    Jan 17 16:13:38.803: INFO: Pod pod-7b268bfd-51d2-463d-b1a4-701381d8b5cd no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 17 16:13:38.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-3813" for this suite. 01/17/23 16:13:38.808
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:13:38.815
Jan 17 16:13:38.815: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename sysctl 01/17/23 16:13:38.816
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:13:38.849
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:13:38.855
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 01/17/23 16:13:38.859
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
Jan 17 16:13:38.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-6464" for this suite. 01/17/23 16:13:38.889
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","completed":341,"skipped":6396,"failed":0}
------------------------------
• [0.085 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:13:38.815
    Jan 17 16:13:38.815: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename sysctl 01/17/23 16:13:38.816
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:13:38.849
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:13:38.855
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 01/17/23 16:13:38.859
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan 17 16:13:38.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sysctl-6464" for this suite. 01/17/23 16:13:38.889
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:585
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:13:38.902
Jan 17 16:13:38.902: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename statefulset 01/17/23 16:13:38.902
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:13:38.933
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:13:38.936
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-2370 01/17/23 16:13:38.938
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:585
STEP: Initializing watcher for selector baz=blah,foo=bar 01/17/23 16:13:38.95
STEP: Creating stateful set ss in namespace statefulset-2370 01/17/23 16:13:38.954
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2370 01/17/23 16:13:38.962
Jan 17 16:13:38.967: INFO: Found 0 stateful pods, waiting for 1
Jan 17 16:13:48.972: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 01/17/23 16:13:48.972
Jan 17 16:13:48.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=statefulset-2370 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 17 16:13:49.102: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 17 16:13:49.102: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 17 16:13:49.102: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 17 16:13:49.105: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan 17 16:13:59.109: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 17 16:13:59.109: INFO: Waiting for statefulset status.replicas updated to 0
Jan 17 16:13:59.124: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999723s
Jan 17 16:14:00.128: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.996756777s
Jan 17 16:14:01.132: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.993094104s
Jan 17 16:14:02.135: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.989112517s
Jan 17 16:14:03.149: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.985419353s
Jan 17 16:14:04.153: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.971022223s
Jan 17 16:14:05.157: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.96774751s
Jan 17 16:14:06.160: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.964058094s
Jan 17 16:14:07.164: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.96028656s
Jan 17 16:14:08.169: INFO: Verifying statefulset ss doesn't scale past 1 for another 955.532607ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2370 01/17/23 16:14:09.169
Jan 17 16:14:09.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=statefulset-2370 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 17 16:14:09.302: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 17 16:14:09.302: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 17 16:14:09.302: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 17 16:14:09.305: INFO: Found 1 stateful pods, waiting for 3
Jan 17 16:14:19.310: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 17 16:14:19.310: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 17 16:14:19.310: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 01/17/23 16:14:19.31
STEP: Scale down will halt with unhealthy stateful pod 01/17/23 16:14:19.311
Jan 17 16:14:19.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=statefulset-2370 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 17 16:14:19.422: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 17 16:14:19.422: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 17 16:14:19.422: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 17 16:14:19.422: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=statefulset-2370 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 17 16:14:19.543: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 17 16:14:19.543: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 17 16:14:19.543: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 17 16:14:19.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=statefulset-2370 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 17 16:14:19.642: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 17 16:14:19.642: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 17 16:14:19.642: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 17 16:14:19.642: INFO: Waiting for statefulset status.replicas updated to 0
Jan 17 16:14:19.646: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Jan 17 16:14:29.652: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 17 16:14:29.652: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan 17 16:14:29.652: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan 17 16:14:29.663: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999804s
Jan 17 16:14:30.667: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996521926s
Jan 17 16:14:31.671: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.992627043s
Jan 17 16:14:32.677: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.988969672s
Jan 17 16:14:33.681: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.982834899s
Jan 17 16:14:34.690: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.97923873s
Jan 17 16:14:35.695: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.96918284s
Jan 17 16:14:36.699: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.964509482s
Jan 17 16:14:37.703: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.961133406s
Jan 17 16:14:38.708: INFO: Verifying statefulset ss doesn't scale past 3 for another 955.953008ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2370 01/17/23 16:14:39.708
Jan 17 16:14:39.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=statefulset-2370 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 17 16:14:39.830: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 17 16:14:39.830: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 17 16:14:39.830: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 17 16:14:39.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=statefulset-2370 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 17 16:14:39.964: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 17 16:14:39.964: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 17 16:14:39.964: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 17 16:14:39.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=statefulset-2370 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 17 16:14:40.082: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 17 16:14:40.082: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 17 16:14:40.082: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 17 16:14:40.082: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 01/17/23 16:14:50.099
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 17 16:14:50.099: INFO: Deleting all statefulset in ns statefulset-2370
Jan 17 16:14:50.105: INFO: Scaling statefulset ss to 0
Jan 17 16:14:50.115: INFO: Waiting for statefulset status.replicas updated to 0
Jan 17 16:14:50.118: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan 17 16:14:50.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2370" for this suite. 01/17/23 16:14:50.147
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","completed":342,"skipped":6440,"failed":0}
------------------------------
• [SLOW TEST] [71.252 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:585

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:13:38.902
    Jan 17 16:13:38.902: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename statefulset 01/17/23 16:13:38.902
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:13:38.933
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:13:38.936
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-2370 01/17/23 16:13:38.938
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:585
    STEP: Initializing watcher for selector baz=blah,foo=bar 01/17/23 16:13:38.95
    STEP: Creating stateful set ss in namespace statefulset-2370 01/17/23 16:13:38.954
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2370 01/17/23 16:13:38.962
    Jan 17 16:13:38.967: INFO: Found 0 stateful pods, waiting for 1
    Jan 17 16:13:48.972: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 01/17/23 16:13:48.972
    Jan 17 16:13:48.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=statefulset-2370 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 17 16:13:49.102: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 17 16:13:49.102: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 17 16:13:49.102: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 17 16:13:49.105: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Jan 17 16:13:59.109: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan 17 16:13:59.109: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 17 16:13:59.124: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999723s
    Jan 17 16:14:00.128: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.996756777s
    Jan 17 16:14:01.132: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.993094104s
    Jan 17 16:14:02.135: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.989112517s
    Jan 17 16:14:03.149: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.985419353s
    Jan 17 16:14:04.153: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.971022223s
    Jan 17 16:14:05.157: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.96774751s
    Jan 17 16:14:06.160: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.964058094s
    Jan 17 16:14:07.164: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.96028656s
    Jan 17 16:14:08.169: INFO: Verifying statefulset ss doesn't scale past 1 for another 955.532607ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2370 01/17/23 16:14:09.169
    Jan 17 16:14:09.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=statefulset-2370 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 17 16:14:09.302: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 17 16:14:09.302: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 17 16:14:09.302: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 17 16:14:09.305: INFO: Found 1 stateful pods, waiting for 3
    Jan 17 16:14:19.310: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 17 16:14:19.310: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 17 16:14:19.310: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 01/17/23 16:14:19.31
    STEP: Scale down will halt with unhealthy stateful pod 01/17/23 16:14:19.311
    Jan 17 16:14:19.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=statefulset-2370 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 17 16:14:19.422: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 17 16:14:19.422: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 17 16:14:19.422: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 17 16:14:19.422: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=statefulset-2370 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 17 16:14:19.543: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 17 16:14:19.543: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 17 16:14:19.543: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 17 16:14:19.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=statefulset-2370 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 17 16:14:19.642: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 17 16:14:19.642: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 17 16:14:19.642: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 17 16:14:19.642: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 17 16:14:19.646: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Jan 17 16:14:29.652: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan 17 16:14:29.652: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Jan 17 16:14:29.652: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Jan 17 16:14:29.663: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999804s
    Jan 17 16:14:30.667: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996521926s
    Jan 17 16:14:31.671: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.992627043s
    Jan 17 16:14:32.677: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.988969672s
    Jan 17 16:14:33.681: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.982834899s
    Jan 17 16:14:34.690: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.97923873s
    Jan 17 16:14:35.695: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.96918284s
    Jan 17 16:14:36.699: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.964509482s
    Jan 17 16:14:37.703: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.961133406s
    Jan 17 16:14:38.708: INFO: Verifying statefulset ss doesn't scale past 3 for another 955.953008ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2370 01/17/23 16:14:39.708
    Jan 17 16:14:39.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=statefulset-2370 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 17 16:14:39.830: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 17 16:14:39.830: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 17 16:14:39.830: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 17 16:14:39.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=statefulset-2370 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 17 16:14:39.964: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 17 16:14:39.964: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 17 16:14:39.964: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 17 16:14:39.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=statefulset-2370 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 17 16:14:40.082: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 17 16:14:40.082: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 17 16:14:40.082: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 17 16:14:40.082: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 01/17/23 16:14:50.099
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan 17 16:14:50.099: INFO: Deleting all statefulset in ns statefulset-2370
    Jan 17 16:14:50.105: INFO: Scaling statefulset ss to 0
    Jan 17 16:14:50.115: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 17 16:14:50.118: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan 17 16:14:50.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-2370" for this suite. 01/17/23 16:14:50.147
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:14:50.154
Jan 17 16:14:50.154: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename emptydir 01/17/23 16:14:50.155
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:14:50.182
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:14:50.184
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126
STEP: Creating a pod to test emptydir 0644 on tmpfs 01/17/23 16:14:50.187
Jan 17 16:14:50.223: INFO: Waiting up to 5m0s for pod "pod-09acc622-a9c0-4376-9d75-edfb60e81dec" in namespace "emptydir-7892" to be "Succeeded or Failed"
Jan 17 16:14:50.230: INFO: Pod "pod-09acc622-a9c0-4376-9d75-edfb60e81dec": Phase="Pending", Reason="", readiness=false. Elapsed: 7.449834ms
Jan 17 16:14:52.234: INFO: Pod "pod-09acc622-a9c0-4376-9d75-edfb60e81dec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011193592s
Jan 17 16:14:54.234: INFO: Pod "pod-09acc622-a9c0-4376-9d75-edfb60e81dec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011847349s
STEP: Saw pod success 01/17/23 16:14:54.234
Jan 17 16:14:54.235: INFO: Pod "pod-09acc622-a9c0-4376-9d75-edfb60e81dec" satisfied condition "Succeeded or Failed"
Jan 17 16:14:54.238: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-09acc622-a9c0-4376-9d75-edfb60e81dec container test-container: <nil>
STEP: delete the pod 01/17/23 16:14:54.243
Jan 17 16:14:54.254: INFO: Waiting for pod pod-09acc622-a9c0-4376-9d75-edfb60e81dec to disappear
Jan 17 16:14:54.256: INFO: Pod pod-09acc622-a9c0-4376-9d75-edfb60e81dec no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 17 16:14:54.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7892" for this suite. 01/17/23 16:14:54.261
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":343,"skipped":6481,"failed":0}
------------------------------
• [4.113 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:14:50.154
    Jan 17 16:14:50.154: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename emptydir 01/17/23 16:14:50.155
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:14:50.182
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:14:50.184
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:126
    STEP: Creating a pod to test emptydir 0644 on tmpfs 01/17/23 16:14:50.187
    Jan 17 16:14:50.223: INFO: Waiting up to 5m0s for pod "pod-09acc622-a9c0-4376-9d75-edfb60e81dec" in namespace "emptydir-7892" to be "Succeeded or Failed"
    Jan 17 16:14:50.230: INFO: Pod "pod-09acc622-a9c0-4376-9d75-edfb60e81dec": Phase="Pending", Reason="", readiness=false. Elapsed: 7.449834ms
    Jan 17 16:14:52.234: INFO: Pod "pod-09acc622-a9c0-4376-9d75-edfb60e81dec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011193592s
    Jan 17 16:14:54.234: INFO: Pod "pod-09acc622-a9c0-4376-9d75-edfb60e81dec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011847349s
    STEP: Saw pod success 01/17/23 16:14:54.234
    Jan 17 16:14:54.235: INFO: Pod "pod-09acc622-a9c0-4376-9d75-edfb60e81dec" satisfied condition "Succeeded or Failed"
    Jan 17 16:14:54.238: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-09acc622-a9c0-4376-9d75-edfb60e81dec container test-container: <nil>
    STEP: delete the pod 01/17/23 16:14:54.243
    Jan 17 16:14:54.254: INFO: Waiting for pod pod-09acc622-a9c0-4376-9d75-edfb60e81dec to disappear
    Jan 17 16:14:54.256: INFO: Pod pod-09acc622-a9c0-4376-9d75-edfb60e81dec no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 17 16:14:54.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-7892" for this suite. 01/17/23 16:14:54.261
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:14:54.267
Jan 17 16:14:54.267: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename taint-multiple-pods 01/17/23 16:14:54.268
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:14:54.306
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:14:54.31
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:348
Jan 17 16:14:54.312: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 17 16:15:54.427: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420
Jan 17 16:15:54.432: INFO: Starting informer...
STEP: Starting pods... 01/17/23 16:15:54.432
Jan 17 16:15:54.655: INFO: Pod1 is running on ip-10-0-151-22.ec2.internal. Tainting Node
Jan 17 16:15:54.871: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-8310" to be "running"
Jan 17 16:15:54.874: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.959952ms
Jan 17 16:15:56.877: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.006607047s
Jan 17 16:15:56.877: INFO: Pod "taint-eviction-b1" satisfied condition "running"
Jan 17 16:15:56.877: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-8310" to be "running"
Jan 17 16:15:56.880: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 3.053265ms
Jan 17 16:15:56.880: INFO: Pod "taint-eviction-b2" satisfied condition "running"
Jan 17 16:15:56.880: INFO: Pod2 is running on ip-10-0-151-22.ec2.internal. Tainting Node
STEP: Trying to apply a taint on the Node 01/17/23 16:15:56.88
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/17/23 16:15:56.892
STEP: Waiting for Pod1 and Pod2 to be deleted 01/17/23 16:15:56.898
Jan 17 16:16:03.059: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Jan 17 16:16:23.097: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/17/23 16:16:23.114
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:187
Jan 17 16:16:23.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-8310" for this suite. 01/17/23 16:16:23.137
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","completed":344,"skipped":6484,"failed":0}
------------------------------
• [SLOW TEST] [88.886 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:14:54.267
    Jan 17 16:14:54.267: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename taint-multiple-pods 01/17/23 16:14:54.268
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:14:54.306
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:14:54.31
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:348
    Jan 17 16:14:54.312: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 17 16:15:54.427: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:420
    Jan 17 16:15:54.432: INFO: Starting informer...
    STEP: Starting pods... 01/17/23 16:15:54.432
    Jan 17 16:15:54.655: INFO: Pod1 is running on ip-10-0-151-22.ec2.internal. Tainting Node
    Jan 17 16:15:54.871: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-8310" to be "running"
    Jan 17 16:15:54.874: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.959952ms
    Jan 17 16:15:56.877: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.006607047s
    Jan 17 16:15:56.877: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    Jan 17 16:15:56.877: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-8310" to be "running"
    Jan 17 16:15:56.880: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 3.053265ms
    Jan 17 16:15:56.880: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    Jan 17 16:15:56.880: INFO: Pod2 is running on ip-10-0-151-22.ec2.internal. Tainting Node
    STEP: Trying to apply a taint on the Node 01/17/23 16:15:56.88
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/17/23 16:15:56.892
    STEP: Waiting for Pod1 and Pod2 to be deleted 01/17/23 16:15:56.898
    Jan 17 16:16:03.059: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    Jan 17 16:16:23.097: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/17/23 16:16:23.114
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/framework.go:187
    Jan 17 16:16:23.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "taint-multiple-pods-8310" for this suite. 01/17/23 16:16:23.137
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:16:23.154
Jan 17 16:16:23.154: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename projected 01/17/23 16:16:23.154
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:16:23.203
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:16:23.209
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374
STEP: Creating configMap with name projected-configmap-test-volume-880be6e9-f034-4a0d-880a-01b20a2b73ce 01/17/23 16:16:23.212
STEP: Creating a pod to test consume configMaps 01/17/23 16:16:23.222
Jan 17 16:16:23.253: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f5611963-2bdb-44dd-9a20-4d6962cf03c1" in namespace "projected-6988" to be "Succeeded or Failed"
Jan 17 16:16:23.284: INFO: Pod "pod-projected-configmaps-f5611963-2bdb-44dd-9a20-4d6962cf03c1": Phase="Pending", Reason="", readiness=false. Elapsed: 31.212772ms
Jan 17 16:16:25.288: INFO: Pod "pod-projected-configmaps-f5611963-2bdb-44dd-9a20-4d6962cf03c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03499917s
Jan 17 16:16:27.293: INFO: Pod "pod-projected-configmaps-f5611963-2bdb-44dd-9a20-4d6962cf03c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039892509s
STEP: Saw pod success 01/17/23 16:16:27.293
Jan 17 16:16:27.293: INFO: Pod "pod-projected-configmaps-f5611963-2bdb-44dd-9a20-4d6962cf03c1" satisfied condition "Succeeded or Failed"
Jan 17 16:16:27.298: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-projected-configmaps-f5611963-2bdb-44dd-9a20-4d6962cf03c1 container projected-configmap-volume-test: <nil>
STEP: delete the pod 01/17/23 16:16:27.321
Jan 17 16:16:27.333: INFO: Waiting for pod pod-projected-configmaps-f5611963-2bdb-44dd-9a20-4d6962cf03c1 to disappear
Jan 17 16:16:27.336: INFO: Pod pod-projected-configmaps-f5611963-2bdb-44dd-9a20-4d6962cf03c1 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan 17 16:16:27.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6988" for this suite. 01/17/23 16:16:27.34
{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","completed":345,"skipped":6490,"failed":0}
------------------------------
• [4.193 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:16:23.154
    Jan 17 16:16:23.154: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename projected 01/17/23 16:16:23.154
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:16:23.203
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:16:23.209
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:374
    STEP: Creating configMap with name projected-configmap-test-volume-880be6e9-f034-4a0d-880a-01b20a2b73ce 01/17/23 16:16:23.212
    STEP: Creating a pod to test consume configMaps 01/17/23 16:16:23.222
    Jan 17 16:16:23.253: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f5611963-2bdb-44dd-9a20-4d6962cf03c1" in namespace "projected-6988" to be "Succeeded or Failed"
    Jan 17 16:16:23.284: INFO: Pod "pod-projected-configmaps-f5611963-2bdb-44dd-9a20-4d6962cf03c1": Phase="Pending", Reason="", readiness=false. Elapsed: 31.212772ms
    Jan 17 16:16:25.288: INFO: Pod "pod-projected-configmaps-f5611963-2bdb-44dd-9a20-4d6962cf03c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03499917s
    Jan 17 16:16:27.293: INFO: Pod "pod-projected-configmaps-f5611963-2bdb-44dd-9a20-4d6962cf03c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039892509s
    STEP: Saw pod success 01/17/23 16:16:27.293
    Jan 17 16:16:27.293: INFO: Pod "pod-projected-configmaps-f5611963-2bdb-44dd-9a20-4d6962cf03c1" satisfied condition "Succeeded or Failed"
    Jan 17 16:16:27.298: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-projected-configmaps-f5611963-2bdb-44dd-9a20-4d6962cf03c1 container projected-configmap-volume-test: <nil>
    STEP: delete the pod 01/17/23 16:16:27.321
    Jan 17 16:16:27.333: INFO: Waiting for pod pod-projected-configmaps-f5611963-2bdb-44dd-9a20-4d6962cf03c1 to disappear
    Jan 17 16:16:27.336: INFO: Pod pod-projected-configmaps-f5611963-2bdb-44dd-9a20-4d6962cf03c1 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan 17 16:16:27.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6988" for this suite. 01/17/23 16:16:27.34
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:16:27.347
Jan 17 16:16:27.347: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename containers 01/17/23 16:16:27.348
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:16:27.374
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:16:27.377
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38
Jan 17 16:16:27.402: INFO: Waiting up to 5m0s for pod "client-containers-f708004e-82a3-42ab-9362-9fccd7c188b9" in namespace "containers-7269" to be "running"
Jan 17 16:16:27.405: INFO: Pod "client-containers-f708004e-82a3-42ab-9362-9fccd7c188b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.500996ms
Jan 17 16:16:29.409: INFO: Pod "client-containers-f708004e-82a3-42ab-9362-9fccd7c188b9": Phase="Running", Reason="", readiness=true. Elapsed: 2.006976547s
Jan 17 16:16:29.409: INFO: Pod "client-containers-f708004e-82a3-42ab-9362-9fccd7c188b9" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Jan 17 16:16:29.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7269" for this suite. 01/17/23 16:16:29.419
{"msg":"PASSED [sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","completed":346,"skipped":6495,"failed":0}
------------------------------
• [2.079 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:16:27.347
    Jan 17 16:16:27.347: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename containers 01/17/23 16:16:27.348
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:16:27.374
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:16:27.377
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:38
    Jan 17 16:16:27.402: INFO: Waiting up to 5m0s for pod "client-containers-f708004e-82a3-42ab-9362-9fccd7c188b9" in namespace "containers-7269" to be "running"
    Jan 17 16:16:27.405: INFO: Pod "client-containers-f708004e-82a3-42ab-9362-9fccd7c188b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.500996ms
    Jan 17 16:16:29.409: INFO: Pod "client-containers-f708004e-82a3-42ab-9362-9fccd7c188b9": Phase="Running", Reason="", readiness=true. Elapsed: 2.006976547s
    Jan 17 16:16:29.409: INFO: Pod "client-containers-f708004e-82a3-42ab-9362-9fccd7c188b9" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Jan 17 16:16:29.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-7269" for this suite. 01/17/23 16:16:29.419
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:16:29.426
Jan 17 16:16:29.426: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename custom-resource-definition 01/17/23 16:16:29.427
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:16:29.454
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:16:29.458
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
Jan 17 16:16:29.460: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 17 16:16:30.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6296" for this suite. 01/17/23 16:16:30.014
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","completed":347,"skipped":6509,"failed":0}
------------------------------
• [0.604 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:16:29.426
    Jan 17 16:16:29.426: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename custom-resource-definition 01/17/23 16:16:29.427
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:16:29.454
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:16:29.458
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    Jan 17 16:16:29.460: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 17 16:16:30.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-6296" for this suite. 01/17/23 16:16:30.014
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:16:30.031
Jan 17 16:16:30.031: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename crd-publish-openapi 01/17/23 16:16:30.032
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:16:30.082
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:16:30.085
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193
Jan 17 16:16:30.090: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/17/23 16:16:40.38
Jan 17 16:16:40.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=crd-publish-openapi-2996 --namespace=crd-publish-openapi-2996 create -f -'
Jan 17 16:16:41.427: INFO: stderr: ""
Jan 17 16:16:41.427: INFO: stdout: "e2e-test-crd-publish-openapi-2938-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jan 17 16:16:41.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=crd-publish-openapi-2996 --namespace=crd-publish-openapi-2996 delete e2e-test-crd-publish-openapi-2938-crds test-cr'
Jan 17 16:16:41.479: INFO: stderr: ""
Jan 17 16:16:41.479: INFO: stdout: "e2e-test-crd-publish-openapi-2938-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Jan 17 16:16:41.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=crd-publish-openapi-2996 --namespace=crd-publish-openapi-2996 apply -f -'
Jan 17 16:16:42.415: INFO: stderr: ""
Jan 17 16:16:42.415: INFO: stdout: "e2e-test-crd-publish-openapi-2938-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jan 17 16:16:42.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=crd-publish-openapi-2996 --namespace=crd-publish-openapi-2996 delete e2e-test-crd-publish-openapi-2938-crds test-cr'
Jan 17 16:16:42.467: INFO: stderr: ""
Jan 17 16:16:42.467: INFO: stdout: "e2e-test-crd-publish-openapi-2938-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 01/17/23 16:16:42.467
Jan 17 16:16:42.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=crd-publish-openapi-2996 explain e2e-test-crd-publish-openapi-2938-crds'
Jan 17 16:16:43.372: INFO: stderr: ""
Jan 17 16:16:43.372: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2938-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 17 16:16:49.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2996" for this suite. 01/17/23 16:16:49.629
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","completed":348,"skipped":6516,"failed":0}
------------------------------
• [SLOW TEST] [19.608 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:16:30.031
    Jan 17 16:16:30.031: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename crd-publish-openapi 01/17/23 16:16:30.032
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:16:30.082
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:16:30.085
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:193
    Jan 17 16:16:30.090: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/17/23 16:16:40.38
    Jan 17 16:16:40.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=crd-publish-openapi-2996 --namespace=crd-publish-openapi-2996 create -f -'
    Jan 17 16:16:41.427: INFO: stderr: ""
    Jan 17 16:16:41.427: INFO: stdout: "e2e-test-crd-publish-openapi-2938-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Jan 17 16:16:41.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=crd-publish-openapi-2996 --namespace=crd-publish-openapi-2996 delete e2e-test-crd-publish-openapi-2938-crds test-cr'
    Jan 17 16:16:41.479: INFO: stderr: ""
    Jan 17 16:16:41.479: INFO: stdout: "e2e-test-crd-publish-openapi-2938-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    Jan 17 16:16:41.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=crd-publish-openapi-2996 --namespace=crd-publish-openapi-2996 apply -f -'
    Jan 17 16:16:42.415: INFO: stderr: ""
    Jan 17 16:16:42.415: INFO: stdout: "e2e-test-crd-publish-openapi-2938-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Jan 17 16:16:42.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=crd-publish-openapi-2996 --namespace=crd-publish-openapi-2996 delete e2e-test-crd-publish-openapi-2938-crds test-cr'
    Jan 17 16:16:42.467: INFO: stderr: ""
    Jan 17 16:16:42.467: INFO: stdout: "e2e-test-crd-publish-openapi-2938-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 01/17/23 16:16:42.467
    Jan 17 16:16:42.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2637184552 --namespace=crd-publish-openapi-2996 explain e2e-test-crd-publish-openapi-2938-crds'
    Jan 17 16:16:43.372: INFO: stderr: ""
    Jan 17 16:16:43.372: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2938-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 17 16:16:49.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-2996" for this suite. 01/17/23 16:16:49.629
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:16:49.639
Jan 17 16:16:49.639: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename subpath 01/17/23 16:16:49.64
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:16:49.657
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:16:49.66
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/17/23 16:16:49.662
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-w4gn 01/17/23 16:16:49.672
STEP: Creating a pod to test atomic-volume-subpath 01/17/23 16:16:49.672
Jan 17 16:16:49.707: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-w4gn" in namespace "subpath-9335" to be "Succeeded or Failed"
Jan 17 16:16:49.714: INFO: Pod "pod-subpath-test-configmap-w4gn": Phase="Pending", Reason="", readiness=false. Elapsed: 6.793013ms
Jan 17 16:16:51.719: INFO: Pod "pod-subpath-test-configmap-w4gn": Phase="Running", Reason="", readiness=true. Elapsed: 2.011522446s
Jan 17 16:16:53.717: INFO: Pod "pod-subpath-test-configmap-w4gn": Phase="Running", Reason="", readiness=true. Elapsed: 4.009864763s
Jan 17 16:16:55.718: INFO: Pod "pod-subpath-test-configmap-w4gn": Phase="Running", Reason="", readiness=true. Elapsed: 6.01073685s
Jan 17 16:16:57.719: INFO: Pod "pod-subpath-test-configmap-w4gn": Phase="Running", Reason="", readiness=true. Elapsed: 8.012030935s
Jan 17 16:16:59.718: INFO: Pod "pod-subpath-test-configmap-w4gn": Phase="Running", Reason="", readiness=true. Elapsed: 10.011155631s
Jan 17 16:17:01.718: INFO: Pod "pod-subpath-test-configmap-w4gn": Phase="Running", Reason="", readiness=true. Elapsed: 12.011307965s
Jan 17 16:17:03.717: INFO: Pod "pod-subpath-test-configmap-w4gn": Phase="Running", Reason="", readiness=true. Elapsed: 14.010044909s
Jan 17 16:17:05.717: INFO: Pod "pod-subpath-test-configmap-w4gn": Phase="Running", Reason="", readiness=true. Elapsed: 16.010107559s
Jan 17 16:17:07.719: INFO: Pod "pod-subpath-test-configmap-w4gn": Phase="Running", Reason="", readiness=true. Elapsed: 18.011961318s
Jan 17 16:17:09.718: INFO: Pod "pod-subpath-test-configmap-w4gn": Phase="Running", Reason="", readiness=true. Elapsed: 20.011428979s
Jan 17 16:17:11.718: INFO: Pod "pod-subpath-test-configmap-w4gn": Phase="Running", Reason="", readiness=false. Elapsed: 22.011420998s
Jan 17 16:17:13.717: INFO: Pod "pod-subpath-test-configmap-w4gn": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.010335325s
STEP: Saw pod success 01/17/23 16:17:13.717
Jan 17 16:17:13.717: INFO: Pod "pod-subpath-test-configmap-w4gn" satisfied condition "Succeeded or Failed"
Jan 17 16:17:13.720: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-subpath-test-configmap-w4gn container test-container-subpath-configmap-w4gn: <nil>
STEP: delete the pod 01/17/23 16:17:13.734
Jan 17 16:17:13.747: INFO: Waiting for pod pod-subpath-test-configmap-w4gn to disappear
Jan 17 16:17:13.751: INFO: Pod pod-subpath-test-configmap-w4gn no longer exists
STEP: Deleting pod pod-subpath-test-configmap-w4gn 01/17/23 16:17:13.751
Jan 17 16:17:13.751: INFO: Deleting pod "pod-subpath-test-configmap-w4gn" in namespace "subpath-9335"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Jan 17 16:17:13.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9335" for this suite. 01/17/23 16:17:13.758
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]","completed":349,"skipped":6517,"failed":0}
------------------------------
• [SLOW TEST] [24.125 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:16:49.639
    Jan 17 16:16:49.639: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename subpath 01/17/23 16:16:49.64
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:16:49.657
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:16:49.66
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/17/23 16:16:49.662
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-w4gn 01/17/23 16:16:49.672
    STEP: Creating a pod to test atomic-volume-subpath 01/17/23 16:16:49.672
    Jan 17 16:16:49.707: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-w4gn" in namespace "subpath-9335" to be "Succeeded or Failed"
    Jan 17 16:16:49.714: INFO: Pod "pod-subpath-test-configmap-w4gn": Phase="Pending", Reason="", readiness=false. Elapsed: 6.793013ms
    Jan 17 16:16:51.719: INFO: Pod "pod-subpath-test-configmap-w4gn": Phase="Running", Reason="", readiness=true. Elapsed: 2.011522446s
    Jan 17 16:16:53.717: INFO: Pod "pod-subpath-test-configmap-w4gn": Phase="Running", Reason="", readiness=true. Elapsed: 4.009864763s
    Jan 17 16:16:55.718: INFO: Pod "pod-subpath-test-configmap-w4gn": Phase="Running", Reason="", readiness=true. Elapsed: 6.01073685s
    Jan 17 16:16:57.719: INFO: Pod "pod-subpath-test-configmap-w4gn": Phase="Running", Reason="", readiness=true. Elapsed: 8.012030935s
    Jan 17 16:16:59.718: INFO: Pod "pod-subpath-test-configmap-w4gn": Phase="Running", Reason="", readiness=true. Elapsed: 10.011155631s
    Jan 17 16:17:01.718: INFO: Pod "pod-subpath-test-configmap-w4gn": Phase="Running", Reason="", readiness=true. Elapsed: 12.011307965s
    Jan 17 16:17:03.717: INFO: Pod "pod-subpath-test-configmap-w4gn": Phase="Running", Reason="", readiness=true. Elapsed: 14.010044909s
    Jan 17 16:17:05.717: INFO: Pod "pod-subpath-test-configmap-w4gn": Phase="Running", Reason="", readiness=true. Elapsed: 16.010107559s
    Jan 17 16:17:07.719: INFO: Pod "pod-subpath-test-configmap-w4gn": Phase="Running", Reason="", readiness=true. Elapsed: 18.011961318s
    Jan 17 16:17:09.718: INFO: Pod "pod-subpath-test-configmap-w4gn": Phase="Running", Reason="", readiness=true. Elapsed: 20.011428979s
    Jan 17 16:17:11.718: INFO: Pod "pod-subpath-test-configmap-w4gn": Phase="Running", Reason="", readiness=false. Elapsed: 22.011420998s
    Jan 17 16:17:13.717: INFO: Pod "pod-subpath-test-configmap-w4gn": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.010335325s
    STEP: Saw pod success 01/17/23 16:17:13.717
    Jan 17 16:17:13.717: INFO: Pod "pod-subpath-test-configmap-w4gn" satisfied condition "Succeeded or Failed"
    Jan 17 16:17:13.720: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-subpath-test-configmap-w4gn container test-container-subpath-configmap-w4gn: <nil>
    STEP: delete the pod 01/17/23 16:17:13.734
    Jan 17 16:17:13.747: INFO: Waiting for pod pod-subpath-test-configmap-w4gn to disappear
    Jan 17 16:17:13.751: INFO: Pod pod-subpath-test-configmap-w4gn no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-w4gn 01/17/23 16:17:13.751
    Jan 17 16:17:13.751: INFO: Deleting pod "pod-subpath-test-configmap-w4gn" in namespace "subpath-9335"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Jan 17 16:17:13.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-9335" for this suite. 01/17/23 16:17:13.758
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:17:13.764
Jan 17 16:17:13.764: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename emptydir 01/17/23 16:17:13.765
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:17:13.782
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:17:13.787
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166
STEP: Creating a pod to test emptydir 0644 on node default medium 01/17/23 16:17:13.793
Jan 17 16:17:13.824: INFO: Waiting up to 5m0s for pod "pod-28b15fc2-3658-45be-8f9f-afdc8a1188c0" in namespace "emptydir-9131" to be "Succeeded or Failed"
Jan 17 16:17:13.837: INFO: Pod "pod-28b15fc2-3658-45be-8f9f-afdc8a1188c0": Phase="Pending", Reason="", readiness=false. Elapsed: 13.877069ms
Jan 17 16:17:15.842: INFO: Pod "pod-28b15fc2-3658-45be-8f9f-afdc8a1188c0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018911309s
Jan 17 16:17:17.843: INFO: Pod "pod-28b15fc2-3658-45be-8f9f-afdc8a1188c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019671075s
STEP: Saw pod success 01/17/23 16:17:17.843
Jan 17 16:17:17.843: INFO: Pod "pod-28b15fc2-3658-45be-8f9f-afdc8a1188c0" satisfied condition "Succeeded or Failed"
Jan 17 16:17:17.847: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-28b15fc2-3658-45be-8f9f-afdc8a1188c0 container test-container: <nil>
STEP: delete the pod 01/17/23 16:17:17.853
Jan 17 16:17:17.864: INFO: Waiting for pod pod-28b15fc2-3658-45be-8f9f-afdc8a1188c0 to disappear
Jan 17 16:17:17.866: INFO: Pod pod-28b15fc2-3658-45be-8f9f-afdc8a1188c0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 17 16:17:17.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9131" for this suite. 01/17/23 16:17:17.87
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":350,"skipped":6524,"failed":0}
------------------------------
• [4.111 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:17:13.764
    Jan 17 16:17:13.764: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename emptydir 01/17/23 16:17:13.765
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:17:13.782
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:17:13.787
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:166
    STEP: Creating a pod to test emptydir 0644 on node default medium 01/17/23 16:17:13.793
    Jan 17 16:17:13.824: INFO: Waiting up to 5m0s for pod "pod-28b15fc2-3658-45be-8f9f-afdc8a1188c0" in namespace "emptydir-9131" to be "Succeeded or Failed"
    Jan 17 16:17:13.837: INFO: Pod "pod-28b15fc2-3658-45be-8f9f-afdc8a1188c0": Phase="Pending", Reason="", readiness=false. Elapsed: 13.877069ms
    Jan 17 16:17:15.842: INFO: Pod "pod-28b15fc2-3658-45be-8f9f-afdc8a1188c0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018911309s
    Jan 17 16:17:17.843: INFO: Pod "pod-28b15fc2-3658-45be-8f9f-afdc8a1188c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019671075s
    STEP: Saw pod success 01/17/23 16:17:17.843
    Jan 17 16:17:17.843: INFO: Pod "pod-28b15fc2-3658-45be-8f9f-afdc8a1188c0" satisfied condition "Succeeded or Failed"
    Jan 17 16:17:17.847: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-28b15fc2-3658-45be-8f9f-afdc8a1188c0 container test-container: <nil>
    STEP: delete the pod 01/17/23 16:17:17.853
    Jan 17 16:17:17.864: INFO: Waiting for pod pod-28b15fc2-3658-45be-8f9f-afdc8a1188c0 to disappear
    Jan 17 16:17:17.866: INFO: Pod pod-28b15fc2-3658-45be-8f9f-afdc8a1188c0 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 17 16:17:17.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-9131" for this suite. 01/17/23 16:17:17.87
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:17:17.877
Jan 17 16:17:17.877: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename secrets 01/17/23 16:17:17.877
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:17:17.893
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:17:17.896
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98
STEP: Creating secret with name secret-test-b7285242-4d51-4195-8d47-d3c6ec3dbe43 01/17/23 16:17:17.936
STEP: Creating a pod to test consume secrets 01/17/23 16:17:17.956
Jan 17 16:17:17.998: INFO: Waiting up to 5m0s for pod "pod-secrets-57f8e60a-10c0-480f-954c-58bec6f4670a" in namespace "secrets-4531" to be "Succeeded or Failed"
Jan 17 16:17:18.007: INFO: Pod "pod-secrets-57f8e60a-10c0-480f-954c-58bec6f4670a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.941142ms
Jan 17 16:17:20.012: INFO: Pod "pod-secrets-57f8e60a-10c0-480f-954c-58bec6f4670a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014336694s
Jan 17 16:17:22.012: INFO: Pod "pod-secrets-57f8e60a-10c0-480f-954c-58bec6f4670a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014171982s
STEP: Saw pod success 01/17/23 16:17:22.012
Jan 17 16:17:22.012: INFO: Pod "pod-secrets-57f8e60a-10c0-480f-954c-58bec6f4670a" satisfied condition "Succeeded or Failed"
Jan 17 16:17:22.016: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-secrets-57f8e60a-10c0-480f-954c-58bec6f4670a container secret-volume-test: <nil>
STEP: delete the pod 01/17/23 16:17:22.047
Jan 17 16:17:22.070: INFO: Waiting for pod pod-secrets-57f8e60a-10c0-480f-954c-58bec6f4670a to disappear
Jan 17 16:17:22.073: INFO: Pod pod-secrets-57f8e60a-10c0-480f-954c-58bec6f4670a no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan 17 16:17:22.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4531" for this suite. 01/17/23 16:17:22.079
STEP: Destroying namespace "secret-namespace-8585" for this suite. 01/17/23 16:17:22.104
{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","completed":351,"skipped":6580,"failed":0}
------------------------------
• [4.238 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:17:17.877
    Jan 17 16:17:17.877: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename secrets 01/17/23 16:17:17.877
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:17:17.893
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:17:17.896
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:98
    STEP: Creating secret with name secret-test-b7285242-4d51-4195-8d47-d3c6ec3dbe43 01/17/23 16:17:17.936
    STEP: Creating a pod to test consume secrets 01/17/23 16:17:17.956
    Jan 17 16:17:17.998: INFO: Waiting up to 5m0s for pod "pod-secrets-57f8e60a-10c0-480f-954c-58bec6f4670a" in namespace "secrets-4531" to be "Succeeded or Failed"
    Jan 17 16:17:18.007: INFO: Pod "pod-secrets-57f8e60a-10c0-480f-954c-58bec6f4670a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.941142ms
    Jan 17 16:17:20.012: INFO: Pod "pod-secrets-57f8e60a-10c0-480f-954c-58bec6f4670a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014336694s
    Jan 17 16:17:22.012: INFO: Pod "pod-secrets-57f8e60a-10c0-480f-954c-58bec6f4670a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014171982s
    STEP: Saw pod success 01/17/23 16:17:22.012
    Jan 17 16:17:22.012: INFO: Pod "pod-secrets-57f8e60a-10c0-480f-954c-58bec6f4670a" satisfied condition "Succeeded or Failed"
    Jan 17 16:17:22.016: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-secrets-57f8e60a-10c0-480f-954c-58bec6f4670a container secret-volume-test: <nil>
    STEP: delete the pod 01/17/23 16:17:22.047
    Jan 17 16:17:22.070: INFO: Waiting for pod pod-secrets-57f8e60a-10c0-480f-954c-58bec6f4670a to disappear
    Jan 17 16:17:22.073: INFO: Pod pod-secrets-57f8e60a-10c0-480f-954c-58bec6f4670a no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan 17 16:17:22.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-4531" for this suite. 01/17/23 16:17:22.079
    STEP: Destroying namespace "secret-namespace-8585" for this suite. 01/17/23 16:17:22.104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:17:22.115
Jan 17 16:17:22.115: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename pod-network-test 01/17/23 16:17:22.116
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:17:22.177
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:17:22.183
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-8448 01/17/23 16:17:22.186
STEP: creating a selector 01/17/23 16:17:22.186
STEP: Creating the service pods in kubernetes 01/17/23 16:17:22.186
Jan 17 16:17:22.186: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 17 16:17:22.309: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-8448" to be "running and ready"
Jan 17 16:17:22.312: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.920852ms
Jan 17 16:17:22.312: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 16:17:24.315: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.005897311s
Jan 17 16:17:24.315: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 17 16:17:26.317: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.007720939s
Jan 17 16:17:26.317: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 17 16:17:28.315: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.006272586s
Jan 17 16:17:28.315: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 17 16:17:30.316: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.006584764s
Jan 17 16:17:30.316: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 17 16:17:32.317: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.007820369s
Jan 17 16:17:32.317: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 17 16:17:34.316: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.007134892s
Jan 17 16:17:34.316: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan 17 16:17:34.316: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan 17 16:17:34.319: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-8448" to be "running and ready"
Jan 17 16:17:34.321: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.166286ms
Jan 17 16:17:34.321: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan 17 16:17:34.321: INFO: Pod "netserver-1" satisfied condition "running and ready"
Jan 17 16:17:34.324: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-8448" to be "running and ready"
Jan 17 16:17:34.327: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.286075ms
Jan 17 16:17:34.327: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Jan 17 16:17:34.327: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 01/17/23 16:17:34.33
Jan 17 16:17:34.354: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-8448" to be "running"
Jan 17 16:17:34.357: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.474734ms
Jan 17 16:17:36.361: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007153861s
Jan 17 16:17:36.361: INFO: Pod "test-container-pod" satisfied condition "running"
Jan 17 16:17:36.363: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-8448" to be "running"
Jan 17 16:17:36.367: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.354884ms
Jan 17 16:17:36.367: INFO: Pod "host-test-container-pod" satisfied condition "running"
Jan 17 16:17:36.370: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Jan 17 16:17:36.370: INFO: Going to poll 10.129.2.170 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Jan 17 16:17:36.372: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.129.2.170:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8448 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 16:17:36.372: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
Jan 17 16:17:36.373: INFO: ExecWithOptions: Clientset creation
Jan 17 16:17:36.373: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-8448/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.129.2.170%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 17 16:17:36.540: INFO: Found all 1 expected endpoints: [netserver-0]
Jan 17 16:17:36.540: INFO: Going to poll 10.131.1.154 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Jan 17 16:17:36.543: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.131.1.154:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8448 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 16:17:36.543: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
Jan 17 16:17:36.543: INFO: ExecWithOptions: Clientset creation
Jan 17 16:17:36.543: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-8448/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.131.1.154%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 17 16:17:36.643: INFO: Found all 1 expected endpoints: [netserver-1]
Jan 17 16:17:36.643: INFO: Going to poll 10.128.2.197 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Jan 17 16:17:36.646: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.128.2.197:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8448 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 16:17:36.646: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
Jan 17 16:17:36.647: INFO: ExecWithOptions: Clientset creation
Jan 17 16:17:36.647: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-8448/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.128.2.197%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 17 16:17:36.721: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Jan 17 16:17:36.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8448" for this suite. 01/17/23 16:17:36.727
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","completed":352,"skipped":6594,"failed":0}
------------------------------
• [SLOW TEST] [14.618 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:17:22.115
    Jan 17 16:17:22.115: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename pod-network-test 01/17/23 16:17:22.116
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:17:22.177
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:17:22.183
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-8448 01/17/23 16:17:22.186
    STEP: creating a selector 01/17/23 16:17:22.186
    STEP: Creating the service pods in kubernetes 01/17/23 16:17:22.186
    Jan 17 16:17:22.186: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan 17 16:17:22.309: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-8448" to be "running and ready"
    Jan 17 16:17:22.312: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.920852ms
    Jan 17 16:17:22.312: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 17 16:17:24.315: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.005897311s
    Jan 17 16:17:24.315: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 17 16:17:26.317: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.007720939s
    Jan 17 16:17:26.317: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 17 16:17:28.315: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.006272586s
    Jan 17 16:17:28.315: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 17 16:17:30.316: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.006584764s
    Jan 17 16:17:30.316: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 17 16:17:32.317: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.007820369s
    Jan 17 16:17:32.317: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 17 16:17:34.316: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.007134892s
    Jan 17 16:17:34.316: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan 17 16:17:34.316: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan 17 16:17:34.319: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-8448" to be "running and ready"
    Jan 17 16:17:34.321: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.166286ms
    Jan 17 16:17:34.321: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan 17 16:17:34.321: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Jan 17 16:17:34.324: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-8448" to be "running and ready"
    Jan 17 16:17:34.327: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.286075ms
    Jan 17 16:17:34.327: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Jan 17 16:17:34.327: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 01/17/23 16:17:34.33
    Jan 17 16:17:34.354: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-8448" to be "running"
    Jan 17 16:17:34.357: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.474734ms
    Jan 17 16:17:36.361: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007153861s
    Jan 17 16:17:36.361: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan 17 16:17:36.363: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-8448" to be "running"
    Jan 17 16:17:36.367: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.354884ms
    Jan 17 16:17:36.367: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Jan 17 16:17:36.370: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Jan 17 16:17:36.370: INFO: Going to poll 10.129.2.170 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Jan 17 16:17:36.372: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.129.2.170:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8448 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 17 16:17:36.372: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    Jan 17 16:17:36.373: INFO: ExecWithOptions: Clientset creation
    Jan 17 16:17:36.373: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-8448/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.129.2.170%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 17 16:17:36.540: INFO: Found all 1 expected endpoints: [netserver-0]
    Jan 17 16:17:36.540: INFO: Going to poll 10.131.1.154 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Jan 17 16:17:36.543: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.131.1.154:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8448 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 17 16:17:36.543: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    Jan 17 16:17:36.543: INFO: ExecWithOptions: Clientset creation
    Jan 17 16:17:36.543: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-8448/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.131.1.154%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 17 16:17:36.643: INFO: Found all 1 expected endpoints: [netserver-1]
    Jan 17 16:17:36.643: INFO: Going to poll 10.128.2.197 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Jan 17 16:17:36.646: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.128.2.197:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8448 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 17 16:17:36.646: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    Jan 17 16:17:36.647: INFO: ExecWithOptions: Clientset creation
    Jan 17 16:17:36.647: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-8448/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.128.2.197%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 17 16:17:36.721: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Jan 17 16:17:36.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-8448" for this suite. 01/17/23 16:17:36.727
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:17:36.733
Jan 17 16:17:36.733: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename downward-api 01/17/23 16:17:36.734
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:17:36.758
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:17:36.761
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206
STEP: Creating a pod to test downward API volume plugin 01/17/23 16:17:36.764
Jan 17 16:17:36.793: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bb1830f6-3bba-4958-9639-82e0c8b85d1f" in namespace "downward-api-7582" to be "Succeeded or Failed"
Jan 17 16:17:36.802: INFO: Pod "downwardapi-volume-bb1830f6-3bba-4958-9639-82e0c8b85d1f": Phase="Pending", Reason="", readiness=false. Elapsed: 9.082741ms
Jan 17 16:17:38.805: INFO: Pod "downwardapi-volume-bb1830f6-3bba-4958-9639-82e0c8b85d1f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012112431s
Jan 17 16:17:40.810: INFO: Pod "downwardapi-volume-bb1830f6-3bba-4958-9639-82e0c8b85d1f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01650645s
STEP: Saw pod success 01/17/23 16:17:40.81
Jan 17 16:17:40.810: INFO: Pod "downwardapi-volume-bb1830f6-3bba-4958-9639-82e0c8b85d1f" satisfied condition "Succeeded or Failed"
Jan 17 16:17:40.813: INFO: Trying to get logs from node ip-10-0-139-213.ec2.internal pod downwardapi-volume-bb1830f6-3bba-4958-9639-82e0c8b85d1f container client-container: <nil>
STEP: delete the pod 01/17/23 16:17:40.825
Jan 17 16:17:40.836: INFO: Waiting for pod downwardapi-volume-bb1830f6-3bba-4958-9639-82e0c8b85d1f to disappear
Jan 17 16:17:40.839: INFO: Pod downwardapi-volume-bb1830f6-3bba-4958-9639-82e0c8b85d1f no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 17 16:17:40.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7582" for this suite. 01/17/23 16:17:40.842
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","completed":353,"skipped":6618,"failed":0}
------------------------------
• [4.114 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:17:36.733
    Jan 17 16:17:36.733: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename downward-api 01/17/23 16:17:36.734
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:17:36.758
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:17:36.761
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:206
    STEP: Creating a pod to test downward API volume plugin 01/17/23 16:17:36.764
    Jan 17 16:17:36.793: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bb1830f6-3bba-4958-9639-82e0c8b85d1f" in namespace "downward-api-7582" to be "Succeeded or Failed"
    Jan 17 16:17:36.802: INFO: Pod "downwardapi-volume-bb1830f6-3bba-4958-9639-82e0c8b85d1f": Phase="Pending", Reason="", readiness=false. Elapsed: 9.082741ms
    Jan 17 16:17:38.805: INFO: Pod "downwardapi-volume-bb1830f6-3bba-4958-9639-82e0c8b85d1f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012112431s
    Jan 17 16:17:40.810: INFO: Pod "downwardapi-volume-bb1830f6-3bba-4958-9639-82e0c8b85d1f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01650645s
    STEP: Saw pod success 01/17/23 16:17:40.81
    Jan 17 16:17:40.810: INFO: Pod "downwardapi-volume-bb1830f6-3bba-4958-9639-82e0c8b85d1f" satisfied condition "Succeeded or Failed"
    Jan 17 16:17:40.813: INFO: Trying to get logs from node ip-10-0-139-213.ec2.internal pod downwardapi-volume-bb1830f6-3bba-4958-9639-82e0c8b85d1f container client-container: <nil>
    STEP: delete the pod 01/17/23 16:17:40.825
    Jan 17 16:17:40.836: INFO: Waiting for pod downwardapi-volume-bb1830f6-3bba-4958-9639-82e0c8b85d1f to disappear
    Jan 17 16:17:40.839: INFO: Pod downwardapi-volume-bb1830f6-3bba-4958-9639-82e0c8b85d1f no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 17 16:17:40.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-7582" for this suite. 01/17/23 16:17:40.842
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:781
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:17:40.849
Jan 17 16:17:40.849: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename services 01/17/23 16:17:40.849
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:17:40.865
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:17:40.868
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:781
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 17 16:17:40.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-596" for this suite. 01/17/23 16:17:40.885
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","completed":354,"skipped":6657,"failed":0}
------------------------------
• [0.046 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:781

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:17:40.849
    Jan 17 16:17:40.849: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename services 01/17/23 16:17:40.849
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:17:40.865
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:17:40.868
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:781
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 17 16:17:40.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-596" for this suite. 01/17/23 16:17:40.885
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:17:40.895
Jan 17 16:17:40.895: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename container-probe 01/17/23 16:17:40.895
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:17:40.912
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:17:40.916
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131
STEP: Creating pod busybox-e78d2998-4936-472c-a275-c8f87832b1e7 in namespace container-probe-1006 01/17/23 16:17:40.919
Jan 17 16:17:40.942: INFO: Waiting up to 5m0s for pod "busybox-e78d2998-4936-472c-a275-c8f87832b1e7" in namespace "container-probe-1006" to be "not pending"
Jan 17 16:17:40.947: INFO: Pod "busybox-e78d2998-4936-472c-a275-c8f87832b1e7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.244695ms
Jan 17 16:17:42.952: INFO: Pod "busybox-e78d2998-4936-472c-a275-c8f87832b1e7": Phase="Running", Reason="", readiness=true. Elapsed: 2.010139234s
Jan 17 16:17:42.952: INFO: Pod "busybox-e78d2998-4936-472c-a275-c8f87832b1e7" satisfied condition "not pending"
Jan 17 16:17:42.952: INFO: Started pod busybox-e78d2998-4936-472c-a275-c8f87832b1e7 in namespace container-probe-1006
STEP: checking the pod's current state and verifying that restartCount is present 01/17/23 16:17:42.952
Jan 17 16:17:42.955: INFO: Initial restart count of pod busybox-e78d2998-4936-472c-a275-c8f87832b1e7 is 0
Jan 17 16:18:33.079: INFO: Restart count of pod container-probe-1006/busybox-e78d2998-4936-472c-a275-c8f87832b1e7 is now 1 (50.123970009s elapsed)
STEP: deleting the pod 01/17/23 16:18:33.079
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan 17 16:18:33.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1006" for this suite. 01/17/23 16:18:33.097
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","completed":355,"skipped":6659,"failed":0}
------------------------------
• [SLOW TEST] [52.208 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:17:40.895
    Jan 17 16:17:40.895: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename container-probe 01/17/23 16:17:40.895
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:17:40.912
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:17:40.916
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:131
    STEP: Creating pod busybox-e78d2998-4936-472c-a275-c8f87832b1e7 in namespace container-probe-1006 01/17/23 16:17:40.919
    Jan 17 16:17:40.942: INFO: Waiting up to 5m0s for pod "busybox-e78d2998-4936-472c-a275-c8f87832b1e7" in namespace "container-probe-1006" to be "not pending"
    Jan 17 16:17:40.947: INFO: Pod "busybox-e78d2998-4936-472c-a275-c8f87832b1e7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.244695ms
    Jan 17 16:17:42.952: INFO: Pod "busybox-e78d2998-4936-472c-a275-c8f87832b1e7": Phase="Running", Reason="", readiness=true. Elapsed: 2.010139234s
    Jan 17 16:17:42.952: INFO: Pod "busybox-e78d2998-4936-472c-a275-c8f87832b1e7" satisfied condition "not pending"
    Jan 17 16:17:42.952: INFO: Started pod busybox-e78d2998-4936-472c-a275-c8f87832b1e7 in namespace container-probe-1006
    STEP: checking the pod's current state and verifying that restartCount is present 01/17/23 16:17:42.952
    Jan 17 16:17:42.955: INFO: Initial restart count of pod busybox-e78d2998-4936-472c-a275-c8f87832b1e7 is 0
    Jan 17 16:18:33.079: INFO: Restart count of pod container-probe-1006/busybox-e78d2998-4936-472c-a275-c8f87832b1e7 is now 1 (50.123970009s elapsed)
    STEP: deleting the pod 01/17/23 16:18:33.079
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan 17 16:18:33.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-1006" for this suite. 01/17/23 16:18:33.097
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:18:33.103
Jan 17 16:18:33.103: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename projected 01/17/23 16:18:33.104
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:18:33.126
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:18:33.131
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108
STEP: Creating configMap with name projected-configmap-test-volume-map-69d38406-5494-431f-a803-86add6186adb 01/17/23 16:18:33.134
STEP: Creating a pod to test consume configMaps 01/17/23 16:18:33.141
Jan 17 16:18:33.169: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c43d4b4d-1f7d-4554-ac45-c621e8990396" in namespace "projected-8284" to be "Succeeded or Failed"
Jan 17 16:18:33.175: INFO: Pod "pod-projected-configmaps-c43d4b4d-1f7d-4554-ac45-c621e8990396": Phase="Pending", Reason="", readiness=false. Elapsed: 5.942248ms
Jan 17 16:18:35.179: INFO: Pod "pod-projected-configmaps-c43d4b4d-1f7d-4554-ac45-c621e8990396": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009933025s
Jan 17 16:18:37.180: INFO: Pod "pod-projected-configmaps-c43d4b4d-1f7d-4554-ac45-c621e8990396": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010814986s
STEP: Saw pod success 01/17/23 16:18:37.18
Jan 17 16:18:37.180: INFO: Pod "pod-projected-configmaps-c43d4b4d-1f7d-4554-ac45-c621e8990396" satisfied condition "Succeeded or Failed"
Jan 17 16:18:37.182: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-projected-configmaps-c43d4b4d-1f7d-4554-ac45-c621e8990396 container agnhost-container: <nil>
STEP: delete the pod 01/17/23 16:18:37.188
Jan 17 16:18:37.198: INFO: Waiting for pod pod-projected-configmaps-c43d4b4d-1f7d-4554-ac45-c621e8990396 to disappear
Jan 17 16:18:37.201: INFO: Pod pod-projected-configmaps-c43d4b4d-1f7d-4554-ac45-c621e8990396 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan 17 16:18:37.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8284" for this suite. 01/17/23 16:18:37.205
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","completed":356,"skipped":6659,"failed":0}
------------------------------
• [4.108 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:18:33.103
    Jan 17 16:18:33.103: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename projected 01/17/23 16:18:33.104
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:18:33.126
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:18:33.131
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:108
    STEP: Creating configMap with name projected-configmap-test-volume-map-69d38406-5494-431f-a803-86add6186adb 01/17/23 16:18:33.134
    STEP: Creating a pod to test consume configMaps 01/17/23 16:18:33.141
    Jan 17 16:18:33.169: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c43d4b4d-1f7d-4554-ac45-c621e8990396" in namespace "projected-8284" to be "Succeeded or Failed"
    Jan 17 16:18:33.175: INFO: Pod "pod-projected-configmaps-c43d4b4d-1f7d-4554-ac45-c621e8990396": Phase="Pending", Reason="", readiness=false. Elapsed: 5.942248ms
    Jan 17 16:18:35.179: INFO: Pod "pod-projected-configmaps-c43d4b4d-1f7d-4554-ac45-c621e8990396": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009933025s
    Jan 17 16:18:37.180: INFO: Pod "pod-projected-configmaps-c43d4b4d-1f7d-4554-ac45-c621e8990396": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010814986s
    STEP: Saw pod success 01/17/23 16:18:37.18
    Jan 17 16:18:37.180: INFO: Pod "pod-projected-configmaps-c43d4b4d-1f7d-4554-ac45-c621e8990396" satisfied condition "Succeeded or Failed"
    Jan 17 16:18:37.182: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod pod-projected-configmaps-c43d4b4d-1f7d-4554-ac45-c621e8990396 container agnhost-container: <nil>
    STEP: delete the pod 01/17/23 16:18:37.188
    Jan 17 16:18:37.198: INFO: Waiting for pod pod-projected-configmaps-c43d4b4d-1f7d-4554-ac45-c621e8990396 to disappear
    Jan 17 16:18:37.201: INFO: Pod pod-projected-configmaps-c43d4b4d-1f7d-4554-ac45-c621e8990396 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan 17 16:18:37.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8284" for this suite. 01/17/23 16:18:37.205
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:18:37.211
Jan 17 16:18:37.211: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename configmap 01/17/23 16:18:37.212
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:18:37.231
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:18:37.238
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 17 16:18:37.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-470" for this suite. 01/17/23 16:18:37.337
{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","completed":357,"skipped":6674,"failed":0}
------------------------------
• [0.136 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:18:37.211
    Jan 17 16:18:37.211: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename configmap 01/17/23 16:18:37.212
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:18:37.231
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:18:37.238
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:503
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 17 16:18:37.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-470" for this suite. 01/17/23 16:18:37.337
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:215
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:18:37.347
Jan 17 16:18:37.347: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename container-runtime 01/17/23 16:18:37.348
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:18:37.366
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:18:37.37
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:215
STEP: create the container 01/17/23 16:18:37.372
STEP: wait for the container to reach Failed 01/17/23 16:18:37.403
STEP: get the container status 01/17/23 16:18:40.439
STEP: the container should be terminated 01/17/23 16:18:40.442
STEP: the termination message should be set 01/17/23 16:18:40.442
Jan 17 16:18:40.442: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 01/17/23 16:18:40.442
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Jan 17 16:18:40.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9358" for this suite. 01/17/23 16:18:40.462
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":358,"skipped":6676,"failed":0}
------------------------------
• [3.124 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:18:37.347
    Jan 17 16:18:37.347: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename container-runtime 01/17/23 16:18:37.348
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:18:37.366
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:18:37.37
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:215
    STEP: create the container 01/17/23 16:18:37.372
    STEP: wait for the container to reach Failed 01/17/23 16:18:37.403
    STEP: get the container status 01/17/23 16:18:40.439
    STEP: the container should be terminated 01/17/23 16:18:40.442
    STEP: the termination message should be set 01/17/23 16:18:40.442
    Jan 17 16:18:40.442: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 01/17/23 16:18:40.442
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Jan 17 16:18:40.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-9358" for this suite. 01/17/23 16:18:40.462
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:18:40.471
Jan 17 16:18:40.471: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename gc 01/17/23 16:18:40.472
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:18:40.485
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:18:40.488
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 01/17/23 16:18:40.501
W0117 16:18:40.510321      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: create the rc2 01/17/23 16:18:40.51
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 01/17/23 16:18:45.522
STEP: delete the rc simpletest-rc-to-be-deleted 01/17/23 16:18:46.358
STEP: wait for the rc to be deleted 01/17/23 16:18:46.367
STEP: Gathering metrics 01/17/23 16:18:51.38
W0117 16:18:51.384184      22 metrics_grabber.go:110] Can't find any pods in namespace kube-system to grab metrics from
W0117 16:18:51.384199      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan 17 16:18:51.384: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jan 17 16:18:51.384: INFO: Deleting pod "simpletest-rc-to-be-deleted-255tk" in namespace "gc-9869"
Jan 17 16:18:51.395: INFO: Deleting pod "simpletest-rc-to-be-deleted-2jm2t" in namespace "gc-9869"
Jan 17 16:18:51.406: INFO: Deleting pod "simpletest-rc-to-be-deleted-2kldq" in namespace "gc-9869"
Jan 17 16:18:51.427: INFO: Deleting pod "simpletest-rc-to-be-deleted-2nn4z" in namespace "gc-9869"
Jan 17 16:18:51.439: INFO: Deleting pod "simpletest-rc-to-be-deleted-4brnz" in namespace "gc-9869"
Jan 17 16:18:51.459: INFO: Deleting pod "simpletest-rc-to-be-deleted-4rzjj" in namespace "gc-9869"
Jan 17 16:18:51.472: INFO: Deleting pod "simpletest-rc-to-be-deleted-4t7xc" in namespace "gc-9869"
Jan 17 16:18:51.488: INFO: Deleting pod "simpletest-rc-to-be-deleted-4v52k" in namespace "gc-9869"
Jan 17 16:18:51.504: INFO: Deleting pod "simpletest-rc-to-be-deleted-4vql2" in namespace "gc-9869"
Jan 17 16:18:51.527: INFO: Deleting pod "simpletest-rc-to-be-deleted-4xxb4" in namespace "gc-9869"
Jan 17 16:18:51.540: INFO: Deleting pod "simpletest-rc-to-be-deleted-4zqxd" in namespace "gc-9869"
Jan 17 16:18:51.555: INFO: Deleting pod "simpletest-rc-to-be-deleted-525cl" in namespace "gc-9869"
Jan 17 16:18:51.571: INFO: Deleting pod "simpletest-rc-to-be-deleted-55rvz" in namespace "gc-9869"
Jan 17 16:18:51.590: INFO: Deleting pod "simpletest-rc-to-be-deleted-5b6zn" in namespace "gc-9869"
Jan 17 16:18:51.604: INFO: Deleting pod "simpletest-rc-to-be-deleted-5jrtb" in namespace "gc-9869"
Jan 17 16:18:51.622: INFO: Deleting pod "simpletest-rc-to-be-deleted-5qcgt" in namespace "gc-9869"
Jan 17 16:18:51.634: INFO: Deleting pod "simpletest-rc-to-be-deleted-5skql" in namespace "gc-9869"
Jan 17 16:18:51.666: INFO: Deleting pod "simpletest-rc-to-be-deleted-5wr5z" in namespace "gc-9869"
Jan 17 16:18:51.690: INFO: Deleting pod "simpletest-rc-to-be-deleted-649tf" in namespace "gc-9869"
Jan 17 16:18:51.707: INFO: Deleting pod "simpletest-rc-to-be-deleted-69ftq" in namespace "gc-9869"
Jan 17 16:18:51.738: INFO: Deleting pod "simpletest-rc-to-be-deleted-6cwt9" in namespace "gc-9869"
Jan 17 16:18:51.774: INFO: Deleting pod "simpletest-rc-to-be-deleted-6h5kg" in namespace "gc-9869"
Jan 17 16:18:51.849: INFO: Deleting pod "simpletest-rc-to-be-deleted-6m7lq" in namespace "gc-9869"
Jan 17 16:18:51.876: INFO: Deleting pod "simpletest-rc-to-be-deleted-6mpzn" in namespace "gc-9869"
Jan 17 16:18:51.907: INFO: Deleting pod "simpletest-rc-to-be-deleted-7gh28" in namespace "gc-9869"
Jan 17 16:18:51.931: INFO: Deleting pod "simpletest-rc-to-be-deleted-7jxxc" in namespace "gc-9869"
Jan 17 16:18:51.946: INFO: Deleting pod "simpletest-rc-to-be-deleted-7lk69" in namespace "gc-9869"
Jan 17 16:18:51.967: INFO: Deleting pod "simpletest-rc-to-be-deleted-7w7vv" in namespace "gc-9869"
Jan 17 16:18:51.985: INFO: Deleting pod "simpletest-rc-to-be-deleted-895gw" in namespace "gc-9869"
Jan 17 16:18:51.999: INFO: Deleting pod "simpletest-rc-to-be-deleted-8xtcz" in namespace "gc-9869"
Jan 17 16:18:52.018: INFO: Deleting pod "simpletest-rc-to-be-deleted-98nmn" in namespace "gc-9869"
Jan 17 16:18:52.031: INFO: Deleting pod "simpletest-rc-to-be-deleted-9rkh8" in namespace "gc-9869"
Jan 17 16:18:52.050: INFO: Deleting pod "simpletest-rc-to-be-deleted-9rsh7" in namespace "gc-9869"
Jan 17 16:18:52.072: INFO: Deleting pod "simpletest-rc-to-be-deleted-9vfgc" in namespace "gc-9869"
Jan 17 16:18:52.096: INFO: Deleting pod "simpletest-rc-to-be-deleted-b8t79" in namespace "gc-9869"
Jan 17 16:18:52.110: INFO: Deleting pod "simpletest-rc-to-be-deleted-bb5wv" in namespace "gc-9869"
Jan 17 16:18:52.125: INFO: Deleting pod "simpletest-rc-to-be-deleted-bk78x" in namespace "gc-9869"
Jan 17 16:18:52.146: INFO: Deleting pod "simpletest-rc-to-be-deleted-bp9j5" in namespace "gc-9869"
Jan 17 16:18:52.169: INFO: Deleting pod "simpletest-rc-to-be-deleted-bwx95" in namespace "gc-9869"
Jan 17 16:18:52.197: INFO: Deleting pod "simpletest-rc-to-be-deleted-c8zj7" in namespace "gc-9869"
Jan 17 16:18:52.213: INFO: Deleting pod "simpletest-rc-to-be-deleted-cnwgp" in namespace "gc-9869"
Jan 17 16:18:52.234: INFO: Deleting pod "simpletest-rc-to-be-deleted-cst9c" in namespace "gc-9869"
Jan 17 16:18:52.249: INFO: Deleting pod "simpletest-rc-to-be-deleted-dgpnq" in namespace "gc-9869"
Jan 17 16:18:52.263: INFO: Deleting pod "simpletest-rc-to-be-deleted-dpgrq" in namespace "gc-9869"
Jan 17 16:18:52.276: INFO: Deleting pod "simpletest-rc-to-be-deleted-f57pg" in namespace "gc-9869"
Jan 17 16:18:52.297: INFO: Deleting pod "simpletest-rc-to-be-deleted-f9jm4" in namespace "gc-9869"
Jan 17 16:18:52.320: INFO: Deleting pod "simpletest-rc-to-be-deleted-fdphh" in namespace "gc-9869"
Jan 17 16:18:52.377: INFO: Deleting pod "simpletest-rc-to-be-deleted-frhkw" in namespace "gc-9869"
Jan 17 16:18:52.391: INFO: Deleting pod "simpletest-rc-to-be-deleted-gd5gw" in namespace "gc-9869"
Jan 17 16:18:52.418: INFO: Deleting pod "simpletest-rc-to-be-deleted-ggcr8" in namespace "gc-9869"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan 17 16:18:52.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9869" for this suite. 01/17/23 16:18:52.448
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","completed":359,"skipped":6678,"failed":0}
------------------------------
• [SLOW TEST] [11.988 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:18:40.471
    Jan 17 16:18:40.471: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename gc 01/17/23 16:18:40.472
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:18:40.485
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:18:40.488
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 01/17/23 16:18:40.501
    W0117 16:18:40.510321      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: create the rc2 01/17/23 16:18:40.51
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 01/17/23 16:18:45.522
    STEP: delete the rc simpletest-rc-to-be-deleted 01/17/23 16:18:46.358
    STEP: wait for the rc to be deleted 01/17/23 16:18:46.367
    STEP: Gathering metrics 01/17/23 16:18:51.38
    W0117 16:18:51.384184      22 metrics_grabber.go:110] Can't find any pods in namespace kube-system to grab metrics from
    W0117 16:18:51.384199      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jan 17 16:18:51.384: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Jan 17 16:18:51.384: INFO: Deleting pod "simpletest-rc-to-be-deleted-255tk" in namespace "gc-9869"
    Jan 17 16:18:51.395: INFO: Deleting pod "simpletest-rc-to-be-deleted-2jm2t" in namespace "gc-9869"
    Jan 17 16:18:51.406: INFO: Deleting pod "simpletest-rc-to-be-deleted-2kldq" in namespace "gc-9869"
    Jan 17 16:18:51.427: INFO: Deleting pod "simpletest-rc-to-be-deleted-2nn4z" in namespace "gc-9869"
    Jan 17 16:18:51.439: INFO: Deleting pod "simpletest-rc-to-be-deleted-4brnz" in namespace "gc-9869"
    Jan 17 16:18:51.459: INFO: Deleting pod "simpletest-rc-to-be-deleted-4rzjj" in namespace "gc-9869"
    Jan 17 16:18:51.472: INFO: Deleting pod "simpletest-rc-to-be-deleted-4t7xc" in namespace "gc-9869"
    Jan 17 16:18:51.488: INFO: Deleting pod "simpletest-rc-to-be-deleted-4v52k" in namespace "gc-9869"
    Jan 17 16:18:51.504: INFO: Deleting pod "simpletest-rc-to-be-deleted-4vql2" in namespace "gc-9869"
    Jan 17 16:18:51.527: INFO: Deleting pod "simpletest-rc-to-be-deleted-4xxb4" in namespace "gc-9869"
    Jan 17 16:18:51.540: INFO: Deleting pod "simpletest-rc-to-be-deleted-4zqxd" in namespace "gc-9869"
    Jan 17 16:18:51.555: INFO: Deleting pod "simpletest-rc-to-be-deleted-525cl" in namespace "gc-9869"
    Jan 17 16:18:51.571: INFO: Deleting pod "simpletest-rc-to-be-deleted-55rvz" in namespace "gc-9869"
    Jan 17 16:18:51.590: INFO: Deleting pod "simpletest-rc-to-be-deleted-5b6zn" in namespace "gc-9869"
    Jan 17 16:18:51.604: INFO: Deleting pod "simpletest-rc-to-be-deleted-5jrtb" in namespace "gc-9869"
    Jan 17 16:18:51.622: INFO: Deleting pod "simpletest-rc-to-be-deleted-5qcgt" in namespace "gc-9869"
    Jan 17 16:18:51.634: INFO: Deleting pod "simpletest-rc-to-be-deleted-5skql" in namespace "gc-9869"
    Jan 17 16:18:51.666: INFO: Deleting pod "simpletest-rc-to-be-deleted-5wr5z" in namespace "gc-9869"
    Jan 17 16:18:51.690: INFO: Deleting pod "simpletest-rc-to-be-deleted-649tf" in namespace "gc-9869"
    Jan 17 16:18:51.707: INFO: Deleting pod "simpletest-rc-to-be-deleted-69ftq" in namespace "gc-9869"
    Jan 17 16:18:51.738: INFO: Deleting pod "simpletest-rc-to-be-deleted-6cwt9" in namespace "gc-9869"
    Jan 17 16:18:51.774: INFO: Deleting pod "simpletest-rc-to-be-deleted-6h5kg" in namespace "gc-9869"
    Jan 17 16:18:51.849: INFO: Deleting pod "simpletest-rc-to-be-deleted-6m7lq" in namespace "gc-9869"
    Jan 17 16:18:51.876: INFO: Deleting pod "simpletest-rc-to-be-deleted-6mpzn" in namespace "gc-9869"
    Jan 17 16:18:51.907: INFO: Deleting pod "simpletest-rc-to-be-deleted-7gh28" in namespace "gc-9869"
    Jan 17 16:18:51.931: INFO: Deleting pod "simpletest-rc-to-be-deleted-7jxxc" in namespace "gc-9869"
    Jan 17 16:18:51.946: INFO: Deleting pod "simpletest-rc-to-be-deleted-7lk69" in namespace "gc-9869"
    Jan 17 16:18:51.967: INFO: Deleting pod "simpletest-rc-to-be-deleted-7w7vv" in namespace "gc-9869"
    Jan 17 16:18:51.985: INFO: Deleting pod "simpletest-rc-to-be-deleted-895gw" in namespace "gc-9869"
    Jan 17 16:18:51.999: INFO: Deleting pod "simpletest-rc-to-be-deleted-8xtcz" in namespace "gc-9869"
    Jan 17 16:18:52.018: INFO: Deleting pod "simpletest-rc-to-be-deleted-98nmn" in namespace "gc-9869"
    Jan 17 16:18:52.031: INFO: Deleting pod "simpletest-rc-to-be-deleted-9rkh8" in namespace "gc-9869"
    Jan 17 16:18:52.050: INFO: Deleting pod "simpletest-rc-to-be-deleted-9rsh7" in namespace "gc-9869"
    Jan 17 16:18:52.072: INFO: Deleting pod "simpletest-rc-to-be-deleted-9vfgc" in namespace "gc-9869"
    Jan 17 16:18:52.096: INFO: Deleting pod "simpletest-rc-to-be-deleted-b8t79" in namespace "gc-9869"
    Jan 17 16:18:52.110: INFO: Deleting pod "simpletest-rc-to-be-deleted-bb5wv" in namespace "gc-9869"
    Jan 17 16:18:52.125: INFO: Deleting pod "simpletest-rc-to-be-deleted-bk78x" in namespace "gc-9869"
    Jan 17 16:18:52.146: INFO: Deleting pod "simpletest-rc-to-be-deleted-bp9j5" in namespace "gc-9869"
    Jan 17 16:18:52.169: INFO: Deleting pod "simpletest-rc-to-be-deleted-bwx95" in namespace "gc-9869"
    Jan 17 16:18:52.197: INFO: Deleting pod "simpletest-rc-to-be-deleted-c8zj7" in namespace "gc-9869"
    Jan 17 16:18:52.213: INFO: Deleting pod "simpletest-rc-to-be-deleted-cnwgp" in namespace "gc-9869"
    Jan 17 16:18:52.234: INFO: Deleting pod "simpletest-rc-to-be-deleted-cst9c" in namespace "gc-9869"
    Jan 17 16:18:52.249: INFO: Deleting pod "simpletest-rc-to-be-deleted-dgpnq" in namespace "gc-9869"
    Jan 17 16:18:52.263: INFO: Deleting pod "simpletest-rc-to-be-deleted-dpgrq" in namespace "gc-9869"
    Jan 17 16:18:52.276: INFO: Deleting pod "simpletest-rc-to-be-deleted-f57pg" in namespace "gc-9869"
    Jan 17 16:18:52.297: INFO: Deleting pod "simpletest-rc-to-be-deleted-f9jm4" in namespace "gc-9869"
    Jan 17 16:18:52.320: INFO: Deleting pod "simpletest-rc-to-be-deleted-fdphh" in namespace "gc-9869"
    Jan 17 16:18:52.377: INFO: Deleting pod "simpletest-rc-to-be-deleted-frhkw" in namespace "gc-9869"
    Jan 17 16:18:52.391: INFO: Deleting pod "simpletest-rc-to-be-deleted-gd5gw" in namespace "gc-9869"
    Jan 17 16:18:52.418: INFO: Deleting pod "simpletest-rc-to-be-deleted-ggcr8" in namespace "gc-9869"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan 17 16:18:52.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-9869" for this suite. 01/17/23 16:18:52.448
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:18:52.46
Jan 17 16:18:52.460: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename configmap 01/17/23 16:18:52.46
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:18:52.489
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:18:52.494
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168
STEP: creating a ConfigMap 01/17/23 16:18:52.497
STEP: fetching the ConfigMap 01/17/23 16:18:52.529
STEP: patching the ConfigMap 01/17/23 16:18:52.546
STEP: listing all ConfigMaps in all namespaces with a label selector 01/17/23 16:18:52.563
STEP: deleting the ConfigMap by collection with a label selector 01/17/23 16:18:52.623
STEP: listing all ConfigMaps in test namespace 01/17/23 16:18:52.634
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Jan 17 16:18:52.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1669" for this suite. 01/17/23 16:18:52.645
{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","completed":360,"skipped":6687,"failed":0}
------------------------------
• [0.204 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:18:52.46
    Jan 17 16:18:52.460: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename configmap 01/17/23 16:18:52.46
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:18:52.489
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:18:52.494
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:168
    STEP: creating a ConfigMap 01/17/23 16:18:52.497
    STEP: fetching the ConfigMap 01/17/23 16:18:52.529
    STEP: patching the ConfigMap 01/17/23 16:18:52.546
    STEP: listing all ConfigMaps in all namespaces with a label selector 01/17/23 16:18:52.563
    STEP: deleting the ConfigMap by collection with a label selector 01/17/23 16:18:52.623
    STEP: listing all ConfigMaps in test namespace 01/17/23 16:18:52.634
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 17 16:18:52.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-1669" for this suite. 01/17/23 16:18:52.645
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:18:52.664
Jan 17 16:18:52.664: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename containers 01/17/23 16:18:52.665
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:18:52.687
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:18:52.699
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72
STEP: Creating a pod to test override command 01/17/23 16:18:52.706
Jan 17 16:18:52.733: INFO: Waiting up to 5m0s for pod "client-containers-7a300bda-c1a7-438e-8d53-e6b02d2c9203" in namespace "containers-2631" to be "Succeeded or Failed"
Jan 17 16:18:52.739: INFO: Pod "client-containers-7a300bda-c1a7-438e-8d53-e6b02d2c9203": Phase="Pending", Reason="", readiness=false. Elapsed: 5.329215ms
Jan 17 16:18:54.742: INFO: Pod "client-containers-7a300bda-c1a7-438e-8d53-e6b02d2c9203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008985581s
Jan 17 16:18:56.743: INFO: Pod "client-containers-7a300bda-c1a7-438e-8d53-e6b02d2c9203": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009980516s
Jan 17 16:18:58.743: INFO: Pod "client-containers-7a300bda-c1a7-438e-8d53-e6b02d2c9203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009617618s
STEP: Saw pod success 01/17/23 16:18:58.743
Jan 17 16:18:58.743: INFO: Pod "client-containers-7a300bda-c1a7-438e-8d53-e6b02d2c9203" satisfied condition "Succeeded or Failed"
Jan 17 16:18:58.746: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod client-containers-7a300bda-c1a7-438e-8d53-e6b02d2c9203 container agnhost-container: <nil>
STEP: delete the pod 01/17/23 16:18:58.752
Jan 17 16:18:58.763: INFO: Waiting for pod client-containers-7a300bda-c1a7-438e-8d53-e6b02d2c9203 to disappear
Jan 17 16:18:58.766: INFO: Pod client-containers-7a300bda-c1a7-438e-8d53-e6b02d2c9203 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Jan 17 16:18:58.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2631" for this suite. 01/17/23 16:18:58.77
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]","completed":361,"skipped":6700,"failed":0}
------------------------------
• [SLOW TEST] [6.112 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:18:52.664
    Jan 17 16:18:52.664: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename containers 01/17/23 16:18:52.665
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:18:52.687
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:18:52.699
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:72
    STEP: Creating a pod to test override command 01/17/23 16:18:52.706
    Jan 17 16:18:52.733: INFO: Waiting up to 5m0s for pod "client-containers-7a300bda-c1a7-438e-8d53-e6b02d2c9203" in namespace "containers-2631" to be "Succeeded or Failed"
    Jan 17 16:18:52.739: INFO: Pod "client-containers-7a300bda-c1a7-438e-8d53-e6b02d2c9203": Phase="Pending", Reason="", readiness=false. Elapsed: 5.329215ms
    Jan 17 16:18:54.742: INFO: Pod "client-containers-7a300bda-c1a7-438e-8d53-e6b02d2c9203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008985581s
    Jan 17 16:18:56.743: INFO: Pod "client-containers-7a300bda-c1a7-438e-8d53-e6b02d2c9203": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009980516s
    Jan 17 16:18:58.743: INFO: Pod "client-containers-7a300bda-c1a7-438e-8d53-e6b02d2c9203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009617618s
    STEP: Saw pod success 01/17/23 16:18:58.743
    Jan 17 16:18:58.743: INFO: Pod "client-containers-7a300bda-c1a7-438e-8d53-e6b02d2c9203" satisfied condition "Succeeded or Failed"
    Jan 17 16:18:58.746: INFO: Trying to get logs from node ip-10-0-151-22.ec2.internal pod client-containers-7a300bda-c1a7-438e-8d53-e6b02d2c9203 container agnhost-container: <nil>
    STEP: delete the pod 01/17/23 16:18:58.752
    Jan 17 16:18:58.763: INFO: Waiting for pod client-containers-7a300bda-c1a7-438e-8d53-e6b02d2c9203 to disappear
    Jan 17 16:18:58.766: INFO: Pod client-containers-7a300bda-c1a7-438e-8d53-e6b02d2c9203 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Jan 17 16:18:58.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-2631" for this suite. 01/17/23 16:18:58.77
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/17/23 16:18:58.776
Jan 17 16:18:58.777: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
STEP: Building a namespace api object, basename daemonsets 01/17/23 16:18:58.777
STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:18:58.793
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:18:58.795
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293
STEP: Creating a simple DaemonSet "daemon-set" 01/17/23 16:18:58.851
STEP: Check that daemon pods launch on every node of the cluster. 01/17/23 16:18:58.867
Jan 17 16:18:58.872: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 16:18:58.872: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 16:18:58.872: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 16:18:58.874: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 17 16:18:58.874: INFO: Node ip-10-0-139-213.ec2.internal is running 0 daemon pod, expected 1
Jan 17 16:18:59.880: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 16:18:59.880: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 16:18:59.880: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 16:18:59.883: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 17 16:18:59.883: INFO: Node ip-10-0-139-213.ec2.internal is running 0 daemon pod, expected 1
Jan 17 16:19:00.880: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 16:19:00.880: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 16:19:00.880: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 16:19:00.883: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan 17 16:19:00.883: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 01/17/23 16:19:00.886
Jan 17 16:19:00.900: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 16:19:00.900: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 16:19:00.900: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 16:19:00.909: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan 17 16:19:00.909: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 01/17/23 16:19:00.909
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 01/17/23 16:19:01.92
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6342, will wait for the garbage collector to delete the pods 01/17/23 16:19:01.92
Jan 17 16:19:01.982: INFO: Deleting DaemonSet.extensions daemon-set took: 9.039984ms
Jan 17 16:19:02.082: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.117353ms
Jan 17 16:19:04.285: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 17 16:19:04.285: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 17 16:19:04.288: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"141223"},"items":null}

Jan 17 16:19:04.291: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"141223"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan 17 16:19:04.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6342" for this suite. 01/17/23 16:19:04.306
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","completed":362,"skipped":6705,"failed":0}
------------------------------
• [SLOW TEST] [5.537 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/17/23 16:18:58.776
    Jan 17 16:18:58.777: INFO: >>> kubeConfig: /tmp/kubeconfig-2637184552
    STEP: Building a namespace api object, basename daemonsets 01/17/23 16:18:58.777
    STEP: Waiting for a default service account to be provisioned in namespace 01/17/23 16:18:58.793
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/17/23 16:18:58.795
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:293
    STEP: Creating a simple DaemonSet "daemon-set" 01/17/23 16:18:58.851
    STEP: Check that daemon pods launch on every node of the cluster. 01/17/23 16:18:58.867
    Jan 17 16:18:58.872: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 16:18:58.872: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 16:18:58.872: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 16:18:58.874: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 17 16:18:58.874: INFO: Node ip-10-0-139-213.ec2.internal is running 0 daemon pod, expected 1
    Jan 17 16:18:59.880: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 16:18:59.880: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 16:18:59.880: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 16:18:59.883: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 17 16:18:59.883: INFO: Node ip-10-0-139-213.ec2.internal is running 0 daemon pod, expected 1
    Jan 17 16:19:00.880: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 16:19:00.880: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 16:19:00.880: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 16:19:00.883: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jan 17 16:19:00.883: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 01/17/23 16:19:00.886
    Jan 17 16:19:00.900: INFO: DaemonSet pods can't tolerate node ip-10-0-135-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 16:19:00.900: INFO: DaemonSet pods can't tolerate node ip-10-0-159-167.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 16:19:00.900: INFO: DaemonSet pods can't tolerate node ip-10-0-161-62.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 17 16:19:00.909: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jan 17 16:19:00.909: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 01/17/23 16:19:00.909
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 01/17/23 16:19:01.92
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6342, will wait for the garbage collector to delete the pods 01/17/23 16:19:01.92
    Jan 17 16:19:01.982: INFO: Deleting DaemonSet.extensions daemon-set took: 9.039984ms
    Jan 17 16:19:02.082: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.117353ms
    Jan 17 16:19:04.285: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 17 16:19:04.285: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 17 16:19:04.288: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"141223"},"items":null}

    Jan 17 16:19:04.291: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"141223"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan 17 16:19:04.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-6342" for this suite. 01/17/23 16:19:04.306
  << End Captured GinkgoWriter Output
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:87
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:87
{"msg":"Test Suite completed","completed":362,"skipped":6705,"failed":0}
Jan 17 16:19:04.314: INFO: Running AfterSuite actions on all nodes
Jan 17 16:19:04.314: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func20.2
Jan 17 16:19:04.314: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func10.2
Jan 17 16:19:04.314: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
Jan 17 16:19:04.314: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
Jan 17 16:19:04.314: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
Jan 17 16:19:04.314: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
Jan 17 16:19:04.314: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:87
Jan 17 16:19:04.314: INFO: Running AfterSuite actions on node 1
Jan 17 16:19:04.314: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.000 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:87

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:87
    Jan 17 16:19:04.314: INFO: Running AfterSuite actions on all nodes
    Jan 17 16:19:04.314: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func20.2
    Jan 17 16:19:04.314: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func10.2
    Jan 17 16:19:04.314: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
    Jan 17 16:19:04.314: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
    Jan 17 16:19:04.314: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
    Jan 17 16:19:04.314: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
    Jan 17 16:19:04.314: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:87
    Jan 17 16:19:04.314: INFO: Running AfterSuite actions on node 1
    Jan 17 16:19:04.314: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:146
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:146
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:146

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:146
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:559
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:559
------------------------------
[ReportAfterSuite] PASSED [0.044 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:559

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:559
  << End Captured GinkgoWriter Output
------------------------------

Ran 362 of 7067 Specs in 5491.247 seconds
SUCCESS! -- 362 Passed | 0 Failed | 0 Pending | 6705 Skipped
PASS

Ginkgo ran 1 suite in 1h31m31.456686502s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.1.6[0m

